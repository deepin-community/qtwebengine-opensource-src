From 52be8c7f2169d8fbca1c516a33878eb3540fb251 Mon Sep 17 00:00:00 2001
From: Ma Aiguo <maaiguo@uniontech.com>
Date: Thu, 5 May 2022 16:57:36 +0800
Subject: [PATCH] chore:[sw_64] qmake ok on sw_64

Change-Id: I06f2d204a0658d92170da7dd7ba91c73d7c281f3
---
 configure.pri                                 |    1 +
 mkspecs/features/functions.prf                |    1 +
 .../page_allocator_constants.h                |    2 +
 .../partition_alloc_constants.h               |    4 +
 .../chromium/base/process/launch_posix.cc     |    2 +-
 .../double-conversion/utils.h                 |    2 +-
 src/3rdparty/chromium/build/build_config.h    |    5 +
 src/3rdparty/chromium/build/config/sw.gni     |   37 +
 src/3rdparty/chromium/media/media_options.gni |    3 +-
 src/3rdparty/chromium/sandbox/features.gni    |    1 +
 .../linux/bpf_dsl/linux_syscall_ranges.h      |    5 +
 .../sandbox/linux/bpf_dsl/seccomp_macros.h    |   59 +
 .../seccomp-bpf-helpers/baseline_policy.cc    |    2 +
 .../syscall_parameters_restrictions.cc        |    4 +-
 .../linux/seccomp-bpf-helpers/syscall_sets.cc |   12 +-
 .../sandbox/linux/seccomp-bpf/syscall.cc      |   24 +-
 .../sandbox/linux/seccomp-bpf/syscall.h       |    2 +-
 .../sandbox/linux/services/credentials.cc     |    2 +-
 .../linux/system_headers/linux_seccomp.h      |    8 +
 .../linux/system_headers/linux_signal.h       |   24 +
 .../sandbox/linux/system_headers/linux_stat.h |   18 +
 .../linux/system_headers/linux_syscalls.h     |    4 +
 .../linux/system_headers/linux_ucontext.h     |    2 +
 .../system_headers/sw_64_linux_syscalls.h     |   31 +
 .../system_headers/sw_64_linux_ucontext.h     |   35 +
 .../policy/linux/bpf_cdm_policy_linux.cc      |    2 +-
 .../policy/linux/bpf_gpu_policy_linux.cc      |    4 +-
 .../policy/linux/bpf_utility_policy_linux.cc  |    2 +-
 .../policy/linux/sandbox_seccomp_bpf_linux.cc |    6 +-
 src/3rdparty/chromium/skia/BUILD.gn           |    6 +
 .../absl/debugging/internal/examine_stack.cc  |    2 +
 .../chromium/third_party/angle/gni/angle.gni  |    2 +-
 .../blink/renderer/platform/heap/asm/BUILD.gn |    6 +-
 .../platform/heap/asm/SaveRegisters_sw64.S    |   42 +
 .../boringssl/src/include/openssl/base.h      |    3 +
 .../dump_writer_common/raw_context_cpu.h      |    2 +
 .../linux/dump_writer_common/thread_info.cc   |   26 +
 .../linux/dump_writer_common/thread_info.h    |    2 +-
 .../dump_writer_common/ucontext_reader.cc     |   13 +
 .../client/linux/handler/exception_handler.cc |   13 +-
 .../client/linux/handler/exception_handler.h  |    2 +-
 .../microdump_writer/microdump_writer.cc      |    8 +-
 .../minidump_writer/linux_core_dumper.cc      |    7 +
 .../linux/minidump_writer/linux_dumper.h      |    2 +-
 .../minidump_writer/linux_ptrace_dumper.cc    |    3 +
 .../linux/minidump_writer/minidump_writer.cc  |   10 +-
 .../linux/minidump_writer/minidump_writer.h   |    2 +-
 .../src/common/linux/breakpad_getcontext.S    |    2 +-
 .../src/common/linux/breakpad_getcontext.h    |    4 +
 .../src/common/linux/memory_mapped_file.cc    |    2 +-
 .../src/common/linux/ucontext_constants.h     |    1 +
 .../breakpad/src/common/memory_allocator.h    |    6 +
 .../common/minidump_cpu_sw64.h                |  165 +
 .../google_breakpad/common/minidump_format.h  |    2 +
 .../minidump/minidump_misc_info_writer.cc     |    2 +
 .../crashpad/snapshot/capture_memory.cc       |    4 +
 .../crashpad/snapshot/cpu_architecture.h      |    4 +-
 .../crashpad/crashpad/snapshot/cpu_context.h  |   24 +
 .../snapshot/linux/cpu_context_linux.h        |   38 +
 .../linux/exception_snapshot_linux.cc         |   52 +
 .../snapshot/linux/exception_snapshot_linux.h |    2 +
 .../snapshot/linux/process_reader_linux.cc    |    2 +
 .../crashpad/snapshot/linux/signal_context.h  |  104 +
 .../snapshot/linux/system_snapshot_linux.cc   |   11 +
 .../snapshot/linux/thread_snapshot_linux.cc   |   11 +
 .../snapshot/linux/thread_snapshot_linux.h    |    2 +
 .../crashpad/crashpad/util/linux/ptracer.cc   |   34 +
 .../crashpad/util/linux/thread_info.h         |   17 +-
 .../third_party/ffmpeg/ffmpeg_options.gni     |    4 +
 .../icu/source/i18n/double-conversion-utils.h |    2 +-
 .../chromium/third_party/libvpx/BUILD.gn      |    3 +
 .../source/config/linux/sw_64/vp8_rtcd.h      |  357 +
 .../source/config/linux/sw_64/vp9_rtcd.h      |  275 +
 .../source/config/linux/sw_64/vpx_config.asm  |   98 +
 .../source/config/linux/sw_64/vpx_config.c    |   10 +
 .../source/config/linux/sw_64/vpx_config.h    |  107 +
 .../source/config/linux/sw_64/vpx_dsp_rtcd.h  | 3868 +++++++
 .../config/linux/sw_64/vpx_scale_rtcd.h       |   96 +
 .../third_party/lss/linux_syscall_support.h   |  205 +-
 .../chromium/third_party/node/node.py         |   13 +
 .../third_party/llvm-10.0/BUILD.gn            |    7 +
 .../modules/desktop_capture/differ_block.cc   |    2 +-
 .../third_party/webrtc/rtc_base/system/arch.h |    4 +
 src/3rdparty/chromium/v8/BUILD.gn             |   40 +
 .../chromium/v8/src/base/build_config.h       |   14 +-
 src/3rdparty/chromium/v8/src/base/cpu.cc      |    4 +-
 .../v8/src/base/platform/platform-posix.cc    |    5 +
 .../builtins-sharedarraybuffer-gen.cc         |    4 +-
 .../chromium/v8/src/builtins/builtins.cc      |    2 +-
 .../v8/src/builtins/sw64/builtins-sw64.cc     | 3222 ++++++
 .../chromium/v8/src/codegen/assembler-arch.h  |    2 +
 .../chromium/v8/src/codegen/assembler-inl.h   |    2 +
 .../v8/src/codegen/assembler-sw64-inl.h       |  329 +
 .../chromium/v8/src/codegen/constants-arch.h  |    2 +
 .../chromium/v8/src/codegen/cpu-features.h    |    5 +
 .../v8/src/codegen/external-reference.cc      |   99 +-
 .../v8/src/codegen/external-reference.h       |    8 +
 .../v8/src/codegen/interface-descriptors.cc   |    2 +-
 .../chromium/v8/src/codegen/macro-assembler.h |    3 +
 .../chromium/v8/src/codegen/register-arch.h   |    2 +
 .../v8/src/codegen/register-configuration.cc  |    2 +
 .../chromium/v8/src/codegen/reloc-info.cc     |    2 +-
 .../chromium/v8/src/codegen/sw64/OWNERS       |    3 +
 .../v8/src/codegen/sw64/assembler-sw64-inl.h  |  329 +
 .../v8/src/codegen/sw64/assembler-sw64.cc     | 4379 ++++++++
 .../v8/src/codegen/sw64/assembler-sw64.h      | 1636 +++
 .../v8/src/codegen/sw64/constants-sw64.cc     |  160 +
 .../v8/src/codegen/sw64/constants-sw64.h      | 2756 +++++
 .../chromium/v8/src/codegen/sw64/cpu-sw64.cc  |   40 +
 .../sw64/interface-descriptors-sw64.cc        |  351 +
 .../src/codegen/sw64/macro-assembler-sw64.cc  | 5089 ++++++++++
 .../src/codegen/sw64/macro-assembler-sw64.h   | 1321 +++
 .../v8/src/codegen/sw64/register-sw64.h       |  406 +
 src/3rdparty/chromium/v8/src/common/globals.h |    3 +
 .../src/compiler/backend/instruction-codes.h  |    2 +
 .../compiler/backend/instruction-selector.cc  |    4 +-
 .../v8/src/compiler/backend/sw64/OWNERS       |    3 +
 .../backend/sw64/code-generator-sw64.cc       | 4209 ++++++++
 .../backend/sw64/instruction-codes-sw64.h     |  427 +
 .../sw64/instruction-scheduler-sw64.cc        | 1558 +++
 .../backend/sw64/instruction-selector-sw64.cc | 3291 ++++++
 .../sw64/unwinding-info-writer-sw64.cc        |  108 +
 .../backend/sw64/unwinding-info-writer-sw64.h |   73 +
 .../compiler/backend/unwinding-info-writer.h  |    2 +
 .../chromium/v8/src/compiler/c-linkage.cc     |   10 +
 .../chromium/v8/src/debug/debug-evaluate.cc   |    2 +
 .../chromium/v8/src/debug/sw64/debug-sw64.cc  |   57 +
 .../src/deoptimizer/sw64/deoptimizer-sw64.cc  |  250 +
 .../chromium/v8/src/diagnostics/gdb-jit.cc    |    2 +
 .../chromium/v8/src/diagnostics/perf-jit.h    |    3 +
 .../v8/src/diagnostics/sw64/disasm-sw64.cc    | 3439 +++++++
 .../v8/src/execution/frame-constants.h        |    2 +
 .../chromium/v8/src/execution/simulator.h     |    2 +
 .../execution/sw64/frame-constants-sw64.cc    |   32 +
 .../src/execution/sw64/frame-constants-sw64.h |   78 +
 .../v8/src/execution/sw64/simulator-sw64.cc   | 8980 +++++++++++++++++
 .../v8/src/execution/sw64/simulator-sw64.h    |  620 ++
 .../chromium/v8/src/flags/flag-definitions.h  |    2 +-
 .../heap/base/asm/sw64/push_registers_asm.cc  |   46 +
 .../src/interpreter/interpreter-assembler.cc  |    2 +-
 .../chromium/v8/src/libsampler/sampler.cc     |    4 +
 src/3rdparty/chromium/v8/src/logging/log.cc   |    2 +
 src/3rdparty/chromium/v8/src/objects/code.cc  |    2 +-
 src/3rdparty/chromium/v8/src/objects/code.h   |    2 +
 .../chromium/v8/src/profiler/tick-sample.cc   |    6 +
 .../src/regexp/regexp-macro-assembler-arch.h  |    2 +
 .../v8/src/regexp/regexp-macro-assembler.h    |    1 +
 src/3rdparty/chromium/v8/src/regexp/regexp.cc |    3 +
 .../chromium/v8/src/regexp/sw64/OWNERS        |    3 +
 .../sw64/regexp-macro-assembler-sw64.cc       | 1370 +++
 .../regexp/sw64/regexp-macro-assembler-sw64.h |  224 +
 .../v8/src/runtime/runtime-atomics.cc         |    2 +-
 .../chromium/v8/src/runtime/runtime-utils.h   |   13 +
 .../chromium/v8/src/snapshot/deserializer.h   |    2 +-
 .../platform-embedded-file-writer-generic.cc  |    2 +-
 .../wasm/baseline/liftoff-assembler-defs.h    |   29 +
 .../v8/src/wasm/baseline/liftoff-assembler.h  |    2 +
 .../baseline/sw64/liftoff-assembler-sw64.h    | 2951 ++++++
 .../v8/src/wasm/jump-table-assembler.cc       |   35 +
 .../v8/src/wasm/jump-table-assembler.h        |    5 +
 .../chromium/v8/src/wasm/wasm-debug.cc        |    6 +-
 .../chromium/v8/src/wasm/wasm-linkage.h       |    9 +
 src/3rdparty/gn/tools/gn/args.cc              |    3 +
 src/3rdparty/gn/util/build_config.h           |    5 +
 164 files changed, 54037 insertions(+), 67 deletions(-)
 create mode 100644 src/3rdparty/chromium/build/config/sw.gni
 create mode 100644 src/3rdparty/chromium/sandbox/linux/system_headers/sw_64_linux_syscalls.h
 create mode 100644 src/3rdparty/chromium/sandbox/linux/system_headers/sw_64_linux_ucontext.h
 create mode 100755 src/3rdparty/chromium/third_party/blink/renderer/platform/heap/asm/SaveRegisters_sw64.S
 create mode 100644 src/3rdparty/chromium/third_party/breakpad/breakpad/src/google_breakpad/common/minidump_cpu_sw64.h
 create mode 100644 src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vp8_rtcd.h
 create mode 100644 src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vp9_rtcd.h
 create mode 100644 src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vpx_config.asm
 create mode 100644 src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vpx_config.c
 create mode 100644 src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vpx_config.h
 create mode 100644 src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vpx_dsp_rtcd.h
 create mode 100644 src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vpx_scale_rtcd.h
 create mode 100755 src/3rdparty/chromium/v8/src/builtins/sw64/builtins-sw64.cc
 create mode 100755 src/3rdparty/chromium/v8/src/codegen/assembler-sw64-inl.h
 create mode 100755 src/3rdparty/chromium/v8/src/codegen/sw64/OWNERS
 create mode 100755 src/3rdparty/chromium/v8/src/codegen/sw64/assembler-sw64-inl.h
 create mode 100755 src/3rdparty/chromium/v8/src/codegen/sw64/assembler-sw64.cc
 create mode 100755 src/3rdparty/chromium/v8/src/codegen/sw64/assembler-sw64.h
 create mode 100755 src/3rdparty/chromium/v8/src/codegen/sw64/constants-sw64.cc
 create mode 100755 src/3rdparty/chromium/v8/src/codegen/sw64/constants-sw64.h
 create mode 100755 src/3rdparty/chromium/v8/src/codegen/sw64/cpu-sw64.cc
 create mode 100755 src/3rdparty/chromium/v8/src/codegen/sw64/interface-descriptors-sw64.cc
 create mode 100755 src/3rdparty/chromium/v8/src/codegen/sw64/macro-assembler-sw64.cc
 create mode 100755 src/3rdparty/chromium/v8/src/codegen/sw64/macro-assembler-sw64.h
 create mode 100755 src/3rdparty/chromium/v8/src/codegen/sw64/register-sw64.h
 create mode 100755 src/3rdparty/chromium/v8/src/compiler/backend/sw64/OWNERS
 create mode 100755 src/3rdparty/chromium/v8/src/compiler/backend/sw64/code-generator-sw64.cc
 create mode 100755 src/3rdparty/chromium/v8/src/compiler/backend/sw64/instruction-codes-sw64.h
 create mode 100755 src/3rdparty/chromium/v8/src/compiler/backend/sw64/instruction-scheduler-sw64.cc
 create mode 100755 src/3rdparty/chromium/v8/src/compiler/backend/sw64/instruction-selector-sw64.cc
 create mode 100755 src/3rdparty/chromium/v8/src/compiler/backend/sw64/unwinding-info-writer-sw64.cc
 create mode 100755 src/3rdparty/chromium/v8/src/compiler/backend/sw64/unwinding-info-writer-sw64.h
 create mode 100755 src/3rdparty/chromium/v8/src/debug/sw64/debug-sw64.cc
 create mode 100644 src/3rdparty/chromium/v8/src/deoptimizer/sw64/deoptimizer-sw64.cc
 create mode 100755 src/3rdparty/chromium/v8/src/diagnostics/sw64/disasm-sw64.cc
 create mode 100755 src/3rdparty/chromium/v8/src/execution/sw64/frame-constants-sw64.cc
 create mode 100755 src/3rdparty/chromium/v8/src/execution/sw64/frame-constants-sw64.h
 create mode 100755 src/3rdparty/chromium/v8/src/execution/sw64/simulator-sw64.cc
 create mode 100755 src/3rdparty/chromium/v8/src/execution/sw64/simulator-sw64.h
 create mode 100755 src/3rdparty/chromium/v8/src/heap/base/asm/sw64/push_registers_asm.cc
 create mode 100755 src/3rdparty/chromium/v8/src/regexp/sw64/OWNERS
 create mode 100755 src/3rdparty/chromium/v8/src/regexp/sw64/regexp-macro-assembler-sw64.cc
 create mode 100755 src/3rdparty/chromium/v8/src/regexp/sw64/regexp-macro-assembler-sw64.h
 create mode 100755 src/3rdparty/chromium/v8/src/wasm/baseline/sw64/liftoff-assembler-sw64.h

diff --git a/configure.pri b/configure.pri
index e072961f05..6f2be1048f 100644
--- a/configure.pri
+++ b/configure.pri
@@ -144,6 +144,7 @@ defineTest(qtConfTest_detectArch) {
     contains(QT_ARCH, "arm")|contains(QT_ARCH, "arm64"): return(true)
     contains(QT_ARCH, "mips"): return(true)
     contains(QT_ARCH, "mips64"): return(true)
+    contains(QT_ARCH, "sw_64"): return(true)
     qtLog("Architecture not supported.")
     return(false)
 }
diff --git a/mkspecs/features/functions.prf b/mkspecs/features/functions.prf
index 2750d70717..3dfdeb5699 100644
--- a/mkspecs/features/functions.prf
+++ b/mkspecs/features/functions.prf
@@ -107,6 +107,7 @@ defineReplace(gnArch) {
     contains(qtArch, "mips"): return(mipsel)
     contains(qtArch, "mips64"): return(mips64el)
     contains(qtArch, "mips64el"): return(mips64el)
+    contains(qtArch, "sw_64"): return(sw_64)
     return(unknown)
 }
 
diff --git a/src/3rdparty/chromium/base/allocator/partition_allocator/page_allocator_constants.h b/src/3rdparty/chromium/base/allocator/partition_allocator/page_allocator_constants.h
index c42fe2835f..1f6fc214b4 100644
--- a/src/3rdparty/chromium/base/allocator/partition_allocator/page_allocator_constants.h
+++ b/src/3rdparty/chromium/base/allocator/partition_allocator/page_allocator_constants.h
@@ -49,6 +49,8 @@ constexpr ALWAYS_INLINE int PageAllocationGranularityShift() {
   return 16;  // 64kB
 #elif defined(_MIPS_ARCH_LOONGSON)
   return 14;  // 16kB
+#elif defined(__sw_64__)
+  return 13; // 8KB
 #else
   return 12;  // 4kB
 #endif
diff --git a/src/3rdparty/chromium/base/allocator/partition_allocator/partition_alloc_constants.h b/src/3rdparty/chromium/base/allocator/partition_allocator/partition_alloc_constants.h
index c8268ec30a..09e12809dc 100644
--- a/src/3rdparty/chromium/base/allocator/partition_allocator/partition_alloc_constants.h
+++ b/src/3rdparty/chromium/base/allocator/partition_allocator/partition_alloc_constants.h
@@ -61,6 +61,10 @@ PartitionPageShift() {
 PAGE_ALLOCATOR_CONSTANTS_DECLARE_CONSTEXPR ALWAYS_INLINE int
 PartitionPageShift() {
   return vm_page_shift + 2;
+#elif defined(__sw_64__)
+PAGE_ALLOCATOR_CONSTANTS_DECLARE_CONSTEXPR ALWAYS_INLINE size_t
+PartitionPageShift() {
+  return 15; //16KB
 }
 #else
 PAGE_ALLOCATOR_CONSTANTS_DECLARE_CONSTEXPR ALWAYS_INLINE int
diff --git a/src/3rdparty/chromium/base/process/launch_posix.cc b/src/3rdparty/chromium/base/process/launch_posix.cc
index a8b1f0bdf9..8c36f31d5e 100644
--- a/src/3rdparty/chromium/base/process/launch_posix.cc
+++ b/src/3rdparty/chromium/base/process/launch_posix.cc
@@ -704,7 +704,7 @@ NOINLINE pid_t CloneAndLongjmpInChild(unsigned long flags,
   alignas(16) char stack_buf[PTHREAD_STACK_MIN];
 #if defined(ARCH_CPU_X86_FAMILY) || defined(ARCH_CPU_ARM_FAMILY) ||   \
     defined(ARCH_CPU_MIPS_FAMILY) || defined(ARCH_CPU_S390_FAMILY) || \
-    defined(ARCH_CPU_PPC64_FAMILY)
+    defined(ARCH_CPU_PPC64_FAMILY) || defined(ARCH_CPU_SW64)
   // The stack grows downward.
   void* stack = stack_buf + sizeof(stack_buf);
 #else
diff --git a/src/3rdparty/chromium/base/third_party/double_conversion/double-conversion/utils.h b/src/3rdparty/chromium/base/third_party/double_conversion/double-conversion/utils.h
index 471c3da84c..58c31f1b81 100644
--- a/src/3rdparty/chromium/base/third_party/double_conversion/double-conversion/utils.h
+++ b/src/3rdparty/chromium/base/third_party/double_conversion/double-conversion/utils.h
@@ -99,7 +99,7 @@ int main(int argc, char** argv) {
 #if defined(_M_X64) || defined(__x86_64__) || \
     defined(__ARMEL__) || defined(__avr32__) || defined(_M_ARM) || defined(_M_ARM64) || \
     defined(__hppa__) || defined(__ia64__) || \
-    defined(__mips__) || \
+    defined(__mips__) || defined(__sw_64__) || \
     defined(__nios2__) || \
     defined(__powerpc__) || defined(__ppc__) || defined(__ppc64__) || \
     defined(_POWER) || defined(_ARCH_PPC) || defined(_ARCH_PPC64) || \
diff --git a/src/3rdparty/chromium/build/build_config.h b/src/3rdparty/chromium/build/build_config.h
index 2c3e81ee1d..ae5d28f949 100644
--- a/src/3rdparty/chromium/build/build_config.h
+++ b/src/3rdparty/chromium/build/build_config.h
@@ -193,6 +193,11 @@
 #define ARCH_CPU_32_BITS 1
 #define ARCH_CPU_BIG_ENDIAN 1
 #endif
+#elif defined(__sw_64__)
+#define ARCH_CPU_SW64_FAMILY 1
+#define ARCH_CPU_SW64 1
+#define ARCH_CPU_64_BITS 1
+#define ARCH_CPU_LITTLE_ENDIAN 1
 #else
 #error Please add support for your architecture in build/build_config.h
 #endif
diff --git a/src/3rdparty/chromium/build/config/sw.gni b/src/3rdparty/chromium/build/config/sw.gni
new file mode 100644
index 0000000000..ae79313ed2
--- /dev/null
+++ b/src/3rdparty/chromium/build/config/sw.gni
@@ -0,0 +1,37 @@
+# Copyright 2015 The Chromium Authors. All rights reserved.
+# Use of this source code is governed by a BSD-style license that
+# found in the LICENSE file.
+
+import("//build/config/v8_target_cpu.gni")
+
+# NOT SURE, TODO at BUILD.gn
+if (current_cpu == "sw_64") {
+  declare_args() {
+    # MIPS arch variant. Possible values are:
+    #   "r1"
+    #   "r2"
+    #   "r6"
+    #   "loongson3"
+    sw_arch_variant = "r1"
+
+    # MIPS DSP ASE revision. Possible values are:
+    #   0: unavailable
+    #   1: revision 1
+    #   2: revision 2
+    sw_dsp_rev = 0
+
+    # MIPS SIMD Arch compilation flag.
+    sw_use_msa = false
+
+    # MIPS floating-point ABI. Possible values are:
+    #   "hard": sets the GCC -mhard-float option.
+    #   "soft": sets the GCC -msoft-float option.
+    sw_float_abi = "hard"
+
+    # MIPS32 floating-point register width. Possible values are:
+    #   "fp32": sets the GCC -mfp32 option.
+    #   "fp64": sets the GCC -mfp64 option.
+    #   "fpxx": sets the GCC -mfpxx option.
+    sw_fpu_mode = "fp32"
+  }
+}
diff --git a/src/3rdparty/chromium/media/media_options.gni b/src/3rdparty/chromium/media/media_options.gni
index acacfcd625..8782e883d9 100644
--- a/src/3rdparty/chromium/media/media_options.gni
+++ b/src/3rdparty/chromium/media/media_options.gni
@@ -100,7 +100,8 @@ declare_args() {
   # are combined and we could override more logging than expected.
   enable_logging_override = !use_jumbo_build && is_chromecast
 
-  enable_dav1d_decoder = !is_android && !is_ios && target_cpu != "mipsel" && target_cpu != "mips64el"
+  enable_dav1d_decoder = !is_android && !is_ios && target_cpu != "mipsel" &&
+      target_cpu != "mips64el" && target_cpu != "sw_64"
 
   # Enable browser managed persistent metadata storage for EME persistent
   # session and persistent usage record session.
diff --git a/src/3rdparty/chromium/sandbox/features.gni b/src/3rdparty/chromium/sandbox/features.gni
index db30ae6d63..bc99ae2b9e 100644
--- a/src/3rdparty/chromium/sandbox/features.gni
+++ b/src/3rdparty/chromium/sandbox/features.gni
@@ -11,6 +11,7 @@ import("//build/config/nacl/config.gni")
 use_seccomp_bpf = (is_linux || is_chromeos || is_android) &&
                   (current_cpu == "x86" || current_cpu == "x64" ||
                    current_cpu == "arm" || current_cpu == "arm64" ||
+                   current_cpu == "mips64el" || current_cpu == "sw_64" ||
                    current_cpu == "mipsel" || current_cpu == "mips64el")
 
 use_seccomp_bpf = use_seccomp_bpf || is_nacl_nonsfi
diff --git a/src/3rdparty/chromium/sandbox/linux/bpf_dsl/linux_syscall_ranges.h b/src/3rdparty/chromium/sandbox/linux/bpf_dsl/linux_syscall_ranges.h
index 642ceddae8..3299996498 100644
--- a/src/3rdparty/chromium/sandbox/linux/bpf_dsl/linux_syscall_ranges.h
+++ b/src/3rdparty/chromium/sandbox/linux/bpf_dsl/linux_syscall_ranges.h
@@ -49,6 +49,11 @@
 #define MAX_PUBLIC_SYSCALL __NR_syscalls
 #define MAX_SYSCALL MAX_PUBLIC_SYSCALL
 
+#elif defined(__sw_64__)
+#define MIN_SYSCALL         0u
+#define MAX_PUBLIC_SYSCALL  514u
+#define MAX_SYSCALL         MAX_PUBLIC_SYSCALL
+
 #else
 #error "Unsupported architecture"
 #endif
diff --git a/src/3rdparty/chromium/sandbox/linux/bpf_dsl/seccomp_macros.h b/src/3rdparty/chromium/sandbox/linux/bpf_dsl/seccomp_macros.h
index 1a407b9523..3c5ccaa802 100644
--- a/src/3rdparty/chromium/sandbox/linux/bpf_dsl/seccomp_macros.h
+++ b/src/3rdparty/chromium/sandbox/linux/bpf_dsl/seccomp_macros.h
@@ -245,6 +245,65 @@ struct regs_struct {
 #define SECCOMP_PT_PARM3(_regs)   (_regs).REG_a2
 #define SECCOMP_PT_PARM4(_regs)   (_regs).REG_a3
 
+#elif defined(__sw_64__)
+#define SECCOMP_ARCH        AUDIT_ARCH_SW64
+#define SYSCALL_EIGHT_ARGS
+// NOT SURE
+// See </arch/sw_64/include/uapi/asm/sigcontext.h> in the Linux kernel.
+#define SECCOMP_REG(_ctx, _reg) ((_ctx)->uc_mcontext.sc_regs[_reg])
+// Based on MIPS n64 ABI syscall convention.
+// On MIPS, when an indirect syscall is being made (syscall(__NR_foo)),
+// the real identifier (__NR_foo) is not in v0, but in a0.
+#define SECCOMP_RESULT(_ctx)    SECCOMP_REG(_ctx, 2)
+#define SECCOMP_SYSCALL(_ctx)   SECCOMP_REG(_ctx, 2)
+#define SECCOMP_IP(_ctx)        (_ctx)->uc_mcontext.sc_pc
+#define SECCOMP_PARM1(_ctx)     SECCOMP_REG(_ctx, 4)
+#define SECCOMP_PARM2(_ctx)     SECCOMP_REG(_ctx, 5)
+#define SECCOMP_PARM3(_ctx)     SECCOMP_REG(_ctx, 6)
+#define SECCOMP_PARM4(_ctx)     SECCOMP_REG(_ctx, 7)
+#define SECCOMP_PARM5(_ctx)     SECCOMP_REG(_ctx, 8)
+#define SECCOMP_PARM6(_ctx)     SECCOMP_REG(_ctx, 9)
+#define SECCOMP_PARM7(_ctx)     SECCOMP_REG(_ctx, 10)
+#define SECCOMP_PARM8(_ctx)     SECCOMP_REG(_ctx, 11)
+#define SECCOMP_NR_IDX          (offsetof(struct arch_seccomp_data, nr))
+#define SECCOMP_ARCH_IDX        (offsetof(struct arch_seccomp_data, arch))
+#define SECCOMP_IP_MSB_IDX      (offsetof(struct arch_seccomp_data,           \
+                                          instruction_pointer) + 4)
+#define SECCOMP_IP_LSB_IDX      (offsetof(struct arch_seccomp_data,           \
+                                          instruction_pointer) + 0)
+#define SECCOMP_ARG_MSB_IDX(nr) (offsetof(struct arch_seccomp_data, args) +   \
+                                 8*(nr) + 4)
+#define SECCOMP_ARG_LSB_IDX(nr) (offsetof(struct arch_seccomp_data, args) +   \
+                                 8*(nr) + 0)
+
+// On MIPS we don't have structures like user_regs or user_regs_struct in
+// sys/user.h that we could use, so we just define regs_struct directly.
+struct regs_struct {
+  unsigned long long regs[32];
+};
+
+#define REG_a7 regs[11]
+#define REG_a6 regs[10]
+#define REG_a5 regs[9]
+#define REG_a4 regs[8]
+#define REG_a3 regs[7]
+#define REG_a2 regs[6]
+#define REG_a1 regs[5]
+#define REG_a0 regs[4]
+#define REG_v1 regs[3]
+#define REG_v0 regs[2]
+
+#define SECCOMP_PT_RESULT(_regs)  (_regs).REG_v0
+#define SECCOMP_PT_SYSCALL(_regs) (_regs).REG_v0
+#define SECCOMP_PT_PARM1(_regs)   (_regs).REG_a0
+#define SECCOMP_PT_PARM2(_regs)   (_regs).REG_a1
+#define SECCOMP_PT_PARM3(_regs)   (_regs).REG_a2
+#define SECCOMP_PT_PARM4(_regs)   (_regs).REG_a3
+#define SECCOMP_PT_PARM5(_regs)   (_regs).REG_a4
+#define SECCOMP_PT_PARM6(_regs)   (_regs).REG_a5
+#define SECCOMP_PT_PARM7(_regs)   (_regs).REG_a6
+#define SECCOMP_PT_PARM8(_regs)   (_regs).REG_a7
+
 #elif defined(ARCH_CPU_MIPS_FAMILY) && defined(ARCH_CPU_64_BITS)
 #define SECCOMP_ARCH        AUDIT_ARCH_MIPSEL64
 #define SYSCALL_EIGHT_ARGS
diff --git a/src/3rdparty/chromium/sandbox/linux/seccomp-bpf-helpers/baseline_policy.cc b/src/3rdparty/chromium/sandbox/linux/seccomp-bpf-helpers/baseline_policy.cc
index 11ecb8130b..1ddba31fa6 100644
--- a/src/3rdparty/chromium/sandbox/linux/seccomp-bpf-helpers/baseline_policy.cc
+++ b/src/3rdparty/chromium/sandbox/linux/seccomp-bpf-helpers/baseline_policy.cc
@@ -166,11 +166,13 @@ ResultExpr EvaluateSyscallImpl(int fs_denied_errno,
     return RestrictCloneToThreadsAndEPERMFork();
   }
 
+#if !defined(__sw_64__)
   // clone3 takes a pointer argument which we cannot examine, so return ENOSYS
   // to force the libc to use clone. See https://crbug.com/1213452.
   if (sysno == __NR_clone3) {
     return Error(ENOSYS);
   }
+#endif
 
   if (sysno == __NR_fcntl)
     return RestrictFcntlCommands();
diff --git a/src/3rdparty/chromium/sandbox/linux/seccomp-bpf-helpers/syscall_parameters_restrictions.cc b/src/3rdparty/chromium/sandbox/linux/seccomp-bpf-helpers/syscall_parameters_restrictions.cc
index da22c8f1a1..ae539cffda 100644
--- a/src/3rdparty/chromium/sandbox/linux/seccomp-bpf-helpers/syscall_parameters_restrictions.cc
+++ b/src/3rdparty/chromium/sandbox/linux/seccomp-bpf-helpers/syscall_parameters_restrictions.cc
@@ -37,7 +37,7 @@
 #include <sys/ioctl.h>
 #include <sys/ptrace.h>
 #if defined(OS_LINUX) && !defined(OS_CHROMEOS) && !defined(__arm__) && \
-    !defined(__aarch64__) && !defined(PTRACE_GET_THREAD_AREA)
+    !defined(__aarch64__) && !defined(PTRACE_GET_THREAD_AREA) && !defined(__sw_64__)
 // Also include asm/ptrace-abi.h since ptrace.h in older libc (for instance
 // the one in Ubuntu 16.04 LTS) is missing PTRACE_GET_THREAD_AREA.
 // asm/ptrace-abi.h doesn't exist on arm32 and PTRACE_GET_THREAD_AREA isn't
@@ -410,7 +410,7 @@ ResultExpr RestrictPrlimitToGetrlimit(pid_t target_pid) {
 ResultExpr RestrictPtrace() {
   const Arg<int> request(0);
   return Switch(request).CASES((
-#if !defined(__aarch64__)
+#if !defined(__aarch64__) && !defined(__sw_64__)
         PTRACE_GETREGS,
         PTRACE_GETFPREGS,
 #if defined(TRACE_GET_THREAD_AREA)
diff --git a/src/3rdparty/chromium/sandbox/linux/seccomp-bpf-helpers/syscall_sets.cc b/src/3rdparty/chromium/sandbox/linux/seccomp-bpf-helpers/syscall_sets.cc
index 2c7a9e8351..00a55cebd8 100644
--- a/src/3rdparty/chromium/sandbox/linux/seccomp-bpf-helpers/syscall_sets.cc
+++ b/src/3rdparty/chromium/sandbox/linux/seccomp-bpf-helpers/syscall_sets.cc
@@ -93,7 +93,9 @@ bool SyscallSets::IsFileSystem(int sysno) {
 #if defined(__i386__) || defined(__arm__)
     case __NR_chown32:
 #endif
+#if !defined(__sw_64__)
     case __NR_creat:
+#endif
     case __NR_futimesat:  // Should be called utimesat ?
     case __NR_lchown:
     case __NR_link:
@@ -242,16 +244,18 @@ bool SyscallSets::IsDeniedFileSystemAccessViaFd(int sysno) {
 bool SyscallSets::IsGetSimpleId(int sysno) {
   switch (sysno) {
     case __NR_capget:
+#if !defined(__sw_64__)
     case __NR_getegid:
     case __NR_geteuid:
     case __NR_getgid:
+    case __NR_getuid:
+    case __NR_getppid:
+#endif
     case __NR_getgroups:
     case __NR_getpid:
-    case __NR_getppid:
     case __NR_getresgid:
     case __NR_getsid:
     case __NR_gettid:
-    case __NR_getuid:
     case __NR_getresuid:
 #if defined(__i386__) || defined(__arm__)
     case __NR_getegid32:
@@ -636,7 +640,7 @@ bool SyscallSets::IsSeccomp(int sysno) {
 bool SyscallSets::IsAllowedBasicScheduler(int sysno) {
   switch (sysno) {
     case __NR_sched_yield:
-#if !defined(__aarch64__)
+#if !defined(__aarch64__) && !defined(__sw_64__)
     case __NR_pause:
 #endif
     case __NR_nanosleep:
@@ -707,7 +711,9 @@ bool SyscallSets::IsFsControl(int sysno) {
     (defined(ARCH_CPU_MIPS_FAMILY) && defined(ARCH_CPU_32_BITS))
     case __NR_umount:
 #endif
+#if !defined(__sw_64__)
     case __NR_umount2:
+#endif
       return true;
     default:
       return false;
diff --git a/src/3rdparty/chromium/sandbox/linux/seccomp-bpf/syscall.cc b/src/3rdparty/chromium/sandbox/linux/seccomp-bpf/syscall.cc
index e47e98bf5b..2de344fb2c 100644
--- a/src/3rdparty/chromium/sandbox/linux/seccomp-bpf/syscall.cc
+++ b/src/3rdparty/chromium/sandbox/linux/seccomp-bpf/syscall.cc
@@ -18,7 +18,7 @@ namespace sandbox {
 namespace {
 
 #if defined(ARCH_CPU_X86_FAMILY) || defined(ARCH_CPU_ARM_FAMILY) || \
-    defined(ARCH_CPU_MIPS_FAMILY)
+    defined(ARCH_CPU_MIPS_FAMILY) || defined(ARCH_CPU_SW64_FAMILY)
 // Number that's not currently used by any Linux kernel ABIs.
 const int kInvalidSyscallNumber = 0x351d3;
 #else
@@ -312,6 +312,10 @@ asm(// We need to be able to tell the kernel exactly where we made a
     "2:ret\n"
     ".cfi_endproc\n"
     ".size SyscallAsm, .-SyscallAsm\n"
+#elif defined(__sw_64__)
+    ".text\n"
+    ".option pic2\n"
+    "ret\n"
 #endif
     );  // asm
 
@@ -319,7 +323,7 @@ asm(// We need to be able to tell the kernel exactly where we made a
 extern "C" {
 intptr_t SyscallAsm(intptr_t nr, const intptr_t args[6]);
 }
-#elif defined(__mips__)
+#elif defined(__mips__) || defined(__sw_64__)
 extern "C" {
 intptr_t SyscallAsm(intptr_t nr, const intptr_t args[8]);
 }
@@ -353,7 +357,7 @@ intptr_t Syscall::Call(int nr,
 
   // TODO(nedeljko): Enable use of more than six parameters on architectures
   //                 where that makes sense.
-#if defined(__mips__)
+#if defined(__mips__) || defined(__sw_64__)
   const intptr_t args[8] = {p0, p1, p2, p3, p4, p5, p6, p7};
 #else
   DCHECK_EQ(p6, 0) << " Support for syscalls with more than six arguments not "
@@ -407,7 +411,7 @@ intptr_t Syscall::Call(int nr,
         );
     ret = inout;
   }
-#elif defined(__mips__)
+#elif defined(__mips__) || defined(__sw_64__)
   intptr_t err_status;
   intptr_t ret = Syscall::SandboxSyscallRaw(nr, args, &err_status);
 
@@ -436,7 +440,7 @@ intptr_t Syscall::Call(int nr,
 }
 
 void Syscall::PutValueInUcontext(intptr_t ret_val, ucontext_t* ctx) {
-#if defined(__mips__)
+#if defined(__mips__) || defined(__sw_64__)
   // Mips ABI states that on error a3 CPU register has non zero value and if
   // there is no error, it should be zero.
   if (ret_val <= -1 && ret_val >= -4095) {
@@ -478,6 +482,16 @@ intptr_t Syscall::SandboxSyscallRaw(int nr,
 
   return ret;
 }
+
+#elif defined(__sw_64__)
+// NOT SURE TODO
+intptr_t Syscall::SandboxSyscallRaw(int nr,
+                                    const intptr_t* args,
+                                    intptr_t* err_ret) {
+  register intptr_t ret = nr;
+  return ret;
+}
+
 #endif  // defined(__mips__)
 
 }  // namespace sandbox
diff --git a/src/3rdparty/chromium/sandbox/linux/seccomp-bpf/syscall.h b/src/3rdparty/chromium/sandbox/linux/seccomp-bpf/syscall.h
index 3b02a6723f..45a4788e19 100644
--- a/src/3rdparty/chromium/sandbox/linux/seccomp-bpf/syscall.h
+++ b/src/3rdparty/chromium/sandbox/linux/seccomp-bpf/syscall.h
@@ -143,7 +143,7 @@ class SANDBOX_EXPORT Syscall {
                        intptr_t p6,
                        intptr_t p7);
 
-#if defined(__mips__)
+#if defined(__mips__) || defined(__sw_64__)
   // This function basically does on MIPS what SandboxSyscall() is doing on
   // other architectures. However, because of specificity of MIPS regarding
   // handling syscall errors, SandboxSyscall() is made as a wrapper for this
diff --git a/src/3rdparty/chromium/sandbox/linux/services/credentials.cc b/src/3rdparty/chromium/sandbox/linux/services/credentials.cc
index d7b5d8c441..c8fef20724 100644
--- a/src/3rdparty/chromium/sandbox/linux/services/credentials.cc
+++ b/src/3rdparty/chromium/sandbox/linux/services/credentials.cc
@@ -81,7 +81,7 @@ bool ChrootToSafeEmptyDir() {
   pid_t pid = -1;
   alignas(16) char stack_buf[PTHREAD_STACK_MIN];
 #if defined(ARCH_CPU_X86_FAMILY) || defined(ARCH_CPU_ARM_FAMILY) || \
-    defined(ARCH_CPU_MIPS_FAMILY)
+    defined(ARCH_CPU_MIPS_FAMILY) || defined(ARCH_CPU_SW64_FAMILY)
   // The stack grows downward.
   void* stack = stack_buf + sizeof(stack_buf);
 #else
diff --git a/src/3rdparty/chromium/sandbox/linux/system_headers/linux_seccomp.h b/src/3rdparty/chromium/sandbox/linux/system_headers/linux_seccomp.h
index 1fa47ed09f..678ff35226 100644
--- a/src/3rdparty/chromium/sandbox/linux/system_headers/linux_seccomp.h
+++ b/src/3rdparty/chromium/sandbox/linux/system_headers/linux_seccomp.h
@@ -42,6 +42,10 @@
 #define EM_AARCH64 183
 #endif
 
+#ifndef EM_SW_64
+#define EM_SW_64   0x9906
+#endif
+
 #ifndef __AUDIT_ARCH_64BIT
 #define __AUDIT_ARCH_64BIT 0x80000000
 #endif
@@ -58,6 +62,10 @@
 #define AUDIT_ARCH_I386   (EM_386|__AUDIT_ARCH_LE)
 #endif
 
+#ifndef AUDIT_ARCH_SW64
+#define AUDIT_ARCH_SW64        (EM_SW_64|__AUDIT_ARCH_64BIT|__AUDIT_ARCH_LE)
+#endif
+
 #ifndef AUDIT_ARCH_X86_64
 #define AUDIT_ARCH_X86_64 (EM_X86_64|__AUDIT_ARCH_64BIT|__AUDIT_ARCH_LE)
 #endif
diff --git a/src/3rdparty/chromium/sandbox/linux/system_headers/linux_signal.h b/src/3rdparty/chromium/sandbox/linux/system_headers/linux_signal.h
index f5a7367617..19dea26801 100644
--- a/src/3rdparty/chromium/sandbox/linux/system_headers/linux_signal.h
+++ b/src/3rdparty/chromium/sandbox/linux/system_headers/linux_signal.h
@@ -61,6 +61,30 @@
 
 #define LINUX_SIG_DFL 0
 
+#elif defined(__sw_64__)
+
+#define LINUX_SIGHUP 1
+#define LINUX_SIGINT 2
+#define LINUX_SIGQUIT 3
+#define LINUX_SIGABRT 6
+#define LINUX_SIGBUS 10
+#define LINUX_SIGSEGV 11
+#define LINUX_SIGSYS 12
+#define LINUX_SIGPIPE 13
+#define LINUX_SIGTERM 15
+#define LINUX_SIGUSR1 30
+#define LINUX_SIGUSR2 31
+#define LINUX_SIGCHLD 20
+
+#define LINUX_SIG_BLOCK 1
+#define LINUX_SIG_UNBLOCK 2
+
+#define LINUX_SA_SIGINFO 0x00000040
+#define LINUX_SA_NODEFER 0x00000008
+#define LINUX_SA_RESTART 0x00000002
+
+#define LINUX_SIG_DFL 0
+
 #else
 #error "Unsupported platform"
 #endif
diff --git a/src/3rdparty/chromium/sandbox/linux/system_headers/linux_stat.h b/src/3rdparty/chromium/sandbox/linux/system_headers/linux_stat.h
index 318c2bb8c2..b2062eb8fa 100644
--- a/src/3rdparty/chromium/sandbox/linux/system_headers/linux_stat.h
+++ b/src/3rdparty/chromium/sandbox/linux/system_headers/linux_stat.h
@@ -173,6 +173,24 @@ struct kernel_stat {
   unsigned int __unused4;
   unsigned int __unused5;
 };
+#elif defined(__sw_64__)
+struct kernel_stat {
+    unsigned int st_dev;
+    unsigned int st_ino;
+    unsigned int st_mode;
+    unsigned int st_nlink;
+    unsigned int st_uid;
+    unsigned int st_gid;
+    unsigned int st_rdev;
+    long int st_size;
+    unsigned long int st_atime;
+    unsigned long int st_mtime;
+    unsigned long int st_ctime;
+    unsigned int st_blksize;
+    int st_blocks;
+    unsigned int st_flags;
+    unsigned int st_gen;
+};
 #endif
 
 #if !defined(AT_EMPTY_PATH)
diff --git a/src/3rdparty/chromium/sandbox/linux/system_headers/linux_syscalls.h b/src/3rdparty/chromium/sandbox/linux/system_headers/linux_syscalls.h
index 2b78a0cc3b..1c024ec18f 100644
--- a/src/3rdparty/chromium/sandbox/linux/system_headers/linux_syscalls.h
+++ b/src/3rdparty/chromium/sandbox/linux/system_headers/linux_syscalls.h
@@ -35,5 +35,9 @@
 #include "sandbox/linux/system_headers/arm64_linux_syscalls.h"
 #endif
 
+#if defined(__sw_64__)
+#include "sandbox/linux/system_headers/sw_64_linux_syscalls.h"
+#endif
+
 #endif  // SANDBOX_LINUX_SYSTEM_HEADERS_LINUX_SYSCALLS_H_
 
diff --git a/src/3rdparty/chromium/sandbox/linux/system_headers/linux_ucontext.h b/src/3rdparty/chromium/sandbox/linux/system_headers/linux_ucontext.h
index 22ce780274..468f811a34 100644
--- a/src/3rdparty/chromium/sandbox/linux/system_headers/linux_ucontext.h
+++ b/src/3rdparty/chromium/sandbox/linux/system_headers/linux_ucontext.h
@@ -11,6 +11,8 @@
 #include "sandbox/linux/system_headers/arm_linux_ucontext.h"
 #elif defined(__i386__)
 #include "sandbox/linux/system_headers/i386_linux_ucontext.h"
+#elif defined(__sw_64__)
+#include "sandbox/linux/system_headers/sw_64_linux_ucontext.h"
 #else
 #error "No support for your architecture in PNaCl header"
 #endif
diff --git a/src/3rdparty/chromium/sandbox/linux/system_headers/sw_64_linux_syscalls.h b/src/3rdparty/chromium/sandbox/linux/system_headers/sw_64_linux_syscalls.h
new file mode 100644
index 0000000000..9eed01c77d
--- /dev/null
+++ b/src/3rdparty/chromium/sandbox/linux/system_headers/sw_64_linux_syscalls.h
@@ -0,0 +1,31 @@
+// Copyright 2014 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+// Generated from the Linux kernel's calls.S.
+#ifndef SANDBOX_LINUX_SYSTEM_HEADERS_SW_64_LINUX_SYSCALLS_H_
+#define SANDBOX_LINUX_SYSTEM_HEADERS_SW_64_LINUX_SYSCALLS_H_
+
+#if !defined(__sw_64__)
+#error "Including header on wrong architecture"
+#endif
+
+// __NR_osf_syscall, is defined in <asm/unistd.h>.
+#include <asm/unistd.h>
+
+//#define __NR_creat Unknown
+//#define __NR_getegid Ignore
+//#define __NR_geteuid Ignore
+//#define __NR_getgid Ignore
+
+#ifndef __NR_getpid
+#define __NR_getpid  0x9e
+#endif
+
+//#define __NR_getppid Ignore
+//#define __NR_getuid Ignore
+//#define __NR_pause Ignore
+//#define __NR_umount2 Ignore
+
+#endif
+
diff --git a/src/3rdparty/chromium/sandbox/linux/system_headers/sw_64_linux_ucontext.h b/src/3rdparty/chromium/sandbox/linux/system_headers/sw_64_linux_ucontext.h
new file mode 100644
index 0000000000..dccc016bc5
--- /dev/null
+++ b/src/3rdparty/chromium/sandbox/linux/system_headers/sw_64_linux_ucontext.h
@@ -0,0 +1,35 @@
+// Copyright 2014 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef SANDBOX_LINUX_SYSTEM_HEADERS_SW64_LINUX_UCONTEXT_H_
+#define SANDBOX_LINUX_SYSTEM_HEADERS_SW64_LINUX_UCONTEXT_H_
+
+#include <stdint.h>
+
+// This is mostly copied from breakpad (common/android/include/sys/ucontext.h),
+// except we do use sigset_t for uc_sigmask instead of a custom type.
+#if !defined(__BIONIC_HAVE_UCONTEXT_T)
+// Ensure that 'stack_t' is defined.
+#include <asm/sigcontext.h>
+
+// We also need greg_t for the sandbox, include it in this header as well.
+typedef unsigned long greg_t;
+
+typedef struct sigcontext mcontext_t;
+
+typedef struct ucontext {
+  uint32_t uc_flags;
+  struct ucontext* uc_link;
+  stack_t uc_stack;
+  mcontext_t uc_mcontext;
+  sigset_t uc_sigmask;
+  // Other fields are not used by Google Breakpad. Don't define them.
+} ucontext_t;
+
+#else
+#include <sys/ucontext.h>
+#endif  // __BIONIC_HAVE_UCONTEXT_T
+
+#endif  // SANDBOX_LINUX_SYSTEM_HEADERS_SW64_LINUX_UCONTEXT_H_
+
diff --git a/src/3rdparty/chromium/sandbox/policy/linux/bpf_cdm_policy_linux.cc b/src/3rdparty/chromium/sandbox/policy/linux/bpf_cdm_policy_linux.cc
index 1181e7291d..5ef30868d1 100644
--- a/src/3rdparty/chromium/sandbox/policy/linux/bpf_cdm_policy_linux.cc
+++ b/src/3rdparty/chromium/sandbox/policy/linux/bpf_cdm_policy_linux.cc
@@ -33,7 +33,7 @@ ResultExpr CdmProcessPolicy::EvaluateSyscall(int sysno) const {
     case __NR_ftruncate:
     case __NR_fallocate:
 #if defined(__i386__) || defined(__x86_64__) || defined(__mips__) || \
-    defined(__aarch64__)
+    defined(__aarch64__) || defined(__sw_64__)
     case __NR_getrlimit:
 #endif
 #if defined(__i386__) || defined(__arm__)
diff --git a/src/3rdparty/chromium/sandbox/policy/linux/bpf_gpu_policy_linux.cc b/src/3rdparty/chromium/sandbox/policy/linux/bpf_gpu_policy_linux.cc
index 0b672bb882..5334e30f0b 100644
--- a/src/3rdparty/chromium/sandbox/policy/linux/bpf_gpu_policy_linux.cc
+++ b/src/3rdparty/chromium/sandbox/policy/linux/bpf_gpu_policy_linux.cc
@@ -77,7 +77,7 @@ ResultExpr GpuProcessPolicy::EvaluateSyscall(int sysno) const {
     case __NR_ioctl:
     case __NR_memfd_create:
       return Allow();
-#if defined(__i386__) || defined(__x86_64__) || defined(__mips__)
+#if defined(__i386__) || defined(__x86_64__) || defined(__mips__) || defined(__sw_64__)
     // The Nvidia driver uses flags not in the baseline policy
     // (MAP_LOCKED | MAP_EXECUTABLE | MAP_32BIT)
     case __NR_mmap:
@@ -102,7 +102,7 @@ ResultExpr GpuProcessPolicy::EvaluateSyscall(int sysno) const {
   if (SyscallSets::IsEventFd(sysno))
     return Allow();
 
-#if defined(OS_LINUX) && !defined(OS_CHROMEOS) && defined(USE_X11)
+#if defined(OS_LINUX) && !defined(OS_CHROMEOS) && defined(USE_X11) && !defined(__sw_64__)
   if (SyscallSets::IsSystemVSharedMemory(sysno))
     return Allow();
 #endif
diff --git a/src/3rdparty/chromium/sandbox/policy/linux/bpf_utility_policy_linux.cc b/src/3rdparty/chromium/sandbox/policy/linux/bpf_utility_policy_linux.cc
index dfe9e9c55a..1a65d3eb60 100644
--- a/src/3rdparty/chromium/sandbox/policy/linux/bpf_utility_policy_linux.cc
+++ b/src/3rdparty/chromium/sandbox/policy/linux/bpf_utility_policy_linux.cc
@@ -34,7 +34,7 @@ ResultExpr UtilityProcessPolicy::EvaluateSyscall(int sysno) const {
     case __NR_fdatasync:
     case __NR_fsync:
 #if defined(__i386__) || defined(__x86_64__) || defined(__mips__) || \
-    defined(__aarch64__)
+    defined(__aarch64__) || defined(__sw_64__)
     case __NR_getrlimit:
 #endif
 #if defined(__i386__) || defined(__arm__)
diff --git a/src/3rdparty/chromium/sandbox/policy/linux/sandbox_seccomp_bpf_linux.cc b/src/3rdparty/chromium/sandbox/policy/linux/sandbox_seccomp_bpf_linux.cc
index cf9bd19210..5cbc561532 100644
--- a/src/3rdparty/chromium/sandbox/policy/linux/sandbox_seccomp_bpf_linux.cc
+++ b/src/3rdparty/chromium/sandbox/policy/linux/sandbox_seccomp_bpf_linux.cc
@@ -64,7 +64,7 @@ using sandbox::bpf_dsl::ResultExpr;
 
 // Make sure that seccomp-bpf does not get disabled by mistake. Also make sure
 // that we think twice about this when adding a new architecture.
-#if !defined(ARCH_CPU_ARM64) && !defined(ARCH_CPU_MIPS64EL)
+#if !defined(ARCH_CPU_ARM64) && !defined(ARCH_CPU_MIPS64EL) && !defined(ARCH_CPU_SW64)
 #error "Seccomp-bpf disabled on supported architecture!"
 #endif  // !defined(ARCH_CPU_ARM64) && !defined(ARCH_CPU_MIPS64EL)
 
@@ -127,6 +127,9 @@ std::unique_ptr<BPFBasePolicy> GetGpuProcessSandbox(
 
 // Is seccomp BPF globally enabled?
 bool SandboxSeccompBPF::IsSeccompBPFDesired() {
+#if defined(__sw_64__)
+  return false;
+#else
 #if BUILDFLAG(USE_SECCOMP_BPF)
   const base::CommandLine& command_line =
       *base::CommandLine::ForCurrentProcess();
@@ -135,6 +138,7 @@ bool SandboxSeccompBPF::IsSeccompBPFDesired() {
 #else
   return false;
 #endif  // USE_SECCOMP_BPF
+#endif
 }
 
 bool SandboxSeccompBPF::SupportsSandbox() {
diff --git a/src/3rdparty/chromium/skia/BUILD.gn b/src/3rdparty/chromium/skia/BUILD.gn
index 6a107a51a9..405e845b0f 100644
--- a/src/3rdparty/chromium/skia/BUILD.gn
+++ b/src/3rdparty/chromium/skia/BUILD.gn
@@ -23,6 +23,10 @@ if (current_cpu == "mipsel" || current_cpu == "mips64el") {
   import("//build/config/mips.gni")
 }
 
+if (current_cpu == "sw_64") {
+  import("//build/config/sw.gni")
+}
+
 skia_support_gpu = !is_ios
 skia_support_pdf = !is_ios && enable_basic_printing
 skia_support_skottie = true
@@ -819,6 +823,8 @@ skia_source_set("skia_opts") {
     sources = skia_opts.none_sources
   } else if (current_cpu == "ppc64") {
     sources = skia_opts.none_sources
+  } else if (current_cpu == "sw_64") {
+    sources = skia_opts.none_sources
   } else if (current_cpu == "s390x") {
     sources = skia_opts.none_sources
   } else {
diff --git a/src/3rdparty/chromium/third_party/abseil-cpp/absl/debugging/internal/examine_stack.cc b/src/3rdparty/chromium/third_party/abseil-cpp/absl/debugging/internal/examine_stack.cc
index 6e5ff1fbd8..85daad1899 100644
--- a/src/3rdparty/chromium/third_party/abseil-cpp/absl/debugging/internal/examine_stack.cc
+++ b/src/3rdparty/chromium/third_party/abseil-cpp/absl/debugging/internal/examine_stack.cc
@@ -66,6 +66,8 @@ void* GetProgramCounter(void* vuc) {
 #elif defined(__x86_64__)
     if (16 < ABSL_ARRAYSIZE(context->uc_mcontext.gregs))
       return reinterpret_cast<void*>(context->uc_mcontext.gregs[16]);
+#elif defined(__sw_64__)
+    return reinterpret_cast<void*>(context->uc_mcontext.sc_pc);
 #else
 #error "Undefined Architecture."
 #endif
diff --git a/src/3rdparty/chromium/third_party/angle/gni/angle.gni b/src/3rdparty/chromium/third_party/angle/gni/angle.gni
index c4fe8c83b1..6b1b2185b1 100644
--- a/src/3rdparty/chromium/third_party/angle/gni/angle.gni
+++ b/src/3rdparty/chromium/third_party/angle/gni/angle.gni
@@ -54,7 +54,7 @@ angle_data_dir = "angledata"
 declare_args() {
   if (current_cpu == "arm64" || current_cpu == "x64" ||
       current_cpu == "mips64el" || current_cpu == "s390x" ||
-      current_cpu == "ppc64") {
+      current_cpu == "ppc" || current_cpu == "sw_64") {
     angle_64bit_current_cpu = true
   } else if (current_cpu == "arm" || current_cpu == "x86" ||
              current_cpu == "mipsel" || current_cpu == "s390" ||
diff --git a/src/3rdparty/chromium/third_party/blink/renderer/platform/heap/asm/BUILD.gn b/src/3rdparty/chromium/third_party/blink/renderer/platform/heap/asm/BUILD.gn
index fe44daf27a..c26723a2d3 100644
--- a/src/3rdparty/chromium/third_party/blink/renderer/platform/heap/asm/BUILD.gn
+++ b/src/3rdparty/chromium/third_party/blink/renderer/platform/heap/asm/BUILD.gn
@@ -38,9 +38,9 @@ if (current_cpu == "x86" || current_cpu == "x64") {
       sources = [ "SaveRegisters_mips64.S" ]
     } else if (current_cpu == "ppc64") {
       sources = [ "SaveRegisters_ppc64.S" ]
-    }
-
-    if (current_cpu == "arm") {
+    } else if (current_cpu == "sw_64") {
+      sources = [ "SaveRegisters_sw64.S" ]
+    }if (current_cpu == "arm") {
       defines = [ "ARM=1" ]
     }
   }
diff --git a/src/3rdparty/chromium/third_party/blink/renderer/platform/heap/asm/SaveRegisters_sw64.S b/src/3rdparty/chromium/third_party/blink/renderer/platform/heap/asm/SaveRegisters_sw64.S
new file mode 100755
index 0000000000..1862fc4076
--- /dev/null
+++ b/src/3rdparty/chromium/third_party/blink/renderer/platform/heap/asm/SaveRegisters_sw64.S
@@ -0,0 +1,42 @@
+// Copyright 2014 The Chromium Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+/*
+ * typedef void (*PushAllRegistersCallback)(ThreadState*, intptr_t*);
+ * extern "C" void PushAllRegisters(ThreadState*, PushAllRegistersCallback)
+ */
+
+#include <asm/regdef.h>
+
+.type PushAllRegisters, %function
+.global PushAllRegisters
+.hidden PushAllRegisters
+PushAllRegisters:
+        // Push all callee-saves registers to get them
+        // on the stack for conservative stack scanning.
+        // Reserve space for callee-saved registers and return address.
+        subl sp,64,sp
+        // Save the callee-saved registers and the return address.
+        stl s0,0(sp)
+        stl s1,8(sp)
+        stl s2,16(sp)
+        stl s3,24(sp)
+        stl s4,32(sp)
+        stl s5,40(sp)
+        stl ra,48(sp)
+        // Note: the callee-saved floating point registers do not need to be
+        // copied to the stack, because fp registers never hold heap pointers
+        // and so do not need to be kept visible to the garbage collector.
+        // Pass the two first arguments untouched in a0 and the
+        // stack pointer to the callback.
+        bis $31, a1, t12
+        bis $31, sp, a1
+        call ra,(t12),0
+        // Restore return address, adjust stack and return.
+        // Note: the copied registers do not need to be reloaded here,
+        // because they were preserved by the called routine.
+        ldl ra,48(sp)
+        addl sp,64,sp
+	ret $31,(ra),0
+
diff --git a/src/3rdparty/chromium/third_party/boringssl/src/include/openssl/base.h b/src/3rdparty/chromium/third_party/boringssl/src/include/openssl/base.h
index d681bdf81e..8de1b689fa 100644
--- a/src/3rdparty/chromium/third_party/boringssl/src/include/openssl/base.h
+++ b/src/3rdparty/chromium/third_party/boringssl/src/include/openssl/base.h
@@ -114,6 +114,9 @@ extern "C" {
 #define OPENSSL_32_BIT
 #elif defined(__myriad2__)
 #define OPENSSL_32_BIT
+#elif defined(__sw_64__)
+#define OPENSSL_64_BIT
+#define OPENSSL_SW64
 #else
 // Note BoringSSL only supports standard 32-bit and 64-bit two's-complement,
 // little-endian architectures. Functions will not produce the correct answer
diff --git a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/dump_writer_common/raw_context_cpu.h b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/dump_writer_common/raw_context_cpu.h
index 07d9171a0a..2a81a42267 100644
--- a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/dump_writer_common/raw_context_cpu.h
+++ b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/dump_writer_common/raw_context_cpu.h
@@ -44,6 +44,8 @@ typedef MDRawContextARM RawContextCPU;
 typedef MDRawContextARM64_Old RawContextCPU;
 #elif defined(__mips__)
 typedef MDRawContextMIPS RawContextCPU;
+#elif defined(__sw_64__)
+typedef MDRawContextSW64 RawContextCPU;
 #else
 #error "This code has not been ported to your platform yet."
 #endif
diff --git a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/dump_writer_common/thread_info.cc b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/dump_writer_common/thread_info.cc
index aae1dc13b2..76e2b10c56 100644
--- a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/dump_writer_common/thread_info.cc
+++ b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/dump_writer_common/thread_info.cc
@@ -228,6 +228,24 @@ void ThreadInfo::FillCPUContext(RawContextCPU* out) const {
       MD_FLOATINGSAVEAREA_ARM64_FPR_COUNT * 16);
 }
 
+#elif defined(__sw_64__)
+// NOT SURE TODO
+uintptr_t ThreadInfo::GetInstructionPointer() const {
+  return mcontext.sc_pc;
+}
+
+void ThreadInfo::FillCPUContext(RawContextCPU* out) const {
+  out->context_flags = MD_CONTEXT_SW64_FULL;
+
+  for (int i = 0; i < MD_CONTEXT_SW64_GPR_COUNT; ++i)
+    out->iregs[i] = mcontext.sc_regs[i];
+
+  out->pc = mcontext.sc_pc;
+
+  for (int i = 0; i < MD_FLOATINGSAVEAREA_SW64_FPR_COUNT; ++i)
+    out->float_save.regs[i] = mcontext.sc_fpregs[i];
+}
+
 #elif defined(__mips__)
 
 uintptr_t ThreadInfo::GetInstructionPointer() const {
@@ -279,12 +297,16 @@ void ThreadInfo::GetGeneralPurposeRegisters(void** gp_regs, size_t* size) {
     *gp_regs = mcontext.gregs;
   if (size)
     *size = sizeof(mcontext.gregs);
+#else
+#if defined(__sw_64__)
+  return;
 #else
   if (gp_regs)
     *gp_regs = &regs;
   if (size)
     *size = sizeof(regs);
 #endif
+#endif
 }
 
 void ThreadInfo::GetFloatingPointRegisters(void** fp_regs, size_t* size) {
@@ -294,12 +316,16 @@ void ThreadInfo::GetFloatingPointRegisters(void** fp_regs, size_t* size) {
     *fp_regs = &mcontext.fpregs;
   if (size)
     *size = sizeof(mcontext.fpregs);
+#else
+#if defined(__sw_64__)
+  return;
 #else
   if (fp_regs)
     *fp_regs = &fpregs;
   if (size)
     *size = sizeof(fpregs);
 #endif
+#endif
 }
 
 }  // namespace google_breakpad
diff --git a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/dump_writer_common/thread_info.h b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/dump_writer_common/thread_info.h
index fb216fa6d7..d142c41059 100644
--- a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/dump_writer_common/thread_info.h
+++ b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/dump_writer_common/thread_info.h
@@ -68,7 +68,7 @@ struct ThreadInfo {
   // Use the structures defined in <sys/user.h>
   struct user_regs_struct regs;
   struct user_fpsimd_struct fpregs;
-#elif defined(__mips__)
+#elif defined(__mips__) || defined(__sw_64__)
   // Use the structure defined in <sys/ucontext.h>.
   mcontext_t mcontext;
 #endif
diff --git a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/dump_writer_common/ucontext_reader.cc b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/dump_writer_common/ucontext_reader.cc
index 6eec1be246..62e374cfcf 100644
--- a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/dump_writer_common/ucontext_reader.cc
+++ b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/dump_writer_common/ucontext_reader.cc
@@ -181,9 +181,22 @@ void UContextReader::FillCPUContext(RawContextCPU* out, const ucontext_t* uc) {
   my_memset(&out->float_save.regs, 0, sizeof(out->float_save.regs));
   my_memset(&out->float_save.extra, 0, sizeof(out->float_save.extra));
 }
+#elif defined(__sw_64__)
+
+uintptr_t UContextReader::GetStackPointer(const ucontext_t* uc) {
+  return uc->uc_mcontext.sc_regs[MD_CONTEXT_SW64_REG_SP];
+}
+
+uintptr_t UContextReader::GetInstructionPointer(const ucontext_t* uc) {
+  return uc->uc_mcontext.sc_pc;
+}
+
+void UContextReader::FillCPUContext(RawContextCPU *out, const ucontext_t *uc) {
+}
 
 #elif defined(__aarch64__)
 
+
 uintptr_t UContextReader::GetStackPointer(const ucontext_t* uc) {
   return uc->uc_mcontext.sp;
 }
diff --git a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/handler/exception_handler.cc b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/handler/exception_handler.cc
index 5cdabcf64c..c8e7324891 100644
--- a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/handler/exception_handler.cc
+++ b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/handler/exception_handler.cc
@@ -85,6 +85,7 @@
 #include <algorithm>
 #include <utility>
 #include <vector>
+#include <string.h>
 
 #include "common/basictypes.h"
 #include "common/linux/breakpad_getcontext.h"
@@ -105,6 +106,11 @@
 #define PR_SET_PTRACER 0x59616d61
 #endif
 
+#if defined(__sw_64__)
+#define sys_sigaltstack sigaltstack
+#define sys_clone clone
+#endif
+
 namespace google_breakpad {
 
 namespace {
@@ -461,7 +467,7 @@ bool ExceptionHandler::HandleSignal(int /*sig*/, siginfo_t* info, void* uc) {
     memcpy(&g_crash_context_.float_state, fp_ptr,
            sizeof(g_crash_context_.float_state));
   }
-#elif !defined(__ARM_EABI__) && !defined(__mips__)
+#elif !defined(__ARM_EABI__) && !defined(__mips__) && !defined(__sw_64__)
   // FP state is not part of user ABI on ARM Linux.
   // In case of MIPS Linux FP state is already part of ucontext_t
   // and 'float_state' is not a member of CrashContext.
@@ -701,7 +707,7 @@ bool ExceptionHandler::WriteMinidump() {
   }
 #endif
 
-#if !defined(__ARM_EABI__) && !defined(__aarch64__) && !defined(__mips__)
+#if !defined(__ARM_EABI__) && !defined(__aarch64__) && !defined(__mips__) && !defined(__sw_64__)
   // FPU state is not part of ARM EABI ucontext_t.
   memcpy(&context.float_state, context.context.uc_mcontext.fpregs,
          sizeof(context.float_state));
@@ -726,6 +732,9 @@ bool ExceptionHandler::WriteMinidump() {
 #elif defined(__mips__)
   context.siginfo.si_addr =
       reinterpret_cast<void*>(context.context.uc_mcontext.pc);
+#elif defined(__sw_64__)
+  context.siginfo.si_addr =
+      reinterpret_cast<void*>(context.context.uc_mcontext.sc_pc);
 #else
 #error "This code has not been ported to your platform yet."
 #endif
diff --git a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/handler/exception_handler.h b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/handler/exception_handler.h
index f80843ea72..f5073ab8c9 100644
--- a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/handler/exception_handler.h
+++ b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/handler/exception_handler.h
@@ -192,7 +192,7 @@ class ExceptionHandler {
     siginfo_t siginfo;
     pid_t tid;  // the crashing thread.
     ucontext_t context;
-#if !defined(__ARM_EABI__) && !defined(__mips__)
+#if !defined(__ARM_EABI__) && !defined(__mips__) && !defined(__sw_64__)
     // #ifdef this out because FP state is not part of user ABI for Linux ARM.
     // In case of MIPS Linux FP state is already part of ucontext_t so
     // 'float_state' is not required.
diff --git a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/microdump_writer/microdump_writer.cc b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/microdump_writer/microdump_writer.cc
index fa3c1713a5..9c8a386c30 100644
--- a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/microdump_writer/microdump_writer.cc
+++ b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/microdump_writer/microdump_writer.cc
@@ -138,7 +138,7 @@ class MicrodumpWriter {
                   const MicrodumpExtraInfo& microdump_extra_info,
                   LinuxDumper* dumper)
       : ucontext_(context ? &context->context : NULL),
-#if !defined(__ARM_EABI__) && !defined(__mips__)
+#if !defined(__ARM_EABI__) && !defined(__mips__) && !defined(__sw_64__)
         float_state_(context ? &context->float_state : NULL),
 #endif
         dumper_(dumper),
@@ -337,6 +337,8 @@ class MicrodumpWriter {
 # else
 #  error "This mips ABI is currently not supported (n32)"
 #endif
+#elif defined(__sw_64__)
+    const char kArch[] = "sw_64";
 #else
 #error "This code has not been ported to your platform yet"
 #endif
@@ -409,7 +411,7 @@ class MicrodumpWriter {
   void DumpCPUState() {
     RawContextCPU cpu;
     my_memset(&cpu, 0, sizeof(RawContextCPU));
-#if !defined(__ARM_EABI__) && !defined(__mips__)
+#if !defined(__ARM_EABI__) && !defined(__mips__) && !defined(__sw_64__)
     UContextReader::FillCPUContext(&cpu, ucontext_, float_state_);
 #else
     UContextReader::FillCPUContext(&cpu, ucontext_);
@@ -605,7 +607,7 @@ class MicrodumpWriter {
   void* Alloc(unsigned bytes) { return dumper_->allocator()->Alloc(bytes); }
 
   const ucontext_t* const ucontext_;
-#if !defined(__ARM_EABI__) && !defined(__mips__)
+#if !defined(__ARM_EABI__) && !defined(__mips__) && !defined(__sw_64__)
   const google_breakpad::fpstate_t* const float_state_;
 #endif
   LinuxDumper* dumper_;
diff --git a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/minidump_writer/linux_core_dumper.cc b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/minidump_writer/linux_core_dumper.cc
index 4150689839..b35052039c 100644
--- a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/minidump_writer/linux_core_dumper.cc
+++ b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/minidump_writer/linux_core_dumper.cc
@@ -112,6 +112,10 @@ bool LinuxCoreDumper::GetThreadInfoByIndex(size_t index, ThreadInfo* info) {
 #elif defined(__mips__)
   stack_pointer =
       reinterpret_cast<uint8_t*>(info->mcontext.gregs[MD_CONTEXT_MIPS_REG_SP]);
+#elif defined(__sw_64__)
+  //NOT SURE
+  stack_pointer =
+      reinterpret_cast<uint8_t*>(info->mcontext.sc_regs[MD_CONTEXT_SW64_REG_SP]);
 #else
 #error "This code hasn't been ported to your platform yet."
 #endif
@@ -208,6 +212,9 @@ bool LinuxCoreDumper::EnumerateThreads() {
         info.mcontext.mdlo = status->pr_reg[EF_LO];
         info.mcontext.mdhi = status->pr_reg[EF_HI];
         info.mcontext.pc = status->pr_reg[EF_CP0_EPC];
+#elif defined(__sw_64__)
+        for (int i = 0; i <= 31; i++)
+          info.mcontext.sc_regs[i] = status->pr_reg[i];
 #else  // __mips__
         memcpy(&info.regs, status->pr_reg, sizeof(info.regs));
 #endif  // __mips__
diff --git a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/minidump_writer/linux_dumper.h b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/minidump_writer/linux_dumper.h
index 7bee160f1a..bd3ba036e9 100644
--- a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/minidump_writer/linux_dumper.h
+++ b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/minidump_writer/linux_dumper.h
@@ -62,7 +62,7 @@ namespace google_breakpad {
 #if defined(__i386) || defined(__ARM_EABI__) || \
  (defined(__mips__) && _MIPS_SIM == _ABIO32)
 typedef Elf32_auxv_t elf_aux_entry;
-#elif defined(__x86_64) || defined(__aarch64__) || \
+#elif defined(__x86_64) || defined(__aarch64__) || defined(__sw_64__) || \
      (defined(__mips__) && _MIPS_SIM != _ABIO32)
 typedef Elf64_auxv_t elf_aux_entry;
 #endif
diff --git a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/minidump_writer/linux_ptrace_dumper.cc b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/minidump_writer/linux_ptrace_dumper.cc
index e3ddb81a65..3da66154c3 100644
--- a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/minidump_writer/linux_ptrace_dumper.cc
+++ b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/minidump_writer/linux_ptrace_dumper.cc
@@ -298,6 +298,9 @@ bool LinuxPtraceDumper::GetThreadInfoByIndex(size_t index, ThreadInfo* info) {
 #elif defined(__mips__)
   stack_pointer =
       reinterpret_cast<uint8_t*>(info->mcontext.gregs[MD_CONTEXT_MIPS_REG_SP]);
+#elif defined(__sw_64__)
+  stack_pointer =
+      reinterpret_cast<uint8_t*>(info->mcontext.sc_regs[MD_CONTEXT_SW64_REG_SP]);
 #else
 #error "This code hasn't been ported to your platform yet."
 #endif
diff --git a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/minidump_writer/minidump_writer.cc b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/minidump_writer/minidump_writer.cc
index 32634ef002..633d6096d8 100644
--- a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/minidump_writer/minidump_writer.cc
+++ b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/minidump_writer/minidump_writer.cc
@@ -136,7 +136,7 @@ class MinidumpWriter {
       : fd_(minidump_fd),
         path_(minidump_path),
         ucontext_(context ? &context->context : NULL),
-#if !defined(__ARM_EABI__) && !defined(__mips__)
+#if !defined(__ARM_EABI__) && !defined(__mips__) && !defined(__sw_64__) 
         float_state_(context ? &context->float_state : NULL),
 #endif
         dumper_(dumper),
@@ -468,7 +468,7 @@ class MinidumpWriter {
         if (!cpu.Allocate())
           return false;
         my_memset(cpu.get(), 0, sizeof(RawContextCPU));
-#if !defined(__ARM_EABI__) && !defined(__mips__)
+#if !defined(__ARM_EABI__) && !defined(__mips__) && !defined(__sw_64__)
         UContextReader::FillCPUContext(cpu.get(), ucontext_, float_state_);
 #else
         UContextReader::FillCPUContext(cpu.get(), ucontext_);
@@ -897,7 +897,7 @@ class MinidumpWriter {
     dirent->location.rva = 0;
   }
 
-#if defined(__i386__) || defined(__x86_64__) || defined(__mips__)
+#if defined(__i386__) || defined(__x86_64__) || defined(__mips__) || defined(__sw_64__)
   bool WriteCPUInformation(MDRawSystemInfo* sys_info) {
     char vendor_id[sizeof(sys_info->cpu.x86_cpu_info.vendor_id) + 1] = {0};
     static const char vendor_id_name[] = "vendor_id";
@@ -922,6 +922,8 @@ class MinidumpWriter {
         MD_CPU_ARCHITECTURE_MIPS;
 # elif _MIPS_SIM == _ABI64
         MD_CPU_ARCHITECTURE_MIPS64;
+#elif defined(__sw_64__)
+        MD_CPU_ARCHITECTURE_SW64;
 # else
 #  error "This mips ABI is currently not supported (n32)"
 #endif
@@ -1333,7 +1335,7 @@ class MinidumpWriter {
   const char* path_;  // Path to the file where the minidum should be written.
 
   const ucontext_t* const ucontext_;  // also from the signal handler
-#if !defined(__ARM_EABI__) && !defined(__mips__)
+#if !defined(__ARM_EABI__) && !defined(__mips__) && !defined(__sw_64__)
   const google_breakpad::fpstate_t* const float_state_;  // ditto
 #endif
   LinuxDumper* dumper_;
diff --git a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/minidump_writer/minidump_writer.h b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/minidump_writer/minidump_writer.h
index e3b0b16dae..8df697c161 100644
--- a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/minidump_writer/minidump_writer.h
+++ b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/client/linux/minidump_writer/minidump_writer.h
@@ -48,7 +48,7 @@ class ExceptionHandler;
 
 #if defined(__aarch64__)
 typedef struct fpsimd_context fpstate_t;
-#elif !defined(__ARM_EABI__) && !defined(__mips__)
+#elif !defined(__ARM_EABI__) && !defined(__mips__) && !defined(__sw_64__)
 typedef std::remove_pointer<fpregset_t>::type fpstate_t;
 #endif
 
diff --git a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/common/linux/breakpad_getcontext.S b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/common/linux/breakpad_getcontext.S
index b7fa855802..a2a1314b0a 100644
--- a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/common/linux/breakpad_getcontext.S
+++ b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/common/linux/breakpad_getcontext.S
@@ -480,7 +480,7 @@ breakpad_getcontext:
   ret
   .cfi_endproc
   .size breakpad_getcontext, . - breakpad_getcontext
-
+#elif defined(__sw_64__)
 #else
 #error "This file has not been ported for your CPU!"
 #endif
diff --git a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/common/linux/breakpad_getcontext.h b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/common/linux/breakpad_getcontext.h
index 1418cde621..d98d945056 100644
--- a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/common/linux/breakpad_getcontext.h
+++ b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/common/linux/breakpad_getcontext.h
@@ -42,6 +42,10 @@
 extern "C" {
 #endif  // __cplusplus
 
+#if defined(__sw_64__)
+#include <ucontext.h>
+#endif
+
 // Provided by src/common/linux/breakpad_getcontext.S
 int breakpad_getcontext(ucontext_t* ucp);
 
diff --git a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/common/linux/memory_mapped_file.cc b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/common/linux/memory_mapped_file.cc
index 99362945ca..6766c5a22a 100644
--- a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/common/linux/memory_mapped_file.cc
+++ b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/common/linux/memory_mapped_file.cc
@@ -65,7 +65,7 @@ bool MemoryMappedFile::Map(const char* path, size_t offset) {
   }
 
 #if defined(__x86_64__) || defined(__aarch64__) || \
-   (defined(__mips__) && _MIPS_SIM == _ABI64)
+   (defined(__mips__) && _MIPS_SIM == _ABI64) || defined(__sw_64__)
 
   struct kernel_stat st;
   if (sys_fstat(fd, &st) == -1 || st.st_size < 0) {
diff --git a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/common/linux/ucontext_constants.h b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/common/linux/ucontext_constants.h
index c390508a1a..d08e2de8cc 100644
--- a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/common/linux/ucontext_constants.h
+++ b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/common/linux/ucontext_constants.h
@@ -145,6 +145,7 @@
 #define MCONTEXT_FPREGS_MEM  424
 #endif
 #define FPREGS_OFFSET_MXCSR  24
+#elif defined(__sw_64__)
 
 #else
 #error "This header has not been ported for your CPU"
diff --git a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/common/memory_allocator.h b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/common/memory_allocator.h
index 69055a1582..227e9d3e56 100644
--- a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/common/memory_allocator.h
+++ b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/common/memory_allocator.h
@@ -50,6 +50,12 @@
 #include "third_party/lss/linux_syscall_support.h"
 #endif
 
+#if defined(__sw_64__)
+#define sys_mmap mmap
+#define sys_munmap munmap
+#endif
+
+
 namespace google_breakpad {
 
 // This is very simple allocator which fetches pages from the kernel directly.
diff --git a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/google_breakpad/common/minidump_cpu_sw64.h b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/google_breakpad/common/minidump_cpu_sw64.h
new file mode 100644
index 0000000000..c0f9d181b9
--- /dev/null
+++ b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/google_breakpad/common/minidump_cpu_sw64.h
@@ -0,0 +1,165 @@
+/* Copyright (c) 2013, Google Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ * notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above
+ * copyright notice, this list of conditions and the following disclaimer
+ * in the documentation and/or other materials provided with the
+ * distribution.
+ *     * Neither the name of Google Inc. nor the names of its
+ * contributors may be used to endorse or promote products derived from
+ * this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. */
+
+/* minidump_format.h: A cross-platform reimplementation of minidump-related
+ * portions of DbgHelp.h from the Windows Platform SDK.
+ *
+ * (This is C99 source, please don't corrupt it with C++.)
+ *
+ * This file contains the necessary definitions to read minidump files
+ * produced on SW64.  These files may be read on any platform provided
+ * that the alignments of these structures on the processing system are
+ * identical to the alignments of these structures on the producing system.
+ * For this reason, precise-sized types are used.  The structures defined
+ * by this file have been laid out to minimize alignment problems by
+ * ensuring that all members are aligned on their natural boundaries.
+ * In some cases, tail-padding may be significant when different ABIs specify
+ * different tail-padding behaviors.  To avoid problems when reading or
+ * writing affected structures, MD_*_SIZE macros are provided where needed,
+ * containing the useful size of the structures without padding.
+ *
+ * Structures that are defined by Microsoft to contain a zero-length array
+ * are instead defined here to contain an array with one element, as
+ * zero-length arrays are forbidden by standard C and C++.  In these cases,
+ * *_minsize constants are provided to be used in place of sizeof.  For a
+ * cleaner interface to these sizes when using C++, see minidump_size.h.
+ *
+ * These structures are also sufficient to populate minidump files.
+ *
+ * Because precise data type sizes are crucial for this implementation to
+ * function properly and portably, a set of primitive types with known sizes
+ * are used as the basis of each structure defined by this file.
+ *
+ * Author: Chris Dearman
+ */
+
+/*
+ * SW64 support
+ */
+
+#ifndef GOOGLE_BREAKPAD_COMMON_MINIDUMP_CPU_SW64_H__
+#define GOOGLE_BREAKPAD_COMMON_MINIDUMP_CPU_SW64_H__
+
+#define MD_CONTEXT_SW64_GPR_COUNT 32
+#define MD_FLOATINGSAVEAREA_SW64_FPR_COUNT 32
+#define MD_CONTEXT_SW64_DSP_COUNT 0
+
+/*
+ * Note that these structures *do not* map directly to the CONTEXT
+ * structure defined in WinNT.h in the Windows Mobile SDK. That structure
+ * does not accomodate VFPv3, and I'm unsure if it was ever used in the
+ * wild anyway, as Windows CE only seems to produce "cedumps" which
+ * are not exactly minidumps.
+ */
+typedef struct {
+  /* 32 64-bit floating point registers, f0..f31 */
+  uint64_t regs[MD_FLOATINGSAVEAREA_SW64_FPR_COUNT];
+
+  uint32_t fpcsr; /* FPU status register. */
+  uint32_t fir; /* FPU implementation register. */
+} MDFloatingSaveAreaSW64;
+
+typedef struct {
+  /* The next field determines the layout of the structure, and which parts
+   * of it are populated.
+   */
+  uint32_t context_flags;
+  uint32_t _pad0;
+
+  /* 32 64-bit integer registers, r0..r31.
+   * Note the following fixed uses:
+   *   r29 is the stack pointer.
+   *   r31 is the return address.
+   */
+  uint64_t iregs[MD_CONTEXT_SW64_GPR_COUNT];
+  uint64_t pc;
+
+
+  /* The next field is included with MD_CONTEXT_SW64_FLOATING_POINT. */
+  MDFloatingSaveAreaSW64 float_save;
+
+} MDRawContextSW64;
+
+/* Indices into iregs for registers with a dedicated or conventional
+ * purpose.
+ */
+enum MDSW64RegisterNumbers {
+  MD_CONTEXT_SW64_REG_S0     = 9,
+  MD_CONTEXT_SW64_REG_S1     = 10,
+  MD_CONTEXT_SW64_REG_S2     = 11,
+  MD_CONTEXT_SW64_REG_S3     = 12,
+  MD_CONTEXT_SW64_REG_S4     = 13,
+  MD_CONTEXT_SW64_REG_S5     = 14,
+  MD_CONTEXT_SW64_REG_S6     = 15,
+  MD_CONTEXT_SW64_REG_S7     = 27, //t12
+  MD_CONTEXT_SW64_REG_GP     = 29,
+  MD_CONTEXT_SW64_REG_SP     = 30,
+  MD_CONTEXT_SW64_REG_FP     = 15,
+  MD_CONTEXT_SW64_REG_RA     = 26,
+};
+
+/* For (MDRawContextSW64).context_flags.  These values indicate the type of
+ * context stored in the structure. */
+/* CONTEXT_SW64 from the Windows CE 5.0 SDK. This value isn't correct
+ * because this bit can be used for flags. Presumably this value was
+ * never actually used in minidumps, but only in "CEDumps" which
+ * are a whole parallel minidump file format for Windows CE.
+ * Therefore, Breakpad defines its own value for SW64 CPUs.
+ */
+#define MD_CONTEXT_SW64  0x00040000
+#define MD_CONTEXT_SW64_INTEGER           (MD_CONTEXT_SW64 | 0x00000002)
+#define MD_CONTEXT_SW64_FLOATING_POINT    (MD_CONTEXT_SW64 | 0x00000004)
+#define MD_CONTEXT_SW64_DSP               (MD_CONTEXT_SW64 | 0x00000008)
+
+#define MD_CONTEXT_SW64_FULL              (MD_CONTEXT_SW64_INTEGER | \
+                                           MD_CONTEXT_SW64_FLOATING_POINT | \
+                                           MD_CONTEXT_SW64_DSP)
+
+#define MD_CONTEXT_SW64_ALL               (MD_CONTEXT_SW64_INTEGER | \
+                                           MD_CONTEXT_SW64_FLOATING_POINT \
+                                           MD_CONTEXT_SW64_DSP)
+
+/**
+ * Breakpad defines for SW6464
+ */
+#define MD_CONTEXT_SW6464  0x00080000
+#define MD_CONTEXT_SW6464_INTEGER           (MD_CONTEXT_SW6464 | 0x00000002)
+#define MD_CONTEXT_SW6464_FLOATING_POINT    (MD_CONTEXT_SW6464 | 0x00000004)
+#define MD_CONTEXT_SW6464_DSP               (MD_CONTEXT_SW6464 | 0x00000008)
+
+#define MD_CONTEXT_SW6464_FULL              (MD_CONTEXT_SW6464_INTEGER | \
+                                             MD_CONTEXT_SW6464_FLOATING_POINT | \
+                                             MD_CONTEXT_SW6464_DSP)
+
+#define MD_CONTEXT_SW6464_ALL               (MD_CONTEXT_SW6464_INTEGER | \
+                                             MD_CONTEXT_SW6464_FLOATING_POINT \
+                                             MD_CONTEXT_SW6464_DSP)
+
+#endif  // GOOGLE_BREAKPAD_COMMON_MINIDUMP_CPU_SW64_H__
+
diff --git a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/google_breakpad/common/minidump_format.h b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/google_breakpad/common/minidump_format.h
index 7b36d1127d..77039b1a57 100644
--- a/src/3rdparty/chromium/third_party/breakpad/breakpad/src/google_breakpad/common/minidump_format.h
+++ b/src/3rdparty/chromium/third_party/breakpad/breakpad/src/google_breakpad/common/minidump_format.h
@@ -120,6 +120,7 @@ typedef struct {
 #include "minidump_cpu_ppc64.h"
 #include "minidump_cpu_sparc.h"
 #include "minidump_cpu_x86.h"
+#include "minidump_cpu_sw64.h"
 
 /*
  * WinVer.h
@@ -660,6 +661,7 @@ typedef enum {
   MD_CPU_ARCHITECTURE_PPC64     = 0x8002, /* Breakpad-defined value for PPC64 */
   MD_CPU_ARCHITECTURE_ARM64_OLD = 0x8003, /* Breakpad-defined value for ARM64 */
   MD_CPU_ARCHITECTURE_MIPS64    = 0x8004, /* Breakpad-defined value for MIPS64 */
+  MD_CPU_ARCHITECTURE_SW64      = 0x8005, /* Breakpad-defined value for SW64 */
   MD_CPU_ARCHITECTURE_UNKNOWN   = 0xffff  /* PROCESSOR_ARCHITECTURE_UNKNOWN */
 } MDCPUArchitecture;
 
diff --git a/src/3rdparty/chromium/third_party/crashpad/crashpad/minidump/minidump_misc_info_writer.cc b/src/3rdparty/chromium/third_party/crashpad/crashpad/minidump/minidump_misc_info_writer.cc
index 0974e3ddf5..352a759df0 100644
--- a/src/3rdparty/chromium/third_party/crashpad/crashpad/minidump/minidump_misc_info_writer.cc
+++ b/src/3rdparty/chromium/third_party/crashpad/crashpad/minidump/minidump_misc_info_writer.cc
@@ -135,6 +135,8 @@ std::string MinidumpMiscInfoDebugBuildString() {
   static constexpr char kCPU[] = "mips";
 #elif defined(ARCH_CPU_MIPS64EL)
   static constexpr char kCPU[] = "mips64";
+#elif defined(ARCH_CPU_SW64_FAMILY)
+  static constexpr char kCPU[] = "sw_64";
 #else
 #error define kCPU for this CPU
 #endif
diff --git a/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/capture_memory.cc b/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/capture_memory.cc
index 7a1b2763c0..ebbd2e7f01 100644
--- a/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/capture_memory.cc
+++ b/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/capture_memory.cc
@@ -112,6 +112,10 @@ void CaptureMemory::PointedToByContext(const CPUContext& context,
   for (size_t i = 0; i < base::size(context.mipsel->regs); ++i) {
     MaybeCaptureMemoryAround(delegate, context.mipsel->regs[i]);
   }
+#elif defined(ARCH_CPU_SW64_FAMILY)
+  for (size_t i = 0; i < base::size(context.mipsel->regs); ++i) {
+    MaybeCaptureMemoryAround(delegate, context.mipsel->regs[i]);
+  }
 #else
 #error Port.
 #endif
diff --git a/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/cpu_architecture.h b/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/cpu_architecture.h
index 811a720958..0029e66a3f 100644
--- a/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/cpu_architecture.h
+++ b/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/cpu_architecture.h
@@ -43,7 +43,9 @@ enum CPUArchitecture {
   kCPUArchitectureMIPSEL,
 
   //! \brief 64-bit MIPSEL.
-  kCPUArchitectureMIPS64EL
+  kCPUArchitectureMIPS64EL,
+
+  kCPUArchitectureSW64EL
 };
 
 }  // namespace crashpad
diff --git a/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/cpu_context.h b/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/cpu_context.h
index fb23c4679f..b50fea4a32 100644
--- a/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/cpu_context.h
+++ b/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/cpu_context.h
@@ -352,6 +352,29 @@ struct CPUContextMIPS64 {
   uint64_t fir;
 };
 
+//! \brief A context structure carrying SW64 CPU state.
+struct CPUContextSW64 {
+  uint64_t regs[32];
+  uint64_t mdlo;
+  uint64_t mdhi;
+  uint64_t cp0_epc;
+  uint64_t cp0_badvaddr;
+  uint64_t cp0_status;
+  uint64_t cp0_cause;
+  uint64_t hi[3];
+  uint64_t lo[3];
+  uint64_t dsp_control;
+  union {
+    double dregs[32];
+    struct {
+      float _fp_fregs;
+      uint32_t _fp_pad;
+    } fregs[32];
+  } fpregs;
+  uint64_t fpcsr;
+  uint64_t fir;
+};
+
 //! \brief A context structure capable of carrying the context of any supported
 //!     CPU architecture.
 struct CPUContext {
@@ -382,6 +405,7 @@ struct CPUContext {
     CPUContextARM64* arm64;
     CPUContextMIPS* mipsel;
     CPUContextMIPS64* mips64;
+    CPUContextSW64* sw64;
   };
 };
 
diff --git a/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/cpu_context_linux.h b/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/cpu_context_linux.h
index 9f46a48977..ebbd9405f5 100644
--- a/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/cpu_context_linux.h
+++ b/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/cpu_context_linux.h
@@ -174,6 +174,44 @@ void InitializeCPUContextMIPS(
 
 #endif  // ARCH_CPU_MIPS_FAMILY || DOXYGEN
 
+
+#if defined(ARCH_CPU_SW64_FAMILY) || DOXYGEN
+
+//! \brief Initializes a CPUContextMIPS structure from native context
+//!     structures on Linux.
+//!
+//! This function has template specializations for MIPSEL and MIPS64EL
+//! architecture contexts, using ContextTraits32 or ContextTraits64 as template
+//! parameter, respectively.
+//!
+//! \param[in] thread_context The native thread context.
+//! \param[in] float_context The native float context.
+//! \param[out] context The CPUContextMIPS structure to initialize.
+template <typename Traits>
+void InitializeCPUContextSW64(
+    const typename Traits::SignalThreadContext& thread_context,
+    const typename Traits::SignalFloatContext& float_context,
+    typename Traits::CPUContext* context) {
+  static_assert(sizeof(context->regs) == sizeof(thread_context.regs),
+                "registers size mismatch");
+  static_assert(sizeof(context->fpregs) == sizeof(float_context.fpregs),
+                "fp registers size mismatch");
+  memcpy(&context->regs, &thread_context.regs, sizeof(context->regs));
+  context->mdlo = thread_context.lo;
+  context->mdhi = thread_context.hi;
+  context->cp0_epc = thread_context.cp0_epc;
+  context->cp0_badvaddr = thread_context.cp0_badvaddr;
+  context->cp0_status = thread_context.cp0_status;
+  context->cp0_cause = thread_context.cp0_cause;
+
+  memcpy(&context->fpregs, &float_context.fpregs, sizeof(context->fpregs));
+  context->fpcsr = float_context.fpcsr;
+  context->fir = float_context.fpu_id;
+};
+
+#endif  // ARCH_CPU_MIPS_FAMILY || DOXYGEN
+
+
 }  // namespace internal
 }  // namespace crashpad
 
diff --git a/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/exception_snapshot_linux.cc b/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/exception_snapshot_linux.cc
index cd40b3b12d..f3fe429e6b 100644
--- a/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/exception_snapshot_linux.cc
+++ b/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/exception_snapshot_linux.cc
@@ -268,6 +268,58 @@ bool ExceptionSnapshotLinux::ReadContext<ContextTraits64>(
   } while (true);
 }
 
+#elif defined(ARCH_CPU_SW64_FAMILY)
+
+template <typename Traits>
+static bool ReadContext(ProcessReaderLinux* reader,
+                        LinuxVMAddress context_address,
+                        typename Traits::CPUContext* dest_context) {
+  const ProcessMemory* memory = reader->Memory();
+
+  LinuxVMAddress gregs_address = context_address +
+                                 offsetof(UContext<Traits>, mcontext) +
+                                 offsetof(typename Traits::MContext, gregs);
+
+  typename Traits::SignalThreadContext thread_context;
+  if (!memory->Read(gregs_address, sizeof(thread_context), &thread_context)) {
+    LOG(ERROR) << "Couldn't read gregs";
+    return false;
+  }
+
+  LinuxVMAddress fpregs_address = context_address +
+                                  offsetof(UContext<Traits>, mcontext) +
+                                  offsetof(typename Traits::MContext, fpregs);
+
+  typename Traits::SignalFloatContext fp_context;
+  if (!memory->Read(fpregs_address, sizeof(fp_context), &fp_context)) {
+    LOG(ERROR) << "Couldn't read fpregs";
+    return false;
+  }
+
+  InitializeCPUContextSW64<Traits>(thread_context, fp_context, dest_context);
+
+  return true;
+}
+
+template <>
+bool ExceptionSnapshotLinux::ReadContext<ContextTraits32>(
+    ProcessReaderLinux* reader,
+    LinuxVMAddress context_address) {
+
+    return false;
+}
+
+template <>
+bool ExceptionSnapshotLinux::ReadContext<ContextTraits64>(
+    ProcessReaderLinux* reader,
+    LinuxVMAddress context_address) {
+  context_.architecture = kCPUArchitectureSW64EL;
+  context_.sw64 = &context_union_.sw64;
+
+  return internal::ReadContext<ContextTraits64>(
+      reader, context_address, context_.mips64);
+}
+
 #elif defined(ARCH_CPU_MIPS_FAMILY)
 
 template <typename Traits>
diff --git a/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/exception_snapshot_linux.h b/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/exception_snapshot_linux.h
index ea0cd21066..74d30a983c 100644
--- a/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/exception_snapshot_linux.h
+++ b/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/exception_snapshot_linux.h
@@ -84,6 +84,8 @@ class ExceptionSnapshotLinux final : public ExceptionSnapshot {
 #elif defined(ARCH_CPU_MIPS_FAMILY)
     CPUContextMIPS mipsel;
     CPUContextMIPS64 mips64;
+#elif defined(ARCH_CPU_SW64_FAMILY)
+    CPUContextSW64 sw64;
 #endif
   } context_union_;
   CPUContext context_;
diff --git a/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/process_reader_linux.cc b/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/process_reader_linux.cc
index ee246e8bcb..c50e81794e 100644
--- a/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/process_reader_linux.cc
+++ b/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/process_reader_linux.cc
@@ -108,6 +108,8 @@ void ProcessReaderLinux::Thread::InitializeStack(ProcessReaderLinux* reader) {
 #elif defined(ARCH_CPU_MIPS_FAMILY)
   stack_pointer = reader->Is64Bit() ? thread_info.thread_context.t64.regs[29]
                                     : thread_info.thread_context.t32.regs[29];
+#elif defined(ARCH_CPU_SW64_FAMILY)
+  stack_pointer = thread_info.thread_context.t64.regs[29];
 #else
 #error Port.
 #endif
diff --git a/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/signal_context.h b/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/signal_context.h
index 110024680b..c9ea3e4209 100644
--- a/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/signal_context.h
+++ b/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/signal_context.h
@@ -422,6 +422,110 @@ static_assert(offsetof(UContext<ContextTraits64>, mcontext.fpregs) ==
               "context offset mismatch");
 #endif
 
+#elif defined(ARCH_CPU_SW64_FAMILY)
+
+struct MContext32 {
+  uint32_t regmask;
+  uint32_t status;
+  uint64_t pc;
+  uint64_t gregs[32];
+  struct {
+    float _fp_fregs;
+    unsigned int _fp_pad;
+  } fpregs[32];
+  uint32_t fp_owned;
+  uint32_t fpc_csr;
+  uint32_t fpc_eir;
+  uint32_t used_math;
+  uint32_t dsp;
+  uint64_t mdhi;
+  uint64_t mdlo;
+  uint32_t hi1;
+  uint32_t lo1;
+  uint32_t hi2;
+  uint32_t lo2;
+  uint32_t hi3;
+  uint32_t lo3;
+};
+
+struct MContext64 {
+  uint64_t gregs[32];
+  double fpregs[32];
+  uint64_t mdhi;
+  uint64_t hi1;
+  uint64_t hi2;
+  uint64_t hi3;
+  uint64_t mdlo;
+  uint64_t lo1;
+  uint64_t lo2;
+  uint64_t lo3;
+  uint64_t pc;
+  uint32_t fpc_csr;
+  uint32_t used_math;
+  uint32_t dsp;
+  uint32_t __glibc_reserved1;
+};
+
+struct SignalThreadContext32 {
+  uint64_t regs[32];
+  uint32_t lo;
+  uint32_t hi;
+  uint32_t cp0_epc;
+  uint32_t cp0_badvaddr;
+  uint32_t cp0_status;
+  uint32_t cp0_cause;
+
+  SignalThreadContext32() {}
+  explicit SignalThreadContext32(
+      const struct ThreadContext::t32_t& thread_context) {
+    for (size_t reg = 0; reg < 32; ++reg) {
+      regs[reg] = thread_context.regs[reg];
+    }
+    lo = thread_context.lo;
+    hi = thread_context.hi;
+    cp0_epc = thread_context.cp0_epc;
+    cp0_badvaddr = thread_context.cp0_badvaddr;
+    cp0_status = thread_context.cp0_status;
+    cp0_cause = thread_context.cp0_cause;
+  }
+};
+
+struct ContextTraits32 : public Traits32 {
+  using MContext = MContext32;
+  using SignalThreadContext = SignalThreadContext32;
+  using SignalFloatContext = FloatContext::f32_t;
+  using CPUContext = CPUContextMIPS;
+};
+
+struct ContextTraits64 : public Traits64 {
+  using MContext = MContext64;
+  using SignalThreadContext = ThreadContext::t64_t;
+  using SignalFloatContext = FloatContext::f64_t;
+  using CPUContext = CPUContextMIPS64;
+};
+
+template <typename Traits>
+struct UContext {
+  typename Traits::ULong flags;
+  typename Traits::Address link;
+  SignalStack<Traits> stack;
+  typename Traits::ULong_32Only alignment_padding_;
+  typename Traits::MContext mcontext;
+  Sigset<Traits> sigmask;
+};
+
+#if 0//TODO
+static_assert(offsetof(UContext<ContextTraits64>, mcontext) ==
+                  offsetof(ucontext_t, uc_mcontext),
+              "context offset mismtach");
+static_assert(offsetof(UContext<ContextTraits64>, mcontext.gregs) ==
+                  offsetof(ucontext_t, uc_mcontext.gregs),
+              "context offset mismatch");
+static_assert(offsetof(UContext<ContextTraits64>, mcontext.fpregs) ==
+                  offsetof(ucontext_t, uc_mcontext.fpregs),
+              "context offset mismatch");
+#endif
+
 #else
 #error Port.
 #endif  // ARCH_CPU_X86_FAMILY
diff --git a/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/system_snapshot_linux.cc b/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/system_snapshot_linux.cc
index a99da3e4b6..9e7d8b4b80 100644
--- a/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/system_snapshot_linux.cc
+++ b/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/system_snapshot_linux.cc
@@ -204,6 +204,8 @@ CPUArchitecture SystemSnapshotLinux::GetCPUArchitecture() const {
 #elif defined(ARCH_CPU_MIPS_FAMILY)
   return process_reader_->Is64Bit() ? kCPUArchitectureMIPS64EL
                                     : kCPUArchitectureMIPSEL;
+#elif defined(ARCH_CPU_SW64_FAMILY)
+  return kCPUArchitectureSW64EL;
 #else
 #error port to your architecture
 #endif
@@ -219,6 +221,9 @@ uint32_t SystemSnapshotLinux::CPURevision() const {
 #elif defined(ARCH_CPU_MIPS_FAMILY)
   // Not implementable on MIPS
   return 0;
+#elif defined(ARCH_CPU_SW64_FAMILY)
+  // Not implementable on SW64
+  return 0;
 #else
 #error port to your architecture
 #endif
@@ -239,6 +244,9 @@ std::string SystemSnapshotLinux::CPUVendor() const {
 #elif defined(ARCH_CPU_MIPS_FAMILY)
   // Not implementable on MIPS
   return std::string();
+#elif defined(ARCH_CPU_SW64_FAMILY)
+  // Not implementable on SW64
+  return std::string();
 #else
 #error port to your architecture
 #endif
@@ -372,6 +380,9 @@ bool SystemSnapshotLinux::NXEnabled() const {
 #elif defined(ARCH_CPU_MIPS_FAMILY)
   // Not implementable on MIPS
   return false;
+#elif defined(ARCH_CPU_SW64_FAMILY)
+  // Not implementable on SW64
+  return false;
 #else
 #error Port.
 #endif  // ARCH_CPU_X86_FAMILY
diff --git a/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/thread_snapshot_linux.cc b/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/thread_snapshot_linux.cc
index e3e2bebddb..26745f0a7f 100644
--- a/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/thread_snapshot_linux.cc
+++ b/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/thread_snapshot_linux.cc
@@ -186,6 +186,17 @@ bool ThreadSnapshotLinux::Initialize(ProcessReaderLinux* process_reader,
         thread.thread_info.float_context.f32,
         context_.mipsel);
   }
+#elif defined(ARCH_CPU_SW64_FAMILY)
+  if (process_reader->Is64Bit()) {
+    context_.architecture = kCPUArchitectureSW64EL;
+    context_.sw64 = &context_union_.sw64;
+#if 0//TODO
+    InitializeCPUContextSW64<ContextTraits64>(
+        thread.thread_info.thread_context.t64,
+        thread.thread_info.float_context.f64,
+        context_.sw64);
+#endif
+  }
 #else
 #error Port.
 #endif
diff --git a/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/thread_snapshot_linux.h b/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/thread_snapshot_linux.h
index 44cc6f6d97..5a31a2bfa4 100644
--- a/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/thread_snapshot_linux.h
+++ b/src/3rdparty/chromium/third_party/crashpad/crashpad/snapshot/linux/thread_snapshot_linux.h
@@ -68,6 +68,8 @@ class ThreadSnapshotLinux final : public ThreadSnapshot {
 #elif defined(ARCH_CPU_MIPS_FAMILY)
     CPUContextMIPS mipsel;
     CPUContextMIPS64 mips64;
+#elif defined(ARCH_CPU_SW64_FAMILY)
+    CPUContextSW64 sw64;
 #else
 #error Port.
 #endif  // ARCH_CPU_X86_FAMILY
diff --git a/src/3rdparty/chromium/third_party/crashpad/crashpad/util/linux/ptracer.cc b/src/3rdparty/chromium/third_party/crashpad/crashpad/util/linux/ptracer.cc
index 557e0d3635..bd796dfb3d 100644
--- a/src/3rdparty/chromium/third_party/crashpad/crashpad/util/linux/ptracer.cc
+++ b/src/3rdparty/chromium/third_party/crashpad/crashpad/util/linux/ptracer.cc
@@ -398,6 +398,40 @@ bool GetThreadArea64(pid_t tid,
   return true;
 }
 
+#elif defined(ARCH_CPU_SW64)
+// NOT SURE TODO
+bool GetGeneralPurposeRegistersLegacy(pid_t tid,
+                                      ThreadContext* context,
+                                      bool can_log) {
+    return false;
+}
+
+bool GetFloatingPointRegisters64(pid_t tid,
+                                 FloatContext* context,
+                                 bool can_log) {
+    return false;
+}
+
+bool GetFloatingPointRegisters32(pid_t tid,
+                                 FloatContext* context,
+                                 bool can_log) {
+    return false;
+}
+
+bool GetThreadArea32(pid_t tid,
+                     const ThreadContext& context,
+                     LinuxVMAddress* address,
+                     bool can_log) {
+    return false;
+}
+
+bool GetThreadArea64(pid_t tid,
+                     const ThreadContext& context,
+                     LinuxVMAddress* address,
+                     bool can_log) {
+    return false;
+}
+
 #else
 #error Port.
 #endif  // ARCH_CPU_X86_FAMILY
diff --git a/src/3rdparty/chromium/third_party/crashpad/crashpad/util/linux/thread_info.h b/src/3rdparty/chromium/third_party/crashpad/crashpad/util/linux/thread_info.h
index 5b55c24a76..6316f7f776 100644
--- a/src/3rdparty/chromium/third_party/crashpad/crashpad/util/linux/thread_info.h
+++ b/src/3rdparty/chromium/third_party/crashpad/crashpad/util/linux/thread_info.h
@@ -15,6 +15,10 @@
 #ifndef CRASHPAD_UTIL_LINUX_THREAD_INFO_H_
 #define CRASHPAD_UTIL_LINUX_THREAD_INFO_H_
 
+#if defined(__sw_64__)
+#include <glob.h>
+#endif
+
 #include <stdint.h>
 #include <sys/user.h>
 
@@ -67,7 +71,7 @@ union ThreadContext {
     uint32_t pc;
     uint32_t cpsr;
     uint32_t orig_r0;
-#elif defined(ARCH_CPU_MIPS_FAMILY)
+#elif defined(ARCH_CPU_MIPS_FAMILY) || defined(ARCH_CPU_SW64_FAMILY)
     // Reflects output format of static int gpr32_get(), defined in
     // arch/mips/kernel/ptrace.c in kernel source
     uint32_t padding0_[6];
@@ -122,7 +126,7 @@ union ThreadContext {
     uint64_t sp;
     uint64_t pc;
     uint64_t pstate;
-#elif defined(ARCH_CPU_MIPS_FAMILY)
+#elif defined(ARCH_CPU_MIPS_FAMILY) || defined(ARCH_CPU_SW64_FAMILY)
     // Reflects output format of static int gpr64_get(), defined in
     // arch/mips/kernel/ptrace.c in kernel source
     uint64_t regs[32];
@@ -143,6 +147,9 @@ union ThreadContext {
   using NativeThreadContext = user_regs;
 #elif defined(ARCH_CPU_MIPS_FAMILY)
 // No appropriate NativeThreadsContext type available for MIPS
+#elif defined(ARCH_CPU_SW64_FAMILY)
+// NOT SURE just for build
+  using NativeThreadContext = t64_t;
 #else
 #error Port.
 #endif  // ARCH_CPU_X86_FAMILY || ARCH_CPU_ARM64
@@ -209,7 +216,7 @@ union FloatContext {
 
     bool have_fpregs;
     bool have_vfp;
-#elif defined(ARCH_CPU_MIPS_FAMILY)
+#elif defined(ARCH_CPU_MIPS_FAMILY) || defined(ARCH_CPU_SW64_FAMILY)
     // Reflects data format filled by ptrace_getfpregs() in
     // arch/mips/kernel/ptrace.c
     struct {
@@ -246,7 +253,7 @@ union FloatContext {
     uint32_t fpsr;
     uint32_t fpcr;
     uint8_t padding[8];
-#elif defined(ARCH_CPU_MIPS_FAMILY)
+#elif defined(ARCH_CPU_MIPS_FAMILY) || defined(ARCH_CPU_SW64_FAMILY)
     // Reflects data format filled by ptrace_getfpregs() in
     // arch/mips/kernel/ptrace.c
     double fpregs[32];
@@ -280,6 +287,8 @@ union FloatContext {
   static_assert(sizeof(f64) == sizeof(user_fpsimd_struct), "Size mismatch");
 #elif defined(ARCH_CPU_MIPS_FAMILY)
 // No appropriate floating point context native type for available MIPS.
+#elif defined(ARCH_CPU_SW64)
+  using user_fpsimd_struct = f64_t;
 #else
 #error Port.
 #endif  // ARCH_CPU_X86
diff --git a/src/3rdparty/chromium/third_party/ffmpeg/ffmpeg_options.gni b/src/3rdparty/chromium/third_party/ffmpeg/ffmpeg_options.gni
index 0654f3ddaf..b63a8511f0 100644
--- a/src/3rdparty/chromium/third_party/ffmpeg/ffmpeg_options.gni
+++ b/src/3rdparty/chromium/third_party/ffmpeg/ffmpeg_options.gni
@@ -53,6 +53,10 @@ declare_args() {
   use_system_ffmpeg = false
 }
 
+if (current_cpu == "sw_64") {
+  use_system_ffmpeg = true
+}
+
 assert(ffmpeg_branding == "Chromium" ||
        ffmpeg_branding == "Chrome" ||
        ffmpeg_branding == "ChromeOS")
diff --git a/src/3rdparty/chromium/third_party/icu/source/i18n/double-conversion-utils.h b/src/3rdparty/chromium/third_party/icu/source/i18n/double-conversion-utils.h
index 8c6a0e16e0..2d98509d42 100644
--- a/src/3rdparty/chromium/third_party/icu/source/i18n/double-conversion-utils.h
+++ b/src/3rdparty/chromium/third_party/icu/source/i18n/double-conversion-utils.h
@@ -117,7 +117,7 @@ int main(int argc, char** argv) {
 #if defined(_M_X64) || defined(__x86_64__) || \
     defined(__ARMEL__) || defined(__avr32__) || defined(_M_ARM) || defined(_M_ARM64) || \
     defined(__hppa__) || defined(__ia64__) || \
-    defined(__mips__) || \
+    defined(__mips__) || defined(__sw_64__) || \
     defined(__nios2__) || \
     defined(__powerpc__) || defined(__ppc__) || defined(__ppc64__) || \
     defined(_POWER) || defined(_ARCH_PPC) || defined(_ARCH_PPC64) || \
diff --git a/src/3rdparty/chromium/third_party/libvpx/BUILD.gn b/src/3rdparty/chromium/third_party/libvpx/BUILD.gn
index 6512629e99..38632422ad 100644
--- a/src/3rdparty/chromium/third_party/libvpx/BUILD.gn
+++ b/src/3rdparty/chromium/third_party/libvpx/BUILD.gn
@@ -344,6 +344,9 @@ static_library("bundled_libvpx") {
     } else {
       sources = libvpx_srcs_arm64
     }
+  }else if (current_cpu == "sw_64") {
+    sources = libvpx_srcs_generic
+    #libvpx_srcs_generic
   }
 
   configs -= [ "//build/config/compiler:chromium_code" ]
diff --git a/src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vp8_rtcd.h b/src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vp8_rtcd.h
new file mode 100644
index 0000000000..aa475b55fa
--- /dev/null
+++ b/src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vp8_rtcd.h
@@ -0,0 +1,357 @@
+// This file is generated. Do not edit.
+#ifndef VP8_RTCD_H_
+#define VP8_RTCD_H_
+
+#ifdef RTCD_C
+#define RTCD_EXTERN
+#else
+#define RTCD_EXTERN extern
+#endif
+
+/*
+ * VP8
+ */
+
+struct blockd;
+struct macroblockd;
+struct loop_filter_info;
+
+/* Encoder forward decls */
+struct block;
+struct macroblock;
+struct variance_vtable;
+union int_mv;
+struct yv12_buffer_config;
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+void vp8_bilinear_predict16x16_c(unsigned char* src_ptr,
+                                 int src_pixels_per_line,
+                                 int xoffset,
+                                 int yoffset,
+                                 unsigned char* dst_ptr,
+                                 int dst_pitch);
+#define vp8_bilinear_predict16x16 vp8_bilinear_predict16x16_c
+
+void vp8_bilinear_predict4x4_c(unsigned char* src_ptr,
+                               int src_pixels_per_line,
+                               int xoffset,
+                               int yoffset,
+                               unsigned char* dst_ptr,
+                               int dst_pitch);
+#define vp8_bilinear_predict4x4 vp8_bilinear_predict4x4_c
+
+void vp8_bilinear_predict8x4_c(unsigned char* src_ptr,
+                               int src_pixels_per_line,
+                               int xoffset,
+                               int yoffset,
+                               unsigned char* dst_ptr,
+                               int dst_pitch);
+#define vp8_bilinear_predict8x4 vp8_bilinear_predict8x4_c
+
+void vp8_bilinear_predict8x8_c(unsigned char* src_ptr,
+                               int src_pixels_per_line,
+                               int xoffset,
+                               int yoffset,
+                               unsigned char* dst_ptr,
+                               int dst_pitch);
+#define vp8_bilinear_predict8x8 vp8_bilinear_predict8x8_c
+
+void vp8_blend_b_c(unsigned char* y,
+                   unsigned char* u,
+                   unsigned char* v,
+                   int y_1,
+                   int u_1,
+                   int v_1,
+                   int alpha,
+                   int stride);
+#define vp8_blend_b vp8_blend_b_c
+
+void vp8_blend_mb_inner_c(unsigned char* y,
+                          unsigned char* u,
+                          unsigned char* v,
+                          int y_1,
+                          int u_1,
+                          int v_1,
+                          int alpha,
+                          int stride);
+#define vp8_blend_mb_inner vp8_blend_mb_inner_c
+
+void vp8_blend_mb_outer_c(unsigned char* y,
+                          unsigned char* u,
+                          unsigned char* v,
+                          int y_1,
+                          int u_1,
+                          int v_1,
+                          int alpha,
+                          int stride);
+#define vp8_blend_mb_outer vp8_blend_mb_outer_c
+
+int vp8_block_error_c(short* coeff, short* dqcoeff);
+#define vp8_block_error vp8_block_error_c
+
+void vp8_copy32xn_c(const unsigned char* src_ptr,
+                    int src_stride,
+                    unsigned char* dst_ptr,
+                    int dst_stride,
+                    int height);
+#define vp8_copy32xn vp8_copy32xn_c
+
+void vp8_copy_mem16x16_c(unsigned char* src,
+                         int src_stride,
+                         unsigned char* dst,
+                         int dst_stride);
+#define vp8_copy_mem16x16 vp8_copy_mem16x16_c
+
+void vp8_copy_mem8x4_c(unsigned char* src,
+                       int src_stride,
+                       unsigned char* dst,
+                       int dst_stride);
+#define vp8_copy_mem8x4 vp8_copy_mem8x4_c
+
+void vp8_copy_mem8x8_c(unsigned char* src,
+                       int src_stride,
+                       unsigned char* dst,
+                       int dst_stride);
+#define vp8_copy_mem8x8 vp8_copy_mem8x8_c
+
+void vp8_dc_only_idct_add_c(short input_dc,
+                            unsigned char* pred_ptr,
+                            int pred_stride,
+                            unsigned char* dst_ptr,
+                            int dst_stride);
+#define vp8_dc_only_idct_add vp8_dc_only_idct_add_c
+
+int vp8_denoiser_filter_c(unsigned char* mc_running_avg_y,
+                          int mc_avg_y_stride,
+                          unsigned char* running_avg_y,
+                          int avg_y_stride,
+                          unsigned char* sig,
+                          int sig_stride,
+                          unsigned int motion_magnitude,
+                          int increase_denoising);
+#define vp8_denoiser_filter vp8_denoiser_filter_c
+
+int vp8_denoiser_filter_uv_c(unsigned char* mc_running_avg,
+                             int mc_avg_stride,
+                             unsigned char* running_avg,
+                             int avg_stride,
+                             unsigned char* sig,
+                             int sig_stride,
+                             unsigned int motion_magnitude,
+                             int increase_denoising);
+#define vp8_denoiser_filter_uv vp8_denoiser_filter_uv_c
+
+void vp8_dequant_idct_add_c(short* input,
+                            short* dq,
+                            unsigned char* dest,
+                            int stride);
+#define vp8_dequant_idct_add vp8_dequant_idct_add_c
+
+void vp8_dequant_idct_add_uv_block_c(short* q,
+                                     short* dq,
+                                     unsigned char* dst_u,
+                                     unsigned char* dst_v,
+                                     int stride,
+                                     char* eobs);
+#define vp8_dequant_idct_add_uv_block vp8_dequant_idct_add_uv_block_c
+
+void vp8_dequant_idct_add_y_block_c(short* q,
+                                    short* dq,
+                                    unsigned char* dst,
+                                    int stride,
+                                    char* eobs);
+#define vp8_dequant_idct_add_y_block vp8_dequant_idct_add_y_block_c
+
+void vp8_dequantize_b_c(struct blockd*, short* DQC);
+#define vp8_dequantize_b vp8_dequantize_b_c
+
+int vp8_diamond_search_sad_c(struct macroblock* x,
+                             struct block* b,
+                             struct blockd* d,
+                             union int_mv* ref_mv,
+                             union int_mv* best_mv,
+                             int search_param,
+                             int sad_per_bit,
+                             int* num00,
+                             struct variance_vtable* fn_ptr,
+                             int* mvcost[2],
+                             union int_mv* center_mv);
+#define vp8_diamond_search_sad vp8_diamond_search_sad_c
+
+void vp8_fast_quantize_b_c(struct block*, struct blockd*);
+#define vp8_fast_quantize_b vp8_fast_quantize_b_c
+
+void vp8_filter_by_weight16x16_c(unsigned char* src,
+                                 int src_stride,
+                                 unsigned char* dst,
+                                 int dst_stride,
+                                 int src_weight);
+#define vp8_filter_by_weight16x16 vp8_filter_by_weight16x16_c
+
+void vp8_filter_by_weight4x4_c(unsigned char* src,
+                               int src_stride,
+                               unsigned char* dst,
+                               int dst_stride,
+                               int src_weight);
+#define vp8_filter_by_weight4x4 vp8_filter_by_weight4x4_c
+
+void vp8_filter_by_weight8x8_c(unsigned char* src,
+                               int src_stride,
+                               unsigned char* dst,
+                               int dst_stride,
+                               int src_weight);
+#define vp8_filter_by_weight8x8 vp8_filter_by_weight8x8_c
+
+int vp8_full_search_sad_c(struct macroblock* x,
+                          struct block* b,
+                          struct blockd* d,
+                          union int_mv* ref_mv,
+                          int sad_per_bit,
+                          int distance,
+                          struct variance_vtable* fn_ptr,
+                          int* mvcost[2],
+                          union int_mv* center_mv);
+#define vp8_full_search_sad vp8_full_search_sad_c
+
+void vp8_loop_filter_bh_c(unsigned char* y_ptr,
+                          unsigned char* u_ptr,
+                          unsigned char* v_ptr,
+                          int y_stride,
+                          int uv_stride,
+                          struct loop_filter_info* lfi);
+#define vp8_loop_filter_bh vp8_loop_filter_bh_c
+
+void vp8_loop_filter_bv_c(unsigned char* y_ptr,
+                          unsigned char* u_ptr,
+                          unsigned char* v_ptr,
+                          int y_stride,
+                          int uv_stride,
+                          struct loop_filter_info* lfi);
+#define vp8_loop_filter_bv vp8_loop_filter_bv_c
+
+void vp8_loop_filter_mbh_c(unsigned char* y_ptr,
+                           unsigned char* u_ptr,
+                           unsigned char* v_ptr,
+                           int y_stride,
+                           int uv_stride,
+                           struct loop_filter_info* lfi);
+#define vp8_loop_filter_mbh vp8_loop_filter_mbh_c
+
+void vp8_loop_filter_mbv_c(unsigned char* y_ptr,
+                           unsigned char* u_ptr,
+                           unsigned char* v_ptr,
+                           int y_stride,
+                           int uv_stride,
+                           struct loop_filter_info* lfi);
+#define vp8_loop_filter_mbv vp8_loop_filter_mbv_c
+
+void vp8_loop_filter_bhs_c(unsigned char* y_ptr,
+                           int y_stride,
+                           const unsigned char* blimit);
+#define vp8_loop_filter_simple_bh vp8_loop_filter_bhs_c
+
+void vp8_loop_filter_bvs_c(unsigned char* y_ptr,
+                           int y_stride,
+                           const unsigned char* blimit);
+#define vp8_loop_filter_simple_bv vp8_loop_filter_bvs_c
+
+void vp8_loop_filter_simple_horizontal_edge_c(unsigned char* y_ptr,
+                                              int y_stride,
+                                              const unsigned char* blimit);
+#define vp8_loop_filter_simple_mbh vp8_loop_filter_simple_horizontal_edge_c
+
+void vp8_loop_filter_simple_vertical_edge_c(unsigned char* y_ptr,
+                                            int y_stride,
+                                            const unsigned char* blimit);
+#define vp8_loop_filter_simple_mbv vp8_loop_filter_simple_vertical_edge_c
+
+int vp8_mbblock_error_c(struct macroblock* mb, int dc);
+#define vp8_mbblock_error vp8_mbblock_error_c
+
+int vp8_mbuverror_c(struct macroblock* mb);
+#define vp8_mbuverror vp8_mbuverror_c
+
+int vp8_refining_search_sad_c(struct macroblock* x,
+                              struct block* b,
+                              struct blockd* d,
+                              union int_mv* ref_mv,
+                              int error_per_bit,
+                              int search_range,
+                              struct variance_vtable* fn_ptr,
+                              int* mvcost[2],
+                              union int_mv* center_mv);
+#define vp8_refining_search_sad vp8_refining_search_sad_c
+
+void vp8_regular_quantize_b_c(struct block*, struct blockd*);
+#define vp8_regular_quantize_b vp8_regular_quantize_b_c
+
+void vp8_short_fdct4x4_c(short* input, short* output, int pitch);
+#define vp8_short_fdct4x4 vp8_short_fdct4x4_c
+
+void vp8_short_fdct8x4_c(short* input, short* output, int pitch);
+#define vp8_short_fdct8x4 vp8_short_fdct8x4_c
+
+void vp8_short_idct4x4llm_c(short* input,
+                            unsigned char* pred_ptr,
+                            int pred_stride,
+                            unsigned char* dst_ptr,
+                            int dst_stride);
+#define vp8_short_idct4x4llm vp8_short_idct4x4llm_c
+
+void vp8_short_inv_walsh4x4_c(short* input, short* mb_dqcoeff);
+#define vp8_short_inv_walsh4x4 vp8_short_inv_walsh4x4_c
+
+void vp8_short_inv_walsh4x4_1_c(short* input, short* mb_dqcoeff);
+#define vp8_short_inv_walsh4x4_1 vp8_short_inv_walsh4x4_1_c
+
+void vp8_short_walsh4x4_c(short* input, short* output, int pitch);
+#define vp8_short_walsh4x4 vp8_short_walsh4x4_c
+
+void vp8_sixtap_predict16x16_c(unsigned char* src_ptr,
+                               int src_pixels_per_line,
+                               int xoffset,
+                               int yoffset,
+                               unsigned char* dst_ptr,
+                               int dst_pitch);
+#define vp8_sixtap_predict16x16 vp8_sixtap_predict16x16_c
+
+void vp8_sixtap_predict4x4_c(unsigned char* src_ptr,
+                             int src_pixels_per_line,
+                             int xoffset,
+                             int yoffset,
+                             unsigned char* dst_ptr,
+                             int dst_pitch);
+#define vp8_sixtap_predict4x4 vp8_sixtap_predict4x4_c
+
+void vp8_sixtap_predict8x4_c(unsigned char* src_ptr,
+                             int src_pixels_per_line,
+                             int xoffset,
+                             int yoffset,
+                             unsigned char* dst_ptr,
+                             int dst_pitch);
+#define vp8_sixtap_predict8x4 vp8_sixtap_predict8x4_c
+
+void vp8_sixtap_predict8x8_c(unsigned char* src_ptr,
+                             int src_pixels_per_line,
+                             int xoffset,
+                             int yoffset,
+                             unsigned char* dst_ptr,
+                             int dst_pitch);
+#define vp8_sixtap_predict8x8 vp8_sixtap_predict8x8_c
+
+void vp8_rtcd(void);
+
+#include "vpx_config.h"
+
+#ifdef RTCD_C
+static void setup_rtcd_internal(void) {}
+#endif
+
+#ifdef __cplusplus
+}  // extern "C"
+#endif
+
+#endif
diff --git a/src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vp9_rtcd.h b/src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vp9_rtcd.h
new file mode 100644
index 0000000000..0091393148
--- /dev/null
+++ b/src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vp9_rtcd.h
@@ -0,0 +1,275 @@
+// This file is generated. Do not edit.
+#ifndef VP9_RTCD_H_
+#define VP9_RTCD_H_
+
+#ifdef RTCD_C
+#define RTCD_EXTERN
+#else
+#define RTCD_EXTERN extern
+#endif
+
+/*
+ * VP9
+ */
+
+#include "vp9/common/vp9_common.h"
+#include "vp9/common/vp9_enums.h"
+#include "vp9/common/vp9_filter.h"
+#include "vpx/vpx_integer.h"
+
+struct macroblockd;
+
+/* Encoder forward decls */
+struct macroblock;
+struct vp9_variance_vtable;
+struct search_site_config;
+struct mv;
+union int_mv;
+struct yv12_buffer_config;
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+int64_t vp9_block_error_c(const tran_low_t* coeff,
+                          const tran_low_t* dqcoeff,
+                          intptr_t block_size,
+                          int64_t* ssz);
+#define vp9_block_error vp9_block_error_c
+
+int64_t vp9_block_error_fp_c(const tran_low_t* coeff,
+                             const tran_low_t* dqcoeff,
+                             int block_size);
+#define vp9_block_error_fp vp9_block_error_fp_c
+
+int vp9_denoiser_filter_c(const uint8_t* sig,
+                          int sig_stride,
+                          const uint8_t* mc_avg,
+                          int mc_avg_stride,
+                          uint8_t* avg,
+                          int avg_stride,
+                          int increase_denoising,
+                          BLOCK_SIZE bs,
+                          int motion_magnitude);
+#define vp9_denoiser_filter vp9_denoiser_filter_c
+
+int vp9_diamond_search_sad_c(const struct macroblock* x,
+                             const struct search_site_config* cfg,
+                             struct mv* ref_mv,
+                             struct mv* best_mv,
+                             int search_param,
+                             int sad_per_bit,
+                             int* num00,
+                             const struct vp9_variance_vtable* fn_ptr,
+                             const struct mv* center_mv);
+#define vp9_diamond_search_sad vp9_diamond_search_sad_c
+
+void vp9_fht16x16_c(const int16_t* input,
+                    tran_low_t* output,
+                    int stride,
+                    int tx_type);
+#define vp9_fht16x16 vp9_fht16x16_c
+
+void vp9_fht4x4_c(const int16_t* input,
+                  tran_low_t* output,
+                  int stride,
+                  int tx_type);
+#define vp9_fht4x4 vp9_fht4x4_c
+
+void vp9_fht8x8_c(const int16_t* input,
+                  tran_low_t* output,
+                  int stride,
+                  int tx_type);
+#define vp9_fht8x8 vp9_fht8x8_c
+
+void vp9_filter_by_weight16x16_c(const uint8_t* src,
+                                 int src_stride,
+                                 uint8_t* dst,
+                                 int dst_stride,
+                                 int src_weight);
+#define vp9_filter_by_weight16x16 vp9_filter_by_weight16x16_c
+
+void vp9_filter_by_weight8x8_c(const uint8_t* src,
+                               int src_stride,
+                               uint8_t* dst,
+                               int dst_stride,
+                               int src_weight);
+#define vp9_filter_by_weight8x8 vp9_filter_by_weight8x8_c
+
+void vp9_fwht4x4_c(const int16_t* input, tran_low_t* output, int stride);
+#define vp9_fwht4x4 vp9_fwht4x4_c
+
+int64_t vp9_highbd_block_error_c(const tran_low_t* coeff,
+                                 const tran_low_t* dqcoeff,
+                                 intptr_t block_size,
+                                 int64_t* ssz,
+                                 int bd);
+#define vp9_highbd_block_error vp9_highbd_block_error_c
+
+void vp9_highbd_fht16x16_c(const int16_t* input,
+                           tran_low_t* output,
+                           int stride,
+                           int tx_type);
+#define vp9_highbd_fht16x16 vp9_highbd_fht16x16_c
+
+void vp9_highbd_fht4x4_c(const int16_t* input,
+                         tran_low_t* output,
+                         int stride,
+                         int tx_type);
+#define vp9_highbd_fht4x4 vp9_highbd_fht4x4_c
+
+void vp9_highbd_fht8x8_c(const int16_t* input,
+                         tran_low_t* output,
+                         int stride,
+                         int tx_type);
+#define vp9_highbd_fht8x8 vp9_highbd_fht8x8_c
+
+void vp9_highbd_fwht4x4_c(const int16_t* input, tran_low_t* output, int stride);
+#define vp9_highbd_fwht4x4 vp9_highbd_fwht4x4_c
+
+void vp9_highbd_iht16x16_256_add_c(const tran_low_t* input,
+                                   uint16_t* dest,
+                                   int stride,
+                                   int tx_type,
+                                   int bd);
+#define vp9_highbd_iht16x16_256_add vp9_highbd_iht16x16_256_add_c
+
+void vp9_highbd_iht4x4_16_add_c(const tran_low_t* input,
+                                uint16_t* dest,
+                                int stride,
+                                int tx_type,
+                                int bd);
+#define vp9_highbd_iht4x4_16_add vp9_highbd_iht4x4_16_add_c
+
+void vp9_highbd_iht8x8_64_add_c(const tran_low_t* input,
+                                uint16_t* dest,
+                                int stride,
+                                int tx_type,
+                                int bd);
+#define vp9_highbd_iht8x8_64_add vp9_highbd_iht8x8_64_add_c
+
+void vp9_highbd_mbpost_proc_across_ip_c(uint16_t* src,
+                                        int pitch,
+                                        int rows,
+                                        int cols,
+                                        int flimit);
+#define vp9_highbd_mbpost_proc_across_ip vp9_highbd_mbpost_proc_across_ip_c
+
+void vp9_highbd_mbpost_proc_down_c(uint16_t* dst,
+                                   int pitch,
+                                   int rows,
+                                   int cols,
+                                   int flimit);
+#define vp9_highbd_mbpost_proc_down vp9_highbd_mbpost_proc_down_c
+
+void vp9_highbd_post_proc_down_and_across_c(const uint16_t* src_ptr,
+                                            uint16_t* dst_ptr,
+                                            int src_pixels_per_line,
+                                            int dst_pixels_per_line,
+                                            int rows,
+                                            int cols,
+                                            int flimit);
+#define vp9_highbd_post_proc_down_and_across \
+  vp9_highbd_post_proc_down_and_across_c
+
+void vp9_highbd_quantize_fp_c(const tran_low_t* coeff_ptr,
+                              intptr_t n_coeffs,
+                              int skip_block,
+                              const int16_t* round_ptr,
+                              const int16_t* quant_ptr,
+                              tran_low_t* qcoeff_ptr,
+                              tran_low_t* dqcoeff_ptr,
+                              const int16_t* dequant_ptr,
+                              uint16_t* eob_ptr,
+                              const int16_t* scan,
+                              const int16_t* iscan);
+#define vp9_highbd_quantize_fp vp9_highbd_quantize_fp_c
+
+void vp9_highbd_quantize_fp_32x32_c(const tran_low_t* coeff_ptr,
+                                    intptr_t n_coeffs,
+                                    int skip_block,
+                                    const int16_t* round_ptr,
+                                    const int16_t* quant_ptr,
+                                    tran_low_t* qcoeff_ptr,
+                                    tran_low_t* dqcoeff_ptr,
+                                    const int16_t* dequant_ptr,
+                                    uint16_t* eob_ptr,
+                                    const int16_t* scan,
+                                    const int16_t* iscan);
+#define vp9_highbd_quantize_fp_32x32 vp9_highbd_quantize_fp_32x32_c
+
+void vp9_highbd_temporal_filter_apply_c(const uint8_t* frame1,
+                                        unsigned int stride,
+                                        const uint8_t* frame2,
+                                        unsigned int block_width,
+                                        unsigned int block_height,
+                                        int strength,
+                                        int* blk_fw,
+                                        int use_32x32,
+                                        uint32_t* accumulator,
+                                        uint16_t* count);
+#define vp9_highbd_temporal_filter_apply vp9_highbd_temporal_filter_apply_c
+
+void vp9_iht16x16_256_add_c(const tran_low_t* input,
+                            uint8_t* dest,
+                            int stride,
+                            int tx_type);
+#define vp9_iht16x16_256_add vp9_iht16x16_256_add_c
+
+void vp9_iht4x4_16_add_c(const tran_low_t* input,
+                         uint8_t* dest,
+                         int stride,
+                         int tx_type);
+#define vp9_iht4x4_16_add vp9_iht4x4_16_add_c
+
+void vp9_iht8x8_64_add_c(const tran_low_t* input,
+                         uint8_t* dest,
+                         int stride,
+                         int tx_type);
+#define vp9_iht8x8_64_add vp9_iht8x8_64_add_c
+
+void vp9_quantize_fp_c(const tran_low_t* coeff_ptr,
+                       intptr_t n_coeffs,
+                       int skip_block,
+                       const int16_t* round_ptr,
+                       const int16_t* quant_ptr,
+                       tran_low_t* qcoeff_ptr,
+                       tran_low_t* dqcoeff_ptr,
+                       const int16_t* dequant_ptr,
+                       uint16_t* eob_ptr,
+                       const int16_t* scan,
+                       const int16_t* iscan);
+#define vp9_quantize_fp vp9_quantize_fp_c
+
+void vp9_quantize_fp_32x32_c(const tran_low_t* coeff_ptr,
+                             intptr_t n_coeffs,
+                             int skip_block,
+                             const int16_t* round_ptr,
+                             const int16_t* quant_ptr,
+                             tran_low_t* qcoeff_ptr,
+                             tran_low_t* dqcoeff_ptr,
+                             const int16_t* dequant_ptr,
+                             uint16_t* eob_ptr,
+                             const int16_t* scan,
+                             const int16_t* iscan);
+#define vp9_quantize_fp_32x32 vp9_quantize_fp_32x32_c
+
+void vp9_scale_and_extend_frame_c(const struct yv12_buffer_config* src,
+                                  struct yv12_buffer_config* dst,
+                                  INTERP_FILTER filter_type,
+                                  int phase_scaler);
+#define vp9_scale_and_extend_frame vp9_scale_and_extend_frame_c
+
+void vp9_rtcd(void);
+
+#include "vpx_config.h"
+
+#ifdef RTCD_C
+static void setup_rtcd_internal(void) {}
+#endif
+
+#ifdef __cplusplus
+}  // extern "C"
+#endif
+
+#endif
diff --git a/src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vpx_config.asm b/src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vpx_config.asm
new file mode 100644
index 0000000000..00712e52bb
--- /dev/null
+++ b/src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vpx_config.asm
@@ -0,0 +1,98 @@
+@ This file was created from a .asm file
+@  using the ads2gas.pl script.
+	.syntax unified
+.equ VPX_ARCH_ARM ,  0
+.equ ARCH_ARM ,  0
+.equ VPX_ARCH_MIPS ,  0
+.equ ARCH_MIPS ,  0
+.equ VPX_ARCH_X86 ,  0
+.equ ARCH_X86 ,  0
+.equ VPX_ARCH_X86_64 ,  0
+.equ ARCH_X86_64 ,  0
+.equ VPX_ARCH_PPC ,  0
+.equ ARCH_PPC ,  0
+.equ HAVE_NEON ,  0
+.equ HAVE_NEON_ASM ,  0
+.equ HAVE_MIPS32 ,  0
+.equ HAVE_DSPR2 ,  0
+.equ HAVE_MSA ,  0
+.equ HAVE_MIPS64 ,  0
+.equ HAVE_MMX ,  0
+.equ HAVE_SSE ,  0
+.equ HAVE_SSE2 ,  0
+.equ HAVE_SSE3 ,  0
+.equ HAVE_SSSE3 ,  0
+.equ HAVE_SSE4_1 ,  0
+.equ HAVE_AVX ,  0
+.equ HAVE_AVX2 ,  0
+.equ HAVE_AVX512 ,  0
+.equ HAVE_VSX ,  0
+.equ HAVE_MMI ,  0
+.equ HAVE_VPX_PORTS ,  1
+.equ HAVE_PTHREAD_H ,  1
+.equ HAVE_UNISTD_H ,  0
+.equ CONFIG_DEPENDENCY_TRACKING ,  1
+.equ CONFIG_EXTERNAL_BUILD ,  1
+.equ CONFIG_INSTALL_DOCS ,  0
+.equ CONFIG_INSTALL_BINS ,  1
+.equ CONFIG_INSTALL_LIBS ,  1
+.equ CONFIG_INSTALL_SRCS ,  0
+.equ CONFIG_DEBUG ,  0
+.equ CONFIG_GPROF ,  0
+.equ CONFIG_GCOV ,  0
+.equ CONFIG_RVCT ,  0
+.equ CONFIG_GCC ,  1
+.equ CONFIG_MSVS ,  0
+.equ CONFIG_PIC ,  0
+.equ CONFIG_BIG_ENDIAN ,  0
+.equ CONFIG_CODEC_SRCS ,  0
+.equ CONFIG_DEBUG_LIBS ,  0
+.equ CONFIG_DEQUANT_TOKENS ,  0
+.equ CONFIG_DC_RECON ,  0
+.equ CONFIG_RUNTIME_CPU_DETECT ,  0
+.equ CONFIG_POSTPROC ,  1
+.equ CONFIG_VP9_POSTPROC ,  1
+.equ CONFIG_MULTITHREAD ,  1
+.equ CONFIG_INTERNAL_STATS ,  0
+.equ CONFIG_VP8_ENCODER ,  1
+.equ CONFIG_VP8_DECODER ,  1
+.equ CONFIG_VP9_ENCODER ,  1
+.equ CONFIG_VP9_DECODER ,  1
+.equ CONFIG_VP8 ,  1
+.equ CONFIG_VP9 ,  1
+.equ CONFIG_ENCODERS ,  1
+.equ CONFIG_DECODERS ,  1
+.equ CONFIG_STATIC_MSVCRT ,  0
+.equ CONFIG_SPATIAL_RESAMPLING ,  1
+.equ CONFIG_REALTIME_ONLY ,  1
+.equ CONFIG_ONTHEFLY_BITPACKING ,  0
+.equ CONFIG_ERROR_CONCEALMENT ,  0
+.equ CONFIG_SHARED ,  0
+.equ CONFIG_STATIC ,  1
+.equ CONFIG_SMALL ,  0
+.equ CONFIG_POSTPROC_VISUALIZER ,  0
+.equ CONFIG_OS_SUPPORT ,  1
+.equ CONFIG_UNIT_TESTS ,  1
+.equ CONFIG_WEBM_IO ,  1
+.equ CONFIG_LIBYUV ,  0
+.equ CONFIG_DECODE_PERF_TESTS ,  0
+.equ CONFIG_ENCODE_PERF_TESTS ,  0
+.equ CONFIG_MULTI_RES_ENCODING ,  1
+.equ CONFIG_TEMPORAL_DENOISING ,  1
+.equ CONFIG_VP9_TEMPORAL_DENOISING ,  1
+.equ CONFIG_CONSISTENT_RECODE ,  0
+.equ CONFIG_COEFFICIENT_RANGE_CHECKING ,  0
+.equ CONFIG_VP9_HIGHBITDEPTH ,  1
+.equ CONFIG_BETTER_HW_COMPATIBILITY ,  0
+.equ CONFIG_EXPERIMENTAL ,  0
+.equ CONFIG_SIZE_LIMIT ,  1
+.equ CONFIG_ALWAYS_ADJUST_BPM ,  0
+.equ CONFIG_BITSTREAM_DEBUG ,  0
+.equ CONFIG_MISMATCH_DEBUG ,  0
+.equ CONFIG_FP_MB_STATS ,  0
+.equ CONFIG_EMULATE_HARDWARE ,  0
+.equ CONFIG_NON_GREEDY_MV ,  0
+.equ CONFIG_RATE_CTRL ,  0
+.equ DECODE_WIDTH_LIMIT ,  16384
+.equ DECODE_HEIGHT_LIMIT ,  16384
+	.section	.note.GNU-stack,"",%progbits
diff --git a/src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vpx_config.c b/src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vpx_config.c
new file mode 100644
index 0000000000..8aad25ff17
--- /dev/null
+++ b/src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vpx_config.c
@@ -0,0 +1,10 @@
+/* Copyright (c) 2011 The WebM project authors. All Rights Reserved. */
+/*  */
+/* Use of this source code is governed by a BSD-style license */
+/* that can be found in the LICENSE file in the root of the source */
+/* tree. An additional intellectual property rights grant can be found */
+/* in the file PATENTS.  All contributing project authors may */
+/* be found in the AUTHORS file in the root of the source tree. */
+#include "vpx/vpx_codec.h"
+static const char* const cfg = "--target=generic-gnu --enable-vp9-highbitdepth --enable-external-build --enable-postproc --enable-multi-res-encoding --enable-temporal-denoising --enable-vp9-temporal-denoising --enable-vp9-postproc --size-limit=16384x16384 --enable-realtime-only --disable-install-docs --disable-libyuv";
+const char *vpx_codec_build_config(void) {return cfg;}
diff --git a/src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vpx_config.h b/src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vpx_config.h
new file mode 100644
index 0000000000..fddb76bd2f
--- /dev/null
+++ b/src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vpx_config.h
@@ -0,0 +1,107 @@
+/* Copyright (c) 2011 The WebM project authors. All Rights Reserved. */
+/*  */
+/* Use of this source code is governed by a BSD-style license */
+/* that can be found in the LICENSE file in the root of the source */
+/* tree. An additional intellectual property rights grant can be found */
+/* in the file PATENTS.  All contributing project authors may */
+/* be found in the AUTHORS file in the root of the source tree. */
+/* This file automatically generated by configure. Do not edit! */
+#ifndef VPX_CONFIG_H
+#define VPX_CONFIG_H
+#define RESTRICT    
+#define INLINE      inline
+#define VPX_ARCH_ARM 0
+#define ARCH_ARM 0
+#define VPX_ARCH_MIPS 0
+#define ARCH_MIPS 0
+#define VPX_ARCH_X86 0
+#define ARCH_X86 0
+#define VPX_ARCH_X86_64 0
+#define ARCH_X86_64 0
+#define VPX_ARCH_PPC 0
+#define ARCH_PPC 0
+#define HAVE_NEON 0
+#define HAVE_NEON_ASM 0
+#define HAVE_MIPS32 0
+#define HAVE_DSPR2 0
+#define HAVE_MSA 0
+#define HAVE_MIPS64 0
+#define HAVE_MMX 0
+#define HAVE_SSE 0
+#define HAVE_SSE2 0
+#define HAVE_SSE3 0
+#define HAVE_SSSE3 0
+#define HAVE_SSE4_1 0
+#define HAVE_AVX 0
+#define HAVE_AVX2 0
+#define HAVE_AVX512 0
+#define HAVE_VSX 0
+#define HAVE_MMI 0
+#define HAVE_VPX_PORTS 1
+#define HAVE_PTHREAD_H 1
+#define HAVE_UNISTD_H 0
+#define CONFIG_DEPENDENCY_TRACKING 1
+#define CONFIG_EXTERNAL_BUILD 1
+#define CONFIG_INSTALL_DOCS 0
+#define CONFIG_INSTALL_BINS 1
+#define CONFIG_INSTALL_LIBS 1
+#define CONFIG_INSTALL_SRCS 0
+#define CONFIG_DEBUG 0
+#define CONFIG_GPROF 0
+#define CONFIG_GCOV 0
+#define CONFIG_RVCT 0
+#define CONFIG_GCC 1
+#define CONFIG_MSVS 0
+#define CONFIG_PIC 0
+#define CONFIG_BIG_ENDIAN 0
+#define CONFIG_CODEC_SRCS 0
+#define CONFIG_DEBUG_LIBS 0
+#define CONFIG_DEQUANT_TOKENS 0
+#define CONFIG_DC_RECON 0
+#define CONFIG_RUNTIME_CPU_DETECT 0
+#define CONFIG_POSTPROC 1
+#define CONFIG_VP9_POSTPROC 1
+#define CONFIG_MULTITHREAD 1
+#define CONFIG_INTERNAL_STATS 0
+#define CONFIG_VP8_ENCODER 1
+#define CONFIG_VP8_DECODER 1
+#define CONFIG_VP9_ENCODER 1
+#define CONFIG_VP9_DECODER 1
+#define CONFIG_VP8 1
+#define CONFIG_VP9 1
+#define CONFIG_ENCODERS 1
+#define CONFIG_DECODERS 1
+#define CONFIG_STATIC_MSVCRT 0
+#define CONFIG_SPATIAL_RESAMPLING 1
+#define CONFIG_REALTIME_ONLY 1
+#define CONFIG_ONTHEFLY_BITPACKING 0
+#define CONFIG_ERROR_CONCEALMENT 0
+#define CONFIG_SHARED 0
+#define CONFIG_STATIC 1
+#define CONFIG_SMALL 0
+#define CONFIG_POSTPROC_VISUALIZER 0
+#define CONFIG_OS_SUPPORT 1
+#define CONFIG_UNIT_TESTS 1
+#define CONFIG_WEBM_IO 1
+#define CONFIG_LIBYUV 0
+#define CONFIG_DECODE_PERF_TESTS 0
+#define CONFIG_ENCODE_PERF_TESTS 0
+#define CONFIG_MULTI_RES_ENCODING 1
+#define CONFIG_TEMPORAL_DENOISING 1
+#define CONFIG_VP9_TEMPORAL_DENOISING 1
+#define CONFIG_CONSISTENT_RECODE 0
+#define CONFIG_COEFFICIENT_RANGE_CHECKING 0
+#define CONFIG_VP9_HIGHBITDEPTH 1
+#define CONFIG_BETTER_HW_COMPATIBILITY 0
+#define CONFIG_EXPERIMENTAL 0
+#define CONFIG_SIZE_LIMIT 1
+#define CONFIG_ALWAYS_ADJUST_BPM 0
+#define CONFIG_BITSTREAM_DEBUG 0
+#define CONFIG_MISMATCH_DEBUG 0
+#define CONFIG_FP_MB_STATS 0
+#define CONFIG_EMULATE_HARDWARE 0
+#define CONFIG_NON_GREEDY_MV 0
+#define CONFIG_RATE_CTRL 0
+#define DECODE_WIDTH_LIMIT 16384
+#define DECODE_HEIGHT_LIMIT 16384
+#endif /* VPX_CONFIG_H */
diff --git a/src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vpx_dsp_rtcd.h b/src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vpx_dsp_rtcd.h
new file mode 100644
index 0000000000..8ba4d88055
--- /dev/null
+++ b/src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vpx_dsp_rtcd.h
@@ -0,0 +1,3868 @@
+// This file is generated. Do not edit.
+#ifndef VPX_DSP_RTCD_H_
+#define VPX_DSP_RTCD_H_
+
+#ifdef RTCD_C
+#define RTCD_EXTERN
+#else
+#define RTCD_EXTERN extern
+#endif
+
+/*
+ * DSP
+ */
+
+#include "vpx/vpx_integer.h"
+#include "vpx_dsp/vpx_dsp_common.h"
+#include "vpx_dsp/vpx_filter.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+unsigned int vpx_avg_4x4_c(const uint8_t*, int p);
+#define vpx_avg_4x4 vpx_avg_4x4_c
+
+unsigned int vpx_avg_8x8_c(const uint8_t*, int p);
+#define vpx_avg_8x8 vpx_avg_8x8_c
+
+void vpx_comp_avg_pred_c(uint8_t* comp_pred,
+                         const uint8_t* pred,
+                         int width,
+                         int height,
+                         const uint8_t* ref,
+                         int ref_stride);
+#define vpx_comp_avg_pred vpx_comp_avg_pred_c
+
+void vpx_convolve8_c(const uint8_t* src,
+                     ptrdiff_t src_stride,
+                     uint8_t* dst,
+                     ptrdiff_t dst_stride,
+                     const InterpKernel* filter,
+                     int x0_q4,
+                     int x_step_q4,
+                     int y0_q4,
+                     int y_step_q4,
+                     int w,
+                     int h);
+#define vpx_convolve8 vpx_convolve8_c
+
+void vpx_convolve8_avg_c(const uint8_t* src,
+                         ptrdiff_t src_stride,
+                         uint8_t* dst,
+                         ptrdiff_t dst_stride,
+                         const InterpKernel* filter,
+                         int x0_q4,
+                         int x_step_q4,
+                         int y0_q4,
+                         int y_step_q4,
+                         int w,
+                         int h);
+#define vpx_convolve8_avg vpx_convolve8_avg_c
+
+void vpx_convolve8_avg_horiz_c(const uint8_t* src,
+                               ptrdiff_t src_stride,
+                               uint8_t* dst,
+                               ptrdiff_t dst_stride,
+                               const InterpKernel* filter,
+                               int x0_q4,
+                               int x_step_q4,
+                               int y0_q4,
+                               int y_step_q4,
+                               int w,
+                               int h);
+#define vpx_convolve8_avg_horiz vpx_convolve8_avg_horiz_c
+
+void vpx_convolve8_avg_vert_c(const uint8_t* src,
+                              ptrdiff_t src_stride,
+                              uint8_t* dst,
+                              ptrdiff_t dst_stride,
+                              const InterpKernel* filter,
+                              int x0_q4,
+                              int x_step_q4,
+                              int y0_q4,
+                              int y_step_q4,
+                              int w,
+                              int h);
+#define vpx_convolve8_avg_vert vpx_convolve8_avg_vert_c
+
+void vpx_convolve8_horiz_c(const uint8_t* src,
+                           ptrdiff_t src_stride,
+                           uint8_t* dst,
+                           ptrdiff_t dst_stride,
+                           const InterpKernel* filter,
+                           int x0_q4,
+                           int x_step_q4,
+                           int y0_q4,
+                           int y_step_q4,
+                           int w,
+                           int h);
+#define vpx_convolve8_horiz vpx_convolve8_horiz_c
+
+void vpx_convolve8_vert_c(const uint8_t* src,
+                          ptrdiff_t src_stride,
+                          uint8_t* dst,
+                          ptrdiff_t dst_stride,
+                          const InterpKernel* filter,
+                          int x0_q4,
+                          int x_step_q4,
+                          int y0_q4,
+                          int y_step_q4,
+                          int w,
+                          int h);
+#define vpx_convolve8_vert vpx_convolve8_vert_c
+
+void vpx_convolve_avg_c(const uint8_t* src,
+                        ptrdiff_t src_stride,
+                        uint8_t* dst,
+                        ptrdiff_t dst_stride,
+                        const InterpKernel* filter,
+                        int x0_q4,
+                        int x_step_q4,
+                        int y0_q4,
+                        int y_step_q4,
+                        int w,
+                        int h);
+#define vpx_convolve_avg vpx_convolve_avg_c
+
+void vpx_convolve_copy_c(const uint8_t* src,
+                         ptrdiff_t src_stride,
+                         uint8_t* dst,
+                         ptrdiff_t dst_stride,
+                         const InterpKernel* filter,
+                         int x0_q4,
+                         int x_step_q4,
+                         int y0_q4,
+                         int y_step_q4,
+                         int w,
+                         int h);
+#define vpx_convolve_copy vpx_convolve_copy_c
+
+void vpx_d117_predictor_16x16_c(uint8_t* dst,
+                                ptrdiff_t stride,
+                                const uint8_t* above,
+                                const uint8_t* left);
+#define vpx_d117_predictor_16x16 vpx_d117_predictor_16x16_c
+
+void vpx_d117_predictor_32x32_c(uint8_t* dst,
+                                ptrdiff_t stride,
+                                const uint8_t* above,
+                                const uint8_t* left);
+#define vpx_d117_predictor_32x32 vpx_d117_predictor_32x32_c
+
+void vpx_d117_predictor_4x4_c(uint8_t* dst,
+                              ptrdiff_t stride,
+                              const uint8_t* above,
+                              const uint8_t* left);
+#define vpx_d117_predictor_4x4 vpx_d117_predictor_4x4_c
+
+void vpx_d117_predictor_8x8_c(uint8_t* dst,
+                              ptrdiff_t stride,
+                              const uint8_t* above,
+                              const uint8_t* left);
+#define vpx_d117_predictor_8x8 vpx_d117_predictor_8x8_c
+
+void vpx_d135_predictor_16x16_c(uint8_t* dst,
+                                ptrdiff_t stride,
+                                const uint8_t* above,
+                                const uint8_t* left);
+#define vpx_d135_predictor_16x16 vpx_d135_predictor_16x16_c
+
+void vpx_d135_predictor_32x32_c(uint8_t* dst,
+                                ptrdiff_t stride,
+                                const uint8_t* above,
+                                const uint8_t* left);
+#define vpx_d135_predictor_32x32 vpx_d135_predictor_32x32_c
+
+void vpx_d135_predictor_4x4_c(uint8_t* dst,
+                              ptrdiff_t stride,
+                              const uint8_t* above,
+                              const uint8_t* left);
+#define vpx_d135_predictor_4x4 vpx_d135_predictor_4x4_c
+
+void vpx_d135_predictor_8x8_c(uint8_t* dst,
+                              ptrdiff_t stride,
+                              const uint8_t* above,
+                              const uint8_t* left);
+#define vpx_d135_predictor_8x8 vpx_d135_predictor_8x8_c
+
+void vpx_d153_predictor_16x16_c(uint8_t* dst,
+                                ptrdiff_t stride,
+                                const uint8_t* above,
+                                const uint8_t* left);
+#define vpx_d153_predictor_16x16 vpx_d153_predictor_16x16_c
+
+void vpx_d153_predictor_32x32_c(uint8_t* dst,
+                                ptrdiff_t stride,
+                                const uint8_t* above,
+                                const uint8_t* left);
+#define vpx_d153_predictor_32x32 vpx_d153_predictor_32x32_c
+
+void vpx_d153_predictor_4x4_c(uint8_t* dst,
+                              ptrdiff_t stride,
+                              const uint8_t* above,
+                              const uint8_t* left);
+#define vpx_d153_predictor_4x4 vpx_d153_predictor_4x4_c
+
+void vpx_d153_predictor_8x8_c(uint8_t* dst,
+                              ptrdiff_t stride,
+                              const uint8_t* above,
+                              const uint8_t* left);
+#define vpx_d153_predictor_8x8 vpx_d153_predictor_8x8_c
+
+void vpx_d207_predictor_16x16_c(uint8_t* dst,
+                                ptrdiff_t stride,
+                                const uint8_t* above,
+                                const uint8_t* left);
+#define vpx_d207_predictor_16x16 vpx_d207_predictor_16x16_c
+
+void vpx_d207_predictor_32x32_c(uint8_t* dst,
+                                ptrdiff_t stride,
+                                const uint8_t* above,
+                                const uint8_t* left);
+#define vpx_d207_predictor_32x32 vpx_d207_predictor_32x32_c
+
+void vpx_d207_predictor_4x4_c(uint8_t* dst,
+                              ptrdiff_t stride,
+                              const uint8_t* above,
+                              const uint8_t* left);
+#define vpx_d207_predictor_4x4 vpx_d207_predictor_4x4_c
+
+void vpx_d207_predictor_8x8_c(uint8_t* dst,
+                              ptrdiff_t stride,
+                              const uint8_t* above,
+                              const uint8_t* left);
+#define vpx_d207_predictor_8x8 vpx_d207_predictor_8x8_c
+
+void vpx_d45_predictor_16x16_c(uint8_t* dst,
+                               ptrdiff_t stride,
+                               const uint8_t* above,
+                               const uint8_t* left);
+#define vpx_d45_predictor_16x16 vpx_d45_predictor_16x16_c
+
+void vpx_d45_predictor_32x32_c(uint8_t* dst,
+                               ptrdiff_t stride,
+                               const uint8_t* above,
+                               const uint8_t* left);
+#define vpx_d45_predictor_32x32 vpx_d45_predictor_32x32_c
+
+void vpx_d45_predictor_4x4_c(uint8_t* dst,
+                             ptrdiff_t stride,
+                             const uint8_t* above,
+                             const uint8_t* left);
+#define vpx_d45_predictor_4x4 vpx_d45_predictor_4x4_c
+
+void vpx_d45_predictor_8x8_c(uint8_t* dst,
+                             ptrdiff_t stride,
+                             const uint8_t* above,
+                             const uint8_t* left);
+#define vpx_d45_predictor_8x8 vpx_d45_predictor_8x8_c
+
+void vpx_d45e_predictor_4x4_c(uint8_t* dst,
+                              ptrdiff_t stride,
+                              const uint8_t* above,
+                              const uint8_t* left);
+#define vpx_d45e_predictor_4x4 vpx_d45e_predictor_4x4_c
+
+void vpx_d63_predictor_16x16_c(uint8_t* dst,
+                               ptrdiff_t stride,
+                               const uint8_t* above,
+                               const uint8_t* left);
+#define vpx_d63_predictor_16x16 vpx_d63_predictor_16x16_c
+
+void vpx_d63_predictor_32x32_c(uint8_t* dst,
+                               ptrdiff_t stride,
+                               const uint8_t* above,
+                               const uint8_t* left);
+#define vpx_d63_predictor_32x32 vpx_d63_predictor_32x32_c
+
+void vpx_d63_predictor_4x4_c(uint8_t* dst,
+                             ptrdiff_t stride,
+                             const uint8_t* above,
+                             const uint8_t* left);
+#define vpx_d63_predictor_4x4 vpx_d63_predictor_4x4_c
+
+void vpx_d63_predictor_8x8_c(uint8_t* dst,
+                             ptrdiff_t stride,
+                             const uint8_t* above,
+                             const uint8_t* left);
+#define vpx_d63_predictor_8x8 vpx_d63_predictor_8x8_c
+
+void vpx_d63e_predictor_4x4_c(uint8_t* dst,
+                              ptrdiff_t stride,
+                              const uint8_t* above,
+                              const uint8_t* left);
+#define vpx_d63e_predictor_4x4 vpx_d63e_predictor_4x4_c
+
+void vpx_dc_128_predictor_16x16_c(uint8_t* dst,
+                                  ptrdiff_t stride,
+                                  const uint8_t* above,
+                                  const uint8_t* left);
+#define vpx_dc_128_predictor_16x16 vpx_dc_128_predictor_16x16_c
+
+void vpx_dc_128_predictor_32x32_c(uint8_t* dst,
+                                  ptrdiff_t stride,
+                                  const uint8_t* above,
+                                  const uint8_t* left);
+#define vpx_dc_128_predictor_32x32 vpx_dc_128_predictor_32x32_c
+
+void vpx_dc_128_predictor_4x4_c(uint8_t* dst,
+                                ptrdiff_t stride,
+                                const uint8_t* above,
+                                const uint8_t* left);
+#define vpx_dc_128_predictor_4x4 vpx_dc_128_predictor_4x4_c
+
+void vpx_dc_128_predictor_8x8_c(uint8_t* dst,
+                                ptrdiff_t stride,
+                                const uint8_t* above,
+                                const uint8_t* left);
+#define vpx_dc_128_predictor_8x8 vpx_dc_128_predictor_8x8_c
+
+void vpx_dc_left_predictor_16x16_c(uint8_t* dst,
+                                   ptrdiff_t stride,
+                                   const uint8_t* above,
+                                   const uint8_t* left);
+#define vpx_dc_left_predictor_16x16 vpx_dc_left_predictor_16x16_c
+
+void vpx_dc_left_predictor_32x32_c(uint8_t* dst,
+                                   ptrdiff_t stride,
+                                   const uint8_t* above,
+                                   const uint8_t* left);
+#define vpx_dc_left_predictor_32x32 vpx_dc_left_predictor_32x32_c
+
+void vpx_dc_left_predictor_4x4_c(uint8_t* dst,
+                                 ptrdiff_t stride,
+                                 const uint8_t* above,
+                                 const uint8_t* left);
+#define vpx_dc_left_predictor_4x4 vpx_dc_left_predictor_4x4_c
+
+void vpx_dc_left_predictor_8x8_c(uint8_t* dst,
+                                 ptrdiff_t stride,
+                                 const uint8_t* above,
+                                 const uint8_t* left);
+#define vpx_dc_left_predictor_8x8 vpx_dc_left_predictor_8x8_c
+
+void vpx_dc_predictor_16x16_c(uint8_t* dst,
+                              ptrdiff_t stride,
+                              const uint8_t* above,
+                              const uint8_t* left);
+#define vpx_dc_predictor_16x16 vpx_dc_predictor_16x16_c
+
+void vpx_dc_predictor_32x32_c(uint8_t* dst,
+                              ptrdiff_t stride,
+                              const uint8_t* above,
+                              const uint8_t* left);
+#define vpx_dc_predictor_32x32 vpx_dc_predictor_32x32_c
+
+void vpx_dc_predictor_4x4_c(uint8_t* dst,
+                            ptrdiff_t stride,
+                            const uint8_t* above,
+                            const uint8_t* left);
+#define vpx_dc_predictor_4x4 vpx_dc_predictor_4x4_c
+
+void vpx_dc_predictor_8x8_c(uint8_t* dst,
+                            ptrdiff_t stride,
+                            const uint8_t* above,
+                            const uint8_t* left);
+#define vpx_dc_predictor_8x8 vpx_dc_predictor_8x8_c
+
+void vpx_dc_top_predictor_16x16_c(uint8_t* dst,
+                                  ptrdiff_t stride,
+                                  const uint8_t* above,
+                                  const uint8_t* left);
+#define vpx_dc_top_predictor_16x16 vpx_dc_top_predictor_16x16_c
+
+void vpx_dc_top_predictor_32x32_c(uint8_t* dst,
+                                  ptrdiff_t stride,
+                                  const uint8_t* above,
+                                  const uint8_t* left);
+#define vpx_dc_top_predictor_32x32 vpx_dc_top_predictor_32x32_c
+
+void vpx_dc_top_predictor_4x4_c(uint8_t* dst,
+                                ptrdiff_t stride,
+                                const uint8_t* above,
+                                const uint8_t* left);
+#define vpx_dc_top_predictor_4x4 vpx_dc_top_predictor_4x4_c
+
+void vpx_dc_top_predictor_8x8_c(uint8_t* dst,
+                                ptrdiff_t stride,
+                                const uint8_t* above,
+                                const uint8_t* left);
+#define vpx_dc_top_predictor_8x8 vpx_dc_top_predictor_8x8_c
+
+void vpx_fdct16x16_c(const int16_t* input, tran_low_t* output, int stride);
+#define vpx_fdct16x16 vpx_fdct16x16_c
+
+void vpx_fdct16x16_1_c(const int16_t* input, tran_low_t* output, int stride);
+#define vpx_fdct16x16_1 vpx_fdct16x16_1_c
+
+void vpx_fdct32x32_c(const int16_t* input, tran_low_t* output, int stride);
+#define vpx_fdct32x32 vpx_fdct32x32_c
+
+void vpx_fdct32x32_1_c(const int16_t* input, tran_low_t* output, int stride);
+#define vpx_fdct32x32_1 vpx_fdct32x32_1_c
+
+void vpx_fdct32x32_rd_c(const int16_t* input, tran_low_t* output, int stride);
+#define vpx_fdct32x32_rd vpx_fdct32x32_rd_c
+
+void vpx_fdct4x4_c(const int16_t* input, tran_low_t* output, int stride);
+#define vpx_fdct4x4 vpx_fdct4x4_c
+
+void vpx_fdct4x4_1_c(const int16_t* input, tran_low_t* output, int stride);
+#define vpx_fdct4x4_1 vpx_fdct4x4_1_c
+
+void vpx_fdct8x8_c(const int16_t* input, tran_low_t* output, int stride);
+#define vpx_fdct8x8 vpx_fdct8x8_c
+
+void vpx_fdct8x8_1_c(const int16_t* input, tran_low_t* output, int stride);
+#define vpx_fdct8x8_1 vpx_fdct8x8_1_c
+
+void vpx_get16x16var_c(const uint8_t* src_ptr,
+                       int src_stride,
+                       const uint8_t* ref_ptr,
+                       int ref_stride,
+                       unsigned int* sse,
+                       int* sum);
+#define vpx_get16x16var vpx_get16x16var_c
+
+unsigned int vpx_get4x4sse_cs_c(const unsigned char* src_ptr,
+                                int src_stride,
+                                const unsigned char* ref_ptr,
+                                int ref_stride);
+#define vpx_get4x4sse_cs vpx_get4x4sse_cs_c
+
+void vpx_get8x8var_c(const uint8_t* src_ptr,
+                     int src_stride,
+                     const uint8_t* ref_ptr,
+                     int ref_stride,
+                     unsigned int* sse,
+                     int* sum);
+#define vpx_get8x8var vpx_get8x8var_c
+
+unsigned int vpx_get_mb_ss_c(const int16_t*);
+#define vpx_get_mb_ss vpx_get_mb_ss_c
+
+void vpx_h_predictor_16x16_c(uint8_t* dst,
+                             ptrdiff_t stride,
+                             const uint8_t* above,
+                             const uint8_t* left);
+#define vpx_h_predictor_16x16 vpx_h_predictor_16x16_c
+
+void vpx_h_predictor_32x32_c(uint8_t* dst,
+                             ptrdiff_t stride,
+                             const uint8_t* above,
+                             const uint8_t* left);
+#define vpx_h_predictor_32x32 vpx_h_predictor_32x32_c
+
+void vpx_h_predictor_4x4_c(uint8_t* dst,
+                           ptrdiff_t stride,
+                           const uint8_t* above,
+                           const uint8_t* left);
+#define vpx_h_predictor_4x4 vpx_h_predictor_4x4_c
+
+void vpx_h_predictor_8x8_c(uint8_t* dst,
+                           ptrdiff_t stride,
+                           const uint8_t* above,
+                           const uint8_t* left);
+#define vpx_h_predictor_8x8 vpx_h_predictor_8x8_c
+
+void vpx_hadamard_16x16_c(const int16_t* src_diff,
+                          ptrdiff_t src_stride,
+                          tran_low_t* coeff);
+#define vpx_hadamard_16x16 vpx_hadamard_16x16_c
+
+void vpx_hadamard_32x32_c(const int16_t* src_diff,
+                          ptrdiff_t src_stride,
+                          tran_low_t* coeff);
+#define vpx_hadamard_32x32 vpx_hadamard_32x32_c
+
+void vpx_hadamard_8x8_c(const int16_t* src_diff,
+                        ptrdiff_t src_stride,
+                        tran_low_t* coeff);
+#define vpx_hadamard_8x8 vpx_hadamard_8x8_c
+
+void vpx_he_predictor_4x4_c(uint8_t* dst,
+                            ptrdiff_t stride,
+                            const uint8_t* above,
+                            const uint8_t* left);
+#define vpx_he_predictor_4x4 vpx_he_predictor_4x4_c
+
+void vpx_highbd_10_get16x16var_c(const uint8_t* src_ptr,
+                                 int src_stride,
+                                 const uint8_t* ref_ptr,
+                                 int ref_stride,
+                                 unsigned int* sse,
+                                 int* sum);
+#define vpx_highbd_10_get16x16var vpx_highbd_10_get16x16var_c
+
+void vpx_highbd_10_get8x8var_c(const uint8_t* src_ptr,
+                               int src_stride,
+                               const uint8_t* ref_ptr,
+                               int ref_stride,
+                               unsigned int* sse,
+                               int* sum);
+#define vpx_highbd_10_get8x8var vpx_highbd_10_get8x8var_c
+
+unsigned int vpx_highbd_10_mse16x16_c(const uint8_t* src_ptr,
+                                      int src_stride,
+                                      const uint8_t* ref_ptr,
+                                      int ref_stride,
+                                      unsigned int* sse);
+#define vpx_highbd_10_mse16x16 vpx_highbd_10_mse16x16_c
+
+unsigned int vpx_highbd_10_mse16x8_c(const uint8_t* src_ptr,
+                                     int src_stride,
+                                     const uint8_t* ref_ptr,
+                                     int ref_stride,
+                                     unsigned int* sse);
+#define vpx_highbd_10_mse16x8 vpx_highbd_10_mse16x8_c
+
+unsigned int vpx_highbd_10_mse8x16_c(const uint8_t* src_ptr,
+                                     int src_stride,
+                                     const uint8_t* ref_ptr,
+                                     int ref_stride,
+                                     unsigned int* sse);
+#define vpx_highbd_10_mse8x16 vpx_highbd_10_mse8x16_c
+
+unsigned int vpx_highbd_10_mse8x8_c(const uint8_t* src_ptr,
+                                    int src_stride,
+                                    const uint8_t* ref_ptr,
+                                    int ref_stride,
+                                    unsigned int* sse);
+#define vpx_highbd_10_mse8x8 vpx_highbd_10_mse8x8_c
+
+uint32_t vpx_highbd_10_sub_pixel_avg_variance16x16_c(
+    const uint8_t* src_ptr,
+    int src_stride,
+    int x_offset,
+    int y_offset,
+    const uint8_t* ref_ptr,
+    int ref_stride,
+    uint32_t* sse,
+    const uint8_t* second_pred);
+#define vpx_highbd_10_sub_pixel_avg_variance16x16 \
+  vpx_highbd_10_sub_pixel_avg_variance16x16_c
+
+uint32_t vpx_highbd_10_sub_pixel_avg_variance16x32_c(
+    const uint8_t* src_ptr,
+    int src_stride,
+    int x_offset,
+    int y_offset,
+    const uint8_t* ref_ptr,
+    int ref_stride,
+    uint32_t* sse,
+    const uint8_t* second_pred);
+#define vpx_highbd_10_sub_pixel_avg_variance16x32 \
+  vpx_highbd_10_sub_pixel_avg_variance16x32_c
+
+uint32_t vpx_highbd_10_sub_pixel_avg_variance16x8_c(const uint8_t* src_ptr,
+                                                    int src_stride,
+                                                    int x_offset,
+                                                    int y_offset,
+                                                    const uint8_t* ref_ptr,
+                                                    int ref_stride,
+                                                    uint32_t* sse,
+                                                    const uint8_t* second_pred);
+#define vpx_highbd_10_sub_pixel_avg_variance16x8 \
+  vpx_highbd_10_sub_pixel_avg_variance16x8_c
+
+uint32_t vpx_highbd_10_sub_pixel_avg_variance32x16_c(
+    const uint8_t* src_ptr,
+    int src_stride,
+    int x_offset,
+    int y_offset,
+    const uint8_t* ref_ptr,
+    int ref_stride,
+    uint32_t* sse,
+    const uint8_t* second_pred);
+#define vpx_highbd_10_sub_pixel_avg_variance32x16 \
+  vpx_highbd_10_sub_pixel_avg_variance32x16_c
+
+uint32_t vpx_highbd_10_sub_pixel_avg_variance32x32_c(
+    const uint8_t* src_ptr,
+    int src_stride,
+    int x_offset,
+    int y_offset,
+    const uint8_t* ref_ptr,
+    int ref_stride,
+    uint32_t* sse,
+    const uint8_t* second_pred);
+#define vpx_highbd_10_sub_pixel_avg_variance32x32 \
+  vpx_highbd_10_sub_pixel_avg_variance32x32_c
+
+uint32_t vpx_highbd_10_sub_pixel_avg_variance32x64_c(
+    const uint8_t* src_ptr,
+    int src_stride,
+    int x_offset,
+    int y_offset,
+    const uint8_t* ref_ptr,
+    int ref_stride,
+    uint32_t* sse,
+    const uint8_t* second_pred);
+#define vpx_highbd_10_sub_pixel_avg_variance32x64 \
+  vpx_highbd_10_sub_pixel_avg_variance32x64_c
+
+uint32_t vpx_highbd_10_sub_pixel_avg_variance4x4_c(const uint8_t* src_ptr,
+                                                   int src_stride,
+                                                   int x_offset,
+                                                   int y_offset,
+                                                   const uint8_t* ref_ptr,
+                                                   int ref_stride,
+                                                   uint32_t* sse,
+                                                   const uint8_t* second_pred);
+#define vpx_highbd_10_sub_pixel_avg_variance4x4 \
+  vpx_highbd_10_sub_pixel_avg_variance4x4_c
+
+uint32_t vpx_highbd_10_sub_pixel_avg_variance4x8_c(const uint8_t* src_ptr,
+                                                   int src_stride,
+                                                   int x_offset,
+                                                   int y_offset,
+                                                   const uint8_t* ref_ptr,
+                                                   int ref_stride,
+                                                   uint32_t* sse,
+                                                   const uint8_t* second_pred);
+#define vpx_highbd_10_sub_pixel_avg_variance4x8 \
+  vpx_highbd_10_sub_pixel_avg_variance4x8_c
+
+uint32_t vpx_highbd_10_sub_pixel_avg_variance64x32_c(
+    const uint8_t* src_ptr,
+    int src_stride,
+    int x_offset,
+    int y_offset,
+    const uint8_t* ref_ptr,
+    int ref_stride,
+    uint32_t* sse,
+    const uint8_t* second_pred);
+#define vpx_highbd_10_sub_pixel_avg_variance64x32 \
+  vpx_highbd_10_sub_pixel_avg_variance64x32_c
+
+uint32_t vpx_highbd_10_sub_pixel_avg_variance64x64_c(
+    const uint8_t* src_ptr,
+    int src_stride,
+    int x_offset,
+    int y_offset,
+    const uint8_t* ref_ptr,
+    int ref_stride,
+    uint32_t* sse,
+    const uint8_t* second_pred);
+#define vpx_highbd_10_sub_pixel_avg_variance64x64 \
+  vpx_highbd_10_sub_pixel_avg_variance64x64_c
+
+uint32_t vpx_highbd_10_sub_pixel_avg_variance8x16_c(const uint8_t* src_ptr,
+                                                    int src_stride,
+                                                    int x_offset,
+                                                    int y_offset,
+                                                    const uint8_t* ref_ptr,
+                                                    int ref_stride,
+                                                    uint32_t* sse,
+                                                    const uint8_t* second_pred);
+#define vpx_highbd_10_sub_pixel_avg_variance8x16 \
+  vpx_highbd_10_sub_pixel_avg_variance8x16_c
+
+uint32_t vpx_highbd_10_sub_pixel_avg_variance8x4_c(const uint8_t* src_ptr,
+                                                   int src_stride,
+                                                   int x_offset,
+                                                   int y_offset,
+                                                   const uint8_t* ref_ptr,
+                                                   int ref_stride,
+                                                   uint32_t* sse,
+                                                   const uint8_t* second_pred);
+#define vpx_highbd_10_sub_pixel_avg_variance8x4 \
+  vpx_highbd_10_sub_pixel_avg_variance8x4_c
+
+uint32_t vpx_highbd_10_sub_pixel_avg_variance8x8_c(const uint8_t* src_ptr,
+                                                   int src_stride,
+                                                   int x_offset,
+                                                   int y_offset,
+                                                   const uint8_t* ref_ptr,
+                                                   int ref_stride,
+                                                   uint32_t* sse,
+                                                   const uint8_t* second_pred);
+#define vpx_highbd_10_sub_pixel_avg_variance8x8 \
+  vpx_highbd_10_sub_pixel_avg_variance8x8_c
+
+uint32_t vpx_highbd_10_sub_pixel_variance16x16_c(const uint8_t* src_ptr,
+                                                 int src_stride,
+                                                 int x_offset,
+                                                 int y_offset,
+                                                 const uint8_t* ref_ptr,
+                                                 int ref_stride,
+                                                 uint32_t* sse);
+#define vpx_highbd_10_sub_pixel_variance16x16 \
+  vpx_highbd_10_sub_pixel_variance16x16_c
+
+uint32_t vpx_highbd_10_sub_pixel_variance16x32_c(const uint8_t* src_ptr,
+                                                 int src_stride,
+                                                 int x_offset,
+                                                 int y_offset,
+                                                 const uint8_t* ref_ptr,
+                                                 int ref_stride,
+                                                 uint32_t* sse);
+#define vpx_highbd_10_sub_pixel_variance16x32 \
+  vpx_highbd_10_sub_pixel_variance16x32_c
+
+uint32_t vpx_highbd_10_sub_pixel_variance16x8_c(const uint8_t* src_ptr,
+                                                int src_stride,
+                                                int x_offset,
+                                                int y_offset,
+                                                const uint8_t* ref_ptr,
+                                                int ref_stride,
+                                                uint32_t* sse);
+#define vpx_highbd_10_sub_pixel_variance16x8 \
+  vpx_highbd_10_sub_pixel_variance16x8_c
+
+uint32_t vpx_highbd_10_sub_pixel_variance32x16_c(const uint8_t* src_ptr,
+                                                 int src_stride,
+                                                 int x_offset,
+                                                 int y_offset,
+                                                 const uint8_t* ref_ptr,
+                                                 int ref_stride,
+                                                 uint32_t* sse);
+#define vpx_highbd_10_sub_pixel_variance32x16 \
+  vpx_highbd_10_sub_pixel_variance32x16_c
+
+uint32_t vpx_highbd_10_sub_pixel_variance32x32_c(const uint8_t* src_ptr,
+                                                 int src_stride,
+                                                 int x_offset,
+                                                 int y_offset,
+                                                 const uint8_t* ref_ptr,
+                                                 int ref_stride,
+                                                 uint32_t* sse);
+#define vpx_highbd_10_sub_pixel_variance32x32 \
+  vpx_highbd_10_sub_pixel_variance32x32_c
+
+uint32_t vpx_highbd_10_sub_pixel_variance32x64_c(const uint8_t* src_ptr,
+                                                 int src_stride,
+                                                 int x_offset,
+                                                 int y_offset,
+                                                 const uint8_t* ref_ptr,
+                                                 int ref_stride,
+                                                 uint32_t* sse);
+#define vpx_highbd_10_sub_pixel_variance32x64 \
+  vpx_highbd_10_sub_pixel_variance32x64_c
+
+uint32_t vpx_highbd_10_sub_pixel_variance4x4_c(const uint8_t* src_ptr,
+                                               int src_stride,
+                                               int x_offset,
+                                               int y_offset,
+                                               const uint8_t* ref_ptr,
+                                               int ref_stride,
+                                               uint32_t* sse);
+#define vpx_highbd_10_sub_pixel_variance4x4 \
+  vpx_highbd_10_sub_pixel_variance4x4_c
+
+uint32_t vpx_highbd_10_sub_pixel_variance4x8_c(const uint8_t* src_ptr,
+                                               int src_stride,
+                                               int x_offset,
+                                               int y_offset,
+                                               const uint8_t* ref_ptr,
+                                               int ref_stride,
+                                               uint32_t* sse);
+#define vpx_highbd_10_sub_pixel_variance4x8 \
+  vpx_highbd_10_sub_pixel_variance4x8_c
+
+uint32_t vpx_highbd_10_sub_pixel_variance64x32_c(const uint8_t* src_ptr,
+                                                 int src_stride,
+                                                 int x_offset,
+                                                 int y_offset,
+                                                 const uint8_t* ref_ptr,
+                                                 int ref_stride,
+                                                 uint32_t* sse);
+#define vpx_highbd_10_sub_pixel_variance64x32 \
+  vpx_highbd_10_sub_pixel_variance64x32_c
+
+uint32_t vpx_highbd_10_sub_pixel_variance64x64_c(const uint8_t* src_ptr,
+                                                 int src_stride,
+                                                 int x_offset,
+                                                 int y_offset,
+                                                 const uint8_t* ref_ptr,
+                                                 int ref_stride,
+                                                 uint32_t* sse);
+#define vpx_highbd_10_sub_pixel_variance64x64 \
+  vpx_highbd_10_sub_pixel_variance64x64_c
+
+uint32_t vpx_highbd_10_sub_pixel_variance8x16_c(const uint8_t* src_ptr,
+                                                int src_stride,
+                                                int x_offset,
+                                                int y_offset,
+                                                const uint8_t* ref_ptr,
+                                                int ref_stride,
+                                                uint32_t* sse);
+#define vpx_highbd_10_sub_pixel_variance8x16 \
+  vpx_highbd_10_sub_pixel_variance8x16_c
+
+uint32_t vpx_highbd_10_sub_pixel_variance8x4_c(const uint8_t* src_ptr,
+                                               int src_stride,
+                                               int x_offset,
+                                               int y_offset,
+                                               const uint8_t* ref_ptr,
+                                               int ref_stride,
+                                               uint32_t* sse);
+#define vpx_highbd_10_sub_pixel_variance8x4 \
+  vpx_highbd_10_sub_pixel_variance8x4_c
+
+uint32_t vpx_highbd_10_sub_pixel_variance8x8_c(const uint8_t* src_ptr,
+                                               int src_stride,
+                                               int x_offset,
+                                               int y_offset,
+                                               const uint8_t* ref_ptr,
+                                               int ref_stride,
+                                               uint32_t* sse);
+#define vpx_highbd_10_sub_pixel_variance8x8 \
+  vpx_highbd_10_sub_pixel_variance8x8_c
+
+unsigned int vpx_highbd_10_variance16x16_c(const uint8_t* src_ptr,
+                                           int src_stride,
+                                           const uint8_t* ref_ptr,
+                                           int ref_stride,
+                                           unsigned int* sse);
+#define vpx_highbd_10_variance16x16 vpx_highbd_10_variance16x16_c
+
+unsigned int vpx_highbd_10_variance16x32_c(const uint8_t* src_ptr,
+                                           int src_stride,
+                                           const uint8_t* ref_ptr,
+                                           int ref_stride,
+                                           unsigned int* sse);
+#define vpx_highbd_10_variance16x32 vpx_highbd_10_variance16x32_c
+
+unsigned int vpx_highbd_10_variance16x8_c(const uint8_t* src_ptr,
+                                          int src_stride,
+                                          const uint8_t* ref_ptr,
+                                          int ref_stride,
+                                          unsigned int* sse);
+#define vpx_highbd_10_variance16x8 vpx_highbd_10_variance16x8_c
+
+unsigned int vpx_highbd_10_variance32x16_c(const uint8_t* src_ptr,
+                                           int src_stride,
+                                           const uint8_t* ref_ptr,
+                                           int ref_stride,
+                                           unsigned int* sse);
+#define vpx_highbd_10_variance32x16 vpx_highbd_10_variance32x16_c
+
+unsigned int vpx_highbd_10_variance32x32_c(const uint8_t* src_ptr,
+                                           int src_stride,
+                                           const uint8_t* ref_ptr,
+                                           int ref_stride,
+                                           unsigned int* sse);
+#define vpx_highbd_10_variance32x32 vpx_highbd_10_variance32x32_c
+
+unsigned int vpx_highbd_10_variance32x64_c(const uint8_t* src_ptr,
+                                           int src_stride,
+                                           const uint8_t* ref_ptr,
+                                           int ref_stride,
+                                           unsigned int* sse);
+#define vpx_highbd_10_variance32x64 vpx_highbd_10_variance32x64_c
+
+unsigned int vpx_highbd_10_variance4x4_c(const uint8_t* src_ptr,
+                                         int src_stride,
+                                         const uint8_t* ref_ptr,
+                                         int ref_stride,
+                                         unsigned int* sse);
+#define vpx_highbd_10_variance4x4 vpx_highbd_10_variance4x4_c
+
+unsigned int vpx_highbd_10_variance4x8_c(const uint8_t* src_ptr,
+                                         int src_stride,
+                                         const uint8_t* ref_ptr,
+                                         int ref_stride,
+                                         unsigned int* sse);
+#define vpx_highbd_10_variance4x8 vpx_highbd_10_variance4x8_c
+
+unsigned int vpx_highbd_10_variance64x32_c(const uint8_t* src_ptr,
+                                           int src_stride,
+                                           const uint8_t* ref_ptr,
+                                           int ref_stride,
+                                           unsigned int* sse);
+#define vpx_highbd_10_variance64x32 vpx_highbd_10_variance64x32_c
+
+unsigned int vpx_highbd_10_variance64x64_c(const uint8_t* src_ptr,
+                                           int src_stride,
+                                           const uint8_t* ref_ptr,
+                                           int ref_stride,
+                                           unsigned int* sse);
+#define vpx_highbd_10_variance64x64 vpx_highbd_10_variance64x64_c
+
+unsigned int vpx_highbd_10_variance8x16_c(const uint8_t* src_ptr,
+                                          int src_stride,
+                                          const uint8_t* ref_ptr,
+                                          int ref_stride,
+                                          unsigned int* sse);
+#define vpx_highbd_10_variance8x16 vpx_highbd_10_variance8x16_c
+
+unsigned int vpx_highbd_10_variance8x4_c(const uint8_t* src_ptr,
+                                         int src_stride,
+                                         const uint8_t* ref_ptr,
+                                         int ref_stride,
+                                         unsigned int* sse);
+#define vpx_highbd_10_variance8x4 vpx_highbd_10_variance8x4_c
+
+unsigned int vpx_highbd_10_variance8x8_c(const uint8_t* src_ptr,
+                                         int src_stride,
+                                         const uint8_t* ref_ptr,
+                                         int ref_stride,
+                                         unsigned int* sse);
+#define vpx_highbd_10_variance8x8 vpx_highbd_10_variance8x8_c
+
+void vpx_highbd_12_get16x16var_c(const uint8_t* src_ptr,
+                                 int src_stride,
+                                 const uint8_t* ref_ptr,
+                                 int ref_stride,
+                                 unsigned int* sse,
+                                 int* sum);
+#define vpx_highbd_12_get16x16var vpx_highbd_12_get16x16var_c
+
+void vpx_highbd_12_get8x8var_c(const uint8_t* src_ptr,
+                               int src_stride,
+                               const uint8_t* ref_ptr,
+                               int ref_stride,
+                               unsigned int* sse,
+                               int* sum);
+#define vpx_highbd_12_get8x8var vpx_highbd_12_get8x8var_c
+
+unsigned int vpx_highbd_12_mse16x16_c(const uint8_t* src_ptr,
+                                      int src_stride,
+                                      const uint8_t* ref_ptr,
+                                      int ref_stride,
+                                      unsigned int* sse);
+#define vpx_highbd_12_mse16x16 vpx_highbd_12_mse16x16_c
+
+unsigned int vpx_highbd_12_mse16x8_c(const uint8_t* src_ptr,
+                                     int src_stride,
+                                     const uint8_t* ref_ptr,
+                                     int ref_stride,
+                                     unsigned int* sse);
+#define vpx_highbd_12_mse16x8 vpx_highbd_12_mse16x8_c
+
+unsigned int vpx_highbd_12_mse8x16_c(const uint8_t* src_ptr,
+                                     int src_stride,
+                                     const uint8_t* ref_ptr,
+                                     int ref_stride,
+                                     unsigned int* sse);
+#define vpx_highbd_12_mse8x16 vpx_highbd_12_mse8x16_c
+
+unsigned int vpx_highbd_12_mse8x8_c(const uint8_t* src_ptr,
+                                    int src_stride,
+                                    const uint8_t* ref_ptr,
+                                    int ref_stride,
+                                    unsigned int* sse);
+#define vpx_highbd_12_mse8x8 vpx_highbd_12_mse8x8_c
+
+uint32_t vpx_highbd_12_sub_pixel_avg_variance16x16_c(
+    const uint8_t* src_ptr,
+    int src_stride,
+    int x_offset,
+    int y_offset,
+    const uint8_t* ref_ptr,
+    int ref_stride,
+    uint32_t* sse,
+    const uint8_t* second_pred);
+#define vpx_highbd_12_sub_pixel_avg_variance16x16 \
+  vpx_highbd_12_sub_pixel_avg_variance16x16_c
+
+uint32_t vpx_highbd_12_sub_pixel_avg_variance16x32_c(
+    const uint8_t* src_ptr,
+    int src_stride,
+    int x_offset,
+    int y_offset,
+    const uint8_t* ref_ptr,
+    int ref_stride,
+    uint32_t* sse,
+    const uint8_t* second_pred);
+#define vpx_highbd_12_sub_pixel_avg_variance16x32 \
+  vpx_highbd_12_sub_pixel_avg_variance16x32_c
+
+uint32_t vpx_highbd_12_sub_pixel_avg_variance16x8_c(const uint8_t* src_ptr,
+                                                    int src_stride,
+                                                    int x_offset,
+                                                    int y_offset,
+                                                    const uint8_t* ref_ptr,
+                                                    int ref_stride,
+                                                    uint32_t* sse,
+                                                    const uint8_t* second_pred);
+#define vpx_highbd_12_sub_pixel_avg_variance16x8 \
+  vpx_highbd_12_sub_pixel_avg_variance16x8_c
+
+uint32_t vpx_highbd_12_sub_pixel_avg_variance32x16_c(
+    const uint8_t* src_ptr,
+    int src_stride,
+    int x_offset,
+    int y_offset,
+    const uint8_t* ref_ptr,
+    int ref_stride,
+    uint32_t* sse,
+    const uint8_t* second_pred);
+#define vpx_highbd_12_sub_pixel_avg_variance32x16 \
+  vpx_highbd_12_sub_pixel_avg_variance32x16_c
+
+uint32_t vpx_highbd_12_sub_pixel_avg_variance32x32_c(
+    const uint8_t* src_ptr,
+    int src_stride,
+    int x_offset,
+    int y_offset,
+    const uint8_t* ref_ptr,
+    int ref_stride,
+    uint32_t* sse,
+    const uint8_t* second_pred);
+#define vpx_highbd_12_sub_pixel_avg_variance32x32 \
+  vpx_highbd_12_sub_pixel_avg_variance32x32_c
+
+uint32_t vpx_highbd_12_sub_pixel_avg_variance32x64_c(
+    const uint8_t* src_ptr,
+    int src_stride,
+    int x_offset,
+    int y_offset,
+    const uint8_t* ref_ptr,
+    int ref_stride,
+    uint32_t* sse,
+    const uint8_t* second_pred);
+#define vpx_highbd_12_sub_pixel_avg_variance32x64 \
+  vpx_highbd_12_sub_pixel_avg_variance32x64_c
+
+uint32_t vpx_highbd_12_sub_pixel_avg_variance4x4_c(const uint8_t* src_ptr,
+                                                   int src_stride,
+                                                   int x_offset,
+                                                   int y_offset,
+                                                   const uint8_t* ref_ptr,
+                                                   int ref_stride,
+                                                   uint32_t* sse,
+                                                   const uint8_t* second_pred);
+#define vpx_highbd_12_sub_pixel_avg_variance4x4 \
+  vpx_highbd_12_sub_pixel_avg_variance4x4_c
+
+uint32_t vpx_highbd_12_sub_pixel_avg_variance4x8_c(const uint8_t* src_ptr,
+                                                   int src_stride,
+                                                   int x_offset,
+                                                   int y_offset,
+                                                   const uint8_t* ref_ptr,
+                                                   int ref_stride,
+                                                   uint32_t* sse,
+                                                   const uint8_t* second_pred);
+#define vpx_highbd_12_sub_pixel_avg_variance4x8 \
+  vpx_highbd_12_sub_pixel_avg_variance4x8_c
+
+uint32_t vpx_highbd_12_sub_pixel_avg_variance64x32_c(
+    const uint8_t* src_ptr,
+    int src_stride,
+    int x_offset,
+    int y_offset,
+    const uint8_t* ref_ptr,
+    int ref_stride,
+    uint32_t* sse,
+    const uint8_t* second_pred);
+#define vpx_highbd_12_sub_pixel_avg_variance64x32 \
+  vpx_highbd_12_sub_pixel_avg_variance64x32_c
+
+uint32_t vpx_highbd_12_sub_pixel_avg_variance64x64_c(
+    const uint8_t* src_ptr,
+    int src_stride,
+    int x_offset,
+    int y_offset,
+    const uint8_t* ref_ptr,
+    int ref_stride,
+    uint32_t* sse,
+    const uint8_t* second_pred);
+#define vpx_highbd_12_sub_pixel_avg_variance64x64 \
+  vpx_highbd_12_sub_pixel_avg_variance64x64_c
+
+uint32_t vpx_highbd_12_sub_pixel_avg_variance8x16_c(const uint8_t* src_ptr,
+                                                    int src_stride,
+                                                    int x_offset,
+                                                    int y_offset,
+                                                    const uint8_t* ref_ptr,
+                                                    int ref_stride,
+                                                    uint32_t* sse,
+                                                    const uint8_t* second_pred);
+#define vpx_highbd_12_sub_pixel_avg_variance8x16 \
+  vpx_highbd_12_sub_pixel_avg_variance8x16_c
+
+uint32_t vpx_highbd_12_sub_pixel_avg_variance8x4_c(const uint8_t* src_ptr,
+                                                   int src_stride,
+                                                   int x_offset,
+                                                   int y_offset,
+                                                   const uint8_t* ref_ptr,
+                                                   int ref_stride,
+                                                   uint32_t* sse,
+                                                   const uint8_t* second_pred);
+#define vpx_highbd_12_sub_pixel_avg_variance8x4 \
+  vpx_highbd_12_sub_pixel_avg_variance8x4_c
+
+uint32_t vpx_highbd_12_sub_pixel_avg_variance8x8_c(const uint8_t* src_ptr,
+                                                   int src_stride,
+                                                   int x_offset,
+                                                   int y_offset,
+                                                   const uint8_t* ref_ptr,
+                                                   int ref_stride,
+                                                   uint32_t* sse,
+                                                   const uint8_t* second_pred);
+#define vpx_highbd_12_sub_pixel_avg_variance8x8 \
+  vpx_highbd_12_sub_pixel_avg_variance8x8_c
+
+uint32_t vpx_highbd_12_sub_pixel_variance16x16_c(const uint8_t* src_ptr,
+                                                 int src_stride,
+                                                 int x_offset,
+                                                 int y_offset,
+                                                 const uint8_t* ref_ptr,
+                                                 int ref_stride,
+                                                 uint32_t* sse);
+#define vpx_highbd_12_sub_pixel_variance16x16 \
+  vpx_highbd_12_sub_pixel_variance16x16_c
+
+uint32_t vpx_highbd_12_sub_pixel_variance16x32_c(const uint8_t* src_ptr,
+                                                 int src_stride,
+                                                 int x_offset,
+                                                 int y_offset,
+                                                 const uint8_t* ref_ptr,
+                                                 int ref_stride,
+                                                 uint32_t* sse);
+#define vpx_highbd_12_sub_pixel_variance16x32 \
+  vpx_highbd_12_sub_pixel_variance16x32_c
+
+uint32_t vpx_highbd_12_sub_pixel_variance16x8_c(const uint8_t* src_ptr,
+                                                int src_stride,
+                                                int x_offset,
+                                                int y_offset,
+                                                const uint8_t* ref_ptr,
+                                                int ref_stride,
+                                                uint32_t* sse);
+#define vpx_highbd_12_sub_pixel_variance16x8 \
+  vpx_highbd_12_sub_pixel_variance16x8_c
+
+uint32_t vpx_highbd_12_sub_pixel_variance32x16_c(const uint8_t* src_ptr,
+                                                 int src_stride,
+                                                 int x_offset,
+                                                 int y_offset,
+                                                 const uint8_t* ref_ptr,
+                                                 int ref_stride,
+                                                 uint32_t* sse);
+#define vpx_highbd_12_sub_pixel_variance32x16 \
+  vpx_highbd_12_sub_pixel_variance32x16_c
+
+uint32_t vpx_highbd_12_sub_pixel_variance32x32_c(const uint8_t* src_ptr,
+                                                 int src_stride,
+                                                 int x_offset,
+                                                 int y_offset,
+                                                 const uint8_t* ref_ptr,
+                                                 int ref_stride,
+                                                 uint32_t* sse);
+#define vpx_highbd_12_sub_pixel_variance32x32 \
+  vpx_highbd_12_sub_pixel_variance32x32_c
+
+uint32_t vpx_highbd_12_sub_pixel_variance32x64_c(const uint8_t* src_ptr,
+                                                 int src_stride,
+                                                 int x_offset,
+                                                 int y_offset,
+                                                 const uint8_t* ref_ptr,
+                                                 int ref_stride,
+                                                 uint32_t* sse);
+#define vpx_highbd_12_sub_pixel_variance32x64 \
+  vpx_highbd_12_sub_pixel_variance32x64_c
+
+uint32_t vpx_highbd_12_sub_pixel_variance4x4_c(const uint8_t* src_ptr,
+                                               int src_stride,
+                                               int x_offset,
+                                               int y_offset,
+                                               const uint8_t* ref_ptr,
+                                               int ref_stride,
+                                               uint32_t* sse);
+#define vpx_highbd_12_sub_pixel_variance4x4 \
+  vpx_highbd_12_sub_pixel_variance4x4_c
+
+uint32_t vpx_highbd_12_sub_pixel_variance4x8_c(const uint8_t* src_ptr,
+                                               int src_stride,
+                                               int x_offset,
+                                               int y_offset,
+                                               const uint8_t* ref_ptr,
+                                               int ref_stride,
+                                               uint32_t* sse);
+#define vpx_highbd_12_sub_pixel_variance4x8 \
+  vpx_highbd_12_sub_pixel_variance4x8_c
+
+uint32_t vpx_highbd_12_sub_pixel_variance64x32_c(const uint8_t* src_ptr,
+                                                 int src_stride,
+                                                 int x_offset,
+                                                 int y_offset,
+                                                 const uint8_t* ref_ptr,
+                                                 int ref_stride,
+                                                 uint32_t* sse);
+#define vpx_highbd_12_sub_pixel_variance64x32 \
+  vpx_highbd_12_sub_pixel_variance64x32_c
+
+uint32_t vpx_highbd_12_sub_pixel_variance64x64_c(const uint8_t* src_ptr,
+                                                 int src_stride,
+                                                 int x_offset,
+                                                 int y_offset,
+                                                 const uint8_t* ref_ptr,
+                                                 int ref_stride,
+                                                 uint32_t* sse);
+#define vpx_highbd_12_sub_pixel_variance64x64 \
+  vpx_highbd_12_sub_pixel_variance64x64_c
+
+uint32_t vpx_highbd_12_sub_pixel_variance8x16_c(const uint8_t* src_ptr,
+                                                int src_stride,
+                                                int x_offset,
+                                                int y_offset,
+                                                const uint8_t* ref_ptr,
+                                                int ref_stride,
+                                                uint32_t* sse);
+#define vpx_highbd_12_sub_pixel_variance8x16 \
+  vpx_highbd_12_sub_pixel_variance8x16_c
+
+uint32_t vpx_highbd_12_sub_pixel_variance8x4_c(const uint8_t* src_ptr,
+                                               int src_stride,
+                                               int x_offset,
+                                               int y_offset,
+                                               const uint8_t* ref_ptr,
+                                               int ref_stride,
+                                               uint32_t* sse);
+#define vpx_highbd_12_sub_pixel_variance8x4 \
+  vpx_highbd_12_sub_pixel_variance8x4_c
+
+uint32_t vpx_highbd_12_sub_pixel_variance8x8_c(const uint8_t* src_ptr,
+                                               int src_stride,
+                                               int x_offset,
+                                               int y_offset,
+                                               const uint8_t* ref_ptr,
+                                               int ref_stride,
+                                               uint32_t* sse);
+#define vpx_highbd_12_sub_pixel_variance8x8 \
+  vpx_highbd_12_sub_pixel_variance8x8_c
+
+unsigned int vpx_highbd_12_variance16x16_c(const uint8_t* src_ptr,
+                                           int src_stride,
+                                           const uint8_t* ref_ptr,
+                                           int ref_stride,
+                                           unsigned int* sse);
+#define vpx_highbd_12_variance16x16 vpx_highbd_12_variance16x16_c
+
+unsigned int vpx_highbd_12_variance16x32_c(const uint8_t* src_ptr,
+                                           int src_stride,
+                                           const uint8_t* ref_ptr,
+                                           int ref_stride,
+                                           unsigned int* sse);
+#define vpx_highbd_12_variance16x32 vpx_highbd_12_variance16x32_c
+
+unsigned int vpx_highbd_12_variance16x8_c(const uint8_t* src_ptr,
+                                          int src_stride,
+                                          const uint8_t* ref_ptr,
+                                          int ref_stride,
+                                          unsigned int* sse);
+#define vpx_highbd_12_variance16x8 vpx_highbd_12_variance16x8_c
+
+unsigned int vpx_highbd_12_variance32x16_c(const uint8_t* src_ptr,
+                                           int src_stride,
+                                           const uint8_t* ref_ptr,
+                                           int ref_stride,
+                                           unsigned int* sse);
+#define vpx_highbd_12_variance32x16 vpx_highbd_12_variance32x16_c
+
+unsigned int vpx_highbd_12_variance32x32_c(const uint8_t* src_ptr,
+                                           int src_stride,
+                                           const uint8_t* ref_ptr,
+                                           int ref_stride,
+                                           unsigned int* sse);
+#define vpx_highbd_12_variance32x32 vpx_highbd_12_variance32x32_c
+
+unsigned int vpx_highbd_12_variance32x64_c(const uint8_t* src_ptr,
+                                           int src_stride,
+                                           const uint8_t* ref_ptr,
+                                           int ref_stride,
+                                           unsigned int* sse);
+#define vpx_highbd_12_variance32x64 vpx_highbd_12_variance32x64_c
+
+unsigned int vpx_highbd_12_variance4x4_c(const uint8_t* src_ptr,
+                                         int src_stride,
+                                         const uint8_t* ref_ptr,
+                                         int ref_stride,
+                                         unsigned int* sse);
+#define vpx_highbd_12_variance4x4 vpx_highbd_12_variance4x4_c
+
+unsigned int vpx_highbd_12_variance4x8_c(const uint8_t* src_ptr,
+                                         int src_stride,
+                                         const uint8_t* ref_ptr,
+                                         int ref_stride,
+                                         unsigned int* sse);
+#define vpx_highbd_12_variance4x8 vpx_highbd_12_variance4x8_c
+
+unsigned int vpx_highbd_12_variance64x32_c(const uint8_t* src_ptr,
+                                           int src_stride,
+                                           const uint8_t* ref_ptr,
+                                           int ref_stride,
+                                           unsigned int* sse);
+#define vpx_highbd_12_variance64x32 vpx_highbd_12_variance64x32_c
+
+unsigned int vpx_highbd_12_variance64x64_c(const uint8_t* src_ptr,
+                                           int src_stride,
+                                           const uint8_t* ref_ptr,
+                                           int ref_stride,
+                                           unsigned int* sse);
+#define vpx_highbd_12_variance64x64 vpx_highbd_12_variance64x64_c
+
+unsigned int vpx_highbd_12_variance8x16_c(const uint8_t* src_ptr,
+                                          int src_stride,
+                                          const uint8_t* ref_ptr,
+                                          int ref_stride,
+                                          unsigned int* sse);
+#define vpx_highbd_12_variance8x16 vpx_highbd_12_variance8x16_c
+
+unsigned int vpx_highbd_12_variance8x4_c(const uint8_t* src_ptr,
+                                         int src_stride,
+                                         const uint8_t* ref_ptr,
+                                         int ref_stride,
+                                         unsigned int* sse);
+#define vpx_highbd_12_variance8x4 vpx_highbd_12_variance8x4_c
+
+unsigned int vpx_highbd_12_variance8x8_c(const uint8_t* src_ptr,
+                                         int src_stride,
+                                         const uint8_t* ref_ptr,
+                                         int ref_stride,
+                                         unsigned int* sse);
+#define vpx_highbd_12_variance8x8 vpx_highbd_12_variance8x8_c
+
+void vpx_highbd_8_get16x16var_c(const uint8_t* src_ptr,
+                                int src_stride,
+                                const uint8_t* ref_ptr,
+                                int ref_stride,
+                                unsigned int* sse,
+                                int* sum);
+#define vpx_highbd_8_get16x16var vpx_highbd_8_get16x16var_c
+
+void vpx_highbd_8_get8x8var_c(const uint8_t* src_ptr,
+                              int src_stride,
+                              const uint8_t* ref_ptr,
+                              int ref_stride,
+                              unsigned int* sse,
+                              int* sum);
+#define vpx_highbd_8_get8x8var vpx_highbd_8_get8x8var_c
+
+unsigned int vpx_highbd_8_mse16x16_c(const uint8_t* src_ptr,
+                                     int src_stride,
+                                     const uint8_t* ref_ptr,
+                                     int ref_stride,
+                                     unsigned int* sse);
+#define vpx_highbd_8_mse16x16 vpx_highbd_8_mse16x16_c
+
+unsigned int vpx_highbd_8_mse16x8_c(const uint8_t* src_ptr,
+                                    int src_stride,
+                                    const uint8_t* ref_ptr,
+                                    int ref_stride,
+                                    unsigned int* sse);
+#define vpx_highbd_8_mse16x8 vpx_highbd_8_mse16x8_c
+
+unsigned int vpx_highbd_8_mse8x16_c(const uint8_t* src_ptr,
+                                    int src_stride,
+                                    const uint8_t* ref_ptr,
+                                    int ref_stride,
+                                    unsigned int* sse);
+#define vpx_highbd_8_mse8x16 vpx_highbd_8_mse8x16_c
+
+unsigned int vpx_highbd_8_mse8x8_c(const uint8_t* src_ptr,
+                                   int src_stride,
+                                   const uint8_t* ref_ptr,
+                                   int ref_stride,
+                                   unsigned int* sse);
+#define vpx_highbd_8_mse8x8 vpx_highbd_8_mse8x8_c
+
+uint32_t vpx_highbd_8_sub_pixel_avg_variance16x16_c(const uint8_t* src_ptr,
+                                                    int src_stride,
+                                                    int x_offset,
+                                                    int y_offset,
+                                                    const uint8_t* ref_ptr,
+                                                    int ref_stride,
+                                                    uint32_t* sse,
+                                                    const uint8_t* second_pred);
+#define vpx_highbd_8_sub_pixel_avg_variance16x16 \
+  vpx_highbd_8_sub_pixel_avg_variance16x16_c
+
+uint32_t vpx_highbd_8_sub_pixel_avg_variance16x32_c(const uint8_t* src_ptr,
+                                                    int src_stride,
+                                                    int x_offset,
+                                                    int y_offset,
+                                                    const uint8_t* ref_ptr,
+                                                    int ref_stride,
+                                                    uint32_t* sse,
+                                                    const uint8_t* second_pred);
+#define vpx_highbd_8_sub_pixel_avg_variance16x32 \
+  vpx_highbd_8_sub_pixel_avg_variance16x32_c
+
+uint32_t vpx_highbd_8_sub_pixel_avg_variance16x8_c(const uint8_t* src_ptr,
+                                                   int src_stride,
+                                                   int x_offset,
+                                                   int y_offset,
+                                                   const uint8_t* ref_ptr,
+                                                   int ref_stride,
+                                                   uint32_t* sse,
+                                                   const uint8_t* second_pred);
+#define vpx_highbd_8_sub_pixel_avg_variance16x8 \
+  vpx_highbd_8_sub_pixel_avg_variance16x8_c
+
+uint32_t vpx_highbd_8_sub_pixel_avg_variance32x16_c(const uint8_t* src_ptr,
+                                                    int src_stride,
+                                                    int x_offset,
+                                                    int y_offset,
+                                                    const uint8_t* ref_ptr,
+                                                    int ref_stride,
+                                                    uint32_t* sse,
+                                                    const uint8_t* second_pred);
+#define vpx_highbd_8_sub_pixel_avg_variance32x16 \
+  vpx_highbd_8_sub_pixel_avg_variance32x16_c
+
+uint32_t vpx_highbd_8_sub_pixel_avg_variance32x32_c(const uint8_t* src_ptr,
+                                                    int src_stride,
+                                                    int x_offset,
+                                                    int y_offset,
+                                                    const uint8_t* ref_ptr,
+                                                    int ref_stride,
+                                                    uint32_t* sse,
+                                                    const uint8_t* second_pred);
+#define vpx_highbd_8_sub_pixel_avg_variance32x32 \
+  vpx_highbd_8_sub_pixel_avg_variance32x32_c
+
+uint32_t vpx_highbd_8_sub_pixel_avg_variance32x64_c(const uint8_t* src_ptr,
+                                                    int src_stride,
+                                                    int x_offset,
+                                                    int y_offset,
+                                                    const uint8_t* ref_ptr,
+                                                    int ref_stride,
+                                                    uint32_t* sse,
+                                                    const uint8_t* second_pred);
+#define vpx_highbd_8_sub_pixel_avg_variance32x64 \
+  vpx_highbd_8_sub_pixel_avg_variance32x64_c
+
+uint32_t vpx_highbd_8_sub_pixel_avg_variance4x4_c(const uint8_t* src_ptr,
+                                                  int src_stride,
+                                                  int x_offset,
+                                                  int y_offset,
+                                                  const uint8_t* ref_ptr,
+                                                  int ref_stride,
+                                                  uint32_t* sse,
+                                                  const uint8_t* second_pred);
+#define vpx_highbd_8_sub_pixel_avg_variance4x4 \
+  vpx_highbd_8_sub_pixel_avg_variance4x4_c
+
+uint32_t vpx_highbd_8_sub_pixel_avg_variance4x8_c(const uint8_t* src_ptr,
+                                                  int src_stride,
+                                                  int x_offset,
+                                                  int y_offset,
+                                                  const uint8_t* ref_ptr,
+                                                  int ref_stride,
+                                                  uint32_t* sse,
+                                                  const uint8_t* second_pred);
+#define vpx_highbd_8_sub_pixel_avg_variance4x8 \
+  vpx_highbd_8_sub_pixel_avg_variance4x8_c
+
+uint32_t vpx_highbd_8_sub_pixel_avg_variance64x32_c(const uint8_t* src_ptr,
+                                                    int src_stride,
+                                                    int x_offset,
+                                                    int y_offset,
+                                                    const uint8_t* ref_ptr,
+                                                    int ref_stride,
+                                                    uint32_t* sse,
+                                                    const uint8_t* second_pred);
+#define vpx_highbd_8_sub_pixel_avg_variance64x32 \
+  vpx_highbd_8_sub_pixel_avg_variance64x32_c
+
+uint32_t vpx_highbd_8_sub_pixel_avg_variance64x64_c(const uint8_t* src_ptr,
+                                                    int src_stride,
+                                                    int x_offset,
+                                                    int y_offset,
+                                                    const uint8_t* ref_ptr,
+                                                    int ref_stride,
+                                                    uint32_t* sse,
+                                                    const uint8_t* second_pred);
+#define vpx_highbd_8_sub_pixel_avg_variance64x64 \
+  vpx_highbd_8_sub_pixel_avg_variance64x64_c
+
+uint32_t vpx_highbd_8_sub_pixel_avg_variance8x16_c(const uint8_t* src_ptr,
+                                                   int src_stride,
+                                                   int x_offset,
+                                                   int y_offset,
+                                                   const uint8_t* ref_ptr,
+                                                   int ref_stride,
+                                                   uint32_t* sse,
+                                                   const uint8_t* second_pred);
+#define vpx_highbd_8_sub_pixel_avg_variance8x16 \
+  vpx_highbd_8_sub_pixel_avg_variance8x16_c
+
+uint32_t vpx_highbd_8_sub_pixel_avg_variance8x4_c(const uint8_t* src_ptr,
+                                                  int src_stride,
+                                                  int x_offset,
+                                                  int y_offset,
+                                                  const uint8_t* ref_ptr,
+                                                  int ref_stride,
+                                                  uint32_t* sse,
+                                                  const uint8_t* second_pred);
+#define vpx_highbd_8_sub_pixel_avg_variance8x4 \
+  vpx_highbd_8_sub_pixel_avg_variance8x4_c
+
+uint32_t vpx_highbd_8_sub_pixel_avg_variance8x8_c(const uint8_t* src_ptr,
+                                                  int src_stride,
+                                                  int x_offset,
+                                                  int y_offset,
+                                                  const uint8_t* ref_ptr,
+                                                  int ref_stride,
+                                                  uint32_t* sse,
+                                                  const uint8_t* second_pred);
+#define vpx_highbd_8_sub_pixel_avg_variance8x8 \
+  vpx_highbd_8_sub_pixel_avg_variance8x8_c
+
+uint32_t vpx_highbd_8_sub_pixel_variance16x16_c(const uint8_t* src_ptr,
+                                                int src_stride,
+                                                int x_offset,
+                                                int y_offset,
+                                                const uint8_t* ref_ptr,
+                                                int ref_stride,
+                                                uint32_t* sse);
+#define vpx_highbd_8_sub_pixel_variance16x16 \
+  vpx_highbd_8_sub_pixel_variance16x16_c
+
+uint32_t vpx_highbd_8_sub_pixel_variance16x32_c(const uint8_t* src_ptr,
+                                                int src_stride,
+                                                int x_offset,
+                                                int y_offset,
+                                                const uint8_t* ref_ptr,
+                                                int ref_stride,
+                                                uint32_t* sse);
+#define vpx_highbd_8_sub_pixel_variance16x32 \
+  vpx_highbd_8_sub_pixel_variance16x32_c
+
+uint32_t vpx_highbd_8_sub_pixel_variance16x8_c(const uint8_t* src_ptr,
+                                               int src_stride,
+                                               int x_offset,
+                                               int y_offset,
+                                               const uint8_t* ref_ptr,
+                                               int ref_stride,
+                                               uint32_t* sse);
+#define vpx_highbd_8_sub_pixel_variance16x8 \
+  vpx_highbd_8_sub_pixel_variance16x8_c
+
+uint32_t vpx_highbd_8_sub_pixel_variance32x16_c(const uint8_t* src_ptr,
+                                                int src_stride,
+                                                int x_offset,
+                                                int y_offset,
+                                                const uint8_t* ref_ptr,
+                                                int ref_stride,
+                                                uint32_t* sse);
+#define vpx_highbd_8_sub_pixel_variance32x16 \
+  vpx_highbd_8_sub_pixel_variance32x16_c
+
+uint32_t vpx_highbd_8_sub_pixel_variance32x32_c(const uint8_t* src_ptr,
+                                                int src_stride,
+                                                int x_offset,
+                                                int y_offset,
+                                                const uint8_t* ref_ptr,
+                                                int ref_stride,
+                                                uint32_t* sse);
+#define vpx_highbd_8_sub_pixel_variance32x32 \
+  vpx_highbd_8_sub_pixel_variance32x32_c
+
+uint32_t vpx_highbd_8_sub_pixel_variance32x64_c(const uint8_t* src_ptr,
+                                                int src_stride,
+                                                int x_offset,
+                                                int y_offset,
+                                                const uint8_t* ref_ptr,
+                                                int ref_stride,
+                                                uint32_t* sse);
+#define vpx_highbd_8_sub_pixel_variance32x64 \
+  vpx_highbd_8_sub_pixel_variance32x64_c
+
+uint32_t vpx_highbd_8_sub_pixel_variance4x4_c(const uint8_t* src_ptr,
+                                              int src_stride,
+                                              int x_offset,
+                                              int y_offset,
+                                              const uint8_t* ref_ptr,
+                                              int ref_stride,
+                                              uint32_t* sse);
+#define vpx_highbd_8_sub_pixel_variance4x4 vpx_highbd_8_sub_pixel_variance4x4_c
+
+uint32_t vpx_highbd_8_sub_pixel_variance4x8_c(const uint8_t* src_ptr,
+                                              int src_stride,
+                                              int x_offset,
+                                              int y_offset,
+                                              const uint8_t* ref_ptr,
+                                              int ref_stride,
+                                              uint32_t* sse);
+#define vpx_highbd_8_sub_pixel_variance4x8 vpx_highbd_8_sub_pixel_variance4x8_c
+
+uint32_t vpx_highbd_8_sub_pixel_variance64x32_c(const uint8_t* src_ptr,
+                                                int src_stride,
+                                                int x_offset,
+                                                int y_offset,
+                                                const uint8_t* ref_ptr,
+                                                int ref_stride,
+                                                uint32_t* sse);
+#define vpx_highbd_8_sub_pixel_variance64x32 \
+  vpx_highbd_8_sub_pixel_variance64x32_c
+
+uint32_t vpx_highbd_8_sub_pixel_variance64x64_c(const uint8_t* src_ptr,
+                                                int src_stride,
+                                                int x_offset,
+                                                int y_offset,
+                                                const uint8_t* ref_ptr,
+                                                int ref_stride,
+                                                uint32_t* sse);
+#define vpx_highbd_8_sub_pixel_variance64x64 \
+  vpx_highbd_8_sub_pixel_variance64x64_c
+
+uint32_t vpx_highbd_8_sub_pixel_variance8x16_c(const uint8_t* src_ptr,
+                                               int src_stride,
+                                               int x_offset,
+                                               int y_offset,
+                                               const uint8_t* ref_ptr,
+                                               int ref_stride,
+                                               uint32_t* sse);
+#define vpx_highbd_8_sub_pixel_variance8x16 \
+  vpx_highbd_8_sub_pixel_variance8x16_c
+
+uint32_t vpx_highbd_8_sub_pixel_variance8x4_c(const uint8_t* src_ptr,
+                                              int src_stride,
+                                              int x_offset,
+                                              int y_offset,
+                                              const uint8_t* ref_ptr,
+                                              int ref_stride,
+                                              uint32_t* sse);
+#define vpx_highbd_8_sub_pixel_variance8x4 vpx_highbd_8_sub_pixel_variance8x4_c
+
+uint32_t vpx_highbd_8_sub_pixel_variance8x8_c(const uint8_t* src_ptr,
+                                              int src_stride,
+                                              int x_offset,
+                                              int y_offset,
+                                              const uint8_t* ref_ptr,
+                                              int ref_stride,
+                                              uint32_t* sse);
+#define vpx_highbd_8_sub_pixel_variance8x8 vpx_highbd_8_sub_pixel_variance8x8_c
+
+unsigned int vpx_highbd_8_variance16x16_c(const uint8_t* src_ptr,
+                                          int src_stride,
+                                          const uint8_t* ref_ptr,
+                                          int ref_stride,
+                                          unsigned int* sse);
+#define vpx_highbd_8_variance16x16 vpx_highbd_8_variance16x16_c
+
+unsigned int vpx_highbd_8_variance16x32_c(const uint8_t* src_ptr,
+                                          int src_stride,
+                                          const uint8_t* ref_ptr,
+                                          int ref_stride,
+                                          unsigned int* sse);
+#define vpx_highbd_8_variance16x32 vpx_highbd_8_variance16x32_c
+
+unsigned int vpx_highbd_8_variance16x8_c(const uint8_t* src_ptr,
+                                         int src_stride,
+                                         const uint8_t* ref_ptr,
+                                         int ref_stride,
+                                         unsigned int* sse);
+#define vpx_highbd_8_variance16x8 vpx_highbd_8_variance16x8_c
+
+unsigned int vpx_highbd_8_variance32x16_c(const uint8_t* src_ptr,
+                                          int src_stride,
+                                          const uint8_t* ref_ptr,
+                                          int ref_stride,
+                                          unsigned int* sse);
+#define vpx_highbd_8_variance32x16 vpx_highbd_8_variance32x16_c
+
+unsigned int vpx_highbd_8_variance32x32_c(const uint8_t* src_ptr,
+                                          int src_stride,
+                                          const uint8_t* ref_ptr,
+                                          int ref_stride,
+                                          unsigned int* sse);
+#define vpx_highbd_8_variance32x32 vpx_highbd_8_variance32x32_c
+
+unsigned int vpx_highbd_8_variance32x64_c(const uint8_t* src_ptr,
+                                          int src_stride,
+                                          const uint8_t* ref_ptr,
+                                          int ref_stride,
+                                          unsigned int* sse);
+#define vpx_highbd_8_variance32x64 vpx_highbd_8_variance32x64_c
+
+unsigned int vpx_highbd_8_variance4x4_c(const uint8_t* src_ptr,
+                                        int src_stride,
+                                        const uint8_t* ref_ptr,
+                                        int ref_stride,
+                                        unsigned int* sse);
+#define vpx_highbd_8_variance4x4 vpx_highbd_8_variance4x4_c
+
+unsigned int vpx_highbd_8_variance4x8_c(const uint8_t* src_ptr,
+                                        int src_stride,
+                                        const uint8_t* ref_ptr,
+                                        int ref_stride,
+                                        unsigned int* sse);
+#define vpx_highbd_8_variance4x8 vpx_highbd_8_variance4x8_c
+
+unsigned int vpx_highbd_8_variance64x32_c(const uint8_t* src_ptr,
+                                          int src_stride,
+                                          const uint8_t* ref_ptr,
+                                          int ref_stride,
+                                          unsigned int* sse);
+#define vpx_highbd_8_variance64x32 vpx_highbd_8_variance64x32_c
+
+unsigned int vpx_highbd_8_variance64x64_c(const uint8_t* src_ptr,
+                                          int src_stride,
+                                          const uint8_t* ref_ptr,
+                                          int ref_stride,
+                                          unsigned int* sse);
+#define vpx_highbd_8_variance64x64 vpx_highbd_8_variance64x64_c
+
+unsigned int vpx_highbd_8_variance8x16_c(const uint8_t* src_ptr,
+                                         int src_stride,
+                                         const uint8_t* ref_ptr,
+                                         int ref_stride,
+                                         unsigned int* sse);
+#define vpx_highbd_8_variance8x16 vpx_highbd_8_variance8x16_c
+
+unsigned int vpx_highbd_8_variance8x4_c(const uint8_t* src_ptr,
+                                        int src_stride,
+                                        const uint8_t* ref_ptr,
+                                        int ref_stride,
+                                        unsigned int* sse);
+#define vpx_highbd_8_variance8x4 vpx_highbd_8_variance8x4_c
+
+unsigned int vpx_highbd_8_variance8x8_c(const uint8_t* src_ptr,
+                                        int src_stride,
+                                        const uint8_t* ref_ptr,
+                                        int ref_stride,
+                                        unsigned int* sse);
+#define vpx_highbd_8_variance8x8 vpx_highbd_8_variance8x8_c
+
+unsigned int vpx_highbd_avg_4x4_c(const uint8_t* s8, int p);
+#define vpx_highbd_avg_4x4 vpx_highbd_avg_4x4_c
+
+unsigned int vpx_highbd_avg_8x8_c(const uint8_t* s8, int p);
+#define vpx_highbd_avg_8x8 vpx_highbd_avg_8x8_c
+
+void vpx_highbd_comp_avg_pred_c(uint16_t* comp_pred,
+                                const uint16_t* pred,
+                                int width,
+                                int height,
+                                const uint16_t* ref,
+                                int ref_stride);
+#define vpx_highbd_comp_avg_pred vpx_highbd_comp_avg_pred_c
+
+void vpx_highbd_convolve8_c(const uint16_t* src,
+                            ptrdiff_t src_stride,
+                            uint16_t* dst,
+                            ptrdiff_t dst_stride,
+                            const InterpKernel* filter,
+                            int x0_q4,
+                            int x_step_q4,
+                            int y0_q4,
+                            int y_step_q4,
+                            int w,
+                            int h,
+                            int bd);
+#define vpx_highbd_convolve8 vpx_highbd_convolve8_c
+
+void vpx_highbd_convolve8_avg_c(const uint16_t* src,
+                                ptrdiff_t src_stride,
+                                uint16_t* dst,
+                                ptrdiff_t dst_stride,
+                                const InterpKernel* filter,
+                                int x0_q4,
+                                int x_step_q4,
+                                int y0_q4,
+                                int y_step_q4,
+                                int w,
+                                int h,
+                                int bd);
+#define vpx_highbd_convolve8_avg vpx_highbd_convolve8_avg_c
+
+void vpx_highbd_convolve8_avg_horiz_c(const uint16_t* src,
+                                      ptrdiff_t src_stride,
+                                      uint16_t* dst,
+                                      ptrdiff_t dst_stride,
+                                      const InterpKernel* filter,
+                                      int x0_q4,
+                                      int x_step_q4,
+                                      int y0_q4,
+                                      int y_step_q4,
+                                      int w,
+                                      int h,
+                                      int bd);
+#define vpx_highbd_convolve8_avg_horiz vpx_highbd_convolve8_avg_horiz_c
+
+void vpx_highbd_convolve8_avg_vert_c(const uint16_t* src,
+                                     ptrdiff_t src_stride,
+                                     uint16_t* dst,
+                                     ptrdiff_t dst_stride,
+                                     const InterpKernel* filter,
+                                     int x0_q4,
+                                     int x_step_q4,
+                                     int y0_q4,
+                                     int y_step_q4,
+                                     int w,
+                                     int h,
+                                     int bd);
+#define vpx_highbd_convolve8_avg_vert vpx_highbd_convolve8_avg_vert_c
+
+void vpx_highbd_convolve8_horiz_c(const uint16_t* src,
+                                  ptrdiff_t src_stride,
+                                  uint16_t* dst,
+                                  ptrdiff_t dst_stride,
+                                  const InterpKernel* filter,
+                                  int x0_q4,
+                                  int x_step_q4,
+                                  int y0_q4,
+                                  int y_step_q4,
+                                  int w,
+                                  int h,
+                                  int bd);
+#define vpx_highbd_convolve8_horiz vpx_highbd_convolve8_horiz_c
+
+void vpx_highbd_convolve8_vert_c(const uint16_t* src,
+                                 ptrdiff_t src_stride,
+                                 uint16_t* dst,
+                                 ptrdiff_t dst_stride,
+                                 const InterpKernel* filter,
+                                 int x0_q4,
+                                 int x_step_q4,
+                                 int y0_q4,
+                                 int y_step_q4,
+                                 int w,
+                                 int h,
+                                 int bd);
+#define vpx_highbd_convolve8_vert vpx_highbd_convolve8_vert_c
+
+void vpx_highbd_convolve_avg_c(const uint16_t* src,
+                               ptrdiff_t src_stride,
+                               uint16_t* dst,
+                               ptrdiff_t dst_stride,
+                               const InterpKernel* filter,
+                               int x0_q4,
+                               int x_step_q4,
+                               int y0_q4,
+                               int y_step_q4,
+                               int w,
+                               int h,
+                               int bd);
+#define vpx_highbd_convolve_avg vpx_highbd_convolve_avg_c
+
+void vpx_highbd_convolve_copy_c(const uint16_t* src,
+                                ptrdiff_t src_stride,
+                                uint16_t* dst,
+                                ptrdiff_t dst_stride,
+                                const InterpKernel* filter,
+                                int x0_q4,
+                                int x_step_q4,
+                                int y0_q4,
+                                int y_step_q4,
+                                int w,
+                                int h,
+                                int bd);
+#define vpx_highbd_convolve_copy vpx_highbd_convolve_copy_c
+
+void vpx_highbd_d117_predictor_16x16_c(uint16_t* dst,
+                                       ptrdiff_t stride,
+                                       const uint16_t* above,
+                                       const uint16_t* left,
+                                       int bd);
+#define vpx_highbd_d117_predictor_16x16 vpx_highbd_d117_predictor_16x16_c
+
+void vpx_highbd_d117_predictor_32x32_c(uint16_t* dst,
+                                       ptrdiff_t stride,
+                                       const uint16_t* above,
+                                       const uint16_t* left,
+                                       int bd);
+#define vpx_highbd_d117_predictor_32x32 vpx_highbd_d117_predictor_32x32_c
+
+void vpx_highbd_d117_predictor_4x4_c(uint16_t* dst,
+                                     ptrdiff_t stride,
+                                     const uint16_t* above,
+                                     const uint16_t* left,
+                                     int bd);
+#define vpx_highbd_d117_predictor_4x4 vpx_highbd_d117_predictor_4x4_c
+
+void vpx_highbd_d117_predictor_8x8_c(uint16_t* dst,
+                                     ptrdiff_t stride,
+                                     const uint16_t* above,
+                                     const uint16_t* left,
+                                     int bd);
+#define vpx_highbd_d117_predictor_8x8 vpx_highbd_d117_predictor_8x8_c
+
+void vpx_highbd_d135_predictor_16x16_c(uint16_t* dst,
+                                       ptrdiff_t stride,
+                                       const uint16_t* above,
+                                       const uint16_t* left,
+                                       int bd);
+#define vpx_highbd_d135_predictor_16x16 vpx_highbd_d135_predictor_16x16_c
+
+void vpx_highbd_d135_predictor_32x32_c(uint16_t* dst,
+                                       ptrdiff_t stride,
+                                       const uint16_t* above,
+                                       const uint16_t* left,
+                                       int bd);
+#define vpx_highbd_d135_predictor_32x32 vpx_highbd_d135_predictor_32x32_c
+
+void vpx_highbd_d135_predictor_4x4_c(uint16_t* dst,
+                                     ptrdiff_t stride,
+                                     const uint16_t* above,
+                                     const uint16_t* left,
+                                     int bd);
+#define vpx_highbd_d135_predictor_4x4 vpx_highbd_d135_predictor_4x4_c
+
+void vpx_highbd_d135_predictor_8x8_c(uint16_t* dst,
+                                     ptrdiff_t stride,
+                                     const uint16_t* above,
+                                     const uint16_t* left,
+                                     int bd);
+#define vpx_highbd_d135_predictor_8x8 vpx_highbd_d135_predictor_8x8_c
+
+void vpx_highbd_d153_predictor_16x16_c(uint16_t* dst,
+                                       ptrdiff_t stride,
+                                       const uint16_t* above,
+                                       const uint16_t* left,
+                                       int bd);
+#define vpx_highbd_d153_predictor_16x16 vpx_highbd_d153_predictor_16x16_c
+
+void vpx_highbd_d153_predictor_32x32_c(uint16_t* dst,
+                                       ptrdiff_t stride,
+                                       const uint16_t* above,
+                                       const uint16_t* left,
+                                       int bd);
+#define vpx_highbd_d153_predictor_32x32 vpx_highbd_d153_predictor_32x32_c
+
+void vpx_highbd_d153_predictor_4x4_c(uint16_t* dst,
+                                     ptrdiff_t stride,
+                                     const uint16_t* above,
+                                     const uint16_t* left,
+                                     int bd);
+#define vpx_highbd_d153_predictor_4x4 vpx_highbd_d153_predictor_4x4_c
+
+void vpx_highbd_d153_predictor_8x8_c(uint16_t* dst,
+                                     ptrdiff_t stride,
+                                     const uint16_t* above,
+                                     const uint16_t* left,
+                                     int bd);
+#define vpx_highbd_d153_predictor_8x8 vpx_highbd_d153_predictor_8x8_c
+
+void vpx_highbd_d207_predictor_16x16_c(uint16_t* dst,
+                                       ptrdiff_t stride,
+                                       const uint16_t* above,
+                                       const uint16_t* left,
+                                       int bd);
+#define vpx_highbd_d207_predictor_16x16 vpx_highbd_d207_predictor_16x16_c
+
+void vpx_highbd_d207_predictor_32x32_c(uint16_t* dst,
+                                       ptrdiff_t stride,
+                                       const uint16_t* above,
+                                       const uint16_t* left,
+                                       int bd);
+#define vpx_highbd_d207_predictor_32x32 vpx_highbd_d207_predictor_32x32_c
+
+void vpx_highbd_d207_predictor_4x4_c(uint16_t* dst,
+                                     ptrdiff_t stride,
+                                     const uint16_t* above,
+                                     const uint16_t* left,
+                                     int bd);
+#define vpx_highbd_d207_predictor_4x4 vpx_highbd_d207_predictor_4x4_c
+
+void vpx_highbd_d207_predictor_8x8_c(uint16_t* dst,
+                                     ptrdiff_t stride,
+                                     const uint16_t* above,
+                                     const uint16_t* left,
+                                     int bd);
+#define vpx_highbd_d207_predictor_8x8 vpx_highbd_d207_predictor_8x8_c
+
+void vpx_highbd_d45_predictor_16x16_c(uint16_t* dst,
+                                      ptrdiff_t stride,
+                                      const uint16_t* above,
+                                      const uint16_t* left,
+                                      int bd);
+#define vpx_highbd_d45_predictor_16x16 vpx_highbd_d45_predictor_16x16_c
+
+void vpx_highbd_d45_predictor_32x32_c(uint16_t* dst,
+                                      ptrdiff_t stride,
+                                      const uint16_t* above,
+                                      const uint16_t* left,
+                                      int bd);
+#define vpx_highbd_d45_predictor_32x32 vpx_highbd_d45_predictor_32x32_c
+
+void vpx_highbd_d45_predictor_4x4_c(uint16_t* dst,
+                                    ptrdiff_t stride,
+                                    const uint16_t* above,
+                                    const uint16_t* left,
+                                    int bd);
+#define vpx_highbd_d45_predictor_4x4 vpx_highbd_d45_predictor_4x4_c
+
+void vpx_highbd_d45_predictor_8x8_c(uint16_t* dst,
+                                    ptrdiff_t stride,
+                                    const uint16_t* above,
+                                    const uint16_t* left,
+                                    int bd);
+#define vpx_highbd_d45_predictor_8x8 vpx_highbd_d45_predictor_8x8_c
+
+void vpx_highbd_d63_predictor_16x16_c(uint16_t* dst,
+                                      ptrdiff_t stride,
+                                      const uint16_t* above,
+                                      const uint16_t* left,
+                                      int bd);
+#define vpx_highbd_d63_predictor_16x16 vpx_highbd_d63_predictor_16x16_c
+
+void vpx_highbd_d63_predictor_32x32_c(uint16_t* dst,
+                                      ptrdiff_t stride,
+                                      const uint16_t* above,
+                                      const uint16_t* left,
+                                      int bd);
+#define vpx_highbd_d63_predictor_32x32 vpx_highbd_d63_predictor_32x32_c
+
+void vpx_highbd_d63_predictor_4x4_c(uint16_t* dst,
+                                    ptrdiff_t stride,
+                                    const uint16_t* above,
+                                    const uint16_t* left,
+                                    int bd);
+#define vpx_highbd_d63_predictor_4x4 vpx_highbd_d63_predictor_4x4_c
+
+void vpx_highbd_d63_predictor_8x8_c(uint16_t* dst,
+                                    ptrdiff_t stride,
+                                    const uint16_t* above,
+                                    const uint16_t* left,
+                                    int bd);
+#define vpx_highbd_d63_predictor_8x8 vpx_highbd_d63_predictor_8x8_c
+
+void vpx_highbd_dc_128_predictor_16x16_c(uint16_t* dst,
+                                         ptrdiff_t stride,
+                                         const uint16_t* above,
+                                         const uint16_t* left,
+                                         int bd);
+#define vpx_highbd_dc_128_predictor_16x16 vpx_highbd_dc_128_predictor_16x16_c
+
+void vpx_highbd_dc_128_predictor_32x32_c(uint16_t* dst,
+                                         ptrdiff_t stride,
+                                         const uint16_t* above,
+                                         const uint16_t* left,
+                                         int bd);
+#define vpx_highbd_dc_128_predictor_32x32 vpx_highbd_dc_128_predictor_32x32_c
+
+void vpx_highbd_dc_128_predictor_4x4_c(uint16_t* dst,
+                                       ptrdiff_t stride,
+                                       const uint16_t* above,
+                                       const uint16_t* left,
+                                       int bd);
+#define vpx_highbd_dc_128_predictor_4x4 vpx_highbd_dc_128_predictor_4x4_c
+
+void vpx_highbd_dc_128_predictor_8x8_c(uint16_t* dst,
+                                       ptrdiff_t stride,
+                                       const uint16_t* above,
+                                       const uint16_t* left,
+                                       int bd);
+#define vpx_highbd_dc_128_predictor_8x8 vpx_highbd_dc_128_predictor_8x8_c
+
+void vpx_highbd_dc_left_predictor_16x16_c(uint16_t* dst,
+                                          ptrdiff_t stride,
+                                          const uint16_t* above,
+                                          const uint16_t* left,
+                                          int bd);
+#define vpx_highbd_dc_left_predictor_16x16 vpx_highbd_dc_left_predictor_16x16_c
+
+void vpx_highbd_dc_left_predictor_32x32_c(uint16_t* dst,
+                                          ptrdiff_t stride,
+                                          const uint16_t* above,
+                                          const uint16_t* left,
+                                          int bd);
+#define vpx_highbd_dc_left_predictor_32x32 vpx_highbd_dc_left_predictor_32x32_c
+
+void vpx_highbd_dc_left_predictor_4x4_c(uint16_t* dst,
+                                        ptrdiff_t stride,
+                                        const uint16_t* above,
+                                        const uint16_t* left,
+                                        int bd);
+#define vpx_highbd_dc_left_predictor_4x4 vpx_highbd_dc_left_predictor_4x4_c
+
+void vpx_highbd_dc_left_predictor_8x8_c(uint16_t* dst,
+                                        ptrdiff_t stride,
+                                        const uint16_t* above,
+                                        const uint16_t* left,
+                                        int bd);
+#define vpx_highbd_dc_left_predictor_8x8 vpx_highbd_dc_left_predictor_8x8_c
+
+void vpx_highbd_dc_predictor_16x16_c(uint16_t* dst,
+                                     ptrdiff_t stride,
+                                     const uint16_t* above,
+                                     const uint16_t* left,
+                                     int bd);
+#define vpx_highbd_dc_predictor_16x16 vpx_highbd_dc_predictor_16x16_c
+
+void vpx_highbd_dc_predictor_32x32_c(uint16_t* dst,
+                                     ptrdiff_t stride,
+                                     const uint16_t* above,
+                                     const uint16_t* left,
+                                     int bd);
+#define vpx_highbd_dc_predictor_32x32 vpx_highbd_dc_predictor_32x32_c
+
+void vpx_highbd_dc_predictor_4x4_c(uint16_t* dst,
+                                   ptrdiff_t stride,
+                                   const uint16_t* above,
+                                   const uint16_t* left,
+                                   int bd);
+#define vpx_highbd_dc_predictor_4x4 vpx_highbd_dc_predictor_4x4_c
+
+void vpx_highbd_dc_predictor_8x8_c(uint16_t* dst,
+                                   ptrdiff_t stride,
+                                   const uint16_t* above,
+                                   const uint16_t* left,
+                                   int bd);
+#define vpx_highbd_dc_predictor_8x8 vpx_highbd_dc_predictor_8x8_c
+
+void vpx_highbd_dc_top_predictor_16x16_c(uint16_t* dst,
+                                         ptrdiff_t stride,
+                                         const uint16_t* above,
+                                         const uint16_t* left,
+                                         int bd);
+#define vpx_highbd_dc_top_predictor_16x16 vpx_highbd_dc_top_predictor_16x16_c
+
+void vpx_highbd_dc_top_predictor_32x32_c(uint16_t* dst,
+                                         ptrdiff_t stride,
+                                         const uint16_t* above,
+                                         const uint16_t* left,
+                                         int bd);
+#define vpx_highbd_dc_top_predictor_32x32 vpx_highbd_dc_top_predictor_32x32_c
+
+void vpx_highbd_dc_top_predictor_4x4_c(uint16_t* dst,
+                                       ptrdiff_t stride,
+                                       const uint16_t* above,
+                                       const uint16_t* left,
+                                       int bd);
+#define vpx_highbd_dc_top_predictor_4x4 vpx_highbd_dc_top_predictor_4x4_c
+
+void vpx_highbd_dc_top_predictor_8x8_c(uint16_t* dst,
+                                       ptrdiff_t stride,
+                                       const uint16_t* above,
+                                       const uint16_t* left,
+                                       int bd);
+#define vpx_highbd_dc_top_predictor_8x8 vpx_highbd_dc_top_predictor_8x8_c
+
+void vpx_highbd_fdct16x16_c(const int16_t* input,
+                            tran_low_t* output,
+                            int stride);
+#define vpx_highbd_fdct16x16 vpx_highbd_fdct16x16_c
+
+void vpx_highbd_fdct16x16_1_c(const int16_t* input,
+                              tran_low_t* output,
+                              int stride);
+#define vpx_highbd_fdct16x16_1 vpx_highbd_fdct16x16_1_c
+
+void vpx_highbd_fdct32x32_c(const int16_t* input,
+                            tran_low_t* output,
+                            int stride);
+#define vpx_highbd_fdct32x32 vpx_highbd_fdct32x32_c
+
+void vpx_highbd_fdct32x32_1_c(const int16_t* input,
+                              tran_low_t* output,
+                              int stride);
+#define vpx_highbd_fdct32x32_1 vpx_highbd_fdct32x32_1_c
+
+void vpx_highbd_fdct32x32_rd_c(const int16_t* input,
+                               tran_low_t* output,
+                               int stride);
+#define vpx_highbd_fdct32x32_rd vpx_highbd_fdct32x32_rd_c
+
+void vpx_highbd_fdct4x4_c(const int16_t* input, tran_low_t* output, int stride);
+#define vpx_highbd_fdct4x4 vpx_highbd_fdct4x4_c
+
+void vpx_highbd_fdct8x8_c(const int16_t* input, tran_low_t* output, int stride);
+#define vpx_highbd_fdct8x8 vpx_highbd_fdct8x8_c
+
+void vpx_highbd_fdct8x8_1_c(const int16_t* input,
+                            tran_low_t* output,
+                            int stride);
+#define vpx_highbd_fdct8x8_1 vpx_highbd_fdct8x8_1_c
+
+void vpx_highbd_h_predictor_16x16_c(uint16_t* dst,
+                                    ptrdiff_t stride,
+                                    const uint16_t* above,
+                                    const uint16_t* left,
+                                    int bd);
+#define vpx_highbd_h_predictor_16x16 vpx_highbd_h_predictor_16x16_c
+
+void vpx_highbd_h_predictor_32x32_c(uint16_t* dst,
+                                    ptrdiff_t stride,
+                                    const uint16_t* above,
+                                    const uint16_t* left,
+                                    int bd);
+#define vpx_highbd_h_predictor_32x32 vpx_highbd_h_predictor_32x32_c
+
+void vpx_highbd_h_predictor_4x4_c(uint16_t* dst,
+                                  ptrdiff_t stride,
+                                  const uint16_t* above,
+                                  const uint16_t* left,
+                                  int bd);
+#define vpx_highbd_h_predictor_4x4 vpx_highbd_h_predictor_4x4_c
+
+void vpx_highbd_h_predictor_8x8_c(uint16_t* dst,
+                                  ptrdiff_t stride,
+                                  const uint16_t* above,
+                                  const uint16_t* left,
+                                  int bd);
+#define vpx_highbd_h_predictor_8x8 vpx_highbd_h_predictor_8x8_c
+
+void vpx_highbd_hadamard_16x16_c(const int16_t* src_diff,
+                                 ptrdiff_t src_stride,
+                                 tran_low_t* coeff);
+#define vpx_highbd_hadamard_16x16 vpx_highbd_hadamard_16x16_c
+
+void vpx_highbd_hadamard_32x32_c(const int16_t* src_diff,
+                                 ptrdiff_t src_stride,
+                                 tran_low_t* coeff);
+#define vpx_highbd_hadamard_32x32 vpx_highbd_hadamard_32x32_c
+
+void vpx_highbd_hadamard_8x8_c(const int16_t* src_diff,
+                               ptrdiff_t src_stride,
+                               tran_low_t* coeff);
+#define vpx_highbd_hadamard_8x8 vpx_highbd_hadamard_8x8_c
+
+void vpx_highbd_idct16x16_10_add_c(const tran_low_t* input,
+                                   uint16_t* dest,
+                                   int stride,
+                                   int bd);
+#define vpx_highbd_idct16x16_10_add vpx_highbd_idct16x16_10_add_c
+
+void vpx_highbd_idct16x16_1_add_c(const tran_low_t* input,
+                                  uint16_t* dest,
+                                  int stride,
+                                  int bd);
+#define vpx_highbd_idct16x16_1_add vpx_highbd_idct16x16_1_add_c
+
+void vpx_highbd_idct16x16_256_add_c(const tran_low_t* input,
+                                    uint16_t* dest,
+                                    int stride,
+                                    int bd);
+#define vpx_highbd_idct16x16_256_add vpx_highbd_idct16x16_256_add_c
+
+void vpx_highbd_idct16x16_38_add_c(const tran_low_t* input,
+                                   uint16_t* dest,
+                                   int stride,
+                                   int bd);
+#define vpx_highbd_idct16x16_38_add vpx_highbd_idct16x16_38_add_c
+
+void vpx_highbd_idct32x32_1024_add_c(const tran_low_t* input,
+                                     uint16_t* dest,
+                                     int stride,
+                                     int bd);
+#define vpx_highbd_idct32x32_1024_add vpx_highbd_idct32x32_1024_add_c
+
+void vpx_highbd_idct32x32_135_add_c(const tran_low_t* input,
+                                    uint16_t* dest,
+                                    int stride,
+                                    int bd);
+#define vpx_highbd_idct32x32_135_add vpx_highbd_idct32x32_135_add_c
+
+void vpx_highbd_idct32x32_1_add_c(const tran_low_t* input,
+                                  uint16_t* dest,
+                                  int stride,
+                                  int bd);
+#define vpx_highbd_idct32x32_1_add vpx_highbd_idct32x32_1_add_c
+
+void vpx_highbd_idct32x32_34_add_c(const tran_low_t* input,
+                                   uint16_t* dest,
+                                   int stride,
+                                   int bd);
+#define vpx_highbd_idct32x32_34_add vpx_highbd_idct32x32_34_add_c
+
+void vpx_highbd_idct4x4_16_add_c(const tran_low_t* input,
+                                 uint16_t* dest,
+                                 int stride,
+                                 int bd);
+#define vpx_highbd_idct4x4_16_add vpx_highbd_idct4x4_16_add_c
+
+void vpx_highbd_idct4x4_1_add_c(const tran_low_t* input,
+                                uint16_t* dest,
+                                int stride,
+                                int bd);
+#define vpx_highbd_idct4x4_1_add vpx_highbd_idct4x4_1_add_c
+
+void vpx_highbd_idct8x8_12_add_c(const tran_low_t* input,
+                                 uint16_t* dest,
+                                 int stride,
+                                 int bd);
+#define vpx_highbd_idct8x8_12_add vpx_highbd_idct8x8_12_add_c
+
+void vpx_highbd_idct8x8_1_add_c(const tran_low_t* input,
+                                uint16_t* dest,
+                                int stride,
+                                int bd);
+#define vpx_highbd_idct8x8_1_add vpx_highbd_idct8x8_1_add_c
+
+void vpx_highbd_idct8x8_64_add_c(const tran_low_t* input,
+                                 uint16_t* dest,
+                                 int stride,
+                                 int bd);
+#define vpx_highbd_idct8x8_64_add vpx_highbd_idct8x8_64_add_c
+
+void vpx_highbd_iwht4x4_16_add_c(const tran_low_t* input,
+                                 uint16_t* dest,
+                                 int stride,
+                                 int bd);
+#define vpx_highbd_iwht4x4_16_add vpx_highbd_iwht4x4_16_add_c
+
+void vpx_highbd_iwht4x4_1_add_c(const tran_low_t* input,
+                                uint16_t* dest,
+                                int stride,
+                                int bd);
+#define vpx_highbd_iwht4x4_1_add vpx_highbd_iwht4x4_1_add_c
+
+void vpx_highbd_lpf_horizontal_16_c(uint16_t* s,
+                                    int pitch,
+                                    const uint8_t* blimit,
+                                    const uint8_t* limit,
+                                    const uint8_t* thresh,
+                                    int bd);
+#define vpx_highbd_lpf_horizontal_16 vpx_highbd_lpf_horizontal_16_c
+
+void vpx_highbd_lpf_horizontal_16_dual_c(uint16_t* s,
+                                         int pitch,
+                                         const uint8_t* blimit,
+                                         const uint8_t* limit,
+                                         const uint8_t* thresh,
+                                         int bd);
+#define vpx_highbd_lpf_horizontal_16_dual vpx_highbd_lpf_horizontal_16_dual_c
+
+void vpx_highbd_lpf_horizontal_4_c(uint16_t* s,
+                                   int pitch,
+                                   const uint8_t* blimit,
+                                   const uint8_t* limit,
+                                   const uint8_t* thresh,
+                                   int bd);
+#define vpx_highbd_lpf_horizontal_4 vpx_highbd_lpf_horizontal_4_c
+
+void vpx_highbd_lpf_horizontal_4_dual_c(uint16_t* s,
+                                        int pitch,
+                                        const uint8_t* blimit0,
+                                        const uint8_t* limit0,
+                                        const uint8_t* thresh0,
+                                        const uint8_t* blimit1,
+                                        const uint8_t* limit1,
+                                        const uint8_t* thresh1,
+                                        int bd);
+#define vpx_highbd_lpf_horizontal_4_dual vpx_highbd_lpf_horizontal_4_dual_c
+
+void vpx_highbd_lpf_horizontal_8_c(uint16_t* s,
+                                   int pitch,
+                                   const uint8_t* blimit,
+                                   const uint8_t* limit,
+                                   const uint8_t* thresh,
+                                   int bd);
+#define vpx_highbd_lpf_horizontal_8 vpx_highbd_lpf_horizontal_8_c
+
+void vpx_highbd_lpf_horizontal_8_dual_c(uint16_t* s,
+                                        int pitch,
+                                        const uint8_t* blimit0,
+                                        const uint8_t* limit0,
+                                        const uint8_t* thresh0,
+                                        const uint8_t* blimit1,
+                                        const uint8_t* limit1,
+                                        const uint8_t* thresh1,
+                                        int bd);
+#define vpx_highbd_lpf_horizontal_8_dual vpx_highbd_lpf_horizontal_8_dual_c
+
+void vpx_highbd_lpf_vertical_16_c(uint16_t* s,
+                                  int pitch,
+                                  const uint8_t* blimit,
+                                  const uint8_t* limit,
+                                  const uint8_t* thresh,
+                                  int bd);
+#define vpx_highbd_lpf_vertical_16 vpx_highbd_lpf_vertical_16_c
+
+void vpx_highbd_lpf_vertical_16_dual_c(uint16_t* s,
+                                       int pitch,
+                                       const uint8_t* blimit,
+                                       const uint8_t* limit,
+                                       const uint8_t* thresh,
+                                       int bd);
+#define vpx_highbd_lpf_vertical_16_dual vpx_highbd_lpf_vertical_16_dual_c
+
+void vpx_highbd_lpf_vertical_4_c(uint16_t* s,
+                                 int pitch,
+                                 const uint8_t* blimit,
+                                 const uint8_t* limit,
+                                 const uint8_t* thresh,
+                                 int bd);
+#define vpx_highbd_lpf_vertical_4 vpx_highbd_lpf_vertical_4_c
+
+void vpx_highbd_lpf_vertical_4_dual_c(uint16_t* s,
+                                      int pitch,
+                                      const uint8_t* blimit0,
+                                      const uint8_t* limit0,
+                                      const uint8_t* thresh0,
+                                      const uint8_t* blimit1,
+                                      const uint8_t* limit1,
+                                      const uint8_t* thresh1,
+                                      int bd);
+#define vpx_highbd_lpf_vertical_4_dual vpx_highbd_lpf_vertical_4_dual_c
+
+void vpx_highbd_lpf_vertical_8_c(uint16_t* s,
+                                 int pitch,
+                                 const uint8_t* blimit,
+                                 const uint8_t* limit,
+                                 const uint8_t* thresh,
+                                 int bd);
+#define vpx_highbd_lpf_vertical_8 vpx_highbd_lpf_vertical_8_c
+
+void vpx_highbd_lpf_vertical_8_dual_c(uint16_t* s,
+                                      int pitch,
+                                      const uint8_t* blimit0,
+                                      const uint8_t* limit0,
+                                      const uint8_t* thresh0,
+                                      const uint8_t* blimit1,
+                                      const uint8_t* limit1,
+                                      const uint8_t* thresh1,
+                                      int bd);
+#define vpx_highbd_lpf_vertical_8_dual vpx_highbd_lpf_vertical_8_dual_c
+
+void vpx_highbd_minmax_8x8_c(const uint8_t* s8,
+                             int p,
+                             const uint8_t* d8,
+                             int dp,
+                             int* min,
+                             int* max);
+#define vpx_highbd_minmax_8x8 vpx_highbd_minmax_8x8_c
+
+void vpx_highbd_quantize_b_c(const tran_low_t* coeff_ptr,
+                             intptr_t n_coeffs,
+                             int skip_block,
+                             const int16_t* zbin_ptr,
+                             const int16_t* round_ptr,
+                             const int16_t* quant_ptr,
+                             const int16_t* quant_shift_ptr,
+                             tran_low_t* qcoeff_ptr,
+                             tran_low_t* dqcoeff_ptr,
+                             const int16_t* dequant_ptr,
+                             uint16_t* eob_ptr,
+                             const int16_t* scan,
+                             const int16_t* iscan);
+#define vpx_highbd_quantize_b vpx_highbd_quantize_b_c
+
+void vpx_highbd_quantize_b_32x32_c(const tran_low_t* coeff_ptr,
+                                   intptr_t n_coeffs,
+                                   int skip_block,
+                                   const int16_t* zbin_ptr,
+                                   const int16_t* round_ptr,
+                                   const int16_t* quant_ptr,
+                                   const int16_t* quant_shift_ptr,
+                                   tran_low_t* qcoeff_ptr,
+                                   tran_low_t* dqcoeff_ptr,
+                                   const int16_t* dequant_ptr,
+                                   uint16_t* eob_ptr,
+                                   const int16_t* scan,
+                                   const int16_t* iscan);
+#define vpx_highbd_quantize_b_32x32 vpx_highbd_quantize_b_32x32_c
+
+unsigned int vpx_highbd_sad16x16_c(const uint8_t* src_ptr,
+                                   int src_stride,
+                                   const uint8_t* ref_ptr,
+                                   int ref_stride);
+#define vpx_highbd_sad16x16 vpx_highbd_sad16x16_c
+
+unsigned int vpx_highbd_sad16x16_avg_c(const uint8_t* src_ptr,
+                                       int src_stride,
+                                       const uint8_t* ref_ptr,
+                                       int ref_stride,
+                                       const uint8_t* second_pred);
+#define vpx_highbd_sad16x16_avg vpx_highbd_sad16x16_avg_c
+
+void vpx_highbd_sad16x16x4d_c(const uint8_t* src_ptr,
+                              int src_stride,
+                              const uint8_t* const ref_array[],
+                              int ref_stride,
+                              uint32_t* sad_array);
+#define vpx_highbd_sad16x16x4d vpx_highbd_sad16x16x4d_c
+
+unsigned int vpx_highbd_sad16x32_c(const uint8_t* src_ptr,
+                                   int src_stride,
+                                   const uint8_t* ref_ptr,
+                                   int ref_stride);
+#define vpx_highbd_sad16x32 vpx_highbd_sad16x32_c
+
+unsigned int vpx_highbd_sad16x32_avg_c(const uint8_t* src_ptr,
+                                       int src_stride,
+                                       const uint8_t* ref_ptr,
+                                       int ref_stride,
+                                       const uint8_t* second_pred);
+#define vpx_highbd_sad16x32_avg vpx_highbd_sad16x32_avg_c
+
+void vpx_highbd_sad16x32x4d_c(const uint8_t* src_ptr,
+                              int src_stride,
+                              const uint8_t* const ref_array[],
+                              int ref_stride,
+                              uint32_t* sad_array);
+#define vpx_highbd_sad16x32x4d vpx_highbd_sad16x32x4d_c
+
+unsigned int vpx_highbd_sad16x8_c(const uint8_t* src_ptr,
+                                  int src_stride,
+                                  const uint8_t* ref_ptr,
+                                  int ref_stride);
+#define vpx_highbd_sad16x8 vpx_highbd_sad16x8_c
+
+unsigned int vpx_highbd_sad16x8_avg_c(const uint8_t* src_ptr,
+                                      int src_stride,
+                                      const uint8_t* ref_ptr,
+                                      int ref_stride,
+                                      const uint8_t* second_pred);
+#define vpx_highbd_sad16x8_avg vpx_highbd_sad16x8_avg_c
+
+void vpx_highbd_sad16x8x4d_c(const uint8_t* src_ptr,
+                             int src_stride,
+                             const uint8_t* const ref_array[],
+                             int ref_stride,
+                             uint32_t* sad_array);
+#define vpx_highbd_sad16x8x4d vpx_highbd_sad16x8x4d_c
+
+unsigned int vpx_highbd_sad32x16_c(const uint8_t* src_ptr,
+                                   int src_stride,
+                                   const uint8_t* ref_ptr,
+                                   int ref_stride);
+#define vpx_highbd_sad32x16 vpx_highbd_sad32x16_c
+
+unsigned int vpx_highbd_sad32x16_avg_c(const uint8_t* src_ptr,
+                                       int src_stride,
+                                       const uint8_t* ref_ptr,
+                                       int ref_stride,
+                                       const uint8_t* second_pred);
+#define vpx_highbd_sad32x16_avg vpx_highbd_sad32x16_avg_c
+
+void vpx_highbd_sad32x16x4d_c(const uint8_t* src_ptr,
+                              int src_stride,
+                              const uint8_t* const ref_array[],
+                              int ref_stride,
+                              uint32_t* sad_array);
+#define vpx_highbd_sad32x16x4d vpx_highbd_sad32x16x4d_c
+
+unsigned int vpx_highbd_sad32x32_c(const uint8_t* src_ptr,
+                                   int src_stride,
+                                   const uint8_t* ref_ptr,
+                                   int ref_stride);
+#define vpx_highbd_sad32x32 vpx_highbd_sad32x32_c
+
+unsigned int vpx_highbd_sad32x32_avg_c(const uint8_t* src_ptr,
+                                       int src_stride,
+                                       const uint8_t* ref_ptr,
+                                       int ref_stride,
+                                       const uint8_t* second_pred);
+#define vpx_highbd_sad32x32_avg vpx_highbd_sad32x32_avg_c
+
+void vpx_highbd_sad32x32x4d_c(const uint8_t* src_ptr,
+                              int src_stride,
+                              const uint8_t* const ref_array[],
+                              int ref_stride,
+                              uint32_t* sad_array);
+#define vpx_highbd_sad32x32x4d vpx_highbd_sad32x32x4d_c
+
+unsigned int vpx_highbd_sad32x64_c(const uint8_t* src_ptr,
+                                   int src_stride,
+                                   const uint8_t* ref_ptr,
+                                   int ref_stride);
+#define vpx_highbd_sad32x64 vpx_highbd_sad32x64_c
+
+unsigned int vpx_highbd_sad32x64_avg_c(const uint8_t* src_ptr,
+                                       int src_stride,
+                                       const uint8_t* ref_ptr,
+                                       int ref_stride,
+                                       const uint8_t* second_pred);
+#define vpx_highbd_sad32x64_avg vpx_highbd_sad32x64_avg_c
+
+void vpx_highbd_sad32x64x4d_c(const uint8_t* src_ptr,
+                              int src_stride,
+                              const uint8_t* const ref_array[],
+                              int ref_stride,
+                              uint32_t* sad_array);
+#define vpx_highbd_sad32x64x4d vpx_highbd_sad32x64x4d_c
+
+unsigned int vpx_highbd_sad4x4_c(const uint8_t* src_ptr,
+                                 int src_stride,
+                                 const uint8_t* ref_ptr,
+                                 int ref_stride);
+#define vpx_highbd_sad4x4 vpx_highbd_sad4x4_c
+
+unsigned int vpx_highbd_sad4x4_avg_c(const uint8_t* src_ptr,
+                                     int src_stride,
+                                     const uint8_t* ref_ptr,
+                                     int ref_stride,
+                                     const uint8_t* second_pred);
+#define vpx_highbd_sad4x4_avg vpx_highbd_sad4x4_avg_c
+
+void vpx_highbd_sad4x4x4d_c(const uint8_t* src_ptr,
+                            int src_stride,
+                            const uint8_t* const ref_array[],
+                            int ref_stride,
+                            uint32_t* sad_array);
+#define vpx_highbd_sad4x4x4d vpx_highbd_sad4x4x4d_c
+
+unsigned int vpx_highbd_sad4x8_c(const uint8_t* src_ptr,
+                                 int src_stride,
+                                 const uint8_t* ref_ptr,
+                                 int ref_stride);
+#define vpx_highbd_sad4x8 vpx_highbd_sad4x8_c
+
+unsigned int vpx_highbd_sad4x8_avg_c(const uint8_t* src_ptr,
+                                     int src_stride,
+                                     const uint8_t* ref_ptr,
+                                     int ref_stride,
+                                     const uint8_t* second_pred);
+#define vpx_highbd_sad4x8_avg vpx_highbd_sad4x8_avg_c
+
+void vpx_highbd_sad4x8x4d_c(const uint8_t* src_ptr,
+                            int src_stride,
+                            const uint8_t* const ref_array[],
+                            int ref_stride,
+                            uint32_t* sad_array);
+#define vpx_highbd_sad4x8x4d vpx_highbd_sad4x8x4d_c
+
+unsigned int vpx_highbd_sad64x32_c(const uint8_t* src_ptr,
+                                   int src_stride,
+                                   const uint8_t* ref_ptr,
+                                   int ref_stride);
+#define vpx_highbd_sad64x32 vpx_highbd_sad64x32_c
+
+unsigned int vpx_highbd_sad64x32_avg_c(const uint8_t* src_ptr,
+                                       int src_stride,
+                                       const uint8_t* ref_ptr,
+                                       int ref_stride,
+                                       const uint8_t* second_pred);
+#define vpx_highbd_sad64x32_avg vpx_highbd_sad64x32_avg_c
+
+void vpx_highbd_sad64x32x4d_c(const uint8_t* src_ptr,
+                              int src_stride,
+                              const uint8_t* const ref_array[],
+                              int ref_stride,
+                              uint32_t* sad_array);
+#define vpx_highbd_sad64x32x4d vpx_highbd_sad64x32x4d_c
+
+unsigned int vpx_highbd_sad64x64_c(const uint8_t* src_ptr,
+                                   int src_stride,
+                                   const uint8_t* ref_ptr,
+                                   int ref_stride);
+#define vpx_highbd_sad64x64 vpx_highbd_sad64x64_c
+
+unsigned int vpx_highbd_sad64x64_avg_c(const uint8_t* src_ptr,
+                                       int src_stride,
+                                       const uint8_t* ref_ptr,
+                                       int ref_stride,
+                                       const uint8_t* second_pred);
+#define vpx_highbd_sad64x64_avg vpx_highbd_sad64x64_avg_c
+
+void vpx_highbd_sad64x64x4d_c(const uint8_t* src_ptr,
+                              int src_stride,
+                              const uint8_t* const ref_array[],
+                              int ref_stride,
+                              uint32_t* sad_array);
+#define vpx_highbd_sad64x64x4d vpx_highbd_sad64x64x4d_c
+
+unsigned int vpx_highbd_sad8x16_c(const uint8_t* src_ptr,
+                                  int src_stride,
+                                  const uint8_t* ref_ptr,
+                                  int ref_stride);
+#define vpx_highbd_sad8x16 vpx_highbd_sad8x16_c
+
+unsigned int vpx_highbd_sad8x16_avg_c(const uint8_t* src_ptr,
+                                      int src_stride,
+                                      const uint8_t* ref_ptr,
+                                      int ref_stride,
+                                      const uint8_t* second_pred);
+#define vpx_highbd_sad8x16_avg vpx_highbd_sad8x16_avg_c
+
+void vpx_highbd_sad8x16x4d_c(const uint8_t* src_ptr,
+                             int src_stride,
+                             const uint8_t* const ref_array[],
+                             int ref_stride,
+                             uint32_t* sad_array);
+#define vpx_highbd_sad8x16x4d vpx_highbd_sad8x16x4d_c
+
+unsigned int vpx_highbd_sad8x4_c(const uint8_t* src_ptr,
+                                 int src_stride,
+                                 const uint8_t* ref_ptr,
+                                 int ref_stride);
+#define vpx_highbd_sad8x4 vpx_highbd_sad8x4_c
+
+unsigned int vpx_highbd_sad8x4_avg_c(const uint8_t* src_ptr,
+                                     int src_stride,
+                                     const uint8_t* ref_ptr,
+                                     int ref_stride,
+                                     const uint8_t* second_pred);
+#define vpx_highbd_sad8x4_avg vpx_highbd_sad8x4_avg_c
+
+void vpx_highbd_sad8x4x4d_c(const uint8_t* src_ptr,
+                            int src_stride,
+                            const uint8_t* const ref_array[],
+                            int ref_stride,
+                            uint32_t* sad_array);
+#define vpx_highbd_sad8x4x4d vpx_highbd_sad8x4x4d_c
+
+unsigned int vpx_highbd_sad8x8_c(const uint8_t* src_ptr,
+                                 int src_stride,
+                                 const uint8_t* ref_ptr,
+                                 int ref_stride);
+#define vpx_highbd_sad8x8 vpx_highbd_sad8x8_c
+
+unsigned int vpx_highbd_sad8x8_avg_c(const uint8_t* src_ptr,
+                                     int src_stride,
+                                     const uint8_t* ref_ptr,
+                                     int ref_stride,
+                                     const uint8_t* second_pred);
+#define vpx_highbd_sad8x8_avg vpx_highbd_sad8x8_avg_c
+
+void vpx_highbd_sad8x8x4d_c(const uint8_t* src_ptr,
+                            int src_stride,
+                            const uint8_t* const ref_array[],
+                            int ref_stride,
+                            uint32_t* sad_array);
+#define vpx_highbd_sad8x8x4d vpx_highbd_sad8x8x4d_c
+
+int vpx_highbd_satd_c(const tran_low_t* coeff, int length);
+#define vpx_highbd_satd vpx_highbd_satd_c
+
+void vpx_highbd_subtract_block_c(int rows,
+                                 int cols,
+                                 int16_t* diff_ptr,
+                                 ptrdiff_t diff_stride,
+                                 const uint8_t* src8_ptr,
+                                 ptrdiff_t src_stride,
+                                 const uint8_t* pred8_ptr,
+                                 ptrdiff_t pred_stride,
+                                 int bd);
+#define vpx_highbd_subtract_block vpx_highbd_subtract_block_c
+
+void vpx_highbd_tm_predictor_16x16_c(uint16_t* dst,
+                                     ptrdiff_t stride,
+                                     const uint16_t* above,
+                                     const uint16_t* left,
+                                     int bd);
+#define vpx_highbd_tm_predictor_16x16 vpx_highbd_tm_predictor_16x16_c
+
+void vpx_highbd_tm_predictor_32x32_c(uint16_t* dst,
+                                     ptrdiff_t stride,
+                                     const uint16_t* above,
+                                     const uint16_t* left,
+                                     int bd);
+#define vpx_highbd_tm_predictor_32x32 vpx_highbd_tm_predictor_32x32_c
+
+void vpx_highbd_tm_predictor_4x4_c(uint16_t* dst,
+                                   ptrdiff_t stride,
+                                   const uint16_t* above,
+                                   const uint16_t* left,
+                                   int bd);
+#define vpx_highbd_tm_predictor_4x4 vpx_highbd_tm_predictor_4x4_c
+
+void vpx_highbd_tm_predictor_8x8_c(uint16_t* dst,
+                                   ptrdiff_t stride,
+                                   const uint16_t* above,
+                                   const uint16_t* left,
+                                   int bd);
+#define vpx_highbd_tm_predictor_8x8 vpx_highbd_tm_predictor_8x8_c
+
+void vpx_highbd_v_predictor_16x16_c(uint16_t* dst,
+                                    ptrdiff_t stride,
+                                    const uint16_t* above,
+                                    const uint16_t* left,
+                                    int bd);
+#define vpx_highbd_v_predictor_16x16 vpx_highbd_v_predictor_16x16_c
+
+void vpx_highbd_v_predictor_32x32_c(uint16_t* dst,
+                                    ptrdiff_t stride,
+                                    const uint16_t* above,
+                                    const uint16_t* left,
+                                    int bd);
+#define vpx_highbd_v_predictor_32x32 vpx_highbd_v_predictor_32x32_c
+
+void vpx_highbd_v_predictor_4x4_c(uint16_t* dst,
+                                  ptrdiff_t stride,
+                                  const uint16_t* above,
+                                  const uint16_t* left,
+                                  int bd);
+#define vpx_highbd_v_predictor_4x4 vpx_highbd_v_predictor_4x4_c
+
+void vpx_highbd_v_predictor_8x8_c(uint16_t* dst,
+                                  ptrdiff_t stride,
+                                  const uint16_t* above,
+                                  const uint16_t* left,
+                                  int bd);
+#define vpx_highbd_v_predictor_8x8 vpx_highbd_v_predictor_8x8_c
+
+void vpx_idct16x16_10_add_c(const tran_low_t* input, uint8_t* dest, int stride);
+#define vpx_idct16x16_10_add vpx_idct16x16_10_add_c
+
+void vpx_idct16x16_1_add_c(const tran_low_t* input, uint8_t* dest, int stride);
+#define vpx_idct16x16_1_add vpx_idct16x16_1_add_c
+
+void vpx_idct16x16_256_add_c(const tran_low_t* input,
+                             uint8_t* dest,
+                             int stride);
+#define vpx_idct16x16_256_add vpx_idct16x16_256_add_c
+
+void vpx_idct16x16_38_add_c(const tran_low_t* input, uint8_t* dest, int stride);
+#define vpx_idct16x16_38_add vpx_idct16x16_38_add_c
+
+void vpx_idct32x32_1024_add_c(const tran_low_t* input,
+                              uint8_t* dest,
+                              int stride);
+#define vpx_idct32x32_1024_add vpx_idct32x32_1024_add_c
+
+void vpx_idct32x32_135_add_c(const tran_low_t* input,
+                             uint8_t* dest,
+                             int stride);
+#define vpx_idct32x32_135_add vpx_idct32x32_135_add_c
+
+void vpx_idct32x32_1_add_c(const tran_low_t* input, uint8_t* dest, int stride);
+#define vpx_idct32x32_1_add vpx_idct32x32_1_add_c
+
+void vpx_idct32x32_34_add_c(const tran_low_t* input, uint8_t* dest, int stride);
+#define vpx_idct32x32_34_add vpx_idct32x32_34_add_c
+
+void vpx_idct4x4_16_add_c(const tran_low_t* input, uint8_t* dest, int stride);
+#define vpx_idct4x4_16_add vpx_idct4x4_16_add_c
+
+void vpx_idct4x4_1_add_c(const tran_low_t* input, uint8_t* dest, int stride);
+#define vpx_idct4x4_1_add vpx_idct4x4_1_add_c
+
+void vpx_idct8x8_12_add_c(const tran_low_t* input, uint8_t* dest, int stride);
+#define vpx_idct8x8_12_add vpx_idct8x8_12_add_c
+
+void vpx_idct8x8_1_add_c(const tran_low_t* input, uint8_t* dest, int stride);
+#define vpx_idct8x8_1_add vpx_idct8x8_1_add_c
+
+void vpx_idct8x8_64_add_c(const tran_low_t* input, uint8_t* dest, int stride);
+#define vpx_idct8x8_64_add vpx_idct8x8_64_add_c
+
+int16_t vpx_int_pro_col_c(const uint8_t* ref, const int width);
+#define vpx_int_pro_col vpx_int_pro_col_c
+
+void vpx_int_pro_row_c(int16_t* hbuf,
+                       const uint8_t* ref,
+                       const int ref_stride,
+                       const int height);
+#define vpx_int_pro_row vpx_int_pro_row_c
+
+void vpx_iwht4x4_16_add_c(const tran_low_t* input, uint8_t* dest, int stride);
+#define vpx_iwht4x4_16_add vpx_iwht4x4_16_add_c
+
+void vpx_iwht4x4_1_add_c(const tran_low_t* input, uint8_t* dest, int stride);
+#define vpx_iwht4x4_1_add vpx_iwht4x4_1_add_c
+
+void vpx_lpf_horizontal_16_c(uint8_t* s,
+                             int pitch,
+                             const uint8_t* blimit,
+                             const uint8_t* limit,
+                             const uint8_t* thresh);
+#define vpx_lpf_horizontal_16 vpx_lpf_horizontal_16_c
+
+void vpx_lpf_horizontal_16_dual_c(uint8_t* s,
+                                  int pitch,
+                                  const uint8_t* blimit,
+                                  const uint8_t* limit,
+                                  const uint8_t* thresh);
+#define vpx_lpf_horizontal_16_dual vpx_lpf_horizontal_16_dual_c
+
+void vpx_lpf_horizontal_4_c(uint8_t* s,
+                            int pitch,
+                            const uint8_t* blimit,
+                            const uint8_t* limit,
+                            const uint8_t* thresh);
+#define vpx_lpf_horizontal_4 vpx_lpf_horizontal_4_c
+
+void vpx_lpf_horizontal_4_dual_c(uint8_t* s,
+                                 int pitch,
+                                 const uint8_t* blimit0,
+                                 const uint8_t* limit0,
+                                 const uint8_t* thresh0,
+                                 const uint8_t* blimit1,
+                                 const uint8_t* limit1,
+                                 const uint8_t* thresh1);
+#define vpx_lpf_horizontal_4_dual vpx_lpf_horizontal_4_dual_c
+
+void vpx_lpf_horizontal_8_c(uint8_t* s,
+                            int pitch,
+                            const uint8_t* blimit,
+                            const uint8_t* limit,
+                            const uint8_t* thresh);
+#define vpx_lpf_horizontal_8 vpx_lpf_horizontal_8_c
+
+void vpx_lpf_horizontal_8_dual_c(uint8_t* s,
+                                 int pitch,
+                                 const uint8_t* blimit0,
+                                 const uint8_t* limit0,
+                                 const uint8_t* thresh0,
+                                 const uint8_t* blimit1,
+                                 const uint8_t* limit1,
+                                 const uint8_t* thresh1);
+#define vpx_lpf_horizontal_8_dual vpx_lpf_horizontal_8_dual_c
+
+void vpx_lpf_vertical_16_c(uint8_t* s,
+                           int pitch,
+                           const uint8_t* blimit,
+                           const uint8_t* limit,
+                           const uint8_t* thresh);
+#define vpx_lpf_vertical_16 vpx_lpf_vertical_16_c
+
+void vpx_lpf_vertical_16_dual_c(uint8_t* s,
+                                int pitch,
+                                const uint8_t* blimit,
+                                const uint8_t* limit,
+                                const uint8_t* thresh);
+#define vpx_lpf_vertical_16_dual vpx_lpf_vertical_16_dual_c
+
+void vpx_lpf_vertical_4_c(uint8_t* s,
+                          int pitch,
+                          const uint8_t* blimit,
+                          const uint8_t* limit,
+                          const uint8_t* thresh);
+#define vpx_lpf_vertical_4 vpx_lpf_vertical_4_c
+
+void vpx_lpf_vertical_4_dual_c(uint8_t* s,
+                               int pitch,
+                               const uint8_t* blimit0,
+                               const uint8_t* limit0,
+                               const uint8_t* thresh0,
+                               const uint8_t* blimit1,
+                               const uint8_t* limit1,
+                               const uint8_t* thresh1);
+#define vpx_lpf_vertical_4_dual vpx_lpf_vertical_4_dual_c
+
+void vpx_lpf_vertical_8_c(uint8_t* s,
+                          int pitch,
+                          const uint8_t* blimit,
+                          const uint8_t* limit,
+                          const uint8_t* thresh);
+#define vpx_lpf_vertical_8 vpx_lpf_vertical_8_c
+
+void vpx_lpf_vertical_8_dual_c(uint8_t* s,
+                               int pitch,
+                               const uint8_t* blimit0,
+                               const uint8_t* limit0,
+                               const uint8_t* thresh0,
+                               const uint8_t* blimit1,
+                               const uint8_t* limit1,
+                               const uint8_t* thresh1);
+#define vpx_lpf_vertical_8_dual vpx_lpf_vertical_8_dual_c
+
+void vpx_mbpost_proc_across_ip_c(unsigned char* src,
+                                 int pitch,
+                                 int rows,
+                                 int cols,
+                                 int flimit);
+#define vpx_mbpost_proc_across_ip vpx_mbpost_proc_across_ip_c
+
+void vpx_mbpost_proc_down_c(unsigned char* dst,
+                            int pitch,
+                            int rows,
+                            int cols,
+                            int flimit);
+#define vpx_mbpost_proc_down vpx_mbpost_proc_down_c
+
+void vpx_minmax_8x8_c(const uint8_t* s,
+                      int p,
+                      const uint8_t* d,
+                      int dp,
+                      int* min,
+                      int* max);
+#define vpx_minmax_8x8 vpx_minmax_8x8_c
+
+unsigned int vpx_mse16x16_c(const uint8_t* src_ptr,
+                            int src_stride,
+                            const uint8_t* ref_ptr,
+                            int ref_stride,
+                            unsigned int* sse);
+#define vpx_mse16x16 vpx_mse16x16_c
+
+unsigned int vpx_mse16x8_c(const uint8_t* src_ptr,
+                           int src_stride,
+                           const uint8_t* ref_ptr,
+                           int ref_stride,
+                           unsigned int* sse);
+#define vpx_mse16x8 vpx_mse16x8_c
+
+unsigned int vpx_mse8x16_c(const uint8_t* src_ptr,
+                           int src_stride,
+                           const uint8_t* ref_ptr,
+                           int ref_stride,
+                           unsigned int* sse);
+#define vpx_mse8x16 vpx_mse8x16_c
+
+unsigned int vpx_mse8x8_c(const uint8_t* src_ptr,
+                          int src_stride,
+                          const uint8_t* ref_ptr,
+                          int ref_stride,
+                          unsigned int* sse);
+#define vpx_mse8x8 vpx_mse8x8_c
+
+void vpx_plane_add_noise_c(uint8_t* start,
+                           const int8_t* noise,
+                           int blackclamp,
+                           int whiteclamp,
+                           int width,
+                           int height,
+                           int pitch);
+#define vpx_plane_add_noise vpx_plane_add_noise_c
+
+void vpx_post_proc_down_and_across_mb_row_c(unsigned char* src,
+                                            unsigned char* dst,
+                                            int src_pitch,
+                                            int dst_pitch,
+                                            int cols,
+                                            unsigned char* flimits,
+                                            int size);
+#define vpx_post_proc_down_and_across_mb_row \
+  vpx_post_proc_down_and_across_mb_row_c
+
+void vpx_quantize_b_c(const tran_low_t* coeff_ptr,
+                      intptr_t n_coeffs,
+                      int skip_block,
+                      const int16_t* zbin_ptr,
+                      const int16_t* round_ptr,
+                      const int16_t* quant_ptr,
+                      const int16_t* quant_shift_ptr,
+                      tran_low_t* qcoeff_ptr,
+                      tran_low_t* dqcoeff_ptr,
+                      const int16_t* dequant_ptr,
+                      uint16_t* eob_ptr,
+                      const int16_t* scan,
+                      const int16_t* iscan);
+#define vpx_quantize_b vpx_quantize_b_c
+
+void vpx_quantize_b_32x32_c(const tran_low_t* coeff_ptr,
+                            intptr_t n_coeffs,
+                            int skip_block,
+                            const int16_t* zbin_ptr,
+                            const int16_t* round_ptr,
+                            const int16_t* quant_ptr,
+                            const int16_t* quant_shift_ptr,
+                            tran_low_t* qcoeff_ptr,
+                            tran_low_t* dqcoeff_ptr,
+                            const int16_t* dequant_ptr,
+                            uint16_t* eob_ptr,
+                            const int16_t* scan,
+                            const int16_t* iscan);
+#define vpx_quantize_b_32x32 vpx_quantize_b_32x32_c
+
+unsigned int vpx_sad16x16_c(const uint8_t* src_ptr,
+                            int src_stride,
+                            const uint8_t* ref_ptr,
+                            int ref_stride);
+#define vpx_sad16x16 vpx_sad16x16_c
+
+unsigned int vpx_sad16x16_avg_c(const uint8_t* src_ptr,
+                                int src_stride,
+                                const uint8_t* ref_ptr,
+                                int ref_stride,
+                                const uint8_t* second_pred);
+#define vpx_sad16x16_avg vpx_sad16x16_avg_c
+
+void vpx_sad16x16x3_c(const uint8_t* src_ptr,
+                      int src_stride,
+                      const uint8_t* ref_ptr,
+                      int ref_stride,
+                      uint32_t* sad_array);
+#define vpx_sad16x16x3 vpx_sad16x16x3_c
+
+void vpx_sad16x16x4d_c(const uint8_t* src_ptr,
+                       int src_stride,
+                       const uint8_t* const ref_array[],
+                       int ref_stride,
+                       uint32_t* sad_array);
+#define vpx_sad16x16x4d vpx_sad16x16x4d_c
+
+void vpx_sad16x16x8_c(const uint8_t* src_ptr,
+                      int src_stride,
+                      const uint8_t* ref_ptr,
+                      int ref_stride,
+                      uint32_t* sad_array);
+#define vpx_sad16x16x8 vpx_sad16x16x8_c
+
+unsigned int vpx_sad16x32_c(const uint8_t* src_ptr,
+                            int src_stride,
+                            const uint8_t* ref_ptr,
+                            int ref_stride);
+#define vpx_sad16x32 vpx_sad16x32_c
+
+unsigned int vpx_sad16x32_avg_c(const uint8_t* src_ptr,
+                                int src_stride,
+                                const uint8_t* ref_ptr,
+                                int ref_stride,
+                                const uint8_t* second_pred);
+#define vpx_sad16x32_avg vpx_sad16x32_avg_c
+
+void vpx_sad16x32x4d_c(const uint8_t* src_ptr,
+                       int src_stride,
+                       const uint8_t* const ref_array[],
+                       int ref_stride,
+                       uint32_t* sad_array);
+#define vpx_sad16x32x4d vpx_sad16x32x4d_c
+
+unsigned int vpx_sad16x8_c(const uint8_t* src_ptr,
+                           int src_stride,
+                           const uint8_t* ref_ptr,
+                           int ref_stride);
+#define vpx_sad16x8 vpx_sad16x8_c
+
+unsigned int vpx_sad16x8_avg_c(const uint8_t* src_ptr,
+                               int src_stride,
+                               const uint8_t* ref_ptr,
+                               int ref_stride,
+                               const uint8_t* second_pred);
+#define vpx_sad16x8_avg vpx_sad16x8_avg_c
+
+void vpx_sad16x8x3_c(const uint8_t* src_ptr,
+                     int src_stride,
+                     const uint8_t* ref_ptr,
+                     int ref_stride,
+                     uint32_t* sad_array);
+#define vpx_sad16x8x3 vpx_sad16x8x3_c
+
+void vpx_sad16x8x4d_c(const uint8_t* src_ptr,
+                      int src_stride,
+                      const uint8_t* const ref_array[],
+                      int ref_stride,
+                      uint32_t* sad_array);
+#define vpx_sad16x8x4d vpx_sad16x8x4d_c
+
+void vpx_sad16x8x8_c(const uint8_t* src_ptr,
+                     int src_stride,
+                     const uint8_t* ref_ptr,
+                     int ref_stride,
+                     uint32_t* sad_array);
+#define vpx_sad16x8x8 vpx_sad16x8x8_c
+
+unsigned int vpx_sad32x16_c(const uint8_t* src_ptr,
+                            int src_stride,
+                            const uint8_t* ref_ptr,
+                            int ref_stride);
+#define vpx_sad32x16 vpx_sad32x16_c
+
+unsigned int vpx_sad32x16_avg_c(const uint8_t* src_ptr,
+                                int src_stride,
+                                const uint8_t* ref_ptr,
+                                int ref_stride,
+                                const uint8_t* second_pred);
+#define vpx_sad32x16_avg vpx_sad32x16_avg_c
+
+void vpx_sad32x16x4d_c(const uint8_t* src_ptr,
+                       int src_stride,
+                       const uint8_t* const ref_array[],
+                       int ref_stride,
+                       uint32_t* sad_array);
+#define vpx_sad32x16x4d vpx_sad32x16x4d_c
+
+unsigned int vpx_sad32x32_c(const uint8_t* src_ptr,
+                            int src_stride,
+                            const uint8_t* ref_ptr,
+                            int ref_stride);
+#define vpx_sad32x32 vpx_sad32x32_c
+
+unsigned int vpx_sad32x32_avg_c(const uint8_t* src_ptr,
+                                int src_stride,
+                                const uint8_t* ref_ptr,
+                                int ref_stride,
+                                const uint8_t* second_pred);
+#define vpx_sad32x32_avg vpx_sad32x32_avg_c
+
+void vpx_sad32x32x4d_c(const uint8_t* src_ptr,
+                       int src_stride,
+                       const uint8_t* const ref_array[],
+                       int ref_stride,
+                       uint32_t* sad_array);
+#define vpx_sad32x32x4d vpx_sad32x32x4d_c
+
+void vpx_sad32x32x8_c(const uint8_t* src_ptr,
+                      int src_stride,
+                      const uint8_t* ref_ptr,
+                      int ref_stride,
+                      uint32_t* sad_array);
+#define vpx_sad32x32x8 vpx_sad32x32x8_c
+
+unsigned int vpx_sad32x64_c(const uint8_t* src_ptr,
+                            int src_stride,
+                            const uint8_t* ref_ptr,
+                            int ref_stride);
+#define vpx_sad32x64 vpx_sad32x64_c
+
+unsigned int vpx_sad32x64_avg_c(const uint8_t* src_ptr,
+                                int src_stride,
+                                const uint8_t* ref_ptr,
+                                int ref_stride,
+                                const uint8_t* second_pred);
+#define vpx_sad32x64_avg vpx_sad32x64_avg_c
+
+void vpx_sad32x64x4d_c(const uint8_t* src_ptr,
+                       int src_stride,
+                       const uint8_t* const ref_array[],
+                       int ref_stride,
+                       uint32_t* sad_array);
+#define vpx_sad32x64x4d vpx_sad32x64x4d_c
+
+unsigned int vpx_sad4x4_c(const uint8_t* src_ptr,
+                          int src_stride,
+                          const uint8_t* ref_ptr,
+                          int ref_stride);
+#define vpx_sad4x4 vpx_sad4x4_c
+
+unsigned int vpx_sad4x4_avg_c(const uint8_t* src_ptr,
+                              int src_stride,
+                              const uint8_t* ref_ptr,
+                              int ref_stride,
+                              const uint8_t* second_pred);
+#define vpx_sad4x4_avg vpx_sad4x4_avg_c
+
+void vpx_sad4x4x3_c(const uint8_t* src_ptr,
+                    int src_stride,
+                    const uint8_t* ref_ptr,
+                    int ref_stride,
+                    uint32_t* sad_array);
+#define vpx_sad4x4x3 vpx_sad4x4x3_c
+
+void vpx_sad4x4x4d_c(const uint8_t* src_ptr,
+                     int src_stride,
+                     const uint8_t* const ref_array[],
+                     int ref_stride,
+                     uint32_t* sad_array);
+#define vpx_sad4x4x4d vpx_sad4x4x4d_c
+
+void vpx_sad4x4x8_c(const uint8_t* src_ptr,
+                    int src_stride,
+                    const uint8_t* ref_ptr,
+                    int ref_stride,
+                    uint32_t* sad_array);
+#define vpx_sad4x4x8 vpx_sad4x4x8_c
+
+unsigned int vpx_sad4x8_c(const uint8_t* src_ptr,
+                          int src_stride,
+                          const uint8_t* ref_ptr,
+                          int ref_stride);
+#define vpx_sad4x8 vpx_sad4x8_c
+
+unsigned int vpx_sad4x8_avg_c(const uint8_t* src_ptr,
+                              int src_stride,
+                              const uint8_t* ref_ptr,
+                              int ref_stride,
+                              const uint8_t* second_pred);
+#define vpx_sad4x8_avg vpx_sad4x8_avg_c
+
+void vpx_sad4x8x4d_c(const uint8_t* src_ptr,
+                     int src_stride,
+                     const uint8_t* const ref_array[],
+                     int ref_stride,
+                     uint32_t* sad_array);
+#define vpx_sad4x8x4d vpx_sad4x8x4d_c
+
+unsigned int vpx_sad64x32_c(const uint8_t* src_ptr,
+                            int src_stride,
+                            const uint8_t* ref_ptr,
+                            int ref_stride);
+#define vpx_sad64x32 vpx_sad64x32_c
+
+unsigned int vpx_sad64x32_avg_c(const uint8_t* src_ptr,
+                                int src_stride,
+                                const uint8_t* ref_ptr,
+                                int ref_stride,
+                                const uint8_t* second_pred);
+#define vpx_sad64x32_avg vpx_sad64x32_avg_c
+
+void vpx_sad64x32x4d_c(const uint8_t* src_ptr,
+                       int src_stride,
+                       const uint8_t* const ref_array[],
+                       int ref_stride,
+                       uint32_t* sad_array);
+#define vpx_sad64x32x4d vpx_sad64x32x4d_c
+
+unsigned int vpx_sad64x64_c(const uint8_t* src_ptr,
+                            int src_stride,
+                            const uint8_t* ref_ptr,
+                            int ref_stride);
+#define vpx_sad64x64 vpx_sad64x64_c
+
+unsigned int vpx_sad64x64_avg_c(const uint8_t* src_ptr,
+                                int src_stride,
+                                const uint8_t* ref_ptr,
+                                int ref_stride,
+                                const uint8_t* second_pred);
+#define vpx_sad64x64_avg vpx_sad64x64_avg_c
+
+void vpx_sad64x64x4d_c(const uint8_t* src_ptr,
+                       int src_stride,
+                       const uint8_t* const ref_array[],
+                       int ref_stride,
+                       uint32_t* sad_array);
+#define vpx_sad64x64x4d vpx_sad64x64x4d_c
+
+unsigned int vpx_sad8x16_c(const uint8_t* src_ptr,
+                           int src_stride,
+                           const uint8_t* ref_ptr,
+                           int ref_stride);
+#define vpx_sad8x16 vpx_sad8x16_c
+
+unsigned int vpx_sad8x16_avg_c(const uint8_t* src_ptr,
+                               int src_stride,
+                               const uint8_t* ref_ptr,
+                               int ref_stride,
+                               const uint8_t* second_pred);
+#define vpx_sad8x16_avg vpx_sad8x16_avg_c
+
+void vpx_sad8x16x3_c(const uint8_t* src_ptr,
+                     int src_stride,
+                     const uint8_t* ref_ptr,
+                     int ref_stride,
+                     uint32_t* sad_array);
+#define vpx_sad8x16x3 vpx_sad8x16x3_c
+
+void vpx_sad8x16x4d_c(const uint8_t* src_ptr,
+                      int src_stride,
+                      const uint8_t* const ref_array[],
+                      int ref_stride,
+                      uint32_t* sad_array);
+#define vpx_sad8x16x4d vpx_sad8x16x4d_c
+
+void vpx_sad8x16x8_c(const uint8_t* src_ptr,
+                     int src_stride,
+                     const uint8_t* ref_ptr,
+                     int ref_stride,
+                     uint32_t* sad_array);
+#define vpx_sad8x16x8 vpx_sad8x16x8_c
+
+unsigned int vpx_sad8x4_c(const uint8_t* src_ptr,
+                          int src_stride,
+                          const uint8_t* ref_ptr,
+                          int ref_stride);
+#define vpx_sad8x4 vpx_sad8x4_c
+
+unsigned int vpx_sad8x4_avg_c(const uint8_t* src_ptr,
+                              int src_stride,
+                              const uint8_t* ref_ptr,
+                              int ref_stride,
+                              const uint8_t* second_pred);
+#define vpx_sad8x4_avg vpx_sad8x4_avg_c
+
+void vpx_sad8x4x4d_c(const uint8_t* src_ptr,
+                     int src_stride,
+                     const uint8_t* const ref_array[],
+                     int ref_stride,
+                     uint32_t* sad_array);
+#define vpx_sad8x4x4d vpx_sad8x4x4d_c
+
+unsigned int vpx_sad8x8_c(const uint8_t* src_ptr,
+                          int src_stride,
+                          const uint8_t* ref_ptr,
+                          int ref_stride);
+#define vpx_sad8x8 vpx_sad8x8_c
+
+unsigned int vpx_sad8x8_avg_c(const uint8_t* src_ptr,
+                              int src_stride,
+                              const uint8_t* ref_ptr,
+                              int ref_stride,
+                              const uint8_t* second_pred);
+#define vpx_sad8x8_avg vpx_sad8x8_avg_c
+
+void vpx_sad8x8x3_c(const uint8_t* src_ptr,
+                    int src_stride,
+                    const uint8_t* ref_ptr,
+                    int ref_stride,
+                    uint32_t* sad_array);
+#define vpx_sad8x8x3 vpx_sad8x8x3_c
+
+void vpx_sad8x8x4d_c(const uint8_t* src_ptr,
+                     int src_stride,
+                     const uint8_t* const ref_array[],
+                     int ref_stride,
+                     uint32_t* sad_array);
+#define vpx_sad8x8x4d vpx_sad8x8x4d_c
+
+void vpx_sad8x8x8_c(const uint8_t* src_ptr,
+                    int src_stride,
+                    const uint8_t* ref_ptr,
+                    int ref_stride,
+                    uint32_t* sad_array);
+#define vpx_sad8x8x8 vpx_sad8x8x8_c
+
+int vpx_satd_c(const tran_low_t* coeff, int length);
+#define vpx_satd vpx_satd_c
+
+void vpx_scaled_2d_c(const uint8_t* src,
+                     ptrdiff_t src_stride,
+                     uint8_t* dst,
+                     ptrdiff_t dst_stride,
+                     const InterpKernel* filter,
+                     int x0_q4,
+                     int x_step_q4,
+                     int y0_q4,
+                     int y_step_q4,
+                     int w,
+                     int h);
+#define vpx_scaled_2d vpx_scaled_2d_c
+
+void vpx_scaled_avg_2d_c(const uint8_t* src,
+                         ptrdiff_t src_stride,
+                         uint8_t* dst,
+                         ptrdiff_t dst_stride,
+                         const InterpKernel* filter,
+                         int x0_q4,
+                         int x_step_q4,
+                         int y0_q4,
+                         int y_step_q4,
+                         int w,
+                         int h);
+#define vpx_scaled_avg_2d vpx_scaled_avg_2d_c
+
+void vpx_scaled_avg_horiz_c(const uint8_t* src,
+                            ptrdiff_t src_stride,
+                            uint8_t* dst,
+                            ptrdiff_t dst_stride,
+                            const InterpKernel* filter,
+                            int x0_q4,
+                            int x_step_q4,
+                            int y0_q4,
+                            int y_step_q4,
+                            int w,
+                            int h);
+#define vpx_scaled_avg_horiz vpx_scaled_avg_horiz_c
+
+void vpx_scaled_avg_vert_c(const uint8_t* src,
+                           ptrdiff_t src_stride,
+                           uint8_t* dst,
+                           ptrdiff_t dst_stride,
+                           const InterpKernel* filter,
+                           int x0_q4,
+                           int x_step_q4,
+                           int y0_q4,
+                           int y_step_q4,
+                           int w,
+                           int h);
+#define vpx_scaled_avg_vert vpx_scaled_avg_vert_c
+
+void vpx_scaled_horiz_c(const uint8_t* src,
+                        ptrdiff_t src_stride,
+                        uint8_t* dst,
+                        ptrdiff_t dst_stride,
+                        const InterpKernel* filter,
+                        int x0_q4,
+                        int x_step_q4,
+                        int y0_q4,
+                        int y_step_q4,
+                        int w,
+                        int h);
+#define vpx_scaled_horiz vpx_scaled_horiz_c
+
+void vpx_scaled_vert_c(const uint8_t* src,
+                       ptrdiff_t src_stride,
+                       uint8_t* dst,
+                       ptrdiff_t dst_stride,
+                       const InterpKernel* filter,
+                       int x0_q4,
+                       int x_step_q4,
+                       int y0_q4,
+                       int y_step_q4,
+                       int w,
+                       int h);
+#define vpx_scaled_vert vpx_scaled_vert_c
+
+uint32_t vpx_sub_pixel_avg_variance16x16_c(const uint8_t* src_ptr,
+                                           int src_stride,
+                                           int x_offset,
+                                           int y_offset,
+                                           const uint8_t* ref_ptr,
+                                           int ref_stride,
+                                           uint32_t* sse,
+                                           const uint8_t* second_pred);
+#define vpx_sub_pixel_avg_variance16x16 vpx_sub_pixel_avg_variance16x16_c
+
+uint32_t vpx_sub_pixel_avg_variance16x32_c(const uint8_t* src_ptr,
+                                           int src_stride,
+                                           int x_offset,
+                                           int y_offset,
+                                           const uint8_t* ref_ptr,
+                                           int ref_stride,
+                                           uint32_t* sse,
+                                           const uint8_t* second_pred);
+#define vpx_sub_pixel_avg_variance16x32 vpx_sub_pixel_avg_variance16x32_c
+
+uint32_t vpx_sub_pixel_avg_variance16x8_c(const uint8_t* src_ptr,
+                                          int src_stride,
+                                          int x_offset,
+                                          int y_offset,
+                                          const uint8_t* ref_ptr,
+                                          int ref_stride,
+                                          uint32_t* sse,
+                                          const uint8_t* second_pred);
+#define vpx_sub_pixel_avg_variance16x8 vpx_sub_pixel_avg_variance16x8_c
+
+uint32_t vpx_sub_pixel_avg_variance32x16_c(const uint8_t* src_ptr,
+                                           int src_stride,
+                                           int x_offset,
+                                           int y_offset,
+                                           const uint8_t* ref_ptr,
+                                           int ref_stride,
+                                           uint32_t* sse,
+                                           const uint8_t* second_pred);
+#define vpx_sub_pixel_avg_variance32x16 vpx_sub_pixel_avg_variance32x16_c
+
+uint32_t vpx_sub_pixel_avg_variance32x32_c(const uint8_t* src_ptr,
+                                           int src_stride,
+                                           int x_offset,
+                                           int y_offset,
+                                           const uint8_t* ref_ptr,
+                                           int ref_stride,
+                                           uint32_t* sse,
+                                           const uint8_t* second_pred);
+#define vpx_sub_pixel_avg_variance32x32 vpx_sub_pixel_avg_variance32x32_c
+
+uint32_t vpx_sub_pixel_avg_variance32x64_c(const uint8_t* src_ptr,
+                                           int src_stride,
+                                           int x_offset,
+                                           int y_offset,
+                                           const uint8_t* ref_ptr,
+                                           int ref_stride,
+                                           uint32_t* sse,
+                                           const uint8_t* second_pred);
+#define vpx_sub_pixel_avg_variance32x64 vpx_sub_pixel_avg_variance32x64_c
+
+uint32_t vpx_sub_pixel_avg_variance4x4_c(const uint8_t* src_ptr,
+                                         int src_stride,
+                                         int x_offset,
+                                         int y_offset,
+                                         const uint8_t* ref_ptr,
+                                         int ref_stride,
+                                         uint32_t* sse,
+                                         const uint8_t* second_pred);
+#define vpx_sub_pixel_avg_variance4x4 vpx_sub_pixel_avg_variance4x4_c
+
+uint32_t vpx_sub_pixel_avg_variance4x8_c(const uint8_t* src_ptr,
+                                         int src_stride,
+                                         int x_offset,
+                                         int y_offset,
+                                         const uint8_t* ref_ptr,
+                                         int ref_stride,
+                                         uint32_t* sse,
+                                         const uint8_t* second_pred);
+#define vpx_sub_pixel_avg_variance4x8 vpx_sub_pixel_avg_variance4x8_c
+
+uint32_t vpx_sub_pixel_avg_variance64x32_c(const uint8_t* src_ptr,
+                                           int src_stride,
+                                           int x_offset,
+                                           int y_offset,
+                                           const uint8_t* ref_ptr,
+                                           int ref_stride,
+                                           uint32_t* sse,
+                                           const uint8_t* second_pred);
+#define vpx_sub_pixel_avg_variance64x32 vpx_sub_pixel_avg_variance64x32_c
+
+uint32_t vpx_sub_pixel_avg_variance64x64_c(const uint8_t* src_ptr,
+                                           int src_stride,
+                                           int x_offset,
+                                           int y_offset,
+                                           const uint8_t* ref_ptr,
+                                           int ref_stride,
+                                           uint32_t* sse,
+                                           const uint8_t* second_pred);
+#define vpx_sub_pixel_avg_variance64x64 vpx_sub_pixel_avg_variance64x64_c
+
+uint32_t vpx_sub_pixel_avg_variance8x16_c(const uint8_t* src_ptr,
+                                          int src_stride,
+                                          int x_offset,
+                                          int y_offset,
+                                          const uint8_t* ref_ptr,
+                                          int ref_stride,
+                                          uint32_t* sse,
+                                          const uint8_t* second_pred);
+#define vpx_sub_pixel_avg_variance8x16 vpx_sub_pixel_avg_variance8x16_c
+
+uint32_t vpx_sub_pixel_avg_variance8x4_c(const uint8_t* src_ptr,
+                                         int src_stride,
+                                         int x_offset,
+                                         int y_offset,
+                                         const uint8_t* ref_ptr,
+                                         int ref_stride,
+                                         uint32_t* sse,
+                                         const uint8_t* second_pred);
+#define vpx_sub_pixel_avg_variance8x4 vpx_sub_pixel_avg_variance8x4_c
+
+uint32_t vpx_sub_pixel_avg_variance8x8_c(const uint8_t* src_ptr,
+                                         int src_stride,
+                                         int x_offset,
+                                         int y_offset,
+                                         const uint8_t* ref_ptr,
+                                         int ref_stride,
+                                         uint32_t* sse,
+                                         const uint8_t* second_pred);
+#define vpx_sub_pixel_avg_variance8x8 vpx_sub_pixel_avg_variance8x8_c
+
+uint32_t vpx_sub_pixel_variance16x16_c(const uint8_t* src_ptr,
+                                       int src_stride,
+                                       int x_offset,
+                                       int y_offset,
+                                       const uint8_t* ref_ptr,
+                                       int ref_stride,
+                                       uint32_t* sse);
+#define vpx_sub_pixel_variance16x16 vpx_sub_pixel_variance16x16_c
+
+uint32_t vpx_sub_pixel_variance16x32_c(const uint8_t* src_ptr,
+                                       int src_stride,
+                                       int x_offset,
+                                       int y_offset,
+                                       const uint8_t* ref_ptr,
+                                       int ref_stride,
+                                       uint32_t* sse);
+#define vpx_sub_pixel_variance16x32 vpx_sub_pixel_variance16x32_c
+
+uint32_t vpx_sub_pixel_variance16x8_c(const uint8_t* src_ptr,
+                                      int src_stride,
+                                      int x_offset,
+                                      int y_offset,
+                                      const uint8_t* ref_ptr,
+                                      int ref_stride,
+                                      uint32_t* sse);
+#define vpx_sub_pixel_variance16x8 vpx_sub_pixel_variance16x8_c
+
+uint32_t vpx_sub_pixel_variance32x16_c(const uint8_t* src_ptr,
+                                       int src_stride,
+                                       int x_offset,
+                                       int y_offset,
+                                       const uint8_t* ref_ptr,
+                                       int ref_stride,
+                                       uint32_t* sse);
+#define vpx_sub_pixel_variance32x16 vpx_sub_pixel_variance32x16_c
+
+uint32_t vpx_sub_pixel_variance32x32_c(const uint8_t* src_ptr,
+                                       int src_stride,
+                                       int x_offset,
+                                       int y_offset,
+                                       const uint8_t* ref_ptr,
+                                       int ref_stride,
+                                       uint32_t* sse);
+#define vpx_sub_pixel_variance32x32 vpx_sub_pixel_variance32x32_c
+
+uint32_t vpx_sub_pixel_variance32x64_c(const uint8_t* src_ptr,
+                                       int src_stride,
+                                       int x_offset,
+                                       int y_offset,
+                                       const uint8_t* ref_ptr,
+                                       int ref_stride,
+                                       uint32_t* sse);
+#define vpx_sub_pixel_variance32x64 vpx_sub_pixel_variance32x64_c
+
+uint32_t vpx_sub_pixel_variance4x4_c(const uint8_t* src_ptr,
+                                     int src_stride,
+                                     int x_offset,
+                                     int y_offset,
+                                     const uint8_t* ref_ptr,
+                                     int ref_stride,
+                                     uint32_t* sse);
+#define vpx_sub_pixel_variance4x4 vpx_sub_pixel_variance4x4_c
+
+uint32_t vpx_sub_pixel_variance4x8_c(const uint8_t* src_ptr,
+                                     int src_stride,
+                                     int x_offset,
+                                     int y_offset,
+                                     const uint8_t* ref_ptr,
+                                     int ref_stride,
+                                     uint32_t* sse);
+#define vpx_sub_pixel_variance4x8 vpx_sub_pixel_variance4x8_c
+
+uint32_t vpx_sub_pixel_variance64x32_c(const uint8_t* src_ptr,
+                                       int src_stride,
+                                       int x_offset,
+                                       int y_offset,
+                                       const uint8_t* ref_ptr,
+                                       int ref_stride,
+                                       uint32_t* sse);
+#define vpx_sub_pixel_variance64x32 vpx_sub_pixel_variance64x32_c
+
+uint32_t vpx_sub_pixel_variance64x64_c(const uint8_t* src_ptr,
+                                       int src_stride,
+                                       int x_offset,
+                                       int y_offset,
+                                       const uint8_t* ref_ptr,
+                                       int ref_stride,
+                                       uint32_t* sse);
+#define vpx_sub_pixel_variance64x64 vpx_sub_pixel_variance64x64_c
+
+uint32_t vpx_sub_pixel_variance8x16_c(const uint8_t* src_ptr,
+                                      int src_stride,
+                                      int x_offset,
+                                      int y_offset,
+                                      const uint8_t* ref_ptr,
+                                      int ref_stride,
+                                      uint32_t* sse);
+#define vpx_sub_pixel_variance8x16 vpx_sub_pixel_variance8x16_c
+
+uint32_t vpx_sub_pixel_variance8x4_c(const uint8_t* src_ptr,
+                                     int src_stride,
+                                     int x_offset,
+                                     int y_offset,
+                                     const uint8_t* ref_ptr,
+                                     int ref_stride,
+                                     uint32_t* sse);
+#define vpx_sub_pixel_variance8x4 vpx_sub_pixel_variance8x4_c
+
+uint32_t vpx_sub_pixel_variance8x8_c(const uint8_t* src_ptr,
+                                     int src_stride,
+                                     int x_offset,
+                                     int y_offset,
+                                     const uint8_t* ref_ptr,
+                                     int ref_stride,
+                                     uint32_t* sse);
+#define vpx_sub_pixel_variance8x8 vpx_sub_pixel_variance8x8_c
+
+void vpx_subtract_block_c(int rows,
+                          int cols,
+                          int16_t* diff_ptr,
+                          ptrdiff_t diff_stride,
+                          const uint8_t* src_ptr,
+                          ptrdiff_t src_stride,
+                          const uint8_t* pred_ptr,
+                          ptrdiff_t pred_stride);
+#define vpx_subtract_block vpx_subtract_block_c
+
+uint64_t vpx_sum_squares_2d_i16_c(const int16_t* src, int stride, int size);
+#define vpx_sum_squares_2d_i16 vpx_sum_squares_2d_i16_c
+
+void vpx_tm_predictor_16x16_c(uint8_t* dst,
+                              ptrdiff_t stride,
+                              const uint8_t* above,
+                              const uint8_t* left);
+#define vpx_tm_predictor_16x16 vpx_tm_predictor_16x16_c
+
+void vpx_tm_predictor_32x32_c(uint8_t* dst,
+                              ptrdiff_t stride,
+                              const uint8_t* above,
+                              const uint8_t* left);
+#define vpx_tm_predictor_32x32 vpx_tm_predictor_32x32_c
+
+void vpx_tm_predictor_4x4_c(uint8_t* dst,
+                            ptrdiff_t stride,
+                            const uint8_t* above,
+                            const uint8_t* left);
+#define vpx_tm_predictor_4x4 vpx_tm_predictor_4x4_c
+
+void vpx_tm_predictor_8x8_c(uint8_t* dst,
+                            ptrdiff_t stride,
+                            const uint8_t* above,
+                            const uint8_t* left);
+#define vpx_tm_predictor_8x8 vpx_tm_predictor_8x8_c
+
+void vpx_v_predictor_16x16_c(uint8_t* dst,
+                             ptrdiff_t stride,
+                             const uint8_t* above,
+                             const uint8_t* left);
+#define vpx_v_predictor_16x16 vpx_v_predictor_16x16_c
+
+void vpx_v_predictor_32x32_c(uint8_t* dst,
+                             ptrdiff_t stride,
+                             const uint8_t* above,
+                             const uint8_t* left);
+#define vpx_v_predictor_32x32 vpx_v_predictor_32x32_c
+
+void vpx_v_predictor_4x4_c(uint8_t* dst,
+                           ptrdiff_t stride,
+                           const uint8_t* above,
+                           const uint8_t* left);
+#define vpx_v_predictor_4x4 vpx_v_predictor_4x4_c
+
+void vpx_v_predictor_8x8_c(uint8_t* dst,
+                           ptrdiff_t stride,
+                           const uint8_t* above,
+                           const uint8_t* left);
+#define vpx_v_predictor_8x8 vpx_v_predictor_8x8_c
+
+unsigned int vpx_variance16x16_c(const uint8_t* src_ptr,
+                                 int src_stride,
+                                 const uint8_t* ref_ptr,
+                                 int ref_stride,
+                                 unsigned int* sse);
+#define vpx_variance16x16 vpx_variance16x16_c
+
+unsigned int vpx_variance16x32_c(const uint8_t* src_ptr,
+                                 int src_stride,
+                                 const uint8_t* ref_ptr,
+                                 int ref_stride,
+                                 unsigned int* sse);
+#define vpx_variance16x32 vpx_variance16x32_c
+
+unsigned int vpx_variance16x8_c(const uint8_t* src_ptr,
+                                int src_stride,
+                                const uint8_t* ref_ptr,
+                                int ref_stride,
+                                unsigned int* sse);
+#define vpx_variance16x8 vpx_variance16x8_c
+
+unsigned int vpx_variance32x16_c(const uint8_t* src_ptr,
+                                 int src_stride,
+                                 const uint8_t* ref_ptr,
+                                 int ref_stride,
+                                 unsigned int* sse);
+#define vpx_variance32x16 vpx_variance32x16_c
+
+unsigned int vpx_variance32x32_c(const uint8_t* src_ptr,
+                                 int src_stride,
+                                 const uint8_t* ref_ptr,
+                                 int ref_stride,
+                                 unsigned int* sse);
+#define vpx_variance32x32 vpx_variance32x32_c
+
+unsigned int vpx_variance32x64_c(const uint8_t* src_ptr,
+                                 int src_stride,
+                                 const uint8_t* ref_ptr,
+                                 int ref_stride,
+                                 unsigned int* sse);
+#define vpx_variance32x64 vpx_variance32x64_c
+
+unsigned int vpx_variance4x4_c(const uint8_t* src_ptr,
+                               int src_stride,
+                               const uint8_t* ref_ptr,
+                               int ref_stride,
+                               unsigned int* sse);
+#define vpx_variance4x4 vpx_variance4x4_c
+
+unsigned int vpx_variance4x8_c(const uint8_t* src_ptr,
+                               int src_stride,
+                               const uint8_t* ref_ptr,
+                               int ref_stride,
+                               unsigned int* sse);
+#define vpx_variance4x8 vpx_variance4x8_c
+
+unsigned int vpx_variance64x32_c(const uint8_t* src_ptr,
+                                 int src_stride,
+                                 const uint8_t* ref_ptr,
+                                 int ref_stride,
+                                 unsigned int* sse);
+#define vpx_variance64x32 vpx_variance64x32_c
+
+unsigned int vpx_variance64x64_c(const uint8_t* src_ptr,
+                                 int src_stride,
+                                 const uint8_t* ref_ptr,
+                                 int ref_stride,
+                                 unsigned int* sse);
+#define vpx_variance64x64 vpx_variance64x64_c
+
+unsigned int vpx_variance8x16_c(const uint8_t* src_ptr,
+                                int src_stride,
+                                const uint8_t* ref_ptr,
+                                int ref_stride,
+                                unsigned int* sse);
+#define vpx_variance8x16 vpx_variance8x16_c
+
+unsigned int vpx_variance8x4_c(const uint8_t* src_ptr,
+                               int src_stride,
+                               const uint8_t* ref_ptr,
+                               int ref_stride,
+                               unsigned int* sse);
+#define vpx_variance8x4 vpx_variance8x4_c
+
+unsigned int vpx_variance8x8_c(const uint8_t* src_ptr,
+                               int src_stride,
+                               const uint8_t* ref_ptr,
+                               int ref_stride,
+                               unsigned int* sse);
+#define vpx_variance8x8 vpx_variance8x8_c
+
+void vpx_ve_predictor_4x4_c(uint8_t* dst,
+                            ptrdiff_t stride,
+                            const uint8_t* above,
+                            const uint8_t* left);
+#define vpx_ve_predictor_4x4 vpx_ve_predictor_4x4_c
+
+int vpx_vector_var_c(const int16_t* ref, const int16_t* src, const int bwl);
+#define vpx_vector_var vpx_vector_var_c
+
+void vpx_dsp_rtcd(void);
+
+#include "vpx_config.h"
+
+#ifdef RTCD_C
+static void setup_rtcd_internal(void) {}
+#endif
+
+#ifdef __cplusplus
+}  // extern "C"
+#endif
+
+#endif
diff --git a/src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vpx_scale_rtcd.h b/src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vpx_scale_rtcd.h
new file mode 100644
index 0000000000..c5196db4dc
--- /dev/null
+++ b/src/3rdparty/chromium/third_party/libvpx/source/config/linux/sw_64/vpx_scale_rtcd.h
@@ -0,0 +1,96 @@
+// This file is generated. Do not edit.
+#ifndef VPX_SCALE_RTCD_H_
+#define VPX_SCALE_RTCD_H_
+
+#ifdef RTCD_C
+#define RTCD_EXTERN
+#else
+#define RTCD_EXTERN extern
+#endif
+
+struct yv12_buffer_config;
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+void vp8_horizontal_line_2_1_scale_c(const unsigned char* source,
+                                     unsigned int source_width,
+                                     unsigned char* dest,
+                                     unsigned int dest_width);
+#define vp8_horizontal_line_2_1_scale vp8_horizontal_line_2_1_scale_c
+
+void vp8_horizontal_line_5_3_scale_c(const unsigned char* source,
+                                     unsigned int source_width,
+                                     unsigned char* dest,
+                                     unsigned int dest_width);
+#define vp8_horizontal_line_5_3_scale vp8_horizontal_line_5_3_scale_c
+
+void vp8_horizontal_line_5_4_scale_c(const unsigned char* source,
+                                     unsigned int source_width,
+                                     unsigned char* dest,
+                                     unsigned int dest_width);
+#define vp8_horizontal_line_5_4_scale vp8_horizontal_line_5_4_scale_c
+
+void vp8_vertical_band_2_1_scale_c(unsigned char* source,
+                                   unsigned int src_pitch,
+                                   unsigned char* dest,
+                                   unsigned int dest_pitch,
+                                   unsigned int dest_width);
+#define vp8_vertical_band_2_1_scale vp8_vertical_band_2_1_scale_c
+
+void vp8_vertical_band_2_1_scale_i_c(unsigned char* source,
+                                     unsigned int src_pitch,
+                                     unsigned char* dest,
+                                     unsigned int dest_pitch,
+                                     unsigned int dest_width);
+#define vp8_vertical_band_2_1_scale_i vp8_vertical_band_2_1_scale_i_c
+
+void vp8_vertical_band_5_3_scale_c(unsigned char* source,
+                                   unsigned int src_pitch,
+                                   unsigned char* dest,
+                                   unsigned int dest_pitch,
+                                   unsigned int dest_width);
+#define vp8_vertical_band_5_3_scale vp8_vertical_band_5_3_scale_c
+
+void vp8_vertical_band_5_4_scale_c(unsigned char* source,
+                                   unsigned int src_pitch,
+                                   unsigned char* dest,
+                                   unsigned int dest_pitch,
+                                   unsigned int dest_width);
+#define vp8_vertical_band_5_4_scale vp8_vertical_band_5_4_scale_c
+
+void vp8_yv12_copy_frame_c(const struct yv12_buffer_config* src_ybc,
+                           struct yv12_buffer_config* dst_ybc);
+#define vp8_yv12_copy_frame vp8_yv12_copy_frame_c
+
+void vp8_yv12_extend_frame_borders_c(struct yv12_buffer_config* ybf);
+#define vp8_yv12_extend_frame_borders vp8_yv12_extend_frame_borders_c
+
+void vpx_extend_frame_borders_c(struct yv12_buffer_config* ybf);
+#define vpx_extend_frame_borders vpx_extend_frame_borders_c
+
+void vpx_extend_frame_inner_borders_c(struct yv12_buffer_config* ybf);
+#define vpx_extend_frame_inner_borders vpx_extend_frame_inner_borders_c
+
+void vpx_yv12_copy_frame_c(const struct yv12_buffer_config* src_ybc,
+                           struct yv12_buffer_config* dst_ybc);
+#define vpx_yv12_copy_frame vpx_yv12_copy_frame_c
+
+void vpx_yv12_copy_y_c(const struct yv12_buffer_config* src_ybc,
+                       struct yv12_buffer_config* dst_ybc);
+#define vpx_yv12_copy_y vpx_yv12_copy_y_c
+
+void vpx_scale_rtcd(void);
+
+#include "vpx_config.h"
+
+#ifdef RTCD_C
+static void setup_rtcd_internal(void) {}
+#endif
+
+#ifdef __cplusplus
+}  // extern "C"
+#endif
+
+#endif
diff --git a/src/3rdparty/chromium/third_party/lss/linux_syscall_support.h b/src/3rdparty/chromium/third_party/lss/linux_syscall_support.h
index e67df41239..db16f8ccee 100644
--- a/src/3rdparty/chromium/third_party/lss/linux_syscall_support.h
+++ b/src/3rdparty/chromium/third_party/lss/linux_syscall_support.h
@@ -88,7 +88,7 @@
  */
 #if (defined(__i386__) || defined(__x86_64__) || defined(__ARM_ARCH_3__) ||   \
      defined(__mips__) || defined(__PPC__) || defined(__ARM_EABI__) || \
-     defined(__aarch64__) || defined(__s390__)) \
+     defined(__aarch64__) || defined(__s390__))  || defined(__sw_64__) \
   && (defined(__linux) || defined(__ANDROID__))
 
 #ifndef SYS_CPLUSPLUS
@@ -379,6 +379,26 @@ struct kernel_stat64 {
   unsigned           __pad2;
   unsigned long long st_blocks;
 };
+
+#elif defined(__sw_64__)
+struct kernel_stat {
+    unsigned int st_dev;
+    unsigned int st_ino;
+    unsigned int st_mode;
+    unsigned int st_nlink;
+    unsigned int st_uid;
+    unsigned int st_gid;
+    unsigned int st_rdev;
+    long int st_size;
+    unsigned long int st_atime;
+    unsigned long int st_mtime;
+    unsigned long int st_ctime;
+    unsigned int st_blksize;
+    int st_blocks;
+    unsigned int st_flags;
+    unsigned int st_gen;
+};
+
 #elif defined __PPC__
 struct kernel_stat64 {
   unsigned long long st_dev;
@@ -475,6 +495,7 @@ struct kernel_stat {
   uint64_t           st_ctime_nsec_;
   int64_t            __unused4[3];
 };
+
 #elif defined(__PPC__)
 struct kernel_stat {
   unsigned           st_dev;
@@ -823,6 +844,42 @@ struct kernel_statfs {
 #define FUTEX_TRYLOCK_PI_PRIVATE  (FUTEX_TRYLOCK_PI | FUTEX_PRIVATE_FLAG)
 #endif
 
+#if defined(__sw_64__)
+#ifndef __NR_fork
+#define __NR_fork               2
+#endif
+#ifndef __NR_rt_sigaction
+#define __NR_rt_sigaction       352
+#define __NR_rt_sigprocmask     353
+#endif
+#ifndef __NR_stat64
+#define __NR_stat64             425
+#endif
+#ifndef __NR_fstat64
+#define __NR_fstat64            427
+#endif
+#ifndef __NR_socket
+#define __NR_socket             97
+#endif
+#ifndef __NR_getdents64
+#define __NR_getdents64         377
+#endif
+#ifndef __NR_gettid
+#define __NR_gettid             378
+#endif
+#ifndef __NR_getxpid
+#define __NR_getxpid            178
+#endif
+#ifndef __NR_getpid
+#define __NR_getpid             __NR_getxpid
+#endif
+#ifndef __NR_futex
+#define __NR_futex              394
+#endif
+#ifndef __NR_openat
+#define __NR_openat             450
+#endif
+#endif
 
 #if defined(__x86_64__)
 #ifndef ARCH_SET_GS
@@ -1892,7 +1949,7 @@ struct kernel_statfs {
       }                                                                       \
       return (type) (res);                                                    \
     } while (0)
-  #elif defined(__mips__)
+  #elif defined(__mips__)  || defined(__sw_64__)
   /* On MIPS, failing system calls return -1, and set errno in a
    * separate CPU register.
    */
@@ -2833,6 +2890,140 @@ struct kernel_statfs {
       }
       LSS_RETURN(int, __res);
     }
+
+#elif defined(__sw_64__)
+    #undef LSS_REG
+    #define LSS_REG(r,a) register unsigned long __r##r __asm__("$"#r) =       \
+                                 (unsigned long)(a)
+    #undef  LSS_BODY
+    #undef LSS_SYSCALL_CLOBBERS
+    #define LSS_SYSCALL_CLOBBERS "$30", "memory"
+    #define LSS_BODY(type,name,r19,...)                                        \
+          register unsigned long __v0 __asm__("$0") = __NR_##name;            \
+          __asm__ __volatile__ ("sys_call 0x83\n"                                   \
+                                : "=r"(__v0), r19 (__r19)                       \
+                                : "0"(__v0), ##__VA_ARGS__                    \
+                                : LSS_SYSCALL_CLOBBERS);                      \
+          LSS_RETURN(type, __v0, __r19)
+    #undef _syscall0
+    #define _syscall0(type, name)                                             \
+      type LSS_NAME(name)(void) {                                             \
+        register unsigned long __r19 __asm__("$19");                            \
+        LSS_BODY(type, name, "=r");                                           \
+      }
+    #undef _syscall1
+    #define _syscall1(type, name, type1, arg1)                                \
+      type LSS_NAME(name)(type1 arg1) {                                       \
+        register unsigned long __r19 __asm__("$19");                            \
+        LSS_REG(16, arg1); LSS_BODY(type, name, "=r", "r"(__r16));              \
+      }
+    #undef _syscall2
+    #define _syscall2(type, name, type1, arg1, type2, arg2)                   \
+      type LSS_NAME(name)(type1 arg1, type2 arg2) {                           \
+        register unsigned long __r19 __asm__("$19");                            \
+        LSS_REG(16, arg1); LSS_REG(17, arg2);                                   \
+        LSS_BODY(type, name, "=r", "r"(__r16), "r"(__r17));                     \
+      }
+    #undef _syscall3
+    #define _syscall3(type, name, type1, arg1, type2, arg2, type3, arg3)      \
+      type LSS_NAME(name)(type1 arg1, type2 arg2, type3 arg3) {               \
+        register unsigned long __r19 __asm__("$19");                            \
+        LSS_REG(16, arg1); LSS_REG(17, arg2); LSS_REG(18, arg3);                 \
+        LSS_BODY(type, name, "=r", "r"(__r16), "r"(__r17), "r"(__r18));          \
+      }
+    #undef _syscall4
+    #define _syscall4(type,name,type1,arg1,type2,arg2,type3,arg3,type4,arg4)  \
+      type LSS_NAME(name)(type1 arg1, type2 arg2, type3 arg3, type4 arg4) {   \
+        LSS_REG(16, arg1); LSS_REG(17, arg2); LSS_REG(18, arg3);                 \
+        LSS_REG(19, arg4);                                                     \
+        LSS_BODY(type, name, "+r", "r"(__r16), "r"(__r17), "r"(__r18));          \
+      }
+    #undef _syscall5
+    #define _syscall5(type,name,type1,arg1,type2,arg2,type3,arg3,type4,arg4,  \
+                      type5,arg5)                                             \
+      type LSS_NAME(name)(type1 arg1, type2 arg2, type3 arg3, type4 arg4,     \
+                          type5 arg5) {                                       \
+        LSS_REG(16, arg1); LSS_REG(17, arg2); LSS_REG(18, arg3);                 \
+        LSS_REG(19, arg4); LSS_REG(20, arg5);                                   \
+        LSS_BODY(type, name, "+r", "r"(__r16), "r"(__r17), "r"(__r18),           \
+                 "r"(__r20));                                                  \
+      }
+    #undef _syscall6
+    #define _syscall6(type,name,type1,arg1,type2,arg2,type3,arg3,type4,arg4,  \
+                      type5,arg5,type6,arg6)                                  \
+      type LSS_NAME(name)(type1 arg1, type2 arg2, type3 arg3, type4 arg4,     \
+                          type5 arg5,type6 arg6) {                            \
+        LSS_REG(16, arg1); LSS_REG(17, arg2); LSS_REG(18, arg3);                 \
+        LSS_REG(19, arg4); LSS_REG(20, arg5); LSS_REG(21, arg6);                 \
+        LSS_BODY(type, name, "+r", "r"(__r16), "r"(__r17), "r"(__r18),           \
+                 "r"(__r20), "r"(__r21));                                       \
+      }
+    LSS_INLINE int LSS_NAME(clone)(int (*fn)(void *), void *child_stack,
+                                   int flags, void *arg, int *parent_tidptr,
+                                   void *newtls, int *child_tidptr) {
+      register unsigned long __v0 __asm__("$0") = -EINVAL;
+      register unsigned long __r19 __asm__("$19") = (unsigned long)newtls;
+      {
+        register int   __flags __asm__("$16") = flags;
+        register void *__stack __asm__("$17") = child_stack;
+        register void *__ptid  __asm__("$18") = parent_tidptr;
+        register int  *__ctid  __asm__("$20") = child_tidptr;
+        register int (*__func)(void *) __asm__ ("$1")  = fn;
+        register void  *__args  __asm__("$2") = arg;
+        __asm__ __volatile__(
+                             "ldi $30,-16($30)\n"
+                             /* if (fn == NULL || child_stack == NULL)
+                              *   return -EINVAL;
+                              */
+                             "beq $1,1f\n" //t0=fn
+                             "beq $17,1f\n"
+
+                             /* Push "arg" and "fn" onto the stack that will be
+                              * used by the child.
+                              */
+                             "ldi $17, -32($17)\n"
+                             "stl $1 ,0($17)\n" //fn
+                             "stl $2 ,8($17)\n" //arg
+
+                             /* $7 = syscall_clone($4 = flags,
+                              *              $5 = child_stack,
+                              *              $6 = parent_tidptr,
+                              *              $7 = newtls,
+                              *              $8 = child_tidptr)
+                              */
+                             "ldi $0,312($31)\n"
+                             "sys_call 0x83\n"
+
+                             /* if ($7 != 0)
+                              *   return $2;
+                              */
+                             "bne $19,1f\n"
+                             "bne $0,1f\n"
+
+                             /* In the child, now. Call "fn(arg)".
+                              */
+                             "ldl $27,0($30)\n" //NOTE: in sunway must use r27, or fn will not be called
+                             "ldl $16,8($30)\n"
+                             "ldi $30, 32($30)\n"
+                             "call $26, ($27)\n"
+
+                             /* Call _exit($2)
+                              */
+                             "bis $31,$0,$16\n"
+                             "ldi $0,1($31)\n"
+                             "sys_call 0x83\n"
+
+                         "1:\n"
+                             "ldi $30, 16($30)\n"
+                             : "+r" (__v0), "+r" (__r19)
+                             : "i"(__NR_clone), "i"(__NR_exit), "r"(fn),
+                               "r"(__stack), "r"(__flags), "r"(arg),
+                               "r"(__ptid), "r"(__ctid)
+                             : "$30", "memory");
+      }
+      LSS_RETURN(int, __v0, __r19);
+    }
+
   #elif defined(__mips__)
     #undef LSS_REG
     #define LSS_REG(r,a) register unsigned long __r##r __asm__("$"#r) =       \
@@ -3403,6 +3594,10 @@ struct kernel_statfs {
     // fork is polyfilled below when not available.
     LSS_INLINE _syscall0(pid_t,   fork)
   #endif
+  #if defined(__sw_64__)
+  LSS_INLINE _syscall2(int,     fstat64,           int,         f,
+                      struct stat*,   b)
+  #endif
   LSS_INLINE _syscall2(int,     fstat,           int,         f,
                       struct kernel_stat*,   b)
   LSS_INLINE _syscall2(int,     fstatfs,         int,         f,
@@ -3424,13 +3619,17 @@ struct kernel_statfs {
                        struct kernel_dirent*, d, int,    c)
   LSS_INLINE _syscall3(int,     getdents64,      int,         f,
                       struct kernel_dirent64*, d, int,    c)
+#if !defined(__sw_64__)
   LSS_INLINE _syscall0(gid_t,   getegid)
   LSS_INLINE _syscall0(uid_t,   geteuid)
+#endif
   #if defined(__NR_getpgrp)
     LSS_INLINE _syscall0(pid_t,   getpgrp)
   #endif
   LSS_INLINE _syscall0(pid_t,   getpid)
+#if !defined(__sw_64__)
   LSS_INLINE _syscall0(pid_t,   getppid)
+#endif
   LSS_INLINE _syscall2(int,     getpriority,     int,         a,
                        int,            b)
   LSS_INLINE _syscall3(int,     getresgid,       gid_t *,     r,
@@ -3587,7 +3786,7 @@ struct kernel_statfs {
                          unsigned *, node, void *, unused)
   #endif
   #if defined(__x86_64__) ||                                                  \
-     (defined(__mips__) && _MIPS_SIM != _MIPS_SIM_ABI32)
+     (defined(__mips__) && _MIPS_SIM != _MIPS_SIM_ABI32) || defined(__sw_64__)
     LSS_INLINE _syscall3(int, recvmsg,            int,   s,
                         struct kernel_msghdr*,     m, int, f)
     LSS_INLINE _syscall3(int, sendmsg,            int,   s,
diff --git a/src/3rdparty/chromium/third_party/node/node.py b/src/3rdparty/chromium/third_party/node/node.py
index 40afbdc305..1300b4449a 100755
--- a/src/3rdparty/chromium/third_party/node/node.py
+++ b/src/3rdparty/chromium/third_party/node/node.py
@@ -18,6 +18,19 @@ def which(cmd):
     return None
 
 def GetBinaryPath():
+  if platform.machine() == 'sw_64':
+      # npath = os.path.abspath("../../../../src/3rdparty/chromium/third_party/node/linux/node-linux-sw_64/lib/node_modules")
+      # home_path = os.path.expanduser('~')
+      # home_path=home_path+"/node_modules"
+      # os.unlink(home_path)
+      # os.symlink(npath,home_path)
+      os.environ['NODE_PATH']="../../../../src/3rdparty/chromium/third_party/node/linux/node-linux-sw_64/lib/node_modules/npm/node_modules"
+      return os_path.join(os_path.dirname(__file__), *{
+        'Darwin': ('mac', 'node-darwin-x64', 'bin', 'node'),
+        'Linux': ('linux', 'node-linux-sw_64', 'bin', 'node'),
+        'Windows': ('win', 'node.exe'),
+      }[platform.system()])
+
   if sys.platform == 'win32':
     nodejs = which('node.exe')
     if nodejs:
diff --git a/src/3rdparty/chromium/third_party/swiftshader/third_party/llvm-10.0/BUILD.gn b/src/3rdparty/chromium/third_party/swiftshader/third_party/llvm-10.0/BUILD.gn
index 70e14c06fb..38fc03362c 100644
--- a/src/3rdparty/chromium/third_party/swiftshader/third_party/llvm-10.0/BUILD.gn
+++ b/src/3rdparty/chromium/third_party/swiftshader/third_party/llvm-10.0/BUILD.gn
@@ -149,6 +149,8 @@ swiftshader_llvm_source_set("swiftshader_llvm") {
     deps += [ ":swiftshader_llvm_ppc" ]
   } else if (current_cpu == "x86" || current_cpu == "x64") {
     deps += [ ":swiftshader_llvm_x86" ]
+  } else if (current_cpu == "sw_64") {
+    deps += [":swiftshader_llvm_sw64"]
   } else {
     assert(false, "Unsupported current_cpu")
   }
@@ -1308,3 +1310,8 @@ swiftshader_llvm_source_set("swiftshader_llvm_x86") {
     "llvm/lib/Target/X86/X86WinEHState.cpp",
   ]
 }
+
+swiftshader_source_set("swiftshader_llvm_sw64") {
+  sources = [
+ ]
+}
diff --git a/src/3rdparty/chromium/third_party/webrtc/modules/desktop_capture/differ_block.cc b/src/3rdparty/chromium/third_party/webrtc/modules/desktop_capture/differ_block.cc
index 4f0c5430c9..1f18079497 100644
--- a/src/3rdparty/chromium/third_party/webrtc/modules/desktop_capture/differ_block.cc
+++ b/src/3rdparty/chromium/third_party/webrtc/modules/desktop_capture/differ_block.cc
@@ -30,7 +30,7 @@ bool VectorDifference(const uint8_t* image1, const uint8_t* image2) {
   static bool (*diff_proc)(const uint8_t*, const uint8_t*) = nullptr;
 
   if (!diff_proc) {
-#if defined(WEBRTC_ARCH_ARM_FAMILY) || defined(WEBRTC_ARCH_MIPS_FAMILY)
+#if defined(WEBRTC_ARCH_ARM_FAMILY) || defined(WEBRTC_ARCH_MIPS_FAMILY) || defined(WEBRTC_ARCH_SW64_FAMILY)
     // For ARM and MIPS processors, always use C version.
     // TODO(hclam): Implement a NEON version.
     diff_proc = &VectorDifference_C;
diff --git a/src/3rdparty/chromium/third_party/webrtc/rtc_base/system/arch.h b/src/3rdparty/chromium/third_party/webrtc/rtc_base/system/arch.h
index ed216e660f..e4b2d8aefd 100644
--- a/src/3rdparty/chromium/third_party/webrtc/rtc_base/system/arch.h
+++ b/src/3rdparty/chromium/third_party/webrtc/rtc_base/system/arch.h
@@ -50,6 +50,10 @@
 #elif defined(__EMSCRIPTEN__)
 #define WEBRTC_ARCH_32_BITS
 #define WEBRTC_ARCH_LITTLE_ENDIAN
+#elif defined(__sw_64__)
+#define WEBRTC_ARCH_SW64_FAMILY
+#define WEBRTC_ARCH_64_BITS
+#define WEBRTC_ARCH_LITTLE_ENDIAN
 #else
 #error Please add support for your architecture in rtc_base/system/arch.h
 #endif
diff --git a/src/3rdparty/chromium/v8/BUILD.gn b/src/3rdparty/chromium/v8/BUILD.gn
index ba99c75140..bd22fbe746 100644
--- a/src/3rdparty/chromium/v8/BUILD.gn
+++ b/src/3rdparty/chromium/v8/BUILD.gn
@@ -712,6 +712,10 @@ config("toolchain") {
     }
   }
 
+  if (v8_current_cpu == "sw_64") {
+   defines += [ "V8_TARGET_ARCH_SW64", "SW64" ]
+  }
+
   # Mips64el/mipsel simulators.
   if (target_is_simulator &&
       (v8_current_cpu == "mipsel" || v8_current_cpu == "mips64el")) {
@@ -1853,6 +1857,11 @@ v8_source_set("v8_initializers") {
       ### gcmole(arch:x64) ###
       "src/builtins/x64/builtins-x64.cc",
     ]
+  } else if (v8_current_cpu == "sw_64") {
+    sources += [
+      ### gcmole(arch:sw64) ###
+      "src/builtins/sw64/builtins-sw64.cc",
+    ]
   } else if (v8_current_cpu == "arm") {
     sources += [
       ### gcmole(arch:arm) ###
@@ -3618,6 +3627,35 @@ v8_source_set("v8_base_without_compiler") {
       # to be excluded, see the comments inside.
       "src/codegen/arm64/instructions-arm64-constants.cc",
     ]
+  } else if (v8_current_cpu == "sw_64") {
+    sources += [  ### gcmole(arch:sw_64) ###
+      "src/codegen/sw64/assembler-sw64-inl.h",
+      "src/codegen/sw64/assembler-sw64.cc",
+      "src/codegen/sw64/assembler-sw64.h",
+      "src/codegen/sw64/constants-sw64.cc",
+      "src/codegen/sw64/constants-sw64.h",
+      "src/codegen/sw64/cpu-sw64.cc",
+      "src/codegen/sw64/interface-descriptors-sw64.cc",
+      "src/codegen/sw64/macro-assembler-sw64.cc",
+      "src/codegen/sw64/macro-assembler-sw64.h",
+      "src/codegen/sw64/register-sw64.h",
+      "src/compiler/backend/sw64/code-generator-sw64.cc",
+      "src/compiler/backend/sw64/instruction-codes-sw64.h",
+      "src/compiler/backend/sw64/instruction-scheduler-sw64.cc",
+      "src/compiler/backend/sw64/instruction-selector-sw64.cc",
+      "src/compiler/backend/sw64/unwinding-info-writer-sw64.cc",
+      "src/compiler/backend/sw64/unwinding-info-writer-sw64.h",
+      "src/debug/sw64/debug-sw64.cc",
+      "src/deoptimizer/sw64/deoptimizer-sw64.cc",
+      "src/diagnostics/sw64/disasm-sw64.cc",
+      "src/execution/sw64/frame-constants-sw64.cc",
+      "src/execution/sw64/frame-constants-sw64.h",
+      "src/execution/sw64/simulator-sw64.cc",
+      "src/execution/sw64/simulator-sw64.h",
+      "src/regexp/sw64/regexp-macro-assembler-sw64.cc",
+      "src/regexp/sw64/regexp-macro-assembler-sw64.h",
+      "src/wasm/baseline/sw64/liftoff-assembler-sw64.h",
+    ]
   } else if (v8_current_cpu == "mips" || v8_current_cpu == "mipsel") {
     sources += [  ### gcmole(arch:mipsel) ###
       "src/codegen/mips/assembler-mips-inl.h",
@@ -4328,6 +4366,8 @@ v8_source_set("v8_cppgc_shared") {
       sources += [ "src/heap/base/asm/mips/push_registers_asm.cc" ]
     } else if (current_cpu == "mips64el") {
       sources += [ "src/heap/base/asm/mips64/push_registers_asm.cc" ]
+    } else if (current_cpu == "sw_64") {
+      sources += [ "src/heap/base/asm/sw64/push_registers_asm.cc" ]
     }
   } else if (is_win) {
     if (current_cpu == "x64") {
diff --git a/src/3rdparty/chromium/v8/src/base/build_config.h b/src/3rdparty/chromium/v8/src/base/build_config.h
index ad287c9290..56e5abee65 100644
--- a/src/3rdparty/chromium/v8/src/base/build_config.h
+++ b/src/3rdparty/chromium/v8/src/base/build_config.h
@@ -46,6 +46,9 @@
 #else
 #define V8_HOST_ARCH_32_BIT 1
 #endif
+#elif defined(__sw_64__)
+#define V8_HOST_ARCH_SW64 1
+#define V8_HOST_ARCH_64_BIT 1
 #else
 #error "Host architecture was not detected as supported by v8"
 #endif
@@ -77,7 +80,7 @@
 // environment as presented by the compiler.
 #if !V8_TARGET_ARCH_X64 && !V8_TARGET_ARCH_IA32 && !V8_TARGET_ARCH_ARM &&      \
     !V8_TARGET_ARCH_ARM64 && !V8_TARGET_ARCH_MIPS && !V8_TARGET_ARCH_MIPS64 && \
-    !V8_TARGET_ARCH_PPC && !V8_TARGET_ARCH_PPC64 && !V8_TARGET_ARCH_S390
+    !V8_TARGET_ARCH_PPC && !V8_TARGET_ARCH_PPC64 && !V8_TARGET_ARCH_S390 && !V8_TARGET_ARCH_SW64
 #if defined(_M_X64) || defined(__x86_64__)
 #define V8_TARGET_ARCH_X64 1
 #elif defined(_M_IX86) || defined(__i386__)
@@ -94,6 +97,8 @@
 #define V8_TARGET_ARCH_PPC64 1
 #elif defined(_ARCH_PPC)
 #define V8_TARGET_ARCH_PPC 1
+#elif defined(__sw_64__)
+#define V8_TARGET_ARCH_SW64 1
 #else
 #error Target architecture was not detected as supported by v8
 #endif
@@ -128,6 +133,8 @@
 #else
 #define V8_TARGET_ARCH_32_BIT 1
 #endif
+#elif V8_TARGET_ARCH_SW64
+#define V8_TARGET_ARCH_64_BIT 1
 #else
 #error Unknown target architecture pointer size
 #endif
@@ -156,6 +163,9 @@
 #if (V8_TARGET_ARCH_MIPS64 && !(V8_HOST_ARCH_X64 || V8_HOST_ARCH_MIPS64))
 #error Target architecture mips64 is only supported on mips64 and x64 host
 #endif
+#if (V8_TARGET_ARCH_SW64 && !(V8_HOST_ARCH_X64 || V8_HOST_ARCH_SW64))
+#error Target architecture sw64 is only supported on sw64 and x64 host
+#endif
 
 // Determine architecture endianness.
 #if V8_TARGET_ARCH_IA32
@@ -190,6 +200,8 @@
 #else
 #define V8_TARGET_BIG_ENDIAN 1
 #endif
+#elif V8_TARGET_ARCH_SW64
+#define V8_TARGET_LITTLE_ENDIAN 1
 #else
 #error Unknown target architecture endianness
 #endif
diff --git a/src/3rdparty/chromium/v8/src/base/cpu.cc b/src/3rdparty/chromium/v8/src/base/cpu.cc
index c0e9e707aa..1c87abcc4c 100644
--- a/src/3rdparty/chromium/v8/src/base/cpu.cc
+++ b/src/3rdparty/chromium/v8/src/base/cpu.cc
@@ -80,7 +80,7 @@ static V8_INLINE void __cpuid(int cpu_info[4], int info_type) {
 #endif  // !V8_LIBC_MSVCRT
 
 #elif V8_HOST_ARCH_ARM || V8_HOST_ARCH_ARM64 || V8_HOST_ARCH_MIPS || \
-    V8_HOST_ARCH_MIPS64
+    V8_HOST_ARCH_MIPS64 || V8_HOST_ARCH_SW64
 
 #if V8_OS_LINUX
 
@@ -692,7 +692,7 @@ CPU::CPU()
 
 #endif  // V8_OS_LINUX
 
-#elif V8_HOST_ARCH_MIPS || V8_HOST_ARCH_MIPS64
+#elif V8_HOST_ARCH_MIPS || V8_HOST_ARCH_MIPS64 || V8_HOST_ARCH_SW64
 
   // Simple detection of FPU at runtime for Linux.
   // It is based on /proc/cpuinfo, which reveals hardware configuration
diff --git a/src/3rdparty/chromium/v8/src/base/platform/platform-posix.cc b/src/3rdparty/chromium/v8/src/base/platform/platform-posix.cc
index 89173b593a..7b771fb0dc 100644
--- a/src/3rdparty/chromium/v8/src/base/platform/platform-posix.cc
+++ b/src/3rdparty/chromium/v8/src/base/platform/platform-posix.cc
@@ -324,6 +324,9 @@ void* OS::GetRandomMmapAddr() {
   // 42 bits of virtual addressing. Truncate to 40 bits to allow kernel chance
   // to fulfill request.
   raw_addr &= uint64_t{0xFFFFFF0000};
+#elif V8_TARGET_ARCH_SW64
+  raw_addr &= uint64_t{0x07FF0000};
+  raw_addr += 0x20000000000;
 #else
   raw_addr &= 0x3FFFF000;
 
@@ -512,6 +515,8 @@ void OS::DebugBreak() {
 #elif V8_HOST_ARCH_S390
   // Software breakpoint instruction is 0x0001
   asm volatile(".word 0x0001");
+#elif V8_HOST_ARCH_SW64
+  asm("sys_call 0x80");
 #else
 #error Unsupported host architecture.
 #endif
diff --git a/src/3rdparty/chromium/v8/src/builtins/builtins-sharedarraybuffer-gen.cc b/src/3rdparty/chromium/v8/src/builtins/builtins-sharedarraybuffer-gen.cc
index 26cf4fe159..cf24aac892 100644
--- a/src/3rdparty/chromium/v8/src/builtins/builtins-sharedarraybuffer-gen.cc
+++ b/src/3rdparty/chromium/v8/src/builtins/builtins-sharedarraybuffer-gen.cc
@@ -510,7 +510,7 @@ TF_BUILTIN(AtomicsCompareExchange, SharedArrayBufferBuiltinsAssembler) {
   TNode<UintPtrT> index_word = ValidateAtomicAccess(array, index, context);
 
 #if V8_TARGET_ARCH_MIPS || V8_TARGET_ARCH_MIPS64 || V8_TARGET_ARCH_PPC64 || \
-    V8_TARGET_ARCH_PPC || V8_TARGET_ARCH_S390 || V8_TARGET_ARCH_S390X
+    V8_TARGET_ARCH_PPC || V8_TARGET_ARCH_S390 || V8_TARGET_ARCH_S390X || V8_TARGET_ARCH_SW64
   USE(array_buffer);
   TNode<Number> index_number = ChangeUintPtrToTagged(index_word);
   Return(CallRuntime(Runtime::kAtomicsCompareExchange, context, array,
@@ -679,7 +679,7 @@ void SharedArrayBufferBuiltinsAssembler::AtomicBinopBuiltinCommon(
   TNode<UintPtrT> index_word = ValidateAtomicAccess(array, index, context);
 
 #if V8_TARGET_ARCH_MIPS || V8_TARGET_ARCH_MIPS64 || V8_TARGET_ARCH_PPC64 || \
-    V8_TARGET_ARCH_PPC || V8_TARGET_ARCH_S390 || V8_TARGET_ARCH_S390X
+    V8_TARGET_ARCH_PPC || V8_TARGET_ARCH_S390 || V8_TARGET_ARCH_S390X || V8_TARGET_ARCH_SW64
   USE(array_buffer);
   TNode<Number> index_number = ChangeUintPtrToTagged(index_word);
   Return(CallRuntime(runtime_function, context, array, index_number, value));
diff --git a/src/3rdparty/chromium/v8/src/builtins/builtins.cc b/src/3rdparty/chromium/v8/src/builtins/builtins.cc
index 31682f3974..9f09e96ef3 100644
--- a/src/3rdparty/chromium/v8/src/builtins/builtins.cc
+++ b/src/3rdparty/chromium/v8/src/builtins/builtins.cc
@@ -490,7 +490,7 @@ bool Builtins::CodeObjectIsExecutable(int builtin_index) {
     case Builtins::kCEntry_Return1_DontSaveFPRegs_ArgvOnStack_NoBuiltinExit:
       return true;
     default:
-#if V8_TARGET_ARCH_MIPS || V8_TARGET_ARCH_MIPS64
+#if V8_TARGET_ARCH_MIPS || V8_TARGET_ARCH_MIPS64 || V8_TARGET_ARCH_SW64
       // TODO(Loongson): Move non-JS linkage builtins code objects into RO_SPACE
       // caused MIPS platform to crash, and we need some time to handle it. Now
       // disable this change temporarily on MIPS platform.
diff --git a/src/3rdparty/chromium/v8/src/builtins/sw64/builtins-sw64.cc b/src/3rdparty/chromium/v8/src/builtins/sw64/builtins-sw64.cc
new file mode 100755
index 0000000000..c9565819ff
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/builtins/sw64/builtins-sw64.cc
@@ -0,0 +1,3222 @@
+// Copyright 2012 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#if V8_TARGET_ARCH_SW64
+
+#include "src/api/api-arguments.h"
+#include "src/codegen/code-factory.h"
+#include "src/debug/debug.h"
+#include "src/deoptimizer/deoptimizer.h"
+#include "src/execution/frame-constants.h"
+#include "src/execution/frames.h"
+#include "src/logging/counters.h"
+// For interpreter_entry_return_pc_offset. TODO(jkummerow): Drop.
+#include "src/codegen/macro-assembler-inl.h"
+#include "src/codegen/sw64/constants-sw64.h"
+#include "src/codegen/register-configuration.h"
+#include "src/heap/heap-inl.h"
+#include "src/objects/cell.h"
+#include "src/objects/foreign.h"
+#include "src/objects/heap-number.h"
+#include "src/objects/js-generator.h"
+#include "src/objects/objects-inl.h"
+#include "src/objects/smi.h"
+#include "src/runtime/runtime.h"
+#include "src/wasm/wasm-linkage.h"
+#include "src/wasm/wasm-objects.h"
+
+namespace v8 {
+namespace internal {
+
+#define __ ACCESS_MASM(masm)
+
+void Builtins::Generate_Adaptor(MacroAssembler* masm, Address address) {
+  __ li(kJavaScriptCallExtraArg1Register, ExternalReference::Create(address));
+    __ Jump(BUILTIN_CODE(masm->isolate(), AdaptorWithBuiltinExitFrame),
+            RelocInfo::CODE_TARGET);
+}
+
+static void GenerateTailCallToReturnedCode(MacroAssembler* masm,
+                                           Runtime::FunctionId function_id) {
+  // ----------- S t a t e -------------
+  //  -- a0 : actual argument count
+  //  -- a1 : target function (preserved for callee)
+  //  -- a3 : new target (preserved for callee)
+  // -----------------------------------
+  {
+    FrameScope scope(masm, StackFrame::INTERNAL);
+    // Push a copy of the target function, the new target and the actual
+    // argument count.
+    // Push function as parameter to the runtime call.
+    __ SmiTag(kJavaScriptCallArgCountRegister);
+    __ Push(kJavaScriptCallTargetRegister, kJavaScriptCallNewTargetRegister,
+            kJavaScriptCallArgCountRegister, kJavaScriptCallTargetRegister);
+
+    __ CallRuntime(function_id, 1);
+    // Restore target function, new target and actual argument count.
+    __ Pop(kJavaScriptCallTargetRegister, kJavaScriptCallNewTargetRegister,
+           kJavaScriptCallArgCountRegister);
+    __ SmiUntag(kJavaScriptCallArgCountRegister);
+  }
+
+  static_assert(kJavaScriptCallCodeStartRegister == a2, "ABI mismatch");
+  __ Addl(a2, v0, Operand(Code::kHeaderSize - kHeapObjectTag));
+  __ Jump(a2);
+}
+
+namespace {
+
+enum StackLimitKind { kInterruptStackLimit, kRealStackLimit };
+
+void LoadStackLimit(MacroAssembler* masm, Register destination,
+                    StackLimitKind kind) {
+  DCHECK(masm->root_array_available());
+  Isolate* isolate = masm->isolate();
+  ExternalReference limit =
+      kind == StackLimitKind::kRealStackLimit
+          ? ExternalReference::address_of_real_jslimit(isolate)
+          : ExternalReference::address_of_jslimit(isolate);
+  DCHECK(TurboAssembler::IsAddressableThroughRootRegister(isolate, limit));
+
+  intptr_t offset =
+      TurboAssembler::RootRegisterOffsetForExternalReference(isolate, limit);
+  CHECK(is_int32(offset));
+  __ Ldl(destination, MemOperand(kRootRegister, static_cast<int32_t>(offset)));
+}
+
+void Generate_JSBuiltinsConstructStubHelper(MacroAssembler* masm) {
+  // ----------- S t a t e -------------
+  //  -- a0     : number of arguments
+  //  -- a1     : constructor function
+  //  -- a3     : new target
+  //  -- cp     : context
+  //  -- ra     : return address
+  //  -- sp[...]: constructor arguments
+  // -----------------------------------
+
+  // Enter a construct frame.
+  {
+    FrameScope scope(masm, StackFrame::CONSTRUCT);
+
+    // Preserve the incoming parameters on the stack.
+    __ SmiTag(a0);
+    __ Push(cp, a0);
+    __ SmiUntag(a0);
+
+    // Set up pointer to last argument (skip receiver).
+    __ Addl(
+        t2, fp,
+        Operand(StandardFrameConstants::kCallerSPOffset + kSystemPointerSize));
+    // Copy arguments and receiver to the expression stack.
+    __ PushArray(t2, a0, t3, t0);
+    // The receiver for the builtin/api call.
+    __ PushRoot(RootIndex::kTheHoleValue);
+
+    // Call the function.
+    // a0: number of arguments (untagged)
+    // a1: constructor function
+    // a3: new target
+    __ InvokeFunctionWithNewTarget(a1, a3, a0, CALL_FUNCTION);
+
+    // Restore context from the frame.
+    __ Ldl(cp, MemOperand(fp, ConstructFrameConstants::kContextOffset));
+    // Restore smi-tagged arguments count from the frame.
+    __ Ldl(t3, MemOperand(fp, ConstructFrameConstants::kLengthOffset));
+    // Leave construct frame.
+  }
+
+  // Remove caller arguments from the stack and return.
+  __ SmiScale(t3, t3, kPointerSizeLog2);
+  __ Addl(sp, sp, t3);
+  __ Addl(sp, sp, kPointerSize);
+  __ Ret();
+}
+
+static void Generate_StackOverflowCheck(MacroAssembler* masm, Register num_args,
+                                        Register scratch1, Register scratch2,
+                                        Label* stack_overflow) {
+  // Check the stack for overflow. We are not trying to catch
+  // interruptions (e.g. debug break and preemption) here, so the "real stack
+  // limit" is checked.
+  LoadStackLimit(masm, scratch1, StackLimitKind::kRealStackLimit);
+  // Make scratch1 the space we have left. The stack might already be overflowed
+  // here which will cause scratch1 to become negative.
+  __ subl(sp, scratch1, scratch1);
+  // Check if the arguments will overflow the stack.
+  __ slll(num_args, kPointerSizeLog2, scratch2);
+  // Signed comparison.
+  __ Branch(stack_overflow, le, scratch1, Operand(scratch2));
+}
+
+}  // namespace
+
+// The construct stub for ES5 constructor functions and ES6 class constructors.
+void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
+  // ----------- S t a t e -------------
+  //  --      a0: number of arguments (untagged)
+  //  --      a1: constructor function
+  //  --      a3: new target
+  //  --      cp: context
+  //  --      ra: return address
+  //  -- sp[...]: constructor arguments
+  // -----------------------------------
+
+  // Enter a construct frame.
+  {
+    FrameScope scope(masm, StackFrame::CONSTRUCT);
+    Label post_instantiation_deopt_entry, not_create_implicit_receiver;
+
+    // Preserve the incoming parameters on the stack.
+    __ SmiTag(a0);
+    __ Push(cp, a0, a1);
+    __ PushRoot(RootIndex::kUndefinedValue);
+    __ Push(a3);
+
+    // ----------- S t a t e -------------
+    //  --        sp[0*kPointerSize]: new target
+    //  --        sp[1*kPointerSize]: padding
+    //  -- a1 and sp[2*kPointerSize]: constructor function
+    //  --        sp[3*kPointerSize]: number of arguments (tagged)
+    //  --        sp[4*kPointerSize]: context
+    // -----------------------------------
+
+    __ Ldl(t2, FieldMemOperand(a1, JSFunction::kSharedFunctionInfoOffset));
+    __ Ldwu(t2, FieldMemOperand(t2, SharedFunctionInfo::kFlagsOffset));
+    __ DecodeField<SharedFunctionInfo::FunctionKindBits>(t2);
+    __ JumpIfIsInRange(t2, kDefaultDerivedConstructor, kDerivedConstructor,
+                     &not_create_implicit_receiver);
+
+    // If not derived class constructor: Allocate the new receiver object.
+    __ IncrementCounter(masm->isolate()->counters()->constructed_objects(), 1,
+                        t2, t3);
+    __ Call(BUILTIN_CODE(masm->isolate(), FastNewObject),
+            RelocInfo::CODE_TARGET);
+    __ Branch(&post_instantiation_deopt_entry);
+
+    // Else: use TheHoleValue as receiver for constructor call
+    __ bind(&not_create_implicit_receiver);
+    __ LoadRoot(v0, RootIndex::kTheHoleValue);
+
+    // ----------- S t a t e -------------
+    //  --                          v0: receiver
+    //  -- Slot 4 / sp[0*kPointerSize]: new target
+    //  -- Slot 3 / sp[1*kPointerSize]: padding
+    //  -- Slot 2 / sp[2*kPointerSize]: constructor function
+    //  -- Slot 1 / sp[3*kPointerSize]: number of arguments (tagged)
+    //  -- Slot 0 / sp[4*kPointerSize]: context
+    // -----------------------------------
+    // Deoptimizer enters here.
+    masm->isolate()->heap()->SetConstructStubCreateDeoptPCOffset(
+        masm->pc_offset());
+    __ bind(&post_instantiation_deopt_entry);
+
+    // Restore new target.
+    __ Pop(a3);
+
+  // Push the allocated receiver to the stack.
+  __ Push(v0);
+
+  // We need two copies because we may have to return the original one
+  // and the calling conventions dictate that the called function pops the
+  // receiver. The second copy is pushed after the arguments, we saved in t9
+  // since v0 will store the return value of callRuntime.
+  __ mov(t9, v0);
+
+  // Set up pointer to last argument.
+  __ Addl(t2, fp, Operand(StandardFrameConstants::kCallerSPOffset +
+                           kSystemPointerSize));
+
+    // ----------- S t a t e -------------
+    //  --                 r3: new target
+    //  -- sp[0*kPointerSize]: implicit receiver
+    //  -- sp[1*kPointerSize]: implicit receiver
+    //  -- sp[2*kPointerSize]: padding
+    //  -- sp[3*kPointerSize]: constructor function
+    //  -- sp[4*kPointerSize]: number of arguments (tagged)
+    //  -- sp[5*kPointerSize]: context
+    // -----------------------------------
+
+    // Restore constructor function and argument count.
+    __ Ldl(a1, MemOperand(fp, ConstructFrameConstants::kConstructorOffset));
+    __ Ldl(a0, MemOperand(fp, ConstructFrameConstants::kLengthOffset));
+    __ SmiUntag(a0);
+
+    Label enough_stack_space, stack_overflow;
+    Generate_StackOverflowCheck(masm, a0, t0, t1, &stack_overflow);
+    __ Branch(&enough_stack_space);
+
+    __ bind(&stack_overflow);
+    // Restore the context from the frame.
+    __ Ldl(cp, MemOperand(fp, ConstructFrameConstants::kContextOffset));
+    __ CallRuntime(Runtime::kThrowStackOverflow);
+    // Unreachable code.
+    __ sys_call(0x80);
+
+    __ bind(&enough_stack_space);
+
+    // Copy arguments and receiver to the expression stack.
+  __ PushArray(t2, a0, t0, t1);
+  // We need two copies because we may have to return the original one
+  // and the calling conventions dictate that the called function pops the
+  // receiver. The second copy is pushed after the arguments,
+  __ Push(t9);
+
+    // Call the function.
+  __ InvokeFunctionWithNewTarget(a1, a3, a0, CALL_FUNCTION);
+
+    // ----------- S t a t e -------------
+    //  --                 v0: constructor result
+    //  -- sp[0*kPointerSize]: implicit receiver
+    //  -- sp[1*kPointerSize]: padding
+    //  -- sp[2*kPointerSize]: constructor function
+    //  -- sp[3*kPointerSize]: number of arguments
+    //  -- sp[4*kPointerSize]: context
+    // -----------------------------------
+
+    // Store offset of return address for deoptimizer.
+    masm->isolate()->heap()->SetConstructStubInvokeDeoptPCOffset(
+        masm->pc_offset());
+
+    // Restore the context from the frame.
+    __ Ldl(cp, MemOperand(fp, ConstructFrameConstants::kContextOffset));
+
+    // If the result is an object (in the ECMA sense), we should get rid
+    // of the receiver and use the result; see ECMA-262 section 13.2.2-7
+    // on page 74.
+    Label use_receiver, do_throw, leave_frame;
+
+    // If the result is undefined, we jump out to using the implicit receiver.
+    __ JumpIfRoot(v0, RootIndex::kUndefinedValue, &use_receiver);
+
+    // Otherwise we do a smi check and fall through to check if the return value
+    // is a valid receiver.
+
+    // If the result is a smi, it is *not* an object in the ECMA sense.
+    __ JumpIfSmi(v0, &use_receiver);
+
+    // If the type of the result (stored in its map) is less than
+    // FIRST_JS_RECEIVER_TYPE, it is not an object in the ECMA sense.
+    __ GetObjectType(v0, t2, t2);
+    STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+    __ Branch(&leave_frame, greater_equal, t2, Operand(FIRST_JS_RECEIVER_TYPE));
+    __ Branch(&use_receiver);
+
+    __ bind(&do_throw);
+    __ CallRuntime(Runtime::kThrowConstructorReturnedNonObject);
+
+    // Throw away the result of the constructor invocation and use the
+    // on-stack receiver as the result.
+    __ bind(&use_receiver);
+    __ Ldl(v0, MemOperand(sp, 0 * kPointerSize));
+    __ JumpIfRoot(v0, RootIndex::kTheHoleValue, &do_throw);
+
+    __ bind(&leave_frame);
+    // Restore smi-tagged arguments count from the frame.
+    __ Ldl(a1, MemOperand(fp, ConstructFrameConstants::kLengthOffset));
+    // Leave construct frame.
+  }
+    // Remove caller arguments from the stack and return.
+    __ SmiScale(a4, a1, kPointerSizeLog2);
+    __ Addl(sp, sp, a4);
+    __ Addl(sp, sp, kPointerSize);
+    __ Ret();
+}
+
+void Builtins::Generate_JSBuiltinsConstructStub(MacroAssembler* masm) {
+  Generate_JSBuiltinsConstructStubHelper(masm);
+}
+
+static void GetSharedFunctionInfoBytecode(MacroAssembler* masm,
+                                          Register sfi_data,
+                                          Register scratch1) {
+  Label done;
+
+  __ GetObjectType(sfi_data, scratch1, scratch1);
+  __ Branch(&done, ne, scratch1, Operand(INTERPRETER_DATA_TYPE));
+  __ Ldl(sfi_data,
+        FieldMemOperand(sfi_data, InterpreterData::kBytecodeArrayOffset));
+
+  __ bind(&done);
+}
+
+// static
+void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
+  // ----------- S t a t e -------------
+  //  -- v0 : the value to pass to the generator
+  //  -- a1 : the JSGeneratorObject to resume
+  //  -- ra : return address
+  // -----------------------------------
+  __ AssertGeneratorObject(a1);
+
+  // Store input value into generator object.
+  __ Stl(v0, FieldMemOperand(a1, JSGeneratorObject::kInputOrDebugPosOffset));
+  __ RecordWriteField(a1, JSGeneratorObject::kInputOrDebugPosOffset, v0, a3,
+                      kRAHasNotBeenSaved, kDontSaveFPRegs);
+
+  // Load suspended function and context.
+  __ Ldl(a4, FieldMemOperand(a1, JSGeneratorObject::kFunctionOffset));
+  __ Ldl(cp, FieldMemOperand(a4, JSFunction::kContextOffset));
+
+  // Flood function if we are stepping.
+  Label prepare_step_in_if_stepping, prepare_step_in_suspended_generator;
+  Label stepping_prepared;
+  ExternalReference debug_hook =
+      ExternalReference::debug_hook_on_function_call_address(masm->isolate());
+  __ li(a5, debug_hook);
+  __ Ldb(a5, MemOperand(a5));
+  __ Branch(&prepare_step_in_if_stepping, ne, a5, Operand(zero_reg));
+
+  // Flood function if we need to continue stepping in the suspended generator.
+  ExternalReference debug_suspended_generator =
+      ExternalReference::debug_suspended_generator_address(masm->isolate());
+  __ li(a5, debug_suspended_generator);
+  __ Ldl(a5, MemOperand(a5));
+  __ Branch(&prepare_step_in_suspended_generator, eq, a1, Operand(a5));
+  __ bind(&stepping_prepared);
+
+  // Check the stack for overflow. We are not trying to catch interruptions
+  // (i.e. debug break and preemption) here, so check the "real stack limit".
+  Label stack_overflow;
+  LoadStackLimit(masm, kScratchReg, StackLimitKind::kRealStackLimit);
+  __ Branch(&stack_overflow, lo, sp, Operand(kScratchReg));
+
+  // ----------- S t a t e -------------
+  //  -- a1    : the JSGeneratorObject to resume
+  //  -- a4    : generator function
+  //  -- cp    : generator context
+  //  -- ra    : return address
+  //  -- sp[0] : generator receiver
+  // -----------------------------------
+
+  // Push holes for arguments to generator function. Since the parser forced
+  // context allocation for any variables in generators, the actual argument
+  // values have already been copied into the context and these dummy values
+  // will never be used.
+  __ Ldl(a3, FieldMemOperand(a4, JSFunction::kSharedFunctionInfoOffset));
+  __ Ldhu(a3,
+         FieldMemOperand(a3, SharedFunctionInfo::kFormalParameterCountOffset));
+  __ Ldl(t1,
+        FieldMemOperand(a1, JSGeneratorObject::kParametersAndRegistersOffset));
+  {
+    Label done_loop, loop;
+    __ bind(&loop);
+    __ Subl(a3, a3, Operand(1));
+    __ Branch(&done_loop, lt, a3, Operand(zero_reg));
+    __ s8addl(a3, t1, kScratchReg);  DCHECK_EQ(kPointerSizeLog2, 3);
+    __ Ldl(kScratchReg, FieldMemOperand(kScratchReg, FixedArray::kHeaderSize));
+    __ Push(kScratchReg);
+    __ Branch(&loop);
+    __ bind(&done_loop);
+    // Push receiver.
+    __ Ldl(kScratchReg, FieldMemOperand(a1, JSGeneratorObject::kReceiverOffset));
+    __ Push(kScratchReg);
+  }
+
+  // Underlying function needs to have bytecode available.
+  if (FLAG_debug_code) {
+    __ Ldl(a3, FieldMemOperand(a4, JSFunction::kSharedFunctionInfoOffset));
+    __ Ldl(a3, FieldMemOperand(a3, SharedFunctionInfo::kFunctionDataOffset));
+    GetSharedFunctionInfoBytecode(masm, a3, a0);
+    __ GetObjectType(a3, a3, a3);
+    __ Assert(eq, AbortReason::kMissingBytecodeArray, a3,
+              Operand(BYTECODE_ARRAY_TYPE));
+  }
+
+  // Resume (Ignition/TurboFan) generator object.
+  {
+    __ Ldl(a0, FieldMemOperand(a4, JSFunction::kSharedFunctionInfoOffset));
+    __ Ldhu(a0, FieldMemOperand(
+                   a0, SharedFunctionInfo::kFormalParameterCountOffset));
+    // We abuse new.target both to indicate that this is a resume call and to
+    // pass in the generator object.  In ordinary calls, new.target is always
+    // undefined because generator functions are non-constructable.
+    __ Move(a3, a1);
+    __ Move(a1, a4);
+    static_assert(kJavaScriptCallCodeStartRegister == a2, "ABI mismatch");
+    __ Ldl(a2, FieldMemOperand(a1, JSFunction::kCodeOffset));
+    __ Addl(a2, a2, Operand(Code::kHeaderSize - kHeapObjectTag));
+    __ Jump(a2);
+  }
+
+  __ bind(&prepare_step_in_if_stepping);
+  {
+    FrameScope scope(masm, StackFrame::INTERNAL);
+    __ Push(a1, a4);
+    // Push hole as receiver since we do not use it for stepping.
+    __ PushRoot(RootIndex::kTheHoleValue);
+    __ CallRuntime(Runtime::kDebugOnFunctionCall);
+    __ Pop(a1);
+  }
+  __ Ldl(a4, FieldMemOperand(a1, JSGeneratorObject::kFunctionOffset));
+  __ Branch(&stepping_prepared);
+
+  __ bind(&prepare_step_in_suspended_generator);
+  {
+    FrameScope scope(masm, StackFrame::INTERNAL);
+    __ Push(a1);
+    __ CallRuntime(Runtime::kDebugPrepareStepInSuspendedGenerator);
+    __ Pop(a1);
+  }
+  __ Ldl(a4, FieldMemOperand(a1, JSGeneratorObject::kFunctionOffset));
+  __ Branch(&stepping_prepared);
+
+  __ bind(&stack_overflow);
+  {
+    FrameScope scope(masm, StackFrame::INTERNAL);
+    __ CallRuntime(Runtime::kThrowStackOverflow);
+    __ sys_call(0x80);  // This should be unreachable.
+  }
+}
+
+void Builtins::Generate_ConstructedNonConstructable(MacroAssembler* masm) {
+  FrameScope scope(masm, StackFrame::INTERNAL);
+  __ Push(a1);
+  __ CallRuntime(Runtime::kThrowConstructedNonConstructable);
+}
+
+// Clobbers scratch1 and scratch2; preserves all other registers.
+static void Generate_CheckStackOverflow(MacroAssembler* masm, Register argc,
+                                        Register scratch1, Register scratch2) {
+  // Check the stack for overflow. We are not trying to catch
+  // interruptions (e.g. debug break and preemption) here, so the "real stack
+  // limit" is checked.
+  Label okay;
+  LoadStackLimit(masm, scratch1, StackLimitKind::kRealStackLimit);
+  // Make a2 the space we have left. The stack might already be overflowed
+  // here which will cause r2 to become negative.
+  __ Subl(scratch1, sp, scratch1);
+  // Check if the arguments will overflow the stack.
+  __ slll(argc, kPointerSizeLog2,scratch2);
+  __ Branch(&okay, gt, scratch1, Operand(scratch2));  // Signed comparison.
+
+  // Out of stack space.
+  __ CallRuntime(Runtime::kThrowStackOverflow);
+
+  __ bind(&okay);
+}
+
+namespace {
+
+// Called with the native C calling convention. The corresponding function
+// signature is either:
+//
+//   using JSEntryFunction = GeneratedCode<Address(
+//       Address root_register_value, Address new_target, Address target,
+//       Address receiver, intptr_t argc, Address** args)>;
+// or
+//   using JSEntryFunction = GeneratedCode<Address(
+//       Address root_register_value, MicrotaskQueue* microtask_queue)>;
+void Generate_JSEntryVariant(MacroAssembler* masm, StackFrame::Type type,
+                             Builtins::Name entry_trampoline) {
+  Label invoke, handler_entry, exit;
+
+  {
+    NoRootArrayScope no_root_array(masm);
+
+    // TODO(plind): unify the ABI description here.
+    // Registers:
+    //  either
+    //   a0: root register value
+    //   a1: entry address
+    //   a2: function
+    //   a3: receiver
+    //   a4: argc
+    //   a5: argv
+    //  or
+    //   a0: root register value
+    //   a1: microtask_queue
+    //
+    // Stack:
+    // 0 arg slots on sw64
+
+    // Save callee saved registers on the stack.
+    __ MultiPush(kCalleeSaved | ra.bit());
+
+    // Save callee-saved FPU registers.
+    __ MultiPushFPU(kCalleeSavedFPU);
+    // Set up the reserved register for 0.0.
+    //__ Move(kDoubleRegZero, 0.0);
+
+    // Initialize the root register.
+    // C calling convention. The first argument is passed in a0.
+    __ mov(kRootRegister, a0);
+  }
+
+  // a1: entry address
+  // a2: function
+  // a3: receiver
+  // a4: argc
+  // a5: argv
+
+  // We build an EntryFrame.
+  __ li(s1, Operand(-1));  // Push a bad frame pointer to fail if it is used.
+  __ li(s2, Operand(StackFrame::TypeToMarker(type)));
+  __ li(s3, Operand(StackFrame::TypeToMarker(type)));
+  ExternalReference c_entry_fp = ExternalReference::Create(
+      IsolateAddressId::kCEntryFPAddress, masm->isolate());
+  __ li(t8, c_entry_fp);
+  __ Ldl(t8, MemOperand(t8));
+  __ Push(s1, s2, s3, t8);
+  // Set up frame pointer for the frame to be pushed.
+  __ addl(sp, -EntryFrameConstants::kCallerFPOffset, fp); // negtive imm
+
+  // Registers:
+  //  either
+  //   a1: entry address
+  //   a2: function
+  //   a3: receiver
+  //   a4: argc
+  //   a5: argv
+  //  or
+  //   a1: microtask_queue
+  //
+  // Stack:
+  // caller fp          |
+  // function slot      | entry frame
+  // context slot       |
+  // bad fp (0xFF...F)  |
+  // callee saved registers + ra
+  // [ O32: 4 args slots]
+  // args
+
+  // If this is the outermost JS call, set js_entry_sp value.
+  Label non_outermost_js;
+  ExternalReference js_entry_sp = ExternalReference::Create(
+      IsolateAddressId::kJSEntrySPAddress, masm->isolate());
+  __ li(s1, js_entry_sp);
+  __ Ldl(s2, MemOperand(s1));
+  __ Branch(&non_outermost_js, ne, s2, Operand(zero_reg));
+  __ Stl(fp, MemOperand(s1));
+  __ li(s3, Operand(StackFrame::OUTERMOST_JSENTRY_FRAME));
+  Label cont;
+  __ br(&cont);
+  //__ nop();  // Branch delay slot nop.
+  __ bind(&non_outermost_js);
+  __ li(s3, Operand(StackFrame::INNER_JSENTRY_FRAME));
+  __ bind(&cont);
+  __ push(s3);
+
+  // Jump to a faked try block that does the invoke, with a faked catch
+  // block that sets the pending exception.
+  __ jmp(&invoke);
+  __ bind(&handler_entry);
+
+  // Store the current pc as the handler offset. It's used later to create the
+  // handler table.
+  masm->isolate()->builtins()->SetJSEntryHandlerOffset(handler_entry.pos());
+
+  // Caught exception: Store result (exception) in the pending exception
+  // field in the JSEnv and return a failure sentinel.  Coming in here the
+  // fp will be invalid because the PushStackHandler below sets it to 0 to
+  // signal the existence of the JSEntry frame.
+  __ li(s1, ExternalReference::Create(
+                IsolateAddressId::kPendingExceptionAddress, masm->isolate()));
+  __ Stl(v0, MemOperand(s1));  // We come back from 'invoke'. result is in v0.
+  __ LoadRoot(v0, RootIndex::kException);
+  __ br(&exit);  // b exposes branch delay slot.
+  //__ nop();     // Branch delay slot nop.
+
+  // Invoke: Link this frame into the handler chain.
+  __ bind(&invoke);
+  __ PushStackHandler();
+  // If an exception not caught by another handler occurs, this handler
+  // returns control to the code after the bal(&invoke) above, which
+  // restores all kCalleeSaved registers (including cp and fp) to their
+  // saved values before returning a failure to C.
+  //
+  // Registers:
+  //  either
+  //   a0: root register value
+  //   a1: entry address
+  //   a2: function
+  //   a3: receiver
+  //   a4: argc
+  //   a5: argv
+  //  or
+  //   a0: root register value
+  //   a1: microtask_queue
+  //
+  // Stack:
+  // handler frame
+  // entry frame
+  // callee saved registers + ra
+  // [ O32: 4 args slots]
+  // args
+  //
+  // Invoke the function by calling through JS entry trampoline builtin and
+  // pop the faked function when we return.
+
+  Handle<Code> trampoline_code =
+      masm->isolate()->builtins()->builtin_handle(entry_trampoline);
+  __ Call(trampoline_code, RelocInfo::CODE_TARGET);
+
+  // Unlink this frame from the handler chain.
+  __ PopStackHandler();
+
+  __ bind(&exit);  // v0 holds result
+  // Check if the current stack frame is marked as the outermost JS frame.
+  Label non_outermost_js_2;
+  __ pop(a5);
+  __ Branch(&non_outermost_js_2, ne, a5,
+            Operand(StackFrame::OUTERMOST_JSENTRY_FRAME));
+  __ li(a5, js_entry_sp);
+  __ Stl(zero_reg, MemOperand(a5));
+  __ bind(&non_outermost_js_2);
+
+  // Restore the top frame descriptors from the stack.
+  __ pop(a5);
+  __ li(a4, ExternalReference::Create(IsolateAddressId::kCEntryFPAddress,
+                                      masm->isolate()));
+  __ Stl(a5, MemOperand(a4));
+
+  // Reset the stack to the callee saved registers.
+  __ addl(sp, -EntryFrameConstants::kCallerFPOffset, sp);  // negative imm
+
+  // Restore callee-saved fpu registers.
+  __ MultiPopFPU(kCalleeSavedFPU);
+
+  // Restore callee saved registers from the stack.
+  __ MultiPop(kCalleeSaved | ra.bit());
+  // Return.
+  __ Jump(ra);
+}
+
+}  // namespace
+
+void Builtins::Generate_JSEntry(MacroAssembler* masm) {
+  Generate_JSEntryVariant(masm, StackFrame::ENTRY,
+                          Builtins::kJSEntryTrampoline);
+}
+
+void Builtins::Generate_JSConstructEntry(MacroAssembler* masm) {
+  Generate_JSEntryVariant(masm, StackFrame::CONSTRUCT_ENTRY,
+                          Builtins::kJSConstructEntryTrampoline);
+}
+
+void Builtins::Generate_JSRunMicrotasksEntry(MacroAssembler* masm) {
+  Generate_JSEntryVariant(masm, StackFrame::ENTRY,
+                          Builtins::kRunMicrotasksTrampoline);
+}
+
+static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
+                                             bool is_construct) {
+  // ----------- S t a t e -------------
+  //  -- a1: new.target
+  //  -- a2: function
+  //  -- a3: receiver_pointer
+  //  -- a4: argc
+  //  -- a5: argv
+  // -----------------------------------
+
+  // Enter an internal frame.
+  {
+    FrameScope scope(masm, StackFrame::INTERNAL);
+
+    // Setup the context (we need to use the caller context from the isolate).
+    ExternalReference context_address = ExternalReference::Create(
+        IsolateAddressId::kContextAddress, masm->isolate());
+    __ li(cp, context_address);
+    __ Ldl(cp, MemOperand(cp));
+
+    // Push the function onto the stack.
+    __ Push(a2);
+
+    // Check if we have enough stack space to push all arguments.
+    __ addl(a4, 1, t9);
+    Generate_CheckStackOverflow(masm, t9, a0, s2);
+
+    // Copy arguments to the stack in a loop.
+    // a4: argc
+    // a5: argv, i.e. points to first arg
+    Label loop, entry;
+    __ s8addl(a4, a5, s1);  DCHECK_EQ(kPointerSizeLog2, 3);
+    __ br(&entry);
+    // s1 points past last arg.
+    __ bind(&loop);
+    __ subl(s1, kPointerSize, s1);
+    __ Ldl(s2, MemOperand(s1));  // Read next parameter.
+    __ Ldl(s2, MemOperand(s2));  // Dereference handle.
+    __ push(s2);                // Push parameter.
+    __ bind(&entry);
+    __ Branch(&loop, ne, a5, Operand(s1));
+
+    // Push the receive.
+    __ Push(a3);
+
+    // a0: argc
+    // a1: function
+    // a3: new.target
+    __ mov(a3, a1);
+    __ mov(a1, a2);
+    __ mov(a0, a4);
+
+    // Initialize all JavaScript callee-saved registers, since they will be seen
+    // by the garbage collector as part of handlers.
+    __ LoadRoot(a4, RootIndex::kUndefinedValue);
+    __ mov(a5, a4);
+    __ mov(s1, a4);
+    __ mov(s2, a4);
+    __ mov(s3, a4);  //OK.
+    // s4 holds the root address. Do not clobber.
+    // s5 is cp. Do not init.
+
+    // Invoke the code.
+    Handle<Code> builtin = is_construct
+                               ? BUILTIN_CODE(masm->isolate(), Construct)
+                               : masm->isolate()->builtins()->Call();
+    __ Call(builtin, RelocInfo::CODE_TARGET);
+
+    // Leave internal frame.
+  }
+  __ Jump(ra);
+}
+
+void Builtins::Generate_JSEntryTrampoline(MacroAssembler* masm) {
+  Generate_JSEntryTrampolineHelper(masm, false);
+}
+
+void Builtins::Generate_JSConstructEntryTrampoline(MacroAssembler* masm) {
+  Generate_JSEntryTrampolineHelper(masm, true);
+}
+
+void Builtins::Generate_RunMicrotasksTrampoline(MacroAssembler* masm) {
+  // a1: microtask_queue
+  __ mov(RunMicrotasksDescriptor::MicrotaskQueueRegister(), a1);
+  __ Jump(BUILTIN_CODE(masm->isolate(), RunMicrotasks), RelocInfo::CODE_TARGET);
+}
+
+static void ReplaceClosureCodeWithOptimizedCode(MacroAssembler* masm,
+                                                Register optimized_code,
+                                                Register closure,
+                                                Register scratch1,
+                                                Register scratch2) {
+  // Store code entry in the closure.
+  __ Stl(optimized_code, FieldMemOperand(closure, JSFunction::kCodeOffset));
+  __ mov(scratch1, optimized_code);  // Write barrier clobbers scratch1 below.
+  __ RecordWriteField(closure, JSFunction::kCodeOffset, scratch1, scratch2,
+                      kRAHasNotBeenSaved, kDontSaveFPRegs, OMIT_REMEMBERED_SET,
+                      OMIT_SMI_CHECK);
+}
+
+static void LeaveInterpreterFrame(MacroAssembler* masm, Register scratch) {
+  Register args_count = scratch;
+
+  // Get the arguments + receiver count.
+  __ Ldl(args_count,
+        MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));
+  __ Ldw(t0, FieldMemOperand(args_count, BytecodeArray::kParameterSizeOffset));
+
+  // Leave the frame (also dropping the register file).
+  __ LeaveFrame(StackFrame::INTERPRETED);
+
+  // Drop receiver + arguments.
+  __ Addl(sp, sp, args_count);
+}
+
+// Tail-call |function_id| if |smi_entry| == |marker|
+static void TailCallRuntimeIfMarkerEquals(MacroAssembler* masm,
+                                          Register smi_entry,
+                                          OptimizationMarker marker,
+                                          Runtime::FunctionId function_id) {
+  Label no_match;
+  __ Branch(&no_match, ne, smi_entry, Operand(Smi::FromEnum(marker)));
+  GenerateTailCallToReturnedCode(masm, function_id);
+  __ bind(&no_match);
+}
+
+static void TailCallOptimizedCodeSlot(MacroAssembler* masm,
+                                      Register optimized_code_entry,
+                                      Register scratch1, Register scratch2) {
+  // ----------- S t a t e -------------
+  //  -- a0 : actual argument count
+  //  -- a3 : new target (preserved for callee if needed, and caller)
+  //  -- a1 : target function (preserved for callee if needed, and caller)
+  // -----------------------------------
+  DCHECK(!AreAliased(optimized_code_entry, a1, a3, scratch1, scratch2));
+
+  Register closure = a1;
+
+  // Check if the optimized code is marked for deopt. If it is, call the
+  // runtime to clear it.
+  Label found_deoptimized_code;
+  __ Ldl(a5,
+        FieldMemOperand(optimized_code_entry, Code::kCodeDataContainerOffset));
+  __ Ldw(a5, FieldMemOperand(a5, CodeDataContainer::kKindSpecificFlagsOffset));
+  __ And(a5, a5, Operand(1 << Code::kMarkedForDeoptimizationBit));
+  __ Branch(&found_deoptimized_code, ne, a5, Operand(zero_reg));
+
+    // Optimized code is good, get it into the closure and link the closure into
+    // the optimized functions list, then tail call the optimized code.
+    // The feedback vector is no longer used, so re-use it as a scratch
+    // register.
+    ReplaceClosureCodeWithOptimizedCode(masm, optimized_code_entry, closure,
+                                      scratch1, scratch2);
+
+    static_assert(kJavaScriptCallCodeStartRegister == a2, "ABI mismatch");
+    __ Addl(a2, optimized_code_entry,
+             Operand(Code::kHeaderSize - kHeapObjectTag));
+    __ Jump(a2);
+
+  // Optimized code slot contains deoptimized code, evict it and re-enter the
+  // closure's code.
+  __ bind(&found_deoptimized_code);
+  GenerateTailCallToReturnedCode(masm, Runtime::kEvictOptimizedCodeSlot);
+  }
+
+static void MaybeOptimizeCode(MacroAssembler* masm, Register feedback_vector,
+                              Register optimization_marker) {
+  // ----------- S t a t e -------------
+  //  -- a0 : actual argument count
+  //  -- a3 : new target (preserved for callee if needed, and caller)
+  //  -- a1 : target function (preserved for callee if needed, and caller)
+  //  -- feedback vector (preserved for caller if needed)
+  //  -- optimization_marker : a Smi containing a non-zero optimization marker.
+  // -----------------------------------
+  DCHECK(!AreAliased(feedback_vector, a1, a3, optimization_marker));
+
+  // TODO(v8:8394): The logging of first execution will break if
+  // feedback vectors are not allocated. We need to find a different way of
+  // logging these events if required.
+  TailCallRuntimeIfMarkerEquals(masm, optimization_marker,
+                                OptimizationMarker::kLogFirstExecution,
+                                Runtime::kFunctionFirstExecution);
+  TailCallRuntimeIfMarkerEquals(masm, optimization_marker,
+                                OptimizationMarker::kCompileOptimized,
+                                Runtime::kCompileOptimized_NotConcurrent);
+  TailCallRuntimeIfMarkerEquals(masm, optimization_marker,
+                                OptimizationMarker::kCompileOptimizedConcurrent,
+                                Runtime::kCompileOptimized_Concurrent);
+
+  // Otherwise, the marker is InOptimizationQueue, so fall through hoping
+  // that an interrupt will eventually update the slot with optimized code.
+  if (FLAG_debug_code) {
+    __ Assert(eq, AbortReason::kExpectedOptimizationSentinel,
+              optimization_marker,
+              Operand(Smi::FromEnum(OptimizationMarker::kInOptimizationQueue)));
+  }
+}
+
+// Advance the current bytecode offset. This simulates what all bytecode
+// handlers do upon completion of the underlying operation. Will bail out to a
+// label if the bytecode (without prefix) is a return bytecode. Will not advance
+// the bytecode offset if the current bytecode is a JumpLoop, instead just
+// re-executing the JumpLoop to jump to the correct bytecode.
+static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,
+                                          Register bytecode_array,
+                                          Register bytecode_offset,
+                                          Register bytecode, Register scratch1,
+                                          Register scratch2, Register scratch3,
+                                          Label* if_return) {
+  Register bytecode_size_table = scratch1;
+
+  // The bytecode offset value will be increased by one in wide and extra wide
+  // cases. In the case of having a wide or extra wide JumpLoop bytecode, we
+  // will restore the original bytecode. In order to simplify the code, we have
+  // a backup of it.
+  Register original_bytecode_offset = scratch3;
+  DCHECK(!AreAliased(bytecode_array, bytecode_offset, bytecode,
+                     bytecode_size_table, original_bytecode_offset));
+  __ Move(original_bytecode_offset, bytecode_offset);
+  __ li(bytecode_size_table, ExternalReference::bytecode_size_table_address());
+
+  // Check if the bytecode is a Wide or ExtraWide prefix bytecode.
+  Label process_bytecode, extra_wide;
+  STATIC_ASSERT(0 == static_cast<int>(interpreter::Bytecode::kWide));
+  STATIC_ASSERT(1 == static_cast<int>(interpreter::Bytecode::kExtraWide));
+  STATIC_ASSERT(2 == static_cast<int>(interpreter::Bytecode::kDebugBreakWide));
+  STATIC_ASSERT(3 ==
+                static_cast<int>(interpreter::Bytecode::kDebugBreakExtraWide));
+  __ Branch(&process_bytecode, hi, bytecode, Operand(3));
+  __ And(scratch2, bytecode, Operand(1));
+  __ Branch(&extra_wide, ne, scratch2, Operand(zero_reg));
+
+  // Load the next bytecode and update table to the wide scaled table.
+  __ Addl(bytecode_offset, bytecode_offset, Operand(1));
+  __ Addl(scratch2, bytecode_array, bytecode_offset);
+  __ Ldbu(bytecode, MemOperand(scratch2));
+  __ Addl(bytecode_size_table, bytecode_size_table,
+           Operand(kIntSize * interpreter::Bytecodes::kBytecodeCount));
+  __ jmp(&process_bytecode);
+
+  __ bind(&extra_wide);
+  // Load the next bytecode and update table to the extra wide scaled table.
+  __ Addl(bytecode_offset, bytecode_offset, Operand(1));
+  __ Addl(scratch2, bytecode_array, bytecode_offset);
+  __ Ldbu(bytecode, MemOperand(scratch2));
+  __ Addl(bytecode_size_table, bytecode_size_table,
+           Operand(2 * kIntSize * interpreter::Bytecodes::kBytecodeCount));
+
+  __ bind(&process_bytecode);
+
+// Bailout to the return label if this is a return bytecode.
+#define JUMP_IF_EQUAL(NAME)          \
+  __ Branch(if_return, eq, bytecode, \
+            Operand(static_cast<int>(interpreter::Bytecode::k##NAME)));
+  RETURN_BYTECODE_LIST(JUMP_IF_EQUAL)
+#undef JUMP_IF_EQUAL
+
+  // If this is a JumpLoop, re-execute it to perform the jump to the beginning
+  // of the loop.
+  Label end, not_jump_loop;
+  __ Branch(&not_jump_loop, ne, bytecode,
+            Operand(static_cast<int>(interpreter::Bytecode::kJumpLoop)));
+  // We need to restore the original bytecode_offset since we might have
+  // increased it to skip the wide / extra-wide prefix bytecode.
+  __ Move(bytecode_offset, original_bytecode_offset);
+  __ jmp(&end);
+
+  __ bind(&not_jump_loop);
+  // Otherwise, load the size of the current bytecode and advance the offset.
+  __ s4addl(bytecode, bytecode_size_table, scratch2);
+  __ Ldw(scratch2, MemOperand(scratch2));
+  __ Addl(bytecode_offset, bytecode_offset, scratch2);
+
+  __ bind(&end);
+}
+
+// Generate code for entering a JS function with the interpreter.
+// On entry to the function the receiver and arguments have been pushed on the
+// stack left to right.
+//
+// The live registers are:
+//   o a0 : actual argument count (not including the receiver)
+//   o a1: the JS function object being called.
+//   o a3: the incoming new target or generator object
+//   o cp: our context
+//   o fp: the caller's frame pointer
+//   o sp: stack pointer
+//   o ra: return address
+//
+// The function builds an interpreter frame.  See InterpreterFrameConstants in
+// frames.h for its layout.
+void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
+  Register closure = a1;
+  Register feedback_vector = a2;
+
+  // Get the bytecode array from the function object and load it into
+  // kInterpreterBytecodeArrayRegister.
+  __ Ldl(kScratchReg,
+        FieldMemOperand(closure, JSFunction::kSharedFunctionInfoOffset));
+  __ Ldl(kInterpreterBytecodeArrayRegister,
+        FieldMemOperand(kScratchReg, SharedFunctionInfo::kFunctionDataOffset));
+  GetSharedFunctionInfoBytecode(masm, kInterpreterBytecodeArrayRegister,
+                                kScratchReg);
+
+  // The bytecode array could have been flushed from the shared function info,
+  // if so, call into CompileLazy.
+  Label compile_lazy;
+  __ GetObjectType(kInterpreterBytecodeArrayRegister, kScratchReg, kScratchReg);
+  __ Branch(&compile_lazy, ne, kScratchReg, Operand(BYTECODE_ARRAY_TYPE));
+
+  // Load the feedback vector from the closure.
+  __ Ldl(feedback_vector,
+        FieldMemOperand(closure, JSFunction::kFeedbackCellOffset));
+  __ Ldl(feedback_vector, FieldMemOperand(feedback_vector, Cell::kValueOffset));
+
+  Label push_stack_frame;
+  // Check if feedback vector is valid. If valid, check for optimized code
+  // and update invocation count. Otherwise, setup the stack frame.
+  __ Ldl(a4, FieldMemOperand(feedback_vector, HeapObject::kMapOffset));
+  __ Ldhu(a4, FieldMemOperand(a4, Map::kInstanceTypeOffset));
+  __ Branch(&push_stack_frame, ne, a4, Operand(FEEDBACK_VECTOR_TYPE));
+
+  // Read off the optimized code slot in the feedback vector, and if there
+  // is optimized code or an optimization marker, call that instead.
+  Register optimized_code_entry = a4;
+  __ Ldl(optimized_code_entry,
+        FieldMemOperand(feedback_vector,
+                        FeedbackVector::kOptimizedCodeWeakOrSmiOffset));
+
+  // Check if the optimized code slot is not empty.
+  Label optimized_code_slot_not_empty;
+
+  __ Branch(&optimized_code_slot_not_empty, ne, optimized_code_entry,
+            Operand(Smi::FromEnum(OptimizationMarker::kNone)));
+
+  Label not_optimized;
+  __ bind(&not_optimized);
+
+  // Increment invocation count for the function.
+  __ Ldw(a4, FieldMemOperand(feedback_vector,
+                            FeedbackVector::kInvocationCountOffset));
+  __ Addw(a4, a4, Operand(1));
+  __ Stw(a4, FieldMemOperand(feedback_vector,
+                            FeedbackVector::kInvocationCountOffset));
+
+  // Open a frame scope to indicate that there is a frame on the stack.  The
+  // MANUAL indicates that the scope shouldn't actually generate code to set up
+  // the frame (that is done below).
+  __ bind(&push_stack_frame);
+  FrameScope frame_scope(masm, StackFrame::MANUAL);
+  __ PushStandardFrame(closure);
+
+  // Reset code age and the OSR arming. The OSR field and BytecodeAgeOffset are
+  // 8-bit fields next to each other, so we could just optimize by writing a
+  // 16-bit. These static asserts guard our assumption is valid.
+  STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
+                BytecodeArray::kOsrNestingLevelOffset + kCharSize);
+  STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
+  __ Sth(zero_reg, FieldMemOperand(kInterpreterBytecodeArrayRegister,
+                                  BytecodeArray::kOsrNestingLevelOffset));
+
+  // Load initial bytecode offset.
+  __ li(kInterpreterBytecodeOffsetRegister,
+        Operand(BytecodeArray::kHeaderSize - kHeapObjectTag));
+
+  // Push bytecode array and Smi tagged bytecode array offset.
+  __ SmiTag(a4, kInterpreterBytecodeOffsetRegister);
+  __ Push(kInterpreterBytecodeArrayRegister, a4);
+
+  // Allocate the local and temporary register file on the stack.
+  Label stack_overflow;
+  {
+    // Load frame size (word) from the BytecodeArray object.
+    __ Ldw(a4, FieldMemOperand(kInterpreterBytecodeArrayRegister,
+                              BytecodeArray::kFrameSizeOffset));
+
+    // Do a stack check to ensure we don't go over the limit.
+    __ Subl(a5, sp, Operand(a4));
+    LoadStackLimit(masm, a2, StackLimitKind::kRealStackLimit);
+    __ Branch(&stack_overflow, lo, a5, Operand(a2));
+
+    // If ok, push undefined as the initial value for all register file entries.
+    Label loop_header;
+    Label loop_check;
+    __ LoadRoot(a5, RootIndex::kUndefinedValue);
+    __ Branch(&loop_check);
+    __ bind(&loop_header);
+    // TODO(rmcilroy): Consider doing more than one push per loop iteration.
+    __ push(a5);
+    // Continue loop if not done.
+    __ bind(&loop_check);
+    __ Subl(a4, a4, Operand(kPointerSize));
+    __ Branch(&loop_header, ge, a4, Operand(zero_reg));
+  }
+
+  // If the bytecode array has a valid incoming new target or generator object
+  // register, initialize it with incoming value which was passed in r3.
+  Label no_incoming_new_target_or_generator_register;
+  __ Ldw(a5, FieldMemOperand(
+                kInterpreterBytecodeArrayRegister,
+                BytecodeArray::kIncomingNewTargetOrGeneratorRegisterOffset));
+  __ Branch(&no_incoming_new_target_or_generator_register, eq, a5,
+            Operand(zero_reg));
+  __ s8addl(a5, fp, a5);  DCHECK_EQ(kPointerSizeLog2, 3);
+  __ Stl(a3, MemOperand(a5));
+  __ bind(&no_incoming_new_target_or_generator_register);
+
+  // Perform interrupt stack check.
+  // TODO(solanes): Merge with the real stack limit check above.
+  Label stack_check_interrupt, after_stack_check_interrupt;
+  LoadStackLimit(masm, a5, StackLimitKind::kInterruptStackLimit);
+  __ Branch(&stack_check_interrupt, lo, sp, Operand(a5));
+  __ bind(&after_stack_check_interrupt);
+
+  // Load accumulator as undefined.
+  __ LoadRoot(kInterpreterAccumulatorRegister, RootIndex::kUndefinedValue);
+
+  // Load the dispatch table into a register and dispatch to the bytecode
+  // handler at the current bytecode offset.
+  Label do_dispatch;
+  __ bind(&do_dispatch);
+  __ li(kInterpreterDispatchTableRegister,
+        ExternalReference::interpreter_dispatch_table_address(masm->isolate()));
+  __ Addl(a0, kInterpreterBytecodeArrayRegister,
+           kInterpreterBytecodeOffsetRegister);
+  __ Ldbu(t10, MemOperand(a0));
+  __ s8addl(t10, kInterpreterDispatchTableRegister, kScratchReg);  DCHECK_EQ(kPointerSizeLog2, 3);
+  __ Ldl(kJavaScriptCallCodeStartRegister, MemOperand(kScratchReg));
+  __ Call(kJavaScriptCallCodeStartRegister);
+  masm->isolate()->heap()->SetInterpreterEntryReturnPCOffset(masm->pc_offset());
+
+  // Any returns to the entry trampoline are either due to the return bytecode
+  // or the interpreter tail calling a builtin and then a dispatch.
+
+  // Get bytecode array and bytecode offset from the stack frame.
+  __ Ldl(kInterpreterBytecodeArrayRegister,
+        MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));
+  __ Ldl(kInterpreterBytecodeOffsetRegister,
+        MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));
+  __ SmiUntag(kInterpreterBytecodeOffsetRegister);
+
+  // Either return, or advance to the next bytecode and dispatch.
+  Label do_return;
+  __ Addl(a1, kInterpreterBytecodeArrayRegister,
+           kInterpreterBytecodeOffsetRegister);
+  __ Ldbu(a1, MemOperand(a1));
+  AdvanceBytecodeOffsetOrReturn(masm, kInterpreterBytecodeArrayRegister,
+                                kInterpreterBytecodeOffsetRegister, a1, a2, a3,
+                                a4, &do_return);
+  __ jmp(&do_dispatch);
+
+  __ bind(&do_return);
+  // The return value is in v0.
+  LeaveInterpreterFrame(masm, t0);
+  __ Jump(ra);
+
+  __ bind(&stack_check_interrupt);
+  // Modify the bytecode offset in the stack to be kFunctionEntryBytecodeOffset
+  // for the call to the StackGuard.
+  __ li(kInterpreterBytecodeOffsetRegister,
+        Operand(Smi::FromInt(BytecodeArray::kHeaderSize - kHeapObjectTag +
+                             kFunctionEntryBytecodeOffset)));
+  __ Stl(kInterpreterBytecodeOffsetRegister,
+        MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));
+  __ CallRuntime(Runtime::kStackGuard);
+
+  // After the call, restore the bytecode array, bytecode offset and accumulator
+  // registers again. Also, restore the bytecode offset in the stack to its
+  // previous value.
+  __ Ldl(kInterpreterBytecodeArrayRegister,
+        MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));
+  __ li(kInterpreterBytecodeOffsetRegister,
+        Operand(BytecodeArray::kHeaderSize - kHeapObjectTag));
+  __ LoadRoot(kInterpreterAccumulatorRegister, RootIndex::kUndefinedValue);
+
+  __ SmiTag(a5, kInterpreterBytecodeOffsetRegister);
+  __ Stl(a5, MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));
+
+  __ jmp(&after_stack_check_interrupt);
+
+  __ bind(&optimized_code_slot_not_empty);
+  Label maybe_has_optimized_code;
+  // Check if optimized code marker is actually a weak reference to the
+  // optimized code as opposed to an optimization marker.
+  __ JumpIfNotSmi(optimized_code_entry, &maybe_has_optimized_code);
+  MaybeOptimizeCode(masm, feedback_vector, optimized_code_entry);
+  // Fall through if there's no runnable optimized code.
+  __ jmp(&not_optimized);
+
+  __ bind(&maybe_has_optimized_code);
+  // Load code entry from the weak reference, if it was cleared, resume
+  // execution of unoptimized code.
+  __ LoadWeakValue(optimized_code_entry, optimized_code_entry, &not_optimized);
+  TailCallOptimizedCodeSlot(masm, optimized_code_entry, t3, a5);
+
+  __ bind(&compile_lazy);
+  GenerateTailCallToReturnedCode(masm, Runtime::kCompileLazy);
+  // Unreachable code.
+  __ sys_call(0x80);
+
+  __ bind(&stack_overflow);
+  __ CallRuntime(Runtime::kThrowStackOverflow);
+  // Unreachable code.
+  __ sys_call(0x80);
+}
+
+static void Generate_InterpreterPushArgs(MacroAssembler* masm,
+                                         Register num_args,
+                                         Register start_address,
+                                         Register scratch,
+                                         Register scratch2) {
+  // Find the address of the last argument.
+  __ Subl(scratch, num_args, Operand(1));
+  __ slll(scratch, kPointerSizeLog2, scratch);
+  __ Subl(start_address, start_address, scratch);
+
+  // Push the arguments.
+  __ PushArray(start_address, num_args, scratch, scratch2,
+               TurboAssembler::PushArrayOrder::kReverse);
+}
+
+// static
+void Builtins::Generate_InterpreterPushArgsThenCallImpl(
+    MacroAssembler* masm, ConvertReceiverMode receiver_mode,
+    InterpreterPushArgsMode mode) {
+  DCHECK(mode != InterpreterPushArgsMode::kArrayFunction);
+  // ----------- S t a t e -------------
+  //  -- a0 : the number of arguments (not including the receiver)
+  //  -- a2 : the address of the first argument to be pushed. Subsequent
+  //          arguments should be consecutive above this, in the same order as
+  //          they are to be pushed onto the stack.
+  //  -- a1 : the target to call (can be any Object).
+  // -----------------------------------
+  Label stack_overflow;
+  if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
+    // The spread argument should not be pushed.
+    __ Subl(a0, a0, Operand(1));
+  }
+
+  __ Addl(a3, a0, Operand(1));  // Add one for receiver.
+
+  Generate_StackOverflowCheck(masm, a3, a4, t0, &stack_overflow);
+
+  if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
+    // Don't copy receiver.
+    __ mov(a3, a0);
+  }
+
+  // This function modifies a2, t0 and a4.
+  Generate_InterpreterPushArgs(masm, a3, a2, a4, t0);
+
+  if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
+    __ PushRoot(RootIndex::kUndefinedValue);
+  }
+
+  if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
+    // Pass the spread in the register a2.
+    // a2 already points to the penultime argument, the spread
+    // is below that.
+    __ Ldl(a2, MemOperand(a2, -kSystemPointerSize));
+  }
+
+  // Call the target.
+  if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
+    __ Jump(BUILTIN_CODE(masm->isolate(), CallWithSpread),
+            RelocInfo::CODE_TARGET);
+  } else {
+    __ Jump(masm->isolate()->builtins()->Call(ConvertReceiverMode::kAny),
+            RelocInfo::CODE_TARGET);
+  }
+
+  __ bind(&stack_overflow);
+  {
+    __ TailCallRuntime(Runtime::kThrowStackOverflow);
+    // Unreachable code.
+    __ sys_call(0x80);
+  }
+}
+
+// static
+void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
+    MacroAssembler* masm, InterpreterPushArgsMode mode) {
+  // ----------- S t a t e -------------
+  // -- a0 : argument count (not including receiver)
+  // -- a3 : new target
+  // -- a1 : constructor to call
+  // -- a2 : allocation site feedback if available, undefined otherwise.
+  // -- a4 : address of the first argument
+  // -----------------------------------
+  Label stack_overflow;
+  __ addl(a0, 1, t9);
+  Generate_StackOverflowCheck(masm, t9, a5, t0, &stack_overflow);
+
+  if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
+    // The spread argument should not be pushed.
+    __ Subl(a0, a0, Operand(1));
+  }
+
+  // Push the arguments, This function modifies t0, a4 and a5.
+  Generate_InterpreterPushArgs(masm, a0, a4, a5, t0);
+
+  // Push a slot for the receiver.
+  __ push(zero_reg);
+
+  if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
+    // Pass the spread in the register a2.
+    // a4 already points to the penultimate argument, the spread
+    // lies in the next interpreter register.
+    __ Ldl(a2, MemOperand(a4, -kSystemPointerSize));
+  } else {
+    __ AssertUndefinedOrAllocationSite(a2, t0);
+  }
+
+  if (mode == InterpreterPushArgsMode::kArrayFunction) {
+    __ AssertFunction(a1);
+
+    // Tail call to the function-specific construct stub (still in the caller
+    // context at this point).
+    __ Jump(BUILTIN_CODE(masm->isolate(), ArrayConstructorImpl),
+            RelocInfo::CODE_TARGET);
+  } else if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
+    // Call the constructor with a0, a1, and a3 unmodified.
+    __ Jump(BUILTIN_CODE(masm->isolate(), ConstructWithSpread),
+            RelocInfo::CODE_TARGET);
+  } else {
+    DCHECK_EQ(InterpreterPushArgsMode::kOther, mode);
+    // Call the constructor with a0, a1, and a3 unmodified.
+    __ Jump(BUILTIN_CODE(masm->isolate(), Construct), RelocInfo::CODE_TARGET);
+  }
+
+  __ bind(&stack_overflow);
+  {
+    __ TailCallRuntime(Runtime::kThrowStackOverflow);
+    // Unreachable code.
+    __ sys_call(0x80);
+  }
+}
+
+static void Generate_InterpreterEnterBytecode(MacroAssembler* masm) {
+  // Set the return address to the correct point in the interpreter entry
+  // trampoline.
+  Label builtin_trampoline, trampoline_loaded;
+  Smi interpreter_entry_return_pc_offset(
+      masm->isolate()->heap()->interpreter_entry_return_pc_offset());
+  DCHECK_NE(interpreter_entry_return_pc_offset, Smi::zero());
+
+  // If the SFI function_data is an InterpreterData, the function will have a
+  // custom copy of the interpreter entry trampoline for profiling. If so,
+  // get the custom trampoline, otherwise grab the entry address of the global
+  // trampoline.
+  __ Ldl(t0, MemOperand(fp, StandardFrameConstants::kFunctionOffset));
+  __ Ldl(t0, FieldMemOperand(t0, JSFunction::kSharedFunctionInfoOffset));
+  __ Ldl(t0, FieldMemOperand(t0, SharedFunctionInfo::kFunctionDataOffset));
+  __ GetObjectType(t0, kInterpreterDispatchTableRegister,
+                   kInterpreterDispatchTableRegister);
+  __ Branch(&builtin_trampoline, ne, kInterpreterDispatchTableRegister,
+            Operand(INTERPRETER_DATA_TYPE));
+
+  __ Ldl(t0, FieldMemOperand(t0, InterpreterData::kInterpreterTrampolineOffset));
+  __ Addl(t0, t0, Operand(Code::kHeaderSize - kHeapObjectTag));
+  __ Branch(&trampoline_loaded);
+
+  __ bind(&builtin_trampoline);
+  __ li(t0, ExternalReference::
+                address_of_interpreter_entry_trampoline_instruction_start(
+                    masm->isolate()));
+  __ Ldl(t0, MemOperand(t0));
+
+  __ bind(&trampoline_loaded);
+  __ Addl(ra, t0, Operand(interpreter_entry_return_pc_offset.value()));
+
+  // Initialize the dispatch table register.
+  __ li(kInterpreterDispatchTableRegister,
+        ExternalReference::interpreter_dispatch_table_address(masm->isolate()));
+
+  // Get the bytecode array pointer from the frame.
+  __ Ldl(kInterpreterBytecodeArrayRegister,
+        MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));
+
+  if (FLAG_debug_code) {
+    // Check function data field is actually a BytecodeArray object.
+    __ SmiTst(kInterpreterBytecodeArrayRegister, kScratchReg);
+    __ Assert(ne,
+              AbortReason::kFunctionDataShouldBeBytecodeArrayOnInterpreterEntry,
+              kScratchReg, Operand(zero_reg));
+    __ GetObjectType(kInterpreterBytecodeArrayRegister, a1, a1);
+    __ Assert(eq,
+              AbortReason::kFunctionDataShouldBeBytecodeArrayOnInterpreterEntry,
+              a1, Operand(BYTECODE_ARRAY_TYPE));
+  }
+
+  // Get the target bytecode offset from the frame.
+  __ SmiUntag(kInterpreterBytecodeOffsetRegister,
+              MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));
+
+  if (FLAG_debug_code) {
+    Label okay;
+    __ Branch(&okay, ge, kInterpreterBytecodeOffsetRegister,
+              Operand(BytecodeArray::kHeaderSize - kHeapObjectTag));
+    // Unreachable code.
+    __ sys_call(0x80);
+    __ bind(&okay);
+  }
+
+  // Dispatch to the target bytecode.
+  __ Addl(a1, kInterpreterBytecodeArrayRegister,
+           kInterpreterBytecodeOffsetRegister);
+  __ Ldbu(t10, MemOperand(a1));
+  __ s8addl(t10, kInterpreterDispatchTableRegister, a1);  DCHECK_EQ(kPointerSizeLog2, 3);
+  __ Ldl(kJavaScriptCallCodeStartRegister, MemOperand(a1));
+  __ Jump(kJavaScriptCallCodeStartRegister);
+}
+
+void Builtins::Generate_InterpreterEnterBytecodeAdvance(MacroAssembler* masm) {
+  // Advance the current bytecode offset stored within the given interpreter
+  // stack frame. This simulates what all bytecode handlers do upon completion
+  // of the underlying operation.
+  __ Ldl(kInterpreterBytecodeArrayRegister,
+        MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));
+  __ Ldl(kInterpreterBytecodeOffsetRegister,
+        MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));
+  __ SmiUntag(kInterpreterBytecodeOffsetRegister);
+
+  Label enter_bytecode, function_entry_bytecode;
+  __ Branch(&function_entry_bytecode, eq, kInterpreterBytecodeOffsetRegister,
+            Operand(BytecodeArray::kHeaderSize - kHeapObjectTag +
+                    kFunctionEntryBytecodeOffset));
+
+  // Load the current bytecode.
+  __ Addl(a1, kInterpreterBytecodeArrayRegister,
+           kInterpreterBytecodeOffsetRegister);
+  __ Ldbu(a1, MemOperand(a1));
+
+  // Advance to the next bytecode.
+  Label if_return;
+  AdvanceBytecodeOffsetOrReturn(masm, kInterpreterBytecodeArrayRegister,
+                                kInterpreterBytecodeOffsetRegister, a1, a2, a3,
+                                a4, &if_return);
+
+  __ bind(&enter_bytecode);
+  // Convert new bytecode offset to a Smi and save in the stackframe.
+  __ SmiTag(a2, kInterpreterBytecodeOffsetRegister);
+  __ Stl(a2, MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));
+
+  Generate_InterpreterEnterBytecode(masm);
+
+  __ bind(&function_entry_bytecode);
+  // If the code deoptimizes during the implicit function entry stack interrupt
+  // check, it will have a bailout ID of kFunctionEntryBytecodeOffset, which is
+  // not a valid bytecode offset. Detect this case and advance to the first
+  // actual bytecode.
+  __ li(kInterpreterBytecodeOffsetRegister,
+        Operand(BytecodeArray::kHeaderSize - kHeapObjectTag));
+  __ Branch(&enter_bytecode);
+
+  // We should never take the if_return path.
+  __ bind(&if_return);
+  __ Abort(AbortReason::kInvalidBytecodeAdvance);
+}
+
+void Builtins::Generate_InterpreterEnterBytecodeDispatch(MacroAssembler* masm) {
+  Generate_InterpreterEnterBytecode(masm);
+}
+
+namespace {
+void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
+                                      bool java_script_builtin,
+                                      bool with_result) {
+  const RegisterConfiguration* config(RegisterConfiguration::Default());
+  int allocatable_register_count = config->num_allocatable_general_registers();
+  Register scratch = t3;
+  if (with_result) {
+  if (java_script_builtin) {
+    __ mov(scratch, v0);
+  } else {
+    // Overwrite the hole inserted by the deoptimizer with the return value from
+    // the LAZY deopt point.
+    __ Stl(v0,
+          MemOperand(
+              sp, config->num_allocatable_general_registers() * kPointerSize +
+                      BuiltinContinuationFrameConstants::kFixedFrameSize));
+  }
+  }
+  for (int i = allocatable_register_count - 1; i >= 0; --i) {
+    int code = config->GetAllocatableGeneralCode(i);
+    __ Pop(Register::from_code(code));
+    if (java_script_builtin && code == kJavaScriptCallArgCountRegister.code()) {
+      __ SmiUntag(Register::from_code(code));
+    }
+  }
+
+  if (with_result && java_script_builtin) {
+    // Overwrite the hole inserted by the deoptimizer with the return value from
+    // the LAZY deopt point. t0 contains the arguments count, the return value
+    // from LAZY is always the last argument.
+    __ Addl(a0, a0,
+            Operand(BuiltinContinuationFrameConstants::kFixedSlotCount));
+    __ Dlsa(t0, sp, a0, kSystemPointerSizeLog2);
+    __ Stl(scratch, MemOperand(t0));
+    // Recover arguments count.
+    __ Subl(a0, a0,
+            Operand(BuiltinContinuationFrameConstants::kFixedSlotCount));
+  }
+
+  __ Ldl(fp, MemOperand(
+                sp, BuiltinContinuationFrameConstants::kFixedFrameSizeFromFp));
+  // Load builtin index (stored as a Smi) and use it to get the builtin start
+  // address from the builtins table.
+  __ Pop(t0);
+  __ Addl(sp, sp,
+           Operand(BuiltinContinuationFrameConstants::kFixedFrameSizeFromFp));
+  __ Pop(ra);
+  __ LoadEntryFromBuiltinIndex(t0);
+  __ Jump(t0);
+}
+}  // namespace
+
+void Builtins::Generate_ContinueToCodeStubBuiltin(MacroAssembler* masm) {
+  Generate_ContinueToBuiltinHelper(masm, false, false);
+}
+
+void Builtins::Generate_ContinueToCodeStubBuiltinWithResult(
+    MacroAssembler* masm) {
+  Generate_ContinueToBuiltinHelper(masm, false, true);
+}
+
+void Builtins::Generate_ContinueToJavaScriptBuiltin(MacroAssembler* masm) {
+  Generate_ContinueToBuiltinHelper(masm, true, false);
+}
+
+void Builtins::Generate_ContinueToJavaScriptBuiltinWithResult(
+    MacroAssembler* masm) {
+  Generate_ContinueToBuiltinHelper(masm, true, true);
+}
+
+void Builtins::Generate_NotifyDeoptimized(MacroAssembler* masm) {
+  {
+    FrameScope scope(masm, StackFrame::INTERNAL);
+    __ CallRuntime(Runtime::kNotifyDeoptimized);
+  }
+
+  DCHECK_EQ(kInterpreterAccumulatorRegister.code(), v0.code());
+  __ Ldl(v0, MemOperand(sp, 0 * kPointerSize));
+  // Safe to fill delay slot Addw will emit one instruction.
+  __ Addl(sp, sp, Operand(1 * kPointerSize));  // Remove state.
+  __ Ret();
+}
+
+void Builtins::Generate_InterpreterOnStackReplacement(MacroAssembler* masm) {
+  {
+    FrameScope scope(masm, StackFrame::INTERNAL);
+    __ CallRuntime(Runtime::kCompileForOnStackReplacement);
+  }
+
+  // If the code object is null, just return to the caller.
+  __ Ret(eq, v0, Operand(Smi::zero()));
+
+  // Drop the handler frame that is be sitting on top of the actual
+  // JavaScript frame. This is the case then OSR is triggered from bytecode.
+  __ LeaveFrame(StackFrame::STUB);
+
+  // Load deoptimization data from the code object.
+  // <deopt_data> = <code>[#deoptimization_data_offset]
+  __ Ldl(a1, MemOperand(v0, Code::kDeoptimizationDataOffset - kHeapObjectTag));
+
+  // Load the OSR entrypoint offset from the deoptimization data.
+  // <osr_offset> = <deopt_data>[#header_size + #osr_pc_offset]
+  __ SmiUntag(a1, MemOperand(a1, FixedArray::OffsetOfElementAt(
+                                     DeoptimizationData::kOsrPcOffsetIndex) -
+                                     kHeapObjectTag));
+
+  // Compute the target address = code_obj + header_size + osr_offset
+  // <entry_addr> = <code_obj> + #header_size + <osr_offset>
+  __ Addl(v0, v0, a1);
+  __ Addl(ra, v0, Operand(Code::kHeaderSize - kHeapObjectTag));
+
+  // And "return" to the OSR entry point of the function.
+  __ Ret();
+}
+
+// static
+void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {
+  // ----------- S t a t e -------------
+  //  -- a0    : argc
+  //  -- sp[0] : argArray
+  //  -- sp[4] : thisArg
+  //  -- sp[8] : receiver
+  // -----------------------------------
+
+  Register argc = a0;
+  Register arg_array = a2;
+  Register receiver = a1;
+  Register this_arg = a5;
+  Register undefined_value = a3;
+  Register scratch = a4;
+
+  __ LoadRoot(undefined_value, RootIndex::kUndefinedValue);
+
+  // 1. Load receiver into a1, argArray into a2 (if present), remove all
+  // arguments from the stack (including the receiver), and push thisArg (if
+  // present) instead.
+  {
+    // Claim (2 - argc) dummy arguments form the stack, to put the stack in a
+    // consistent state for a simple pop operation.
+
+    __ mov(scratch, argc);
+    __ Ldl(this_arg, MemOperand(sp, kPointerSize));
+    __ Ldl(arg_array, MemOperand(sp, 2 * kPointerSize));
+    __ Seleq(arg_array, undefined_value, scratch);  // if argc == 0
+    __ Seleq(this_arg, undefined_value, scratch);   // if argc == 0
+    __ Subl(scratch, scratch, Operand(1));
+    __ Seleq(arg_array, undefined_value, scratch);  // if argc == 1
+    __ Ldl(receiver, MemOperand(sp));
+    __ Dlsa(sp, sp, argc, kSystemPointerSizeLog2);
+    __ Stl(this_arg, MemOperand(sp));
+  }
+
+  // ----------- S t a t e -------------
+  //  -- a2    : argArray
+  //  -- a1    : receiver
+  //  -- a3    : undefined root value
+  //  -- sp[0] : thisArg
+  // -----------------------------------
+
+  // 2. We don't need to check explicitly for callable receiver here,
+  // since that's the first thing the Call/CallWithArrayLike builtins
+  // will do.
+
+  // 3. Tail call with no arguments if argArray is null or undefined.
+  Label no_arguments;
+  __ JumpIfRoot(arg_array, RootIndex::kNullValue, &no_arguments);
+  __ Branch(&no_arguments, eq, arg_array, Operand(undefined_value));
+
+  // 4a. Apply the receiver to the given argArray.
+  __ Jump(BUILTIN_CODE(masm->isolate(), CallWithArrayLike),
+          RelocInfo::CODE_TARGET);
+
+  // 4b. The argArray is either null or undefined, so we tail call without any
+  // arguments to the receiver.
+  __ bind(&no_arguments);
+  {
+    __ mov(a0, zero_reg);
+    DCHECK(receiver == a1);
+    __ Jump(masm->isolate()->builtins()->Call(), RelocInfo::CODE_TARGET);
+  }
+}
+
+// static
+void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) {
+  // 1. Get the callable to call (passed as receiver) from the stack.
+  {
+    __ Pop(a1);
+  }
+
+  // 2. Make sure we have at least one argument.
+  // a0: actual number of arguments
+  {
+    Label done;
+    __ Branch(&done, ne, a0, Operand(zero_reg));
+    __ PushRoot(RootIndex::kUndefinedValue);
+    __ Addl(a0, a0, Operand(1));
+    __ bind(&done);
+  }
+
+  // 3. Adjust the actual number of arguments.
+  __ subl(a0, 1, a0);
+
+  // 4. Call the callable.
+  __ Jump(masm->isolate()->builtins()->Call(), RelocInfo::CODE_TARGET);
+}
+
+void Builtins::Generate_ReflectApply(MacroAssembler* masm) {
+  // ----------- S t a t e -------------
+  //  -- a0     : argc
+  //  -- sp[0]  : argumentsList  (if argc ==3)
+  //  -- sp[4]  : thisArgument   (if argc >=2)
+  //  -- sp[8]  : target         (if argc >=1)
+  //  -- sp[12] : receiver
+  // -----------------------------------
+
+  Register argc = a0;
+  Register arguments_list = a2;
+  Register target = a1;
+  Register this_argument = a5;
+  Register undefined_value = a3;
+  Register scratch = a4;
+
+  __ LoadRoot(undefined_value, RootIndex::kUndefinedValue);
+
+  // 1. Load target into a1 (if present), argumentsList into a2 (if present),
+  // remove all arguments from the stack (including the receiver), and push
+  // thisArgument (if present) instead.
+  {
+    // Claim (3 - argc) dummy arguments form the stack, to put the stack in a
+    // consistent state for a simple pop operation.
+
+    __ mov(scratch, argc);
+    __ Ldl(target, MemOperand(sp, kPointerSize));
+    __ Ldl(this_argument, MemOperand(sp, 2 * kPointerSize));
+    __ Ldl(arguments_list, MemOperand(sp, 3 * kPointerSize));
+    __ Seleq(arguments_list, undefined_value, scratch);  // if argc == 0
+    __ Seleq(this_argument, undefined_value, scratch);   // if argc == 0
+    __ Seleq(target, undefined_value, scratch);          // if argc == 0
+    __ Subl(scratch, scratch, Operand(1));
+    __ Seleq(arguments_list, undefined_value, scratch);  // if argc == 1
+    __ Seleq(this_argument, undefined_value, scratch);   // if argc == 1
+    __ Subl(scratch, scratch, Operand(1));
+    __ Seleq(arguments_list, undefined_value, scratch);  // if argc == 2
+
+    __ Dlsa(sp, sp, argc, kSystemPointerSizeLog2);
+    __ Stl(this_argument, MemOperand(sp, 0));  // Overwrite receiver
+  }
+
+  // ----------- S t a t e -------------
+  //  -- a2    : argumentsList
+  //  -- a1    : target
+  //  -- a3    : undefined root value
+  //  -- sp[0] : thisArgument
+  // -----------------------------------
+
+  // 2. We don't need to check explicitly for callable target here,
+  // since that's the first thing the Call/CallWithArrayLike builtins
+  // will do.
+
+  // 3. Apply the target to the given argumentsList.
+  __ Jump(BUILTIN_CODE(masm->isolate(), CallWithArrayLike),
+          RelocInfo::CODE_TARGET);
+}
+
+void Builtins::Generate_ReflectConstruct(MacroAssembler* masm) {
+  // ----------- S t a t e -------------
+  //  -- a0     : argc
+  //  -- sp[0]  : new.target (optional) (dummy value if argc <= 2)
+  //  -- sp[4]  : argumentsList         (dummy value if argc <= 1)
+  //  -- sp[8]  : target                (dummy value if argc == 0)
+  //  -- sp[12] : receiver
+  // -----------------------------------
+  // NOTE: The order of args in the stack are reversed if V8_REVERSE_JSARGS
+
+  Register argc = a0;
+  Register arguments_list = a2;
+  Register target = a1;
+  Register new_target = a3;
+  Register undefined_value = a4;
+  Register scratch = a5;
+
+  __ LoadRoot(undefined_value, RootIndex::kUndefinedValue);
+
+  // 1. Load target into a1 (if present), argumentsList into a2 (if present),
+  // new.target into a3 (if present, otherwise use target), remove all
+  // arguments from the stack (including the receiver), and push thisArgument
+  // (if present) instead.
+  {
+    // Claim (3 - argc) dummy arguments form the stack, to put the stack in a
+    // consistent state for a simple pop operation.
+
+    __ mov(scratch, argc);
+    __ Ldl(target, MemOperand(sp, kPointerSize));
+    __ Ldl(arguments_list, MemOperand(sp, 2 * kPointerSize));
+    __ Ldl(new_target, MemOperand(sp, 3 * kPointerSize));
+    __ Seleq(arguments_list, undefined_value, scratch);  // if argc == 0
+    __ Seleq(new_target, undefined_value, scratch);      // if argc == 0
+    __ Seleq(target, undefined_value, scratch);          // if argc == 0
+    __ Subl(scratch, scratch, Operand(1));
+    __ Seleq(arguments_list, undefined_value, scratch);  // if argc == 1
+    __ Seleq(new_target, target, scratch);               // if argc == 1
+    __ Subl(scratch, scratch, Operand(1));
+    __ Seleq(new_target, target, scratch);  // if argc == 2
+
+    __ Dlsa(sp, sp, argc, kSystemPointerSizeLog2);
+    __ Stl(undefined_value, MemOperand(sp, 0));  // Overwrite receiver
+  }
+
+  // ----------- S t a t e -------------
+  //  -- a2    : argumentsList
+  //  -- a1    : target
+  //  -- a3    : new.target
+  //  -- sp[0] : receiver (undefined)
+  // -----------------------------------
+
+  // 2. We don't need to check explicitly for constructor target here,
+  // since that's the first thing the Construct/ConstructWithArrayLike
+  // builtins will do.
+
+  // 3. We don't need to check explicitly for constructor new.target here,
+  // since that's the second thing the Construct/ConstructWithArrayLike
+  // builtins will do.
+
+  // 4. Construct the target with the given new.target and argumentsList.
+  __ Jump(BUILTIN_CODE(masm->isolate(), ConstructWithArrayLike),
+          RelocInfo::CODE_TARGET);
+}
+
+static void EnterArgumentsAdaptorFrame(MacroAssembler* masm) {
+  __ SmiTag(a0);
+  __ li(a4, Operand(StackFrame::TypeToMarker(StackFrame::ARGUMENTS_ADAPTOR)));
+  __ MultiPush(a0.bit() | a1.bit() | a4.bit() | fp.bit() | ra.bit());
+  __ Push(Smi::zero());  // Padding.
+  __ Addl(fp, sp,
+           Operand(ArgumentsAdaptorFrameConstants::kFixedFrameSizeFromFp));
+}
+
+static void LeaveArgumentsAdaptorFrame(MacroAssembler* masm) {
+  // ----------- S t a t e -------------
+  //  -- v0 : result being passed through
+  // -----------------------------------
+  // Get the number of arguments passed (as a smi), tear down the frame and
+  // then tear down the parameters.
+  __ Ldl(a1, MemOperand(fp, ArgumentsAdaptorFrameConstants::kLengthOffset));
+  __ mov(sp, fp);
+  __ MultiPop(fp.bit() | ra.bit());
+  __ SmiScale(a4, a1, kPointerSizeLog2);
+  __ Addl(sp, sp, a4);
+  // Adjust for the receiver.
+  __ Addl(sp, sp, Operand(kPointerSize));
+}
+
+// static
+void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
+                                               Handle<Code> code) {
+  // ----------- S t a t e -------------
+  //  -- a1 : target
+  //  -- a0 : number of parameters on the stack (not including the receiver)
+  //  -- a2 : arguments list (a FixedArray)
+  //  -- a4 : len (number of elements to push from args)
+  //  -- a3 : new.target (for [[Construct]])
+  // -----------------------------------
+  if (masm->emit_debug_code()) {
+    // Allow a2 to be a FixedArray, or a FixedDoubleArray if a4 == 0.
+    Label ok, fail;
+    __ AssertNotSmi(a2);
+    __ GetObjectType(a2, t8, t8);
+    __ Branch(&ok, eq, t8, Operand(FIXED_ARRAY_TYPE));
+    __ Branch(&fail, ne, t8, Operand(FIXED_DOUBLE_ARRAY_TYPE));
+    __ Branch(&ok, eq, a4, Operand(zero_reg));
+    // Fall through.
+    __ bind(&fail);
+    __ Abort(AbortReason::kOperandIsNotAFixedArray);
+
+    __ bind(&ok);
+  }
+
+  Register args = a2;
+  Register len = a4;
+
+  // Check for stack overflow.
+  Label stack_overflow;
+  Generate_StackOverflowCheck(masm, len, kScratchReg, a5, &stack_overflow);
+
+  // Move the arguments already in the stack,
+  // including the receiver and the return address.
+  {
+    Label copy;
+    Register src = t9, dest = t10;
+    __ mov(src, sp);
+    __ slll(a4, kSystemPointerSizeLog2, t0);
+    __ Subl(sp, sp, Operand(t0));
+    // Update stack pointer.
+    __ mov(dest, sp);
+    __ Addl(t0, a0, Operand(zero_reg));
+
+    __ bind(&copy);
+    __ Ldl(t1, MemOperand(src, 0));
+    __ Stl(t1, MemOperand(dest, 0));
+    __ Subl(t0, t0, Operand(1));
+    __ Addl(src, src, Operand(kSystemPointerSize));
+    __ Addl(dest, dest, Operand(kSystemPointerSize));
+    __ Branch(&copy, ge, t0, Operand(zero_reg));
+  }
+
+  // Push arguments onto the stack (thisArgument is already on the stack).
+  {
+    Label done, push, loop;
+    Register src = t9;
+    Register scratch = len;
+
+    __ Addl(src, args, Operand(FixedArray::kHeaderSize - kHeapObjectTag));
+    __ Addl(a0, a0, len);  // The 'len' argument for Call() or Construct().
+    __ Branch(&done, eq, len, Operand(zero_reg));
+    __ slll(len, kPointerSizeLog2, scratch);
+    __ Subl(scratch, sp, Operand(scratch));
+    __ LoadRoot(t1, RootIndex::kTheHoleValue);
+    __ bind(&loop);
+    __ Ldl(a5, MemOperand(src));
+    __ addl(src, kPointerSize, src);
+    __ Branch(&push, ne, a5, Operand(t1));
+    __ LoadRoot(a5, RootIndex::kUndefinedValue);
+    __ bind(&push);
+    __ Stl(a5, MemOperand(t10, 0));
+    __ Addl(t10, t10, Operand(kSystemPointerSize));
+    __ Addl(scratch, scratch, Operand(kSystemPointerSize));
+    __ Branch(&loop, ne, scratch, Operand(sp));
+    __ bind(&done);
+  }
+
+  // Tail-call to the actual Call or Construct builtin.
+  __ Jump(code, RelocInfo::CODE_TARGET);
+
+  __ bind(&stack_overflow);
+  __ TailCallRuntime(Runtime::kThrowStackOverflow);
+}
+
+// static
+void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
+                                                      CallOrConstructMode mode,
+                                                      Handle<Code> code) {
+  // ----------- S t a t e -------------
+  //  -- a0 : the number of arguments (not including the receiver)
+  //  -- a3 : the new.target (for [[Construct]] calls)
+  //  -- a1 : the target to call (can be any Object)
+  //  -- a2 : start index (to support rest parameters)
+  // -----------------------------------
+
+  // Check if new.target has a [[Construct]] internal method.
+  if (mode == CallOrConstructMode::kConstruct) {
+    Label new_target_constructor, new_target_not_constructor;
+    __ JumpIfSmi(a3, &new_target_not_constructor);
+    __ Ldl(t1, FieldMemOperand(a3, HeapObject::kMapOffset));
+    __ Ldbu(t1, FieldMemOperand(t1, Map::kBitFieldOffset));
+    __ And(t1, t1, Operand(Map::Bits1::IsConstructorBit::kMask));
+    __ Branch(&new_target_constructor, ne, t1, Operand(zero_reg));
+    __ bind(&new_target_not_constructor);
+    {
+      FrameScope scope(masm, StackFrame::MANUAL);
+      __ EnterFrame(StackFrame::INTERNAL);
+      __ Push(a3);
+      __ CallRuntime(Runtime::kThrowNotConstructor);
+    }
+    __ bind(&new_target_constructor);
+  }
+
+  // Check if we have an arguments adaptor frame below the function frame.
+  Label arguments_adaptor, arguments_done;
+  __ Ldl(t9, MemOperand(fp, StandardFrameConstants::kCallerFPOffset));
+  __ Ldl(t10, MemOperand(t9, CommonFrameConstants::kContextOrFrameTypeOffset));
+  __ Branch(&arguments_adaptor, eq, t10,
+            Operand(StackFrame::TypeToMarker(StackFrame::ARGUMENTS_ADAPTOR)));
+  {
+    __ Ldl(t10, MemOperand(fp, StandardFrameConstants::kFunctionOffset));
+    __ Ldl(t10, FieldMemOperand(t10, JSFunction::kSharedFunctionInfoOffset));
+    __ Ldhu(t10, FieldMemOperand(
+                   t10, SharedFunctionInfo::kFormalParameterCountOffset));
+    __ mov(t9, fp);
+  }
+  __ Branch(&arguments_done);
+  __ bind(&arguments_adaptor);
+  {
+    // Just get the length from the ArgumentsAdaptorFrame.
+    __ SmiUntag(t10,
+                MemOperand(t9, ArgumentsAdaptorFrameConstants::kLengthOffset));
+  }
+  __ bind(&arguments_done);
+
+  Label stack_done, stack_overflow;
+  __ Subw(t10, t10, a2);
+  __ Branch(&stack_done, le, t10, Operand(zero_reg));
+  {
+    // Check for stack overflow.
+    Generate_StackOverflowCheck(masm, t10, a4, a5, &stack_overflow);
+
+    // Forward the arguments from the caller frame.
+
+    // Point to the first argument to copy (skipping the receiver).
+    __ Addl(t9, t9,
+             Operand(CommonFrameConstants::kFixedFrameSizeAboveFp +
+                     kSystemPointerSize));
+    __ Dlsa(t9, t9, a2, kSystemPointerSizeLog2);
+
+    // Move the arguments already in the stack,
+    // including the receiver and the return address.
+    {
+      Label copy;
+      Register src = t0, dest = a2;
+      __ mov(src, sp);
+      // Update stack pointer.
+      __ slll(t10, kSystemPointerSizeLog2, t1);
+      __ Subl(sp, sp, Operand(t1));
+      __ mov(dest, sp);
+      __ Addl(t2, a0, Operand(zero_reg));
+
+      __ bind(&copy);
+      __ Ldl(t1, MemOperand(src, 0));
+      __ Stl(t1, MemOperand(dest, 0));
+      __ Subl(t2, t2, Operand(1));
+      __ Addl(src, src, Operand(kSystemPointerSize));
+      __ Addl(dest, dest, Operand(kSystemPointerSize));
+      __ Branch(&copy, ge, t2, Operand(zero_reg));
+    }
+
+    // Copy arguments from the caller frame.
+    // TODO(victorgomes): Consider using forward order as potentially more cache
+    // friendly.
+    {
+      Label loop;
+      __ Addl(a0, a0, t10);
+      __ bind(&loop);
+      {
+        __ Subw(t10, t10, Operand(1));
+        __ Dlsa(t0, t9, t10, kPointerSizeLog2);
+        __ Ldl(kScratchReg, MemOperand(t0));
+        __ Dlsa(t0, a2, t10, kPointerSizeLog2);
+        __ Stl(kScratchReg, MemOperand(t0));
+        __ Branch(&loop, ne, t10, Operand(zero_reg));
+      }
+    }
+  }
+  __ Branch(&stack_done);
+  __ bind(&stack_overflow);
+  __ TailCallRuntime(Runtime::kThrowStackOverflow);
+  __ bind(&stack_done);
+
+  // Tail-call to the {code} handler.
+  __ Jump(code, RelocInfo::CODE_TARGET);
+}
+
+// static
+void Builtins::Generate_CallFunction(MacroAssembler* masm,
+                                     ConvertReceiverMode mode) {
+  // ----------- S t a t e -------------
+  //  -- a0 : the number of arguments (not including the receiver)
+  //  -- a1 : the function to call (checked to be a JSFunction)
+  // -----------------------------------
+  __ AssertFunction(a1);
+
+  // See ES6 section 9.2.1 [[Call]] ( thisArgument, argumentsList)
+  // Check that function is not a "classConstructor".
+  Label class_constructor;
+  __ Ldl(a2, FieldMemOperand(a1, JSFunction::kSharedFunctionInfoOffset));
+  __ Ldwu(a3, FieldMemOperand(a2, SharedFunctionInfo::kFlagsOffset));
+  __ And(kScratchReg, a3,
+         Operand(SharedFunctionInfo::IsClassConstructorBit::kMask));
+  __ Branch(&class_constructor, ne, kScratchReg, Operand(zero_reg));
+
+  // Enter the context of the function; ToObject has to run in the function
+  // context, and we also need to take the global proxy from the function
+  // context in case of conversion.
+  __ Ldl(cp, FieldMemOperand(a1, JSFunction::kContextOffset));
+  // We need to convert the receiver for non-native sloppy mode functions.
+  Label done_convert;
+  __ Ldwu(a3, FieldMemOperand(a2, SharedFunctionInfo::kFlagsOffset));
+  __ And(kScratchReg, a3,
+         Operand(SharedFunctionInfo::IsNativeBit::kMask |
+                 SharedFunctionInfo::IsStrictBit::kMask));
+  __ Branch(&done_convert, ne, kScratchReg, Operand(zero_reg));
+  {
+    // ----------- S t a t e -------------
+    //  -- a0 : the number of arguments (not including the receiver)
+    //  -- a1 : the function to call (checked to be a JSFunction)
+    //  -- a2 : the shared function info.
+    //  -- cp : the function context.
+    // -----------------------------------
+
+    if (mode == ConvertReceiverMode::kNullOrUndefined) {
+      // Patch receiver to global proxy.
+      __ LoadGlobalProxy(a3);
+    } else {
+      Label convert_to_object, convert_receiver;
+      __ LoadReceiver(a3, a0);
+      __ JumpIfSmi(a3, &convert_to_object);
+      STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+      __ GetObjectType(a3, a4, a4);
+      __ Branch(&done_convert, hs, a4, Operand(FIRST_JS_RECEIVER_TYPE));
+      if (mode != ConvertReceiverMode::kNotNullOrUndefined) {
+        Label convert_global_proxy;
+        __ JumpIfRoot(a3, RootIndex::kUndefinedValue, &convert_global_proxy);
+        __ JumpIfNotRoot(a3, RootIndex::kNullValue, &convert_to_object);
+        __ bind(&convert_global_proxy);
+        {
+          // Patch receiver to global proxy.
+          __ LoadGlobalProxy(a3);
+        }
+        __ Branch(&convert_receiver);
+      }
+      __ bind(&convert_to_object);
+      {
+        // Convert receiver using ToObject.
+        // TODO(bmeurer): Inline the allocation here to avoid building the frame
+        // in the fast case? (fall back to AllocateInNewSpace?)
+        FrameScope scope(masm, StackFrame::INTERNAL);
+        __ SmiTag(a0);
+        __ Push(a0, a1);
+        __ mov(a0, a3);
+        __ Push(cp);
+        __ Call(BUILTIN_CODE(masm->isolate(), ToObject),
+                RelocInfo::CODE_TARGET);
+        __ Pop(cp);
+        __ mov(a3, v0);
+        __ Pop(a0, a1);
+        __ SmiUntag(a0);
+      }
+      __ Ldl(a2, FieldMemOperand(a1, JSFunction::kSharedFunctionInfoOffset));
+      __ bind(&convert_receiver);
+    }
+    __ StoreReceiver(a3, a0, kScratchReg);
+  }
+  __ bind(&done_convert);
+
+  // ----------- S t a t e -------------
+  //  -- a0 : the number of arguments (not including the receiver)
+  //  -- a1 : the function to call (checked to be a JSFunction)
+  //  -- a2 : the shared function info.
+  //  -- cp : the function context.
+  // -----------------------------------
+
+  __ Ldhu(a2,
+         FieldMemOperand(a2, SharedFunctionInfo::kFormalParameterCountOffset));
+  __ InvokeFunctionCode(a1, no_reg, a2, a0, JUMP_FUNCTION);
+
+  // The function is a "classConstructor", need to raise an exception.
+  __ bind(&class_constructor);
+  {
+    FrameScope frame(masm, StackFrame::INTERNAL);
+    __ Push(a1);
+    __ CallRuntime(Runtime::kThrowConstructorNonCallableError);
+  }
+}
+
+// static
+void Builtins::Generate_CallBoundFunctionImpl(MacroAssembler* masm) {
+  // ----------- S t a t e -------------
+  //  -- a0 : the number of arguments (not including the receiver)
+  //  -- a1 : the function to call (checked to be a JSBoundFunction)
+  // -----------------------------------
+  __ AssertBoundFunction(a1);
+
+  // Patch the receiver to [[BoundThis]].
+  {
+    __ Ldl(t0, FieldMemOperand(a1, JSBoundFunction::kBoundThisOffset));
+    __ StoreReceiver(t0, a0, kScratchReg);
+  }
+
+  // Load [[BoundArguments]] into a2 and length of that into a4.
+  __ Ldl(a2, FieldMemOperand(a1, JSBoundFunction::kBoundArgumentsOffset));
+  __ SmiUntag(a4, FieldMemOperand(a2, FixedArray::kLengthOffset));
+
+  // ----------- S t a t e -------------
+  //  -- a0 : the number of arguments (not including the receiver)
+  //  -- a1 : the function to call (checked to be a JSBoundFunction)
+  //  -- a2 : the [[BoundArguments]] (implemented as FixedArray)
+  //  -- a4 : the number of [[BoundArguments]]
+  // -----------------------------------
+
+  // Reserve stack space for the [[BoundArguments]].
+  {
+    Label done;
+    __ slll(a4, kPointerSizeLog2, a5);
+    __ Subl(t0, sp, Operand(a5));
+    // Check the stack for overflow. We are not trying to catch interruptions
+    // (i.e. debug break and preemption) here, so check the "real stack limit".
+    LoadStackLimit(masm, kScratchReg, StackLimitKind::kRealStackLimit);
+    __ Branch(&done, hs, t0, Operand(kScratchReg));
+    {
+      FrameScope scope(masm, StackFrame::MANUAL);
+      __ EnterFrame(StackFrame::INTERNAL);
+      __ CallRuntime(Runtime::kThrowStackOverflow);
+    }
+    __ bind(&done);
+  }
+
+  // Pop receiver.
+  __ Pop(t0);
+
+  // Push [[BoundArguments]].
+  {
+    Label loop, done_loop;
+    __ SmiUntag(a4, FieldMemOperand(a2, FixedArray::kLengthOffset));
+    __ Addl(a0, a0, Operand(a4));
+    __ Addl(a2, a2, Operand(FixedArray::kHeaderSize - kHeapObjectTag));
+    __ bind(&loop);
+    __ Subl(a4, a4, Operand(1));
+    __ Branch(&done_loop, lt, a4, Operand(zero_reg));
+    __ s8addl(a4, a2, a5);  DCHECK_EQ(kPointerSizeLog2, 3);
+    __ Ldl(kScratchReg, MemOperand(a5));
+    __ Push(kScratchReg);
+    __ Branch(&loop);
+    __ bind(&done_loop);
+  }
+
+  // Push receiver.
+  __ Push(t0);
+
+  // Call the [[BoundTargetFunction]] via the Call builtin.
+  __ Ldl(a1, FieldMemOperand(a1, JSBoundFunction::kBoundTargetFunctionOffset));
+  __ Jump(BUILTIN_CODE(masm->isolate(), Call_ReceiverIsAny),
+          RelocInfo::CODE_TARGET);
+}
+
+// static
+void Builtins::Generate_Call(MacroAssembler* masm, ConvertReceiverMode mode) {
+  // ----------- S t a t e -------------
+  //  -- a0 : the number of arguments (not including the receiver)
+  //  -- a1 : the target to call (can be any Object).
+  // -----------------------------------
+
+  Label non_callable, non_smi;
+  __ JumpIfSmi(a1, &non_callable);
+  __ bind(&non_smi);
+  __ GetObjectType(a1, t1, t2);
+  __ Jump(masm->isolate()->builtins()->CallFunction(mode),
+          RelocInfo::CODE_TARGET, eq, t2, Operand(JS_FUNCTION_TYPE));
+  __ Jump(BUILTIN_CODE(masm->isolate(), CallBoundFunction),
+          RelocInfo::CODE_TARGET, eq, t2, Operand(JS_BOUND_FUNCTION_TYPE));
+
+  // Check if target has a [[Call]] internal method.
+  __ Ldbu(t1, FieldMemOperand(t1, Map::kBitFieldOffset));
+  __ And(t1, t1, Operand(Map::Bits1::IsCallableBit::kMask));
+  __ Branch(&non_callable, eq, t1, Operand(zero_reg));
+
+  __ Jump(BUILTIN_CODE(masm->isolate(), CallProxy),
+          RelocInfo::CODE_TARGET, eq, t2, Operand(JS_PROXY_TYPE));
+
+  // 2. Call to something else, which might have a [[Call]] internal method (if
+  // not we raise an exception).
+  // Overwrite the original receiver with the (original) target.
+  __ StoreReceiver(a1, a0, kScratchReg);
+  // Let the "call_as_function_delegate" take care of the rest.
+  __ LoadNativeContextSlot(Context::CALL_AS_FUNCTION_DELEGATE_INDEX, a1);
+  __ Jump(masm->isolate()->builtins()->CallFunction(
+              ConvertReceiverMode::kNotNullOrUndefined),
+          RelocInfo::CODE_TARGET);
+
+  // 3. Call to something that is not callable.
+  __ bind(&non_callable);
+  {
+    FrameScope scope(masm, StackFrame::INTERNAL);
+    __ Push(a1);
+    __ CallRuntime(Runtime::kThrowCalledNonCallable);
+  }
+}
+
+void Builtins::Generate_ConstructFunction(MacroAssembler* masm) {
+  // ----------- S t a t e -------------
+  //  -- a0 : the number of arguments (not including the receiver)
+  //  -- a1 : the constructor to call (checked to be a JSFunction)
+  //  -- a3 : the new target (checked to be a constructor)
+  // -----------------------------------
+  __ AssertConstructor(a1);
+  __ AssertFunction(a1);
+
+  // Calling convention for function specific ConstructStubs require
+  // a2 to contain either an AllocationSite or undefined.
+  __ LoadRoot(a2, RootIndex::kUndefinedValue);
+
+  Label call_generic_stub;
+
+  // Jump to JSBuiltinsConstructStub or JSConstructStubGeneric.
+  __ Ldl(a4, FieldMemOperand(a1, JSFunction::kSharedFunctionInfoOffset));
+  __ Ldwu(a4, FieldMemOperand(a4, SharedFunctionInfo::kFlagsOffset));
+  __ And(a4, a4, Operand(SharedFunctionInfo::ConstructAsBuiltinBit::kMask));
+  __ Branch(&call_generic_stub, eq, a4, Operand(zero_reg));
+
+  __ Jump(BUILTIN_CODE(masm->isolate(), JSBuiltinsConstructStub),
+          RelocInfo::CODE_TARGET);
+
+  __ bind(&call_generic_stub);
+  __ Jump(BUILTIN_CODE(masm->isolate(), JSConstructStubGeneric),
+          RelocInfo::CODE_TARGET);
+}
+
+// static
+void Builtins::Generate_ConstructBoundFunction(MacroAssembler* masm) {
+  // ----------- S t a t e -------------
+  //  -- a0 : the number of arguments (not including the receiver)
+  //  -- a1 : the function to call (checked to be a JSBoundFunction)
+  //  -- a3 : the new target (checked to be a constructor)
+  // -----------------------------------
+  __ AssertConstructor(a1);
+  __ AssertBoundFunction(a1);
+
+  // Load [[BoundArguments]] into a2 and length of that into a4.
+  __ Ldl(a2, FieldMemOperand(a1, JSBoundFunction::kBoundArgumentsOffset));
+  __ SmiUntag(a4, FieldMemOperand(a2, FixedArray::kLengthOffset));
+
+  // ----------- S t a t e -------------
+  //  -- a0 : the number of arguments (not including the receiver)
+  //  -- a1 : the function to call (checked to be a JSBoundFunction)
+  //  -- a2 : the [[BoundArguments]] (implemented as FixedArray)
+  //  -- a3 : the new target (checked to be a constructor)
+  //  -- a4 : the number of [[BoundArguments]]
+  // -----------------------------------
+
+  // Reserve stack space for the [[BoundArguments]].
+  {
+    Label done;
+    __ slll(a4, kPointerSizeLog2, a5);
+    __ Subl(t0, sp, Operand(a5));
+    // Check the stack for overflow. We are not trying to catch interruptions
+    // (i.e. debug break and preemption) here, so check the "real stack limit".
+    LoadStackLimit(masm, kScratchReg, StackLimitKind::kRealStackLimit);
+    __ Branch(&done, hs, t0, Operand(kScratchReg));
+    {
+      FrameScope scope(masm, StackFrame::MANUAL);
+      __ EnterFrame(StackFrame::INTERNAL);
+      __ CallRuntime(Runtime::kThrowStackOverflow);
+    }
+    __ bind(&done);
+  }
+
+  // Pop receiver.
+  __ Pop(t0);
+
+  // Push [[BoundArguments]].
+  {
+    Label loop, done_loop;
+    __ SmiUntag(a4, FieldMemOperand(a2, FixedArray::kLengthOffset));
+    __ Addl(a0, a0, Operand(a4));
+    __ Addl(a2, a2, Operand(FixedArray::kHeaderSize - kHeapObjectTag));
+    __ bind(&loop);
+    __ Subl(a4, a4, Operand(1));
+    __ Branch(&done_loop, lt, a4, Operand(zero_reg));
+    __ s8addl(a4, a2, a5);  DCHECK_EQ(kPointerSizeLog2, 3);
+    __ Ldl(kScratchReg, MemOperand(a5));
+    __ Push(kScratchReg);
+    __ Branch(&loop);
+    __ bind(&done_loop);
+  }
+
+  // Push receiver.
+  __ Push(t0);
+
+  // Patch new.target to [[BoundTargetFunction]] if new.target equals target.
+  {
+    Label skip_load;
+    __ Branch(&skip_load, ne, a1, Operand(a3));
+    __ Ldl(a3, FieldMemOperand(a1, JSBoundFunction::kBoundTargetFunctionOffset));
+    __ bind(&skip_load);
+  }
+
+  // Construct the [[BoundTargetFunction]] via the Construct builtin.
+  __ Ldl(a1, FieldMemOperand(a1, JSBoundFunction::kBoundTargetFunctionOffset));
+  __ Jump(BUILTIN_CODE(masm->isolate(), Construct), RelocInfo::CODE_TARGET);
+}
+
+// static
+void Builtins::Generate_Construct(MacroAssembler* masm) {
+  // ----------- S t a t e -------------
+  //  -- a0 : the number of arguments (not including the receiver)
+  //  -- a1 : the constructor to call (can be any Object)
+  //  -- a3 : the new target (either the same as the constructor or
+  //          the JSFunction on which new was invoked initially)
+  // -----------------------------------
+
+  // Check if target is a Smi.
+  Label non_constructor, non_proxy;
+  __ JumpIfSmi(a1, &non_constructor);
+
+  // Check if target has a [[Construct]] internal method.
+  __ Ldl(t1, FieldMemOperand(a1, HeapObject::kMapOffset));
+  __ Ldbu(t3, FieldMemOperand(t1, Map::kBitFieldOffset));
+  __ And(t3, t3, Operand(Map::Bits1::IsConstructorBit::kMask));
+  __ Branch(&non_constructor, eq, t3, Operand(zero_reg));
+
+  // Dispatch based on instance type.
+  __ Ldhu(t2, FieldMemOperand(t1, Map::kInstanceTypeOffset));
+  __ Jump(BUILTIN_CODE(masm->isolate(), ConstructFunction),
+          RelocInfo::CODE_TARGET, eq, t2, Operand(JS_FUNCTION_TYPE));
+
+  // Only dispatch to bound functions after checking whether they are
+  // constructors.
+  __ Jump(BUILTIN_CODE(masm->isolate(), ConstructBoundFunction),
+          RelocInfo::CODE_TARGET, eq, t2, Operand(JS_BOUND_FUNCTION_TYPE));
+
+  // Only dispatch to proxies after checking whether they are constructors.
+  __ Branch(&non_proxy, ne, t2, Operand(JS_PROXY_TYPE));
+  __ Jump(BUILTIN_CODE(masm->isolate(), ConstructProxy),
+          RelocInfo::CODE_TARGET);
+
+  // Called Construct on an exotic Object with a [[Construct]] internal method.
+  __ bind(&non_proxy);
+  {
+    // Overwrite the original receiver with the (original) target.
+    __ StoreReceiver(a1, a0, kScratchReg);
+    // Let the "call_as_constructor_delegate" take care of the rest.
+    __ LoadNativeContextSlot(Context::CALL_AS_CONSTRUCTOR_DELEGATE_INDEX, a1);
+    __ Jump(masm->isolate()->builtins()->CallFunction(),
+            RelocInfo::CODE_TARGET);
+  }
+
+  // Called Construct on an Object that doesn't have a [[Construct]] internal
+  // method.
+  __ bind(&non_constructor);
+  __ Jump(BUILTIN_CODE(masm->isolate(), ConstructedNonConstructable),
+          RelocInfo::CODE_TARGET);
+}
+
+void Builtins::Generate_ArgumentsAdaptorTrampoline(MacroAssembler* masm) {
+  // State setup as expected by MacroAssembler::InvokePrologue.
+  // ----------- S t a t e -------------
+  //  -- a0: actual arguments count
+  //  -- a1: function (passed through to callee)
+  //  -- a2: expected arguments count
+  //  -- a3: new target (passed through to callee)
+  // -----------------------------------
+
+  Label invoke, dont_adapt_arguments, stack_overflow;
+
+  Label enough, too_few;
+  __ Branch(&dont_adapt_arguments, eq, a2,
+            Operand(kDontAdaptArgumentsSentinel));
+  // We use Uless as the number of argument should always be greater than 0.
+  __ Branch(&too_few, Uless, a0, Operand(a2));
+
+  {  // Enough parameters: actual >= expected.
+    // a0: actual number of arguments as a smi
+    // a1: function
+    // a2: expected number of arguments
+    // a3: new target (passed through to callee)
+    __ bind(&enough);
+    EnterArgumentsAdaptorFrame(masm);
+    Generate_StackOverflowCheck(masm, a2, a5, kScratchReg, &stack_overflow);
+
+    // Calculate copy start address into a0 and copy end address into a4.
+    __ slll(a2, kPointerSizeLog2, a0);
+    __ Addl(a0, fp, a0);
+    // Adjust for return address and receiver.
+    __ Addl(a0, a0, Operand(2 * kPointerSize));
+    // Compute copy end address.
+    __ slll(a2, kPointerSizeLog2, a4);
+    __ Subl(a4, a0, Operand(a4));
+
+    // Copy the arguments (including the receiver) to the new stack frame.
+    // a0: copy start address
+    // a1: function
+    // a2: expected number of arguments
+    // a3: new target (passed through to callee)
+    // a4: copy end address
+
+    Label copy;
+    __ addl(a0, kPointerSize, a0);  // In delay slot.
+    __ bind(&copy);
+    __ subl(a0, kPointerSize, a0);  // In delay slot.
+    __ Ldl(a5, MemOperand(a0));
+    __ push(a5);
+    __ Branch(&copy, ne, a0, Operand(a4));
+
+    __ jmp(&invoke);
+  }
+
+  {  // Too few parameters: Actual < expected.
+    __ bind(&too_few);
+    EnterArgumentsAdaptorFrame(masm);
+    Generate_StackOverflowCheck(masm, a2, a5, kScratchReg, &stack_overflow);
+
+    // Fill the remaining expected arguments with undefined.
+    __ LoadRoot(t0, RootIndex::kUndefinedValue);
+    __ SmiUntag(t1, a0);
+    __ Subl(t2, a2, Operand(t1));
+    __ slll(t2, kSystemPointerSizeLog2, a4);
+    __ Subl(a4, fp, a4);
+    // Adjust for frame.
+    __ Subl(a4, a4,
+             Operand(ArgumentsAdaptorFrameConstants::kFixedFrameSizeFromFp +
+                     kSystemPointerSize));
+
+    Label fill;
+    __ bind(&fill);
+    __ push(t0);
+    __ Branch(&fill, ne, sp, Operand(a4));
+
+    // Calculate copy start address into r0 and copy end address is fp.
+    __ SmiScale(a0, a0, kPointerSizeLog2);
+    __ Addl(a0, fp, a0);
+
+    // Copy the arguments (including the receiver) to the new stack frame.
+    Label copy;
+    __ Addl(a0, a0, Operand(kSystemPointerSize));  // In delay slot.
+    __ bind(&copy);
+    __ Subl(a0, a0, Operand(kSystemPointerSize));  // In delay slot.
+
+    // Adjust load for return address and receiver.
+    __ Ldl(t0, MemOperand(a0, 2 * kSystemPointerSize));
+    __ push(t0);
+
+    __ Branch(&copy, ne, a0, Operand(fp));
+  }
+
+  // Call the entry point.
+  __ bind(&invoke);
+  __ mov(a0, a2);
+  // a0 : expected number of arguments
+  // a1 : function (passed through to callee)
+  // a3: new target (passed through to callee)
+  static_assert(kJavaScriptCallCodeStartRegister == a2, "ABI mismatch");
+  __ Ldl(a2, FieldMemOperand(a1, JSFunction::kCodeOffset));
+  __ Addl(a2, a2, Operand(Code::kHeaderSize - kHeapObjectTag));
+  __ Call(a2);
+
+  // Store offset of return address for deoptimizer.
+  masm->isolate()->heap()->SetArgumentsAdaptorDeoptPCOffset(masm->pc_offset());
+
+  // Exit frame and return.
+  LeaveArgumentsAdaptorFrame(masm);
+  __ Ret();
+
+  // -------------------------------------------
+  // Don't adapt arguments.
+  // -------------------------------------------
+  __ bind(&dont_adapt_arguments);
+  static_assert(kJavaScriptCallCodeStartRegister == a2, "ABI mismatch");
+  __ Ldl(a2, FieldMemOperand(a1, JSFunction::kCodeOffset));
+  __ Addl(a2, a2, Operand(Code::kHeaderSize - kHeapObjectTag));
+  __ Jump(a2);
+
+  __ bind(&stack_overflow);
+  {
+    FrameScope frame(masm, StackFrame::MANUAL);
+    __ CallRuntime(Runtime::kThrowStackOverflow);
+    __ sys_call(0x80);
+  }
+}
+
+void Builtins::Generate_WasmCompileLazy(MacroAssembler* masm) {
+  // The function index was put in t0 by the jump table trampoline.
+  // Convert to Smi for the runtime call
+  __ SmiTag(kWasmCompileLazyFuncIndexRegister);
+  {
+    HardAbortScope hard_abort(masm);  // Avoid calls to Abort.
+    FrameScope scope(masm, StackFrame::WASM_COMPILE_LAZY);
+
+    // Save all parameter registers (see wasm-linkage.cc). They might be
+    // overwritten in the runtime call below. We don't have any callee-saved
+    // registers in wasm, so no need to store anything else.
+    constexpr RegList gp_regs =
+        Register::ListOf(a0, a1, a2, a3, a4, a5);
+    constexpr RegList fp_regs =
+        DoubleRegister::ListOf(f16, f17, f18, f19, f20, f21);
+    constexpr int16_t num_to_push = base::bits::CountPopulation(gp_regs) +
+                                    base::bits::CountPopulation(fp_regs);
+    // The number of regs to be pushed before kWasmInstanceRegister should be
+    // equal to kNumberOfSavedAllParamRegs.
+    STATIC_ASSERT(num_to_push ==
+                  WasmCompileLazyFrameConstants::kNumberOfSavedAllParamRegs);
+    __ MultiPush(gp_regs);
+    __ MultiPushFPU(fp_regs);
+
+    // Pass instance and function index as an explicit arguments to the runtime
+    // function.
+    __ Push(kWasmInstanceRegister, kWasmCompileLazyFuncIndexRegister);
+    // Initialize the JavaScript context with 0. CEntry will use it to
+    // set the current context on the isolate.
+    __ Move(kContextRegister, Smi::zero());
+    __ CallRuntime(Runtime::kWasmCompileLazy, 2);
+
+    // Restore registers.
+    __ MultiPopFPU(fp_regs);
+    __ MultiPop(gp_regs);
+  }
+  // Finally, jump to the entrypoint.
+  __ Jump(v0);
+}
+
+void Builtins::Generate_WasmDebugBreak(MacroAssembler* masm) {
+  HardAbortScope hard_abort(masm);  // Avoid calls to Abort.
+  {
+    FrameScope scope(masm, StackFrame::WASM_DEBUG_BREAK);
+
+    // Save all parameter registers. They might hold live values, we restore
+    // them after the runtime call.
+    __ MultiPush(WasmDebugBreakFrameConstants::kPushedGpRegs);
+    __ MultiPushFPU(WasmDebugBreakFrameConstants::kPushedFpRegs);
+
+    // Initialize the JavaScript context with 0. CEntry will use it to
+    // set the current context on the isolate.
+    __ Move(cp, Smi::zero());
+    __ CallRuntime(Runtime::kWasmDebugBreak, 0);
+
+    // Restore registers.
+    __ MultiPopFPU(WasmDebugBreakFrameConstants::kPushedFpRegs);
+    __ MultiPop(WasmDebugBreakFrameConstants::kPushedGpRegs);
+  }
+  __ Ret();
+}
+
+void Builtins::Generate_CEntry(MacroAssembler* masm, int result_size,
+                               SaveFPRegsMode save_doubles, ArgvMode argv_mode,
+                               bool builtin_exit_frame) {
+  // Called from JavaScript; parameters are on stack as if calling JS function
+  // a0: number of arguments including receiver
+  // a1: pointer to builtin function
+  // fp: frame pointer    (restored after C call)
+  // sp: stack pointer    (restored as callee's sp after C call)
+  // cp: current context  (C callee-saved)
+  //
+  // If argv_mode == kArgvInRegister:
+  // a2: pointer to the first argument
+
+  if (argv_mode == kArgvInRegister) {
+    // Move argv into the correct register.
+    __ mov(s1, a2);
+  } else {
+    // Compute the argv pointer in a callee-saved register.
+    __ s8addl(a0, sp, s1);  DCHECK_EQ(kPointerSizeLog2, 3);
+    __ Subl(s1, s1, kPointerSize);
+  }
+
+  // Enter the exit frame that transitions from JavaScript to C++.
+  FrameScope scope(masm, StackFrame::MANUAL);
+  __ EnterExitFrame(
+      save_doubles == kSaveFPRegs, 0,
+      builtin_exit_frame ? StackFrame::BUILTIN_EXIT : StackFrame::EXIT);
+
+  // s0: number of arguments  including receiver (C callee-saved)
+  // s1: pointer to first argument (C callee-saved)
+  // s2: pointer to builtin function (C callee-saved)
+
+  // Prepare arguments for C routine.
+  // a0 = argc
+  __ mov(s0, a0);
+  __ mov(s2, a1);
+
+  // We are calling compiled C/C++ code. a0 and a1 hold our two arguments. We
+  // also need to reserve the 4 argument slots on the stack.
+
+  __ AssertStackIsAligned();
+
+  // a0 = argc, a1 = argv, a2 = isolate
+  __ li(a2, ExternalReference::isolate_address(masm->isolate()));
+  __ mov(a1, s1);
+
+  __ StoreReturnAddressAndCall(s2);
+
+  // Result returned in v0 or v1:v0 (a5:v0) - do not destroy these registers!
+
+  // Check result for exception sentinel.
+  Label exception_returned;
+  __ LoadRoot(a4, RootIndex::kException);
+  __ Branch(&exception_returned, eq, a4, Operand(v0));
+
+  // Check that there is no pending exception, otherwise we
+  // should have returned the exception sentinel.
+  if (FLAG_debug_code) {
+    Label okay;
+    ExternalReference pending_exception_address = ExternalReference::Create(
+        IsolateAddressId::kPendingExceptionAddress, masm->isolate());
+    __ li(a2, pending_exception_address);
+    __ Ldl(a2, MemOperand(a2));
+    __ LoadRoot(a4, RootIndex::kTheHoleValue);
+    // Cannot use check here as it attempts to generate call into runtime.
+    __ Branch(&okay, eq, a4, Operand(a2));
+    __ halt();//stop("Unexpected pending exception");
+    __ bind(&okay);
+  }
+
+  // Exit C frame and return.
+  // v0:v1(v0:a5): result
+  // sp: stack pointer
+  // fp: frame pointer
+  Register argc = argv_mode == kArgvInRegister
+                      // We don't want to pop arguments so set argc to no_reg.
+                      ? no_reg
+                      // s0: still holds argc (callee-saved).
+                      : s0;
+  __ LeaveExitFrame(save_doubles == kSaveFPRegs, argc, EMIT_RETURN);
+
+  // Handling of exception.
+  __ bind(&exception_returned);
+
+  ExternalReference pending_handler_context_address = ExternalReference::Create(
+      IsolateAddressId::kPendingHandlerContextAddress, masm->isolate());
+  ExternalReference pending_handler_entrypoint_address =
+      ExternalReference::Create(
+          IsolateAddressId::kPendingHandlerEntrypointAddress, masm->isolate());
+  ExternalReference pending_handler_fp_address = ExternalReference::Create(
+      IsolateAddressId::kPendingHandlerFPAddress, masm->isolate());
+  ExternalReference pending_handler_sp_address = ExternalReference::Create(
+      IsolateAddressId::kPendingHandlerSPAddress, masm->isolate());
+
+  // Ask the runtime for help to determine the handler. This will set v0 to
+  // contain the current pending exception, don't clobber it.
+  ExternalReference find_handler =
+      ExternalReference::Create(Runtime::kUnwindAndFindExceptionHandler);
+  {
+    FrameScope scope(masm, StackFrame::MANUAL);
+    __ PrepareCallCFunction(3, 0, a0);
+    __ mov(a0, zero_reg);
+    __ mov(a1, zero_reg);
+    __ li(a2, ExternalReference::isolate_address(masm->isolate()));
+    __ CallCFunction(find_handler, 3);
+  }
+
+  // Retrieve the handler context, SP and FP.
+  __ li(cp, pending_handler_context_address);
+  __ Ldl(cp, MemOperand(cp));
+  __ li(sp, pending_handler_sp_address);
+  __ Ldl(sp, MemOperand(sp));
+  __ li(fp, pending_handler_fp_address);
+  __ Ldl(fp, MemOperand(fp));
+
+  // If the handler is a JS frame, restore the context to the frame. Note that
+  // the context will be set to (cp == 0) for non-JS frames.
+  Label zero;
+  __ Branch(&zero, eq, cp, Operand(zero_reg));
+  __ Stl(cp, MemOperand(fp, StandardFrameConstants::kContextOffset));
+  __ bind(&zero);
+
+  // Reset the masking register. This is done independent of the underlying
+  // feature flag {FLAG_untrusted_code_mitigations} to make the snapshot work
+  // with both configurations. It is safe to always do this, because the
+  // underlying register is caller-saved and can be arbitrarily clobbered.
+  __ ResetSpeculationPoisonRegister();
+
+  // Compute the handler entry address and jump to it.
+  __ li(t12, pending_handler_entrypoint_address);
+  __ Ldl(t12, MemOperand(t12));
+  __ Jump(t12);
+}
+
+void Builtins::Generate_DoubleToI(MacroAssembler* masm) {
+  Label done;
+  Register result_reg = t0;
+
+  Register scratch = GetRegisterThatIsNotOneOf(result_reg);
+  Register scratch2 = GetRegisterThatIsNotOneOf(result_reg, scratch);
+  Register scratch3 = GetRegisterThatIsNotOneOf(result_reg, scratch, scratch2);
+
+  // Account for saved regs.
+  const int kArgumentOffset = 4 * kPointerSize;
+
+  __ Push(result_reg, scratch, scratch2, scratch3);
+
+#if 0
+  DoubleRegister double_scratch = kScratchDoubleReg;
+  FPURegister fp_scratch  = f16;
+  FPURegister fp_scratch2 = f17;
+  __ MultiPushFPU(fp_scratch.bit() | fp_scratch2.bit());  //SW64
+  kArgumentOffset += 2 * kPointerSize;
+
+  // Load double input.
+  __ Fldd(double_scratch, MemOperand(sp, kArgumentOffset));
+
+  // Clear cumulative exception flags and save the FCSR.
+  // SW64 neednot clear FPCR in 20150513.
+  __ rfpcr(fp_scratch2);
+  //in order to have same effection, we should do four steps in sw:
+  //1) set fpcr = 0
+  //2) Rounding: sw(10), round-to-even
+  //3) set trap bit: sw(62~61,51~49), exception controlled by fpcr but not trap
+  //4) set exception mode: sw(00) setfpec1
+  __ li(scratch, sFCSRControlMask | sFCSRRound1Mask); //1), 2), 3)
+  __ ifmovd(scratch, fp_scratch);
+  __ wfpcr(fp_scratch);
+  __ setfpec1();//4)
+
+  // Try a conversion to a signed integer.
+  __ fcvtdl_z(double_scratch, fp_scratch);
+  __ fcvtlw(fp_scratch, double_scratch);
+  // Move the converted value into the result register.
+  __ fimovs(double_scratch, scratch3);
+
+  // Retrieve and restore the FCSR.
+  __ rfpcr(fp_scratch);
+  __ wfpcr(fp_scratch2);
+  __ setfpec1();
+  __ fimovd(fp_scratch, scratch);
+
+  // Check for overflow and NaNs.
+  __ li(scratch2, sFCSROverflowFlagMask | sFCSRUnderflowFlagMask |
+                  sFCSRInvalidOpFlagMask);
+  __ And(scratch, scratch, Operand(scratch2));
+
+  __ MultiPopFPU(fp_scratch.bit() | fp_scratch2.bit());  //SW64
+  kArgumentOffset -= 2 * kPointerSize;
+
+  // If we had no exceptions then set result_reg and we are done.
+  Label error;
+  __ Branch(&error, ne, scratch, Operand(zero_reg));
+  __ Move(result_reg, scratch3);
+  __ Branch(&done);
+  __ bind(&error);
+#endif
+
+  // Load the double value and perform a manual truncation.
+  Register input_high = scratch2;
+  Register input_low = scratch3;
+
+  __ Ldw(input_low, MemOperand(sp, kArgumentOffset + Register::kMantissaOffset));
+  __ Ldw(input_high,
+        MemOperand(sp, kArgumentOffset + Register::kExponentOffset));
+
+  Label normal_exponent;
+  // Extract the biased exponent in result.
+  __ Ext(result_reg, input_high, HeapNumber::kExponentShift,
+         HeapNumber::kExponentBits);
+
+  // Check for Infinity and NaNs, which should return 0.
+  __ Subw(scratch, result_reg, HeapNumber::kExponentMask);
+  __ Seleq(result_reg, zero_reg, scratch);
+  __ Branch(&done, eq, scratch, Operand(zero_reg));
+
+  // Express exponent as delta to (number of mantissa bits + 31).
+  __ Subw(result_reg, result_reg,
+          Operand(HeapNumber::kExponentBias + HeapNumber::kMantissaBits + 31));
+
+  // If the delta is strictly positive, all bits would be shifted away,
+  // which means that we can return 0.
+  __ Branch(&normal_exponent, le, result_reg, Operand(zero_reg));
+  __ mov(result_reg, zero_reg);
+  __ Branch(&done);
+
+  __ bind(&normal_exponent);
+  const int kShiftBase = HeapNumber::kNonMantissaBitsInTopWord - 1;
+  // Calculate shift.
+  __ Addw(scratch, result_reg, Operand(kShiftBase + HeapNumber::kMantissaBits));
+
+  // Save the sign.
+  Register sign = result_reg;
+  result_reg = no_reg;
+  __ And(sign, input_high, Operand(HeapNumber::kSignMask));
+
+  // On ARM shifts > 31 bits are valid and will result in zero. On SW64 we need
+  // to check for this specific case.
+  Label high_shift_needed, high_shift_done;
+  __ Branch(&high_shift_needed, lt, scratch, Operand(32));
+  __ mov(input_high, zero_reg);
+  __ Branch(&high_shift_done);
+  __ bind(&high_shift_needed);
+
+  // Set the implicit 1 before the mantissa part in input_high.
+  __ Or(input_high, input_high,
+        Operand(1 << HeapNumber::kMantissaBitsInTopWord));
+  // Shift the mantissa bits to the correct position.
+  // We don't need to clear non-mantissa bits as they will be shifted away.
+  // If they weren't, it would mean that the answer is in the 32bit range.
+  __ Sllw(input_high, input_high, scratch);
+
+  __ bind(&high_shift_done);
+
+  // Replace the shifted bits with bits from the lower mantissa word.
+  Label pos_shift, shift_done;
+  __ li(kScratchReg, 32);
+  __ Subw(scratch, kScratchReg, scratch);
+  __ Branch(&pos_shift, ge, scratch, Operand(zero_reg));
+
+  // Negate scratch.
+  __ Subw(scratch, zero_reg, scratch);
+  __ Sllw(input_low, input_low, scratch);
+  __ Branch(&shift_done);
+
+  __ bind(&pos_shift);
+  __ Srlw(input_low, input_low, scratch);
+
+  __ bind(&shift_done);
+  __ Or(input_high, input_high, Operand(input_low));
+  // Restore sign if necessary.
+  __ mov(scratch, sign);
+  result_reg = sign;
+  sign = no_reg;
+  __ Subw(result_reg, zero_reg, input_high);
+  __ Seleq(result_reg, input_high, scratch);
+
+  __ bind(&done);
+
+  __ Stl(result_reg, MemOperand(sp, kArgumentOffset));
+  __ Pop(result_reg, scratch, scratch2, scratch3);
+  __ Ret();
+}
+
+void Builtins::Generate_GenericJSToWasmWrapper(MacroAssembler* masm) {
+  // TODO(v8:10701): Implement for this platform.
+  __ Trap();
+}
+
+namespace {
+
+int AddressOffset(ExternalReference ref0, ExternalReference ref1) {
+  int64_t offset = (ref0.address() - ref1.address());
+  DCHECK(static_cast<int>(offset) == offset);
+  return static_cast<int>(offset);
+}
+
+// Calls an API function.  Allocates HandleScope, extracts returned value
+// from handle and propagates exceptions.  Restores context.  stack_space
+// - space to be unwound on exit (includes the call JS arguments space and
+// the additional space allocated for the fast call).
+void CallApiFunctionAndReturn(MacroAssembler* masm, Register function_address,
+                              ExternalReference thunk_ref, int stack_space,
+                              MemOperand* stack_space_operand,
+                              MemOperand return_value_operand) {
+  Isolate* isolate = masm->isolate();
+  ExternalReference next_address =
+      ExternalReference::handle_scope_next_address(isolate);
+  const int kNextOffset = 0;
+  const int kLimitOffset = AddressOffset(
+      ExternalReference::handle_scope_limit_address(isolate), next_address);
+  const int kLevelOffset = AddressOffset(
+      ExternalReference::handle_scope_level_address(isolate), next_address);
+
+  DCHECK(function_address == a1 || function_address == a2);
+
+  Label profiler_enabled, end_profiler_check;
+  __ li(t12, ExternalReference::is_profiling_address(isolate));
+  __ Ldb(t12, MemOperand(t12, 0));
+  __ Branch(&profiler_enabled, ne, t12, Operand(zero_reg));
+  __ li(t12, ExternalReference::address_of_runtime_stats_flag());
+  __ Ldw(t12, MemOperand(t12, 0));
+  __ Branch(&profiler_enabled, ne, t12, Operand(zero_reg));
+  {
+    // Call the api function directly.
+    __ mov(t12, function_address);
+    __ Branch(&end_profiler_check);
+  }
+
+  __ bind(&profiler_enabled);
+  {
+  // Additional parameter is the address of the actual callback.
+    __ li(t12, thunk_ref);
+  }
+  __ bind(&end_profiler_check);
+
+  // Allocate HandleScope in callee-save registers.
+  __ li(s3, next_address);
+  __ Ldl(s0, MemOperand(s3, kNextOffset));
+  __ Ldl(s1, MemOperand(s3, kLimitOffset));
+  __ Ldw(s2, MemOperand(s3, kLevelOffset));
+  __ Addw(s2, s2, Operand(1));
+  __ Stw(s2, MemOperand(s3, kLevelOffset));
+
+  __ StoreReturnAddressAndCall(t12);
+
+  Label promote_scheduled_exception;
+  Label delete_allocated_handles;
+  Label leave_exit_frame;
+  Label return_value_loaded;
+
+  // Load value from ReturnValue.
+  __ Ldl(v0, return_value_operand);
+  __ bind(&return_value_loaded);
+
+  // No more valid handles (the result handle was the last one). Restore
+  // previous handle scope.
+  __ Stl(s0, MemOperand(s3, kNextOffset));
+  if (__ emit_debug_code()) {
+    __ Ldw(a1, MemOperand(s3, kLevelOffset));
+    __ Check(eq, AbortReason::kUnexpectedLevelAfterReturnFromApiCall, a1,
+             Operand(s2));
+  }
+  __ Subw(s2, s2, Operand(1));
+  __ Stw(s2, MemOperand(s3, kLevelOffset));
+  __ Ldl(kScratchReg, MemOperand(s3, kLimitOffset));
+  __ Branch(&delete_allocated_handles, ne, s1, Operand(kScratchReg));
+
+  // Leave the API exit frame.
+  __ bind(&leave_exit_frame);
+
+  if (stack_space_operand == nullptr) {
+    DCHECK_NE(stack_space, 0);
+    __ li(s0, Operand(stack_space));
+  } else {
+    DCHECK_EQ(stack_space, 0);
+    STATIC_ASSERT(kCArgSlotCount == 0);
+    __ Ldl(s0, *stack_space_operand);
+  }
+
+  static constexpr bool kDontSaveDoubles = false;
+  static constexpr bool kRegisterContainsSlotCount = false;
+  __ LeaveExitFrame(kDontSaveDoubles, s0, NO_EMIT_RETURN,
+                    kRegisterContainsSlotCount);
+
+  // Check if the function scheduled an exception.
+  __ LoadRoot(a4, RootIndex::kTheHoleValue);
+  __ li(kScratchReg, ExternalReference::scheduled_exception_address(isolate));
+  __ Ldl(a5, MemOperand(kScratchReg));
+  __ Branch(&promote_scheduled_exception, ne, a4, Operand(a5));
+
+  __ Ret();
+
+  // Re-throw by promoting a scheduled exception.
+  __ bind(&promote_scheduled_exception);
+  __ TailCallRuntime(Runtime::kPromoteScheduledException);
+
+  // HandleScope limit has changed. Delete allocated extensions.
+  __ bind(&delete_allocated_handles);
+  __ Stl(s1, MemOperand(s3, kLimitOffset));
+  __ mov(s0, v0);
+  __ mov(a0, v0);
+  __ PrepareCallCFunction(1, s1);
+  __ li(a0, ExternalReference::isolate_address(isolate));
+  __ CallCFunction(ExternalReference::delete_handle_scope_extensions(), 1);
+  __ mov(v0, s0);
+  __ jmp(&leave_exit_frame);
+}
+
+}  // namespace
+
+void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
+  // ----------- S t a t e -------------
+  //  -- cp                  : context
+  //  -- a1                  : api function address
+  //  -- a2                  : arguments count (not including the receiver)
+  //  -- a3                  : call data
+  //  -- a0                  : holder
+  //  --
+  //  -- sp[0]               : last argument
+  //  -- ...
+  //  -- sp[(argc - 1) * 8]  : first argument
+  //  -- sp[(argc + 0) * 8]  : receiver
+  // -----------------------------------
+
+  Register api_function_address = a1;
+  Register argc = a2;
+  Register call_data = a3;
+  Register holder = a0;
+  Register scratch = t0;
+  Register base = t1;  // For addressing MemOperands on the stack.
+
+  DCHECK(!AreAliased(api_function_address, argc, call_data,
+                     holder, scratch, base));
+
+  using FCA = FunctionCallbackArguments;
+
+  STATIC_ASSERT(FCA::kArgsLength == 6);
+  STATIC_ASSERT(FCA::kNewTargetIndex == 5);
+  STATIC_ASSERT(FCA::kDataIndex == 4);
+  STATIC_ASSERT(FCA::kReturnValueOffset == 3);
+  STATIC_ASSERT(FCA::kReturnValueDefaultValueIndex == 2);
+  STATIC_ASSERT(FCA::kIsolateIndex == 1);
+  STATIC_ASSERT(FCA::kHolderIndex == 0);
+
+  // Set up FunctionCallbackInfo's implicit_args on the stack as follows:
+  //
+  // Target state:
+  //   sp[0 * kPointerSize]: kHolder
+  //   sp[1 * kPointerSize]: kIsolate
+  //   sp[2 * kPointerSize]: undefined (kReturnValueDefaultValue)
+  //   sp[3 * kPointerSize]: undefined (kReturnValue)
+  //   sp[4 * kPointerSize]: kData
+  //   sp[5 * kPointerSize]: undefined (kNewTarget)
+
+  // Set up the base register for addressing through MemOperands. It will point
+  // at the receiver (located at sp + argc * kPointerSize).
+  __ s8addl(argc, sp, base);  DCHECK_EQ(kPointerSizeLog2, 3);
+
+  // Reserve space on the stack.
+  __ Subl(sp, sp, Operand(FCA::kArgsLength * kPointerSize));
+
+  // kHolder.
+  __ Stl(holder, MemOperand(sp, 0 * kPointerSize));
+
+  // kIsolate.
+  __ li(scratch, ExternalReference::isolate_address(masm->isolate()));
+  __ Stl(scratch, MemOperand(sp, 1 * kPointerSize));
+
+  // kReturnValueDefaultValue and kReturnValue.
+  __ LoadRoot(scratch, RootIndex::kUndefinedValue);
+  __ Stl(scratch, MemOperand(sp, 2 * kPointerSize));
+  __ Stl(scratch, MemOperand(sp, 3 * kPointerSize));
+
+  // kData.
+  __ Stl(call_data, MemOperand(sp, 4 * kPointerSize));
+
+  // kNewTarget.
+  __ Stl(scratch, MemOperand(sp, 5 * kPointerSize));
+
+  // Keep a pointer to kHolder (= implicit_args) in a scratch register.
+  // We use it below to set up the FunctionCallbackInfo object.
+  __ mov(scratch, sp);
+
+  // Allocate the v8::Arguments structure in the arguments' space since
+  // it's not controlled by GC.
+  static constexpr int kApiStackSpace = 4;
+  static constexpr bool kDontSaveDoubles = false;
+  FrameScope frame_scope(masm, StackFrame::MANUAL);
+  __ EnterExitFrame(kDontSaveDoubles, kApiStackSpace);
+
+  // EnterExitFrame may align the sp.
+
+  // FunctionCallbackInfo::implicit_args_ (points at kHolder as set up above).
+  // Arguments are after the return address (pushed by EnterExitFrame()).
+  __ Stl(scratch, MemOperand(sp, 1 * kPointerSize));
+
+  // FunctionCallbackInfo::values_ (points at the first varargs argument passed
+  // on the stack).
+  __ Addl(scratch, scratch,
+          Operand((FCA::kArgsLength + 1) * kSystemPointerSize));
+
+  __ Stl(scratch, MemOperand(sp, 2 * kPointerSize));
+
+  // FunctionCallbackInfo::length_.
+  // Stored as int field, 32-bit integers within struct on stack always left
+  // justified by n64 ABI.
+  __ Stw(argc, MemOperand(sp, 3 * kPointerSize));
+
+  // We also store the number of bytes to drop from the stack after returning
+  // from the API function here.
+  // Note: Unlike on other architectures, this stores the number of slots to
+  // drop, not the number of bytes.
+  __ Addl(scratch, argc, Operand(FCA::kArgsLength + 1 /* receiver */));
+  __ Stl(scratch, MemOperand(sp, 4 * kPointerSize));
+
+  // v8::InvocationCallback's argument.
+  DCHECK(!AreAliased(api_function_address, scratch, a0));
+  __ Addl(a0, sp, Operand(1 * kPointerSize));
+
+  ExternalReference thunk_ref = ExternalReference::invoke_function_callback();
+
+  // There are two stack slots above the arguments we constructed on the stack.
+  // TODO(jgruber): Document what these arguments are.
+  static constexpr int kStackSlotsAboveFCA = 2;
+  MemOperand return_value_operand(
+      fp, (kStackSlotsAboveFCA + FCA::kReturnValueOffset) * kPointerSize);
+
+  static constexpr int kUseStackSpaceOperand = 0;
+  MemOperand stack_space_operand(sp, 4 * kPointerSize);
+
+  AllowExternalCallThatCantCauseGC scope(masm);
+  CallApiFunctionAndReturn(masm, api_function_address, thunk_ref,
+                           kUseStackSpaceOperand, &stack_space_operand,
+                           return_value_operand);
+}
+
+void Builtins::Generate_CallApiGetter(MacroAssembler* masm) {
+  // Build v8::PropertyCallbackInfo::args_ array on the stack and push property
+  // name below the exit frame to make GC aware of them.
+  STATIC_ASSERT(PropertyCallbackArguments::kShouldThrowOnErrorIndex == 0);
+  STATIC_ASSERT(PropertyCallbackArguments::kHolderIndex == 1);
+  STATIC_ASSERT(PropertyCallbackArguments::kIsolateIndex == 2);
+  STATIC_ASSERT(PropertyCallbackArguments::kReturnValueDefaultValueIndex == 3);
+  STATIC_ASSERT(PropertyCallbackArguments::kReturnValueOffset == 4);
+  STATIC_ASSERT(PropertyCallbackArguments::kDataIndex == 5);
+  STATIC_ASSERT(PropertyCallbackArguments::kThisIndex == 6);
+  STATIC_ASSERT(PropertyCallbackArguments::kArgsLength == 7);
+
+  Register receiver = ApiGetterDescriptor::ReceiverRegister();
+  Register holder = ApiGetterDescriptor::HolderRegister();
+  Register callback = ApiGetterDescriptor::CallbackRegister();
+  Register scratch = a4;
+  DCHECK(!AreAliased(receiver, holder, callback, scratch));
+
+  Register api_function_address = a2;
+
+  // Here and below +1 is for name() pushed after the args_ array.
+  using PCA = PropertyCallbackArguments;
+  __ Subl(sp, sp, (PCA::kArgsLength + 1) * kPointerSize);
+  __ Stl(receiver, MemOperand(sp, (PCA::kThisIndex + 1) * kPointerSize));
+  __ Ldl(scratch, FieldMemOperand(callback, AccessorInfo::kDataOffset));
+  __ Stl(scratch, MemOperand(sp, (PCA::kDataIndex + 1) * kPointerSize));
+  __ LoadRoot(scratch, RootIndex::kUndefinedValue);
+  __ Stl(scratch, MemOperand(sp, (PCA::kReturnValueOffset + 1) * kPointerSize));
+  __ Stl(scratch, MemOperand(sp, (PCA::kReturnValueDefaultValueIndex + 1) *
+                                    kPointerSize));
+  __ li(scratch, ExternalReference::isolate_address(masm->isolate()));
+  __ Stl(scratch, MemOperand(sp, (PCA::kIsolateIndex + 1) * kPointerSize));
+  __ Stl(holder, MemOperand(sp, (PCA::kHolderIndex + 1) * kPointerSize));
+  // should_throw_on_error -> false
+  DCHECK_EQ(0, Smi::zero().ptr());
+  __ Stl(zero_reg,
+        MemOperand(sp, (PCA::kShouldThrowOnErrorIndex + 1) * kPointerSize));
+  __ Ldl(scratch, FieldMemOperand(callback, AccessorInfo::kNameOffset));
+  __ Stl(scratch, MemOperand(sp, 0 * kPointerSize));
+
+  // v8::PropertyCallbackInfo::args_ array and name handle.
+  const int kStackUnwindSpace = PropertyCallbackArguments::kArgsLength + 1;
+
+  // Load address of v8::PropertyAccessorInfo::args_ array and name handle.
+  __ mov(a0, sp);                               // a0 = Handle<Name>
+  __ Addl(a1, a0, Operand(1 * kPointerSize));  // a1 = v8::PCI::args_
+
+  const int kApiStackSpace = 1;
+  FrameScope frame_scope(masm, StackFrame::MANUAL);
+  __ EnterExitFrame(false, kApiStackSpace);
+
+  // Create v8::PropertyCallbackInfo object on the stack and initialize
+  // it's args_ field.
+  __ Stl(a1, MemOperand(sp, 1 * kPointerSize));
+  __ Addl(a1, sp, Operand(1 * kPointerSize));
+  // a1 = v8::PropertyCallbackInfo&
+
+  ExternalReference thunk_ref =
+      ExternalReference::invoke_accessor_getter_callback();
+
+  __ Ldl(scratch, FieldMemOperand(callback, AccessorInfo::kJsGetterOffset));
+  __ Ldl(api_function_address,
+        FieldMemOperand(scratch, Foreign::kForeignAddressOffset));
+
+  // +3 is to skip prolog, return address and name handle.
+  MemOperand return_value_operand(
+      fp, (PropertyCallbackArguments::kReturnValueOffset + 3) * kPointerSize);
+  MemOperand* const kUseStackSpaceConstant = nullptr;
+  CallApiFunctionAndReturn(masm, api_function_address, thunk_ref,
+                           kStackUnwindSpace, kUseStackSpaceConstant,
+                           return_value_operand);
+}
+
+void Builtins::Generate_DirectCEntry(MacroAssembler* masm) {
+  // The sole purpose of DirectCEntry is for movable callers (e.g. any general
+  // purpose Code object) to be able to call into C functions that may trigger
+  // GC and thus move the caller.
+  //
+  // DirectCEntry places the return address on the stack (updated by the GC),
+  // making the call GC safe. The irregexp backend relies on this.
+
+  // Make place for arguments to fit C calling convention. Callers use
+  // EnterExitFrame/LeaveExitFrame so they handle stack restoring and we don't
+  // have to do that here. Any caller must drop kCArgsSlotsSize stack space
+  // after the call.
+  __ subl(sp, kCArgsSlotsSize, sp);
+
+  __ Stl(ra, MemOperand(sp, kCArgsSlotsSize));  // Store the return address.
+  __ Call(t12);                                 // Call the C++ function.
+  // set fpec 1 while return from C++.
+  __ setfpec1();
+  __ Ldl(t12, MemOperand(sp, kCArgsSlotsSize));  // Return to calling code.
+
+  if (FLAG_debug_code && FLAG_enable_slow_asserts) {
+    // In case of an error the return address may point to a memory area
+    // filled with kZapValue by the GC. Dereference the address and check for
+    // this.
+    __ Uldl(a4, MemOperand(t12));
+    __ Assert(ne, AbortReason::kReceivedInvalidReturnAddress, a4,
+              Operand(reinterpret_cast<uint64_t>(kZapValue)));
+  }
+
+  __ Jump(t12);
+}
+
+#undef __
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_TARGET_ARCH_SW64
diff --git a/src/3rdparty/chromium/v8/src/codegen/assembler-arch.h b/src/3rdparty/chromium/v8/src/codegen/assembler-arch.h
index d56b372504..9aed27934f 100644
--- a/src/3rdparty/chromium/v8/src/codegen/assembler-arch.h
+++ b/src/3rdparty/chromium/v8/src/codegen/assembler-arch.h
@@ -23,6 +23,8 @@
 #include "src/codegen/mips64/assembler-mips64.h"
 #elif V8_TARGET_ARCH_S390
 #include "src/codegen/s390/assembler-s390.h"
+#elif V8_TARGET_ARCH_SW64
+#include "src/codegen/sw64/assembler-sw64.h"
 #else
 #error Unknown architecture.
 #endif
diff --git a/src/3rdparty/chromium/v8/src/codegen/assembler-inl.h b/src/3rdparty/chromium/v8/src/codegen/assembler-inl.h
index 8c81315d50..59a49ecf91 100644
--- a/src/3rdparty/chromium/v8/src/codegen/assembler-inl.h
+++ b/src/3rdparty/chromium/v8/src/codegen/assembler-inl.h
@@ -23,6 +23,8 @@
 #include "src/codegen/mips64/assembler-mips64-inl.h"
 #elif V8_TARGET_ARCH_S390
 #include "src/codegen/s390/assembler-s390-inl.h"
+#elif V8_TARGET_ARCH_SW64
+#include "src/codegen/sw64/assembler-sw64-inl.h"
 #else
 #error Unknown architecture.
 #endif
diff --git a/src/3rdparty/chromium/v8/src/codegen/assembler-sw64-inl.h b/src/3rdparty/chromium/v8/src/codegen/assembler-sw64-inl.h
new file mode 100755
index 0000000000..22811ec31e
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/codegen/assembler-sw64-inl.h
@@ -0,0 +1,329 @@
+
+// Copyright (c) 1994-2006 Sun Microsystems Inc.
+// All Rights Reserved.
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are
+// met:
+//
+// - Redistributions of source code must retain the above copyright notice,
+// this list of conditions and the following disclaimer.
+//
+// - Redistribution in binary form must reproduce the above copyright
+// notice, this list of conditions and the following disclaimer in the
+// documentation and/or other materials provided with the distribution.
+//
+// - Neither the name of Sun Microsystems or the names of contributors may
+// be used to endorse or promote products derived from this software without
+// specific prior written permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
+// IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
+// THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+// PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+// CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+// EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+// The original source code covered by the above license above has been
+// modified significantly by Google Inc.
+// Copyright 2012 the V8 project authors. All rights reserved.
+
+#ifndef V8_CODEGEN_SW64_ASSEMBLER_SW64_INL_H_
+#define V8_CODEGEN_SW64_ASSEMBLER_SW64_INL_H_
+
+#include "src/codegen/sw64/assembler-sw64.h"
+
+#include "src/codegen/assembler.h"
+#include "src/debug/debug.h"
+#include "src/objects/objects-inl.h"
+
+namespace v8 {
+namespace internal {
+
+bool CpuFeatures::SupportsOptimizer() { return IsSupported(FPU); }
+
+bool CpuFeatures::SupportsWasmSimd128() { return IsSupported(SW64_SIMD); }
+
+// -----------------------------------------------------------------------------
+// Operand and MemOperand.
+
+bool Operand::is_reg() const {
+  return rm_.is_valid();
+}
+
+int64_t Operand::immediate() const {
+  DCHECK(!is_reg());
+  DCHECK(!IsHeapObjectRequest());
+  return value_.immediate;
+}
+
+// -----------------------------------------------------------------------------
+// RelocInfo.
+
+void RelocInfo::apply(intptr_t delta) {
+  if (IsInternalReference(rmode_) || IsInternalReferenceEncoded(rmode_)) {
+    // Absolute code pointer inside code object moves with the code object.
+    Assembler::RelocateInternalReference(rmode_, pc_, delta);
+  }
+}
+
+
+Address RelocInfo::target_address() {
+  DCHECK(IsCodeTarget(rmode_) || IsRuntimeEntry(rmode_) || IsWasmCall(rmode_));
+  return Assembler::target_address_at(pc_, constant_pool_);
+}
+
+Address RelocInfo::target_address_address() {
+  DCHECK(HasTargetAddressAddress());
+  // Read the address of the word containing the target_address in an
+  // instruction stream.
+  // The only architecture-independent user of this function is the serializer.
+  // The serializer uses it to find out how many raw bytes of instruction to
+  // output before the next target.
+  // For an instruction like LUI/ORI where the target bits are mixed into the
+  // instruction bits, the size of the target will be zero, indicating that the
+  // serializer should not step forward in memory after a target is resolved
+  // and written. In this case the target_address_address function should
+  // return the end of the instructions to be patched, allowing the
+  // deserializer to deserialize the instructions as raw bytes and put them in
+  // place, ready to be patched with the target. After jump optimization,
+  // that is the address of the instruction that follows J/JAL/JR/JALR
+  // instruction.
+  return pc_ + Assembler::kInstructionsFor64BitConstant * kInstrSize;
+}
+
+Address RelocInfo::constant_pool_entry_address() { UNREACHABLE(); }
+
+int RelocInfo::target_address_size() { return Assembler::kSpecialTargetSize; }
+
+void Assembler::deserialization_set_special_target_at(
+    Address instruction_payload, Code code, Address target) {
+  set_target_address_at(instruction_payload,
+                        !code.is_null() ? code.constant_pool() : kNullAddress,
+                        target);
+}
+
+int Assembler::deserialization_special_target_size(
+    Address instruction_payload) {
+  return kSpecialTargetSize;
+}
+
+void Assembler::set_target_internal_reference_encoded_at(Address pc,
+                                                         Address target) {
+  // Encoded internal references are j/jal instructions.
+  Instr instr = Assembler::instr_at(pc + 0 * kInstrSize);
+
+  uint64_t imm28 = target & static_cast<uint64_t>(kImm28Mask);
+
+  instr &= ~kImm26Mask;
+  uint64_t imm26 = imm28 >> 2;
+  DCHECK(is_uint26(imm26));
+
+  instr_at_put(pc, instr | (imm26 & kImm26Mask));
+  // Currently used only by deserializer, and all code will be flushed
+  // after complete deserialization, no need to flush on each reference.
+}
+
+void Assembler::deserialization_set_target_internal_reference_at(
+    Address pc, Address target, RelocInfo::Mode mode) {
+  if (mode == RelocInfo::INTERNAL_REFERENCE_ENCODED) {
+    UNREACHABLE();  // DCHECK(IsJ(instr_at(pc)));
+    set_target_internal_reference_encoded_at(pc, target);
+  } else {
+    DCHECK(mode == RelocInfo::INTERNAL_REFERENCE);
+    Memory<Address>(pc) = target;
+  }
+}
+
+HeapObject RelocInfo::target_object() {
+  DCHECK(IsCodeTarget(rmode_) || IsFullEmbeddedObject(rmode_));
+  return HeapObject::cast(
+      Object(Assembler::target_address_at(pc_, constant_pool_)));
+}
+
+HeapObject RelocInfo::target_object_no_host(Isolate* isolate) {
+  return target_object();
+}
+
+Handle<HeapObject> RelocInfo::target_object_handle(Assembler* origin) {
+    DCHECK(IsCodeTarget(rmode_) || IsFullEmbeddedObject(rmode_));
+  return Handle<HeapObject>(reinterpret_cast<Address*>(
+      Assembler::target_address_at(pc_, constant_pool_)));
+}
+
+void RelocInfo::set_target_object(Heap* heap, HeapObject target,
+                                  WriteBarrierMode write_barrier_mode,
+                                  ICacheFlushMode icache_flush_mode) {
+  DCHECK(IsCodeTarget(rmode_) || IsFullEmbeddedObject(rmode_));
+    Assembler::set_target_address_at(pc_, constant_pool_, target.ptr(),
+                                   icache_flush_mode);
+  if (write_barrier_mode == UPDATE_WRITE_BARRIER && !host().is_null() &&
+      !FLAG_disable_write_barriers) {
+    WriteBarrierForCode(host(), this, target);
+  }
+}
+
+
+Address RelocInfo::target_external_reference() {
+  DCHECK(rmode_ == EXTERNAL_REFERENCE);
+  return Assembler::target_address_at(pc_, constant_pool_);
+}
+
+void RelocInfo::set_target_external_reference(
+    Address target, ICacheFlushMode icache_flush_mode) {
+  DCHECK(rmode_ == RelocInfo::EXTERNAL_REFERENCE);
+  Assembler::set_target_address_at(pc_, constant_pool_, target,
+                                   icache_flush_mode);
+}
+
+Address RelocInfo::target_internal_reference() {
+  if (rmode_ == INTERNAL_REFERENCE) {
+    return Memory<Address>(pc_);
+  } else {
+    // Encoded internal references are j/jal instructions.
+    DCHECK(rmode_ == INTERNAL_REFERENCE_ENCODED);
+    Instr instr = Assembler::instr_at(pc_ + 0 * kInstrSize);
+    instr &= kImm26Mask;
+    uint64_t imm28 = instr << 2;
+    uint64_t segment = pc_ & ~static_cast<uint64_t>(kImm28Mask);
+    return static_cast<Address>(segment | imm28);
+  }
+}
+
+
+Address RelocInfo::target_internal_reference_address() {
+  DCHECK(rmode_ == INTERNAL_REFERENCE || rmode_ == INTERNAL_REFERENCE_ENCODED);
+  return pc_;
+}
+
+Address RelocInfo::target_runtime_entry(Assembler* origin) {
+  DCHECK(IsRuntimeEntry(rmode_));
+  return target_address();
+}
+
+void RelocInfo::set_target_runtime_entry(Address target,
+                                         WriteBarrierMode write_barrier_mode,
+                                         ICacheFlushMode icache_flush_mode) {
+  DCHECK(IsRuntimeEntry(rmode_));
+  if (target_address() != target)
+    set_target_address(target, write_barrier_mode, icache_flush_mode);
+}
+
+Address RelocInfo::target_off_heap_target() {
+  DCHECK(IsOffHeapTarget(rmode_));
+  return Assembler::target_address_at(pc_, constant_pool_);
+}
+
+void RelocInfo::WipeOut() {
+  DCHECK(IsFullEmbeddedObject(rmode_) || IsCodeTarget(rmode_) ||
+         IsRuntimeEntry(rmode_) || IsExternalReference(rmode_) ||
+         IsInternalReference(rmode_) || IsInternalReferenceEncoded(rmode_) ||
+         IsOffHeapTarget(rmode_));
+  if (IsInternalReference(rmode_)) {
+    Memory<Address>(pc_) = kNullAddress;
+  } else if (IsInternalReferenceEncoded(rmode_)) {
+    Assembler::set_target_internal_reference_encoded_at(pc_, kNullAddress);
+  } else {
+    Assembler::set_target_address_at(pc_, constant_pool_, kNullAddress);
+  }
+}
+
+// -----------------------------------------------------------------------------
+// Assembler.
+
+
+void Assembler::CheckBuffer() {
+  if (buffer_space() <= kGap) {
+    GrowBuffer();
+  }
+}
+
+
+void Assembler::CheckForEmitInForbiddenSlot() {
+  if (!is_buffer_growth_blocked()) {
+    CheckBuffer();
+  }
+  if (IsPrevInstrCompactBranch()) {
+    // Nop instruction to precede a CTI in forbidden slot:
+    Instr nop = op_ldi | (zero_reg.code() << sRaShift) | (zero_reg.code() << sRbShift);
+    *reinterpret_cast<Instr*>(pc_) = nop;
+    pc_ += kInstrSize;
+
+    ClearCompactBranchState();
+  }
+}
+
+
+void Assembler::EmitHelper(Instr x, CompactBranchType is_compact_branch) {
+  if (IsPrevInstrCompactBranch()) {
+    if (Instruction::IsForbiddenAfterBranchInstr(x)) {
+      // Nop instruction to precede a CTI in forbidden slot:
+      Instr nop = op_ldi | (zero_reg.code() << sRaShift) | (zero_reg.code() << sRbShift);
+      *reinterpret_cast<Instr*>(pc_) = nop;
+      pc_ += kInstrSize;
+    }
+    ClearCompactBranchState();
+  }
+  *reinterpret_cast<Instr*>(pc_) = x;
+  pc_ += kInstrSize;
+  if (is_compact_branch == CompactBranchType::COMPACT_BRANCH) {
+    EmittedCompactBranchInstruction();
+  }
+  CheckTrampolinePoolQuick();
+}
+
+template <>
+inline void Assembler::EmitHelper(uint8_t x);
+
+template <typename T>
+void Assembler::EmitHelper(T x) {
+  *reinterpret_cast<T*>(pc_) = x;
+  pc_ += sizeof(x);
+  CheckTrampolinePoolQuick();
+}
+
+template <>
+void Assembler::EmitHelper(uint8_t x) {
+  *reinterpret_cast<uint8_t*>(pc_) = x;
+  pc_ += sizeof(x);
+  if (reinterpret_cast<intptr_t>(pc_) % kInstrSize == 0) {
+    CheckTrampolinePoolQuick();
+  }
+}
+
+#ifdef ZHJDEL
+void Assembler::emit(Instr x, CompactBranchType is_compact_branch) {
+    UNREACHABLE();  // This should never be reached on sw64.
+}
+
+
+void Assembler::emit(uint64_t data) {
+    UNREACHABLE();  // This should never be reached on sw64.
+}
+#endif
+
+#ifdef SW64
+void Assembler::emitSW(Instr x) {
+  if (!is_buffer_growth_blocked()) {
+    CheckBuffer();
+  }
+  EmitHelper(x);
+}
+
+void Assembler::emitSW(uint64_t data) {
+   CheckForEmitInForbiddenSlot();
+   EmitHelper(data);
+}
+#endif
+
+EnsureSpace::EnsureSpace(Assembler* assembler) { assembler->CheckBuffer(); }
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_SW64_ASSEMBLER_SW64_INL_H_
diff --git a/src/3rdparty/chromium/v8/src/codegen/constants-arch.h b/src/3rdparty/chromium/v8/src/codegen/constants-arch.h
index 7a222c960f..74b11c074f 100644
--- a/src/3rdparty/chromium/v8/src/codegen/constants-arch.h
+++ b/src/3rdparty/chromium/v8/src/codegen/constants-arch.h
@@ -21,6 +21,8 @@
 #include "src/codegen/s390/constants-s390.h"  // NOLINT
 #elif V8_TARGET_ARCH_X64
 #include "src/codegen/x64/constants-x64.h"  // NOLINT
+#elif V8_TARGET_ARCH_SW64
+#include "src/codegen/sw64/constants-sw64.h"  // NOLINT
 #else
 #error Unsupported target architecture.
 #endif
diff --git a/src/3rdparty/chromium/v8/src/codegen/cpu-features.h b/src/3rdparty/chromium/v8/src/codegen/cpu-features.h
index eef98f77e7..4a7ea4ae5e 100644
--- a/src/3rdparty/chromium/v8/src/codegen/cpu-features.h
+++ b/src/3rdparty/chromium/v8/src/codegen/cpu-features.h
@@ -50,6 +50,11 @@ enum CpuFeature {
   MIPSr6,
   MIPS_SIMD,  // MSA instructions
 
+#elif V8_TARGET_ARCH_SW64
+  FPU,
+  FP64FPU,
+  SW64_SIMD,  // SSA instructions
+
 #elif V8_TARGET_ARCH_PPC || V8_TARGET_ARCH_PPC64
   FPU,
   FPR_GPR_MOV,
diff --git a/src/3rdparty/chromium/v8/src/codegen/external-reference.cc b/src/3rdparty/chromium/v8/src/codegen/external-reference.cc
index ba71702e7c..68d039c5e3 100644
--- a/src/3rdparty/chromium/v8/src/codegen/external-reference.cc
+++ b/src/3rdparty/chromium/v8/src/codegen/external-reference.cc
@@ -502,6 +502,8 @@ ExternalReference ExternalReference::invoke_accessor_getter_callback() {
 #define re_stack_check_func RegExpMacroAssemblerMIPS::CheckStackGuardState
 #elif V8_TARGET_ARCH_S390
 #define re_stack_check_func RegExpMacroAssemblerS390::CheckStackGuardState
+#elif V8_TARGET_ARCH_SW64
+#define re_stack_check_func RegExpMacroAssemblerSW64::CheckStackGuardState
 #else
 UNREACHABLE();
 #endif
@@ -935,7 +937,102 @@ static int EnterMicrotaskContextWrapper(HandleScopeImplementer* hsi,
   return 0;
 }
 
-FUNCTION_REFERENCE(call_enter_context_function, EnterMicrotaskContextWrapper)
+FUNCTION_REFERENCE(call_enter_context_function, EnterMicrotaskContextWrapper);
+
+// =====================================================================
+// add start for SW64.
+
+#define SW_CONST64(x)  (x ## LL)
+
+const int sw_min_int = (int)1 << (sizeof(int)*8-1);      // 0x80000000 == smallest int
+const long sw_min_long = SW_CONST64(0x8000000000000000);
+
+static int sw_div(int x, int y) {
+  if (x == sw_min_int && y == SW_CONST64(-1)) {
+    return x;
+  } else {
+#if defined(USE_SIMULATOR)
+   if (0 == y) return 0;
+#endif
+
+    return x / y;
+  }
+}
+
+static int sw_divu(uint32_t x, uint32_t y) {
+  return (int)(x / y);
+}
+
+static long sw_ddiv(long x, long y) {
+  if (x == sw_min_long && y == SW_CONST64(-1)) {
+    return x;
+  } else {
+    return x / y;
+  }
+}
+
+static long sw_ddivu(uint64_t x, uint64_t y) {
+  return (long)(x / y);
+}
+
+static int sw_mod(int x, int y) {
+  if (x == sw_min_int && y == SW_CONST64(-1)) {
+    return 0;
+  } else {
+    return x % y;
+  }
+}
+
+static int sw_modu(uint32_t x, uint32_t y) {
+  return (int)(x % y);
+}
+
+static long sw_dmod(long x, long y) {
+  if (x == sw_min_long && y == SW_CONST64(-1)) {
+    return 0;
+  } else {
+    return x % y;
+  }
+}
+
+static long sw_dmodu(uint64_t x, uint64_t y) {
+  return (long)(x % y);
+}
+
+ExternalReference ExternalReference::math_sw_div_function() {
+  return ExternalReference(Redirect(FUNCTION_ADDR(sw_div)));
+}
+
+ExternalReference ExternalReference::math_sw_divu_function() {
+  return ExternalReference(Redirect(FUNCTION_ADDR(sw_divu)));
+}
+
+ExternalReference ExternalReference::math_sw_ddiv_function() {
+  return ExternalReference(Redirect(FUNCTION_ADDR(sw_ddiv)));
+}
+
+ExternalReference ExternalReference::math_sw_ddivu_function() {
+  return ExternalReference(Redirect(FUNCTION_ADDR(sw_ddivu)));
+}
+
+ExternalReference ExternalReference::math_sw_mod_function() {
+  return ExternalReference(Redirect(FUNCTION_ADDR(sw_mod)));
+}
+
+ExternalReference ExternalReference::math_sw_modu_function() {
+  return ExternalReference(Redirect(FUNCTION_ADDR(sw_modu)));
+}
+
+ExternalReference ExternalReference::math_sw_dmod_function() {
+  return ExternalReference(Redirect(FUNCTION_ADDR(sw_dmod)));
+}
+
+ExternalReference ExternalReference::math_sw_dmodu_function() {
+  return ExternalReference(Redirect(FUNCTION_ADDR(sw_dmodu)));
+}
+// add end for SW64.
+// ======================================================================
+
 
 FUNCTION_REFERENCE(
     js_finalization_registry_remove_cell_from_unregister_token_map,
diff --git a/src/3rdparty/chromium/v8/src/codegen/external-reference.h b/src/3rdparty/chromium/v8/src/codegen/external-reference.h
index e35e12237b..bfeeaba9ac 100644
--- a/src/3rdparty/chromium/v8/src/codegen/external-reference.h
+++ b/src/3rdparty/chromium/v8/src/codegen/external-reference.h
@@ -115,6 +115,14 @@ class StatsCounter;
   V(f64_acos_wrapper_function, "f64_acos_wrapper")                             \
   V(f64_asin_wrapper_function, "f64_asin_wrapper")                             \
   V(f64_mod_wrapper_function, "f64_mod_wrapper")                               \
+  V(math_sw_div_function, "sw_div")                                            \
+  V(math_sw_divu_function, "sw_divu")                                          \
+  V(math_sw_ddiv_function, "sw_ddiv")                                          \
+  V(math_sw_ddivu_function, "sw_ddivu")                                        \
+  V(math_sw_mod_function, "sw_mod")                                            \
+  V(math_sw_modu_function, "sw_modu")                                          \
+  V(math_sw_dmod_function, "sw_dmod")                                          \
+  V(math_sw_dmodu_function, "sw_dmodu")                                        \
   V(get_date_field_function, "JSDate::GetField")                               \
   V(get_or_create_hash_raw, "get_or_create_hash_raw")                          \
   V(ieee754_acos_function, "base::ieee754::acos")                              \
diff --git a/src/3rdparty/chromium/v8/src/codegen/interface-descriptors.cc b/src/3rdparty/chromium/v8/src/codegen/interface-descriptors.cc
index 8a6235fa08..4c8efbe88a 100644
--- a/src/3rdparty/chromium/v8/src/codegen/interface-descriptors.cc
+++ b/src/3rdparty/chromium/v8/src/codegen/interface-descriptors.cc
@@ -397,7 +397,7 @@ void WasmFloat64ToNumberDescriptor::InitializePlatformSpecific(
 }
 #endif  // !V8_TARGET_ARCH_IA32
 
-#if !defined(V8_TARGET_ARCH_MIPS) && !defined(V8_TARGET_ARCH_MIPS64)
+#if !defined(V8_TARGET_ARCH_MIPS) && !defined(V8_TARGET_ARCH_MIPS64) && !defined(V8_TARGET_ARCH_SW64)
 void WasmI32AtomicWait32Descriptor::InitializePlatformSpecific(
     CallInterfaceDescriptorData* data) {
   DefaultInitializePlatformSpecific(data, kParameterCount);
diff --git a/src/3rdparty/chromium/v8/src/codegen/macro-assembler.h b/src/3rdparty/chromium/v8/src/codegen/macro-assembler.h
index a213fc504e..fb6a3e2012 100644
--- a/src/3rdparty/chromium/v8/src/codegen/macro-assembler.h
+++ b/src/3rdparty/chromium/v8/src/codegen/macro-assembler.h
@@ -52,6 +52,9 @@ enum AllocationFlags {
 #elif V8_TARGET_ARCH_S390
 #include "src/codegen/s390/constants-s390.h"
 #include "src/codegen/s390/macro-assembler-s390.h"
+#elif V8_TARGET_ARCH_SW64
+#include "src/codegen/sw64/constants-sw64.h"
+#include "src/codegen/sw64/macro-assembler-sw64.h"
 #else
 #error Unsupported target architecture.
 #endif
diff --git a/src/3rdparty/chromium/v8/src/codegen/register-arch.h b/src/3rdparty/chromium/v8/src/codegen/register-arch.h
index 21a7233016..82aa7c34a4 100644
--- a/src/3rdparty/chromium/v8/src/codegen/register-arch.h
+++ b/src/3rdparty/chromium/v8/src/codegen/register-arch.h
@@ -24,6 +24,8 @@
 #include "src/codegen/mips64/register-mips64.h"
 #elif V8_TARGET_ARCH_S390
 #include "src/codegen/s390/register-s390.h"
+#elif V8_TARGET_ARCH_SW64
+#include "src/codegen/sw64/register-sw64.h"
 #else
 #error Unknown architecture.
 #endif
diff --git a/src/3rdparty/chromium/v8/src/codegen/register-configuration.cc b/src/3rdparty/chromium/v8/src/codegen/register-configuration.cc
index 5752b46339..f7033a18fa 100644
--- a/src/3rdparty/chromium/v8/src/codegen/register-configuration.cc
+++ b/src/3rdparty/chromium/v8/src/codegen/register-configuration.cc
@@ -64,6 +64,8 @@ static int get_num_allocatable_double_registers() {
       kMaxAllocatableDoubleRegisterCount;
 #elif V8_TARGET_ARCH_S390
       kMaxAllocatableDoubleRegisterCount;
+#elif V8_TARGET_ARCH_SW64
+      kMaxAllocatableDoubleRegisterCount;
 #else
 #error Unsupported target architecture.
 #endif
diff --git a/src/3rdparty/chromium/v8/src/codegen/reloc-info.cc b/src/3rdparty/chromium/v8/src/codegen/reloc-info.cc
index d984b1e917..4486604077 100644
--- a/src/3rdparty/chromium/v8/src/codegen/reloc-info.cc
+++ b/src/3rdparty/chromium/v8/src/codegen/reloc-info.cc
@@ -330,7 +330,7 @@ bool RelocInfo::OffHeapTargetIsCodedSpecially() {
   return false;
 #elif defined(V8_TARGET_ARCH_IA32) || defined(V8_TARGET_ARCH_MIPS) || \
     defined(V8_TARGET_ARCH_MIPS64) || defined(V8_TARGET_ARCH_PPC) ||  \
-    defined(V8_TARGET_ARCH_PPC64) || defined(V8_TARGET_ARCH_S390)
+    defined(V8_TARGET_ARCH_PPC64) || defined(V8_TARGET_ARCH_S390) || defined(V8_TARGET_ARCH_SW64)
   return true;
 #endif
 }
diff --git a/src/3rdparty/chromium/v8/src/codegen/sw64/OWNERS b/src/3rdparty/chromium/v8/src/codegen/sw64/OWNERS
new file mode 100755
index 0000000000..42582e993a
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/codegen/sw64/OWNERS
@@ -0,0 +1,3 @@
+ivica.bogosavljevic@sw64.com
+Miran.Karic@sw64.com
+sreten.kovacevic@sw64.com
diff --git a/src/3rdparty/chromium/v8/src/codegen/sw64/assembler-sw64-inl.h b/src/3rdparty/chromium/v8/src/codegen/sw64/assembler-sw64-inl.h
new file mode 100755
index 0000000000..22811ec31e
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/codegen/sw64/assembler-sw64-inl.h
@@ -0,0 +1,329 @@
+
+// Copyright (c) 1994-2006 Sun Microsystems Inc.
+// All Rights Reserved.
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are
+// met:
+//
+// - Redistributions of source code must retain the above copyright notice,
+// this list of conditions and the following disclaimer.
+//
+// - Redistribution in binary form must reproduce the above copyright
+// notice, this list of conditions and the following disclaimer in the
+// documentation and/or other materials provided with the distribution.
+//
+// - Neither the name of Sun Microsystems or the names of contributors may
+// be used to endorse or promote products derived from this software without
+// specific prior written permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
+// IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
+// THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+// PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+// CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+// EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+// The original source code covered by the above license above has been
+// modified significantly by Google Inc.
+// Copyright 2012 the V8 project authors. All rights reserved.
+
+#ifndef V8_CODEGEN_SW64_ASSEMBLER_SW64_INL_H_
+#define V8_CODEGEN_SW64_ASSEMBLER_SW64_INL_H_
+
+#include "src/codegen/sw64/assembler-sw64.h"
+
+#include "src/codegen/assembler.h"
+#include "src/debug/debug.h"
+#include "src/objects/objects-inl.h"
+
+namespace v8 {
+namespace internal {
+
+bool CpuFeatures::SupportsOptimizer() { return IsSupported(FPU); }
+
+bool CpuFeatures::SupportsWasmSimd128() { return IsSupported(SW64_SIMD); }
+
+// -----------------------------------------------------------------------------
+// Operand and MemOperand.
+
+bool Operand::is_reg() const {
+  return rm_.is_valid();
+}
+
+int64_t Operand::immediate() const {
+  DCHECK(!is_reg());
+  DCHECK(!IsHeapObjectRequest());
+  return value_.immediate;
+}
+
+// -----------------------------------------------------------------------------
+// RelocInfo.
+
+void RelocInfo::apply(intptr_t delta) {
+  if (IsInternalReference(rmode_) || IsInternalReferenceEncoded(rmode_)) {
+    // Absolute code pointer inside code object moves with the code object.
+    Assembler::RelocateInternalReference(rmode_, pc_, delta);
+  }
+}
+
+
+Address RelocInfo::target_address() {
+  DCHECK(IsCodeTarget(rmode_) || IsRuntimeEntry(rmode_) || IsWasmCall(rmode_));
+  return Assembler::target_address_at(pc_, constant_pool_);
+}
+
+Address RelocInfo::target_address_address() {
+  DCHECK(HasTargetAddressAddress());
+  // Read the address of the word containing the target_address in an
+  // instruction stream.
+  // The only architecture-independent user of this function is the serializer.
+  // The serializer uses it to find out how many raw bytes of instruction to
+  // output before the next target.
+  // For an instruction like LUI/ORI where the target bits are mixed into the
+  // instruction bits, the size of the target will be zero, indicating that the
+  // serializer should not step forward in memory after a target is resolved
+  // and written. In this case the target_address_address function should
+  // return the end of the instructions to be patched, allowing the
+  // deserializer to deserialize the instructions as raw bytes and put them in
+  // place, ready to be patched with the target. After jump optimization,
+  // that is the address of the instruction that follows J/JAL/JR/JALR
+  // instruction.
+  return pc_ + Assembler::kInstructionsFor64BitConstant * kInstrSize;
+}
+
+Address RelocInfo::constant_pool_entry_address() { UNREACHABLE(); }
+
+int RelocInfo::target_address_size() { return Assembler::kSpecialTargetSize; }
+
+void Assembler::deserialization_set_special_target_at(
+    Address instruction_payload, Code code, Address target) {
+  set_target_address_at(instruction_payload,
+                        !code.is_null() ? code.constant_pool() : kNullAddress,
+                        target);
+}
+
+int Assembler::deserialization_special_target_size(
+    Address instruction_payload) {
+  return kSpecialTargetSize;
+}
+
+void Assembler::set_target_internal_reference_encoded_at(Address pc,
+                                                         Address target) {
+  // Encoded internal references are j/jal instructions.
+  Instr instr = Assembler::instr_at(pc + 0 * kInstrSize);
+
+  uint64_t imm28 = target & static_cast<uint64_t>(kImm28Mask);
+
+  instr &= ~kImm26Mask;
+  uint64_t imm26 = imm28 >> 2;
+  DCHECK(is_uint26(imm26));
+
+  instr_at_put(pc, instr | (imm26 & kImm26Mask));
+  // Currently used only by deserializer, and all code will be flushed
+  // after complete deserialization, no need to flush on each reference.
+}
+
+void Assembler::deserialization_set_target_internal_reference_at(
+    Address pc, Address target, RelocInfo::Mode mode) {
+  if (mode == RelocInfo::INTERNAL_REFERENCE_ENCODED) {
+    UNREACHABLE();  // DCHECK(IsJ(instr_at(pc)));
+    set_target_internal_reference_encoded_at(pc, target);
+  } else {
+    DCHECK(mode == RelocInfo::INTERNAL_REFERENCE);
+    Memory<Address>(pc) = target;
+  }
+}
+
+HeapObject RelocInfo::target_object() {
+  DCHECK(IsCodeTarget(rmode_) || IsFullEmbeddedObject(rmode_));
+  return HeapObject::cast(
+      Object(Assembler::target_address_at(pc_, constant_pool_)));
+}
+
+HeapObject RelocInfo::target_object_no_host(Isolate* isolate) {
+  return target_object();
+}
+
+Handle<HeapObject> RelocInfo::target_object_handle(Assembler* origin) {
+    DCHECK(IsCodeTarget(rmode_) || IsFullEmbeddedObject(rmode_));
+  return Handle<HeapObject>(reinterpret_cast<Address*>(
+      Assembler::target_address_at(pc_, constant_pool_)));
+}
+
+void RelocInfo::set_target_object(Heap* heap, HeapObject target,
+                                  WriteBarrierMode write_barrier_mode,
+                                  ICacheFlushMode icache_flush_mode) {
+  DCHECK(IsCodeTarget(rmode_) || IsFullEmbeddedObject(rmode_));
+    Assembler::set_target_address_at(pc_, constant_pool_, target.ptr(),
+                                   icache_flush_mode);
+  if (write_barrier_mode == UPDATE_WRITE_BARRIER && !host().is_null() &&
+      !FLAG_disable_write_barriers) {
+    WriteBarrierForCode(host(), this, target);
+  }
+}
+
+
+Address RelocInfo::target_external_reference() {
+  DCHECK(rmode_ == EXTERNAL_REFERENCE);
+  return Assembler::target_address_at(pc_, constant_pool_);
+}
+
+void RelocInfo::set_target_external_reference(
+    Address target, ICacheFlushMode icache_flush_mode) {
+  DCHECK(rmode_ == RelocInfo::EXTERNAL_REFERENCE);
+  Assembler::set_target_address_at(pc_, constant_pool_, target,
+                                   icache_flush_mode);
+}
+
+Address RelocInfo::target_internal_reference() {
+  if (rmode_ == INTERNAL_REFERENCE) {
+    return Memory<Address>(pc_);
+  } else {
+    // Encoded internal references are j/jal instructions.
+    DCHECK(rmode_ == INTERNAL_REFERENCE_ENCODED);
+    Instr instr = Assembler::instr_at(pc_ + 0 * kInstrSize);
+    instr &= kImm26Mask;
+    uint64_t imm28 = instr << 2;
+    uint64_t segment = pc_ & ~static_cast<uint64_t>(kImm28Mask);
+    return static_cast<Address>(segment | imm28);
+  }
+}
+
+
+Address RelocInfo::target_internal_reference_address() {
+  DCHECK(rmode_ == INTERNAL_REFERENCE || rmode_ == INTERNAL_REFERENCE_ENCODED);
+  return pc_;
+}
+
+Address RelocInfo::target_runtime_entry(Assembler* origin) {
+  DCHECK(IsRuntimeEntry(rmode_));
+  return target_address();
+}
+
+void RelocInfo::set_target_runtime_entry(Address target,
+                                         WriteBarrierMode write_barrier_mode,
+                                         ICacheFlushMode icache_flush_mode) {
+  DCHECK(IsRuntimeEntry(rmode_));
+  if (target_address() != target)
+    set_target_address(target, write_barrier_mode, icache_flush_mode);
+}
+
+Address RelocInfo::target_off_heap_target() {
+  DCHECK(IsOffHeapTarget(rmode_));
+  return Assembler::target_address_at(pc_, constant_pool_);
+}
+
+void RelocInfo::WipeOut() {
+  DCHECK(IsFullEmbeddedObject(rmode_) || IsCodeTarget(rmode_) ||
+         IsRuntimeEntry(rmode_) || IsExternalReference(rmode_) ||
+         IsInternalReference(rmode_) || IsInternalReferenceEncoded(rmode_) ||
+         IsOffHeapTarget(rmode_));
+  if (IsInternalReference(rmode_)) {
+    Memory<Address>(pc_) = kNullAddress;
+  } else if (IsInternalReferenceEncoded(rmode_)) {
+    Assembler::set_target_internal_reference_encoded_at(pc_, kNullAddress);
+  } else {
+    Assembler::set_target_address_at(pc_, constant_pool_, kNullAddress);
+  }
+}
+
+// -----------------------------------------------------------------------------
+// Assembler.
+
+
+void Assembler::CheckBuffer() {
+  if (buffer_space() <= kGap) {
+    GrowBuffer();
+  }
+}
+
+
+void Assembler::CheckForEmitInForbiddenSlot() {
+  if (!is_buffer_growth_blocked()) {
+    CheckBuffer();
+  }
+  if (IsPrevInstrCompactBranch()) {
+    // Nop instruction to precede a CTI in forbidden slot:
+    Instr nop = op_ldi | (zero_reg.code() << sRaShift) | (zero_reg.code() << sRbShift);
+    *reinterpret_cast<Instr*>(pc_) = nop;
+    pc_ += kInstrSize;
+
+    ClearCompactBranchState();
+  }
+}
+
+
+void Assembler::EmitHelper(Instr x, CompactBranchType is_compact_branch) {
+  if (IsPrevInstrCompactBranch()) {
+    if (Instruction::IsForbiddenAfterBranchInstr(x)) {
+      // Nop instruction to precede a CTI in forbidden slot:
+      Instr nop = op_ldi | (zero_reg.code() << sRaShift) | (zero_reg.code() << sRbShift);
+      *reinterpret_cast<Instr*>(pc_) = nop;
+      pc_ += kInstrSize;
+    }
+    ClearCompactBranchState();
+  }
+  *reinterpret_cast<Instr*>(pc_) = x;
+  pc_ += kInstrSize;
+  if (is_compact_branch == CompactBranchType::COMPACT_BRANCH) {
+    EmittedCompactBranchInstruction();
+  }
+  CheckTrampolinePoolQuick();
+}
+
+template <>
+inline void Assembler::EmitHelper(uint8_t x);
+
+template <typename T>
+void Assembler::EmitHelper(T x) {
+  *reinterpret_cast<T*>(pc_) = x;
+  pc_ += sizeof(x);
+  CheckTrampolinePoolQuick();
+}
+
+template <>
+void Assembler::EmitHelper(uint8_t x) {
+  *reinterpret_cast<uint8_t*>(pc_) = x;
+  pc_ += sizeof(x);
+  if (reinterpret_cast<intptr_t>(pc_) % kInstrSize == 0) {
+    CheckTrampolinePoolQuick();
+  }
+}
+
+#ifdef ZHJDEL
+void Assembler::emit(Instr x, CompactBranchType is_compact_branch) {
+    UNREACHABLE();  // This should never be reached on sw64.
+}
+
+
+void Assembler::emit(uint64_t data) {
+    UNREACHABLE();  // This should never be reached on sw64.
+}
+#endif
+
+#ifdef SW64
+void Assembler::emitSW(Instr x) {
+  if (!is_buffer_growth_blocked()) {
+    CheckBuffer();
+  }
+  EmitHelper(x);
+}
+
+void Assembler::emitSW(uint64_t data) {
+   CheckForEmitInForbiddenSlot();
+   EmitHelper(data);
+}
+#endif
+
+EnsureSpace::EnsureSpace(Assembler* assembler) { assembler->CheckBuffer(); }
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_SW64_ASSEMBLER_SW64_INL_H_
diff --git a/src/3rdparty/chromium/v8/src/codegen/sw64/assembler-sw64.cc b/src/3rdparty/chromium/v8/src/codegen/sw64/assembler-sw64.cc
new file mode 100755
index 0000000000..9f76096027
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/codegen/sw64/assembler-sw64.cc
@@ -0,0 +1,4379 @@
+// Copyright (c) 1994-2006 Sun Microsystems Inc.
+// All Rights Reserved.
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are
+// met:
+//
+// - Redistributions of source code must retain the above copyright notice,
+// this list of conditions and the following disclaimer.
+//
+// - Redistribution in binary form must reproduce the above copyright
+// notice, this list of conditions and the following disclaimer in the
+// documentation and/or other materials provided with the distribution.
+//
+// - Neither the name of Sun Microsystems or the names of contributors may
+// be used to endorse or promote products derived from this software without
+// specific prior written permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
+// IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
+// THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+// PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+// CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+// EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+// The original source code covered by the above license above has been
+// modified significantly by Google Inc.
+// Copyright 2012 the V8 project authors. All rights reserved.
+
+#include "src/codegen/sw64/assembler-sw64.h"
+
+#if V8_TARGET_ARCH_SW64
+
+#include "src/base/cpu.h"
+#include "src/codegen/sw64/assembler-sw64-inl.h"
+#include "src/codegen/safepoint-table.h"
+#include "src/codegen/string-constants.h"
+#include "src/deoptimizer/deoptimizer.h"
+#include "src/objects/heap-number-inl.h"
+
+namespace v8 {
+namespace internal {
+
+
+// Get the CPU features enabled by the build. For cross compilation the
+// preprocessor symbols CAN_USE_FPU_INSTRUCTIONS
+// can be defined to enable FPU instructions when building the
+// snapshot.
+static unsigned CpuFeaturesImpliedByCompiler() {
+  unsigned answer = 0;
+
+  // If the compiler is allowed to use FPU then we can use FPU too in our code
+  // generation even when generating snapshots.  This won't work for cross
+  // compilation.
+  answer |= 1u << FPU;
+
+  return answer;
+}
+
+
+void CpuFeatures::ProbeImpl(bool cross_compile) {
+  supported_ |= CpuFeaturesImpliedByCompiler();
+
+  // Only use statically determined features for cross compile (snapshot).
+  if (cross_compile) return;
+
+  // Probe for additional features at runtime.
+  base::CPU cpu;
+  if (cpu.has_fpu()) supported_ |= 1u << FPU;
+  if (cpu.has_msa()) supported_ |= 1u << SW64_SIMD;
+}
+
+
+void CpuFeatures::PrintTarget() { }
+void CpuFeatures::PrintFeatures() { }
+
+
+int ToNumber(Register reg) {
+  DCHECK(reg.is_valid());
+  const int kNumbers[] = {
+          // (SW64)
+    0,    // v0
+    1,    // t0
+    2,    // t1
+    3,    // t2
+    4,    // t3
+    5,    // t4
+    6,    // t5
+    7,    // t6
+    8,    // t7
+    9,    // s0
+    10,   // s1
+    11,   // s2
+    12,   // s3
+    13,   // s4
+    14,   // s5
+    15,   // fp
+    16,   // a0
+    17,   // a1
+    18,   // a2
+    19,   // a3
+    20,   // a4
+    21,   // a5
+    22,   // t8
+    23,   // t9
+    24,   // t10
+    25,   // t11
+    26,   // ra
+    27,   // t12
+    28,   // at
+    29,   // gp
+    30,   // sp
+    31    // zero_reg
+  };
+  return kNumbers[reg.code()];
+}
+
+
+Register ToRegister(int num) {
+  DCHECK(num >= 0 && num < kNumRegisters);
+  const Register kRegisters[] = {
+    v0,
+    t0, t1, t2, t3, t4, t5, t6, t7,
+    s0, s1, s2, s3, s4, s5, fp,
+    a0, a1, a2, a3, a4, a5,
+    t8, t9, t10, t11,
+    ra,
+    t12,
+    at,
+    gp,
+    sp,
+    zero_reg
+  };
+  return kRegisters[num];
+}
+
+
+// -----------------------------------------------------------------------------
+// Implementation of RelocInfo.
+
+const int RelocInfo::kApplyMask =
+    RelocInfo::ModeMask(RelocInfo::INTERNAL_REFERENCE) |
+    RelocInfo::ModeMask(RelocInfo::INTERNAL_REFERENCE_ENCODED);
+
+bool RelocInfo::IsCodedSpecially() {
+  // The deserializer needs to know whether a pointer is specially coded.  Being
+  // specially coded on SW64 means that it is a lui/ori instruction, and that is
+  // always the case inside code objects.
+  return true;
+}
+
+
+bool RelocInfo::IsInConstantPool() {
+  return false;
+}
+
+uint32_t RelocInfo::wasm_call_tag() const {
+  DCHECK(rmode_ == WASM_CALL || rmode_ == WASM_STUB_CALL);
+  return static_cast<uint32_t>(
+      Assembler::target_address_at(pc_, constant_pool_));
+}
+
+// -----------------------------------------------------------------------------
+// Implementation of Operand and MemOperand.
+// See assembler-sw64-inl.h for inlined constructors.
+
+Operand::Operand(Handle<HeapObject> handle)
+    : rm_(no_reg), rmode_(RelocInfo::FULL_EMBEDDED_OBJECT) {
+  value_.immediate = static_cast<intptr_t>(handle.address());
+}
+
+Operand Operand::EmbeddedNumber(double value) {
+  int32_t smi;
+  if (DoubleToSmiInteger(value, &smi)) return Operand(Smi::FromInt(smi));
+  Operand result(0, RelocInfo::FULL_EMBEDDED_OBJECT);
+  result.is_heap_object_request_ = true;
+  result.value_.heap_object_request = HeapObjectRequest(value);
+  return result;
+}
+
+Operand Operand::EmbeddedStringConstant(const StringConstantBase* str) {
+  Operand result(0, RelocInfo::FULL_EMBEDDED_OBJECT);
+  result.is_heap_object_request_ = true;
+  result.value_.heap_object_request = HeapObjectRequest(str);
+  return result;
+}
+
+MemOperand::MemOperand(Register rm, int32_t offset) : Operand(rm) {
+  offset_ = offset;
+}
+
+
+MemOperand::MemOperand(Register rm, int32_t unit, int32_t multiplier,
+                       OffsetAddend offset_addend)
+    : Operand(rm) {
+  offset_ = unit * multiplier + offset_addend;
+}
+
+void Assembler::AllocateAndInstallRequestedHeapObjects(Isolate* isolate) {
+  DCHECK_IMPLIES(isolate == nullptr, heap_object_requests_.empty());
+  for (auto& request : heap_object_requests_) {
+    Handle<HeapObject> object;
+    switch (request.kind()) {
+      case HeapObjectRequest::kHeapNumber:
+        object = isolate->factory()->NewHeapNumber<AllocationType::kOld>(
+            request.heap_number());
+        break;
+      case HeapObjectRequest::kStringConstant:
+        const StringConstantBase* str = request.string();
+        CHECK_NOT_NULL(str);
+        object = str->AllocateStringConstant(isolate);
+        break;
+    }
+    Address pc = reinterpret_cast<Address>(buffer_start_) + request.offset();
+    set_target_value_at(pc, reinterpret_cast<uint64_t>(object.location()));
+  }
+}
+
+Assembler::Assembler(const AssemblerOptions& options,
+                     std::unique_ptr<AssemblerBuffer> buffer)
+    : AssemblerBase(options, std::move(buffer)),
+      scratch_register_list_(at.bit()) {
+  if (CpuFeatures::IsSupported(SW64_SIMD)) {
+    EnableCpuFeature(SW64_SIMD);
+  }
+  reloc_info_writer.Reposition(buffer_start_ + buffer_->size(), pc_);
+
+  last_trampoline_pool_end_ = 0;
+  no_trampoline_pool_before_ = 0;
+  trampoline_pool_blocked_nesting_ = 0;
+  // We leave space (16 * kTrampolineSlotsSize)
+  // for BlockTrampolinePoolScope buffer.
+  next_buffer_check_ = FLAG_force_long_branches
+      ? kMaxInt : kMaxBranchOffset - kTrampolineSlotsSize * 16;
+  internal_trampoline_exception_ = false;
+  last_bound_pos_ = 0;
+
+  trampoline_emitted_ = FLAG_force_long_branches;
+  unbound_labels_count_ = 0;
+  block_buffer_growth_ = false;
+}
+
+void Assembler::GetCode(Isolate* isolate, CodeDesc* desc,
+                        SafepointTableBuilder* safepoint_table_builder,
+                        int handler_table_offset) {
+  EmitForbiddenSlotInstruction();
+
+  int code_comments_size = WriteCodeComments();
+
+  DCHECK(pc_ <= reloc_info_writer.pos());  // No overlap.
+
+  AllocateAndInstallRequestedHeapObjects(isolate);
+
+  // Set up code descriptor.
+  // TODO(jgruber): Reconsider how these offsets and sizes are maintained up to
+  // this point to make CodeDesc initialization less fiddly.
+
+  static constexpr int kConstantPoolSize = 0;
+  const int instruction_size = pc_offset();
+  const int code_comments_offset = instruction_size - code_comments_size;
+  const int constant_pool_offset = code_comments_offset - kConstantPoolSize;
+  const int handler_table_offset2 = (handler_table_offset == kNoHandlerTable)
+                                        ? constant_pool_offset
+                                        : handler_table_offset;
+  const int safepoint_table_offset =
+      (safepoint_table_builder == kNoSafepointTable)
+          ? handler_table_offset2
+          : safepoint_table_builder->GetCodeOffset();
+  const int reloc_info_offset =
+      static_cast<int>(reloc_info_writer.pos() - buffer_->start());
+  CodeDesc::Initialize(desc, this, safepoint_table_offset,
+                       handler_table_offset2, constant_pool_offset,
+                       code_comments_offset, reloc_info_offset);
+}
+
+void Assembler::Align(int m) {
+  DCHECK(m >= 4 && base::bits::IsPowerOfTwo(m));
+  EmitForbiddenSlotInstruction();
+  while ((pc_offset() & (m - 1)) != 0) {
+    nop();
+  }
+}
+
+
+void Assembler::CodeTargetAlign() {
+  // No advantage to aligning branch/call targets to more than
+  // single instruction, that I am aware of.
+  Align(4);
+}
+
+
+uint32_t Assembler::GetSwRa(Instr instr) {
+  return (instr & sRaFieldMask) >> sRaShift;
+}
+
+
+uint32_t Assembler::GetSwRb(Instr instr) {
+  return (instr & sRbFieldMask) >> sRbShift;
+}
+
+
+uint32_t Assembler::GetSwRc(Instr instr) {
+  return (instr & sRcFieldMask) >> sRcShift;
+}
+
+
+uint32_t Assembler::GetLabelConst(Instr instr) {
+  return instr & ~kImm16Mask;
+}
+
+
+#ifdef SW64  //20181123
+
+#define OP(x)           (((x) & 0x3F) << 26)
+#define OPR(oo,ff)      (OP(oo) | (((ff) & 0xFF) << 5))
+
+int32_t Assembler::GetSwOpcodeField(Instr instr) {
+  return instr & OP(-1);
+}
+
+
+int32_t Assembler::GetSwOpcodeAndFunctionField(Instr instr) {
+  return instr & OPR(-1, -1);
+}
+
+#undef OP
+#undef OPR
+
+uint32_t Assembler::GetSwImmediate8(Instr instr) {
+  return (instr & sImm8Mask) >> sImm8Shift;
+}
+
+
+uint32_t Assembler::GetSwImmediate16(Instr instr) {
+  return (instr & sImm16Mask) >> sImm16Shift;
+}
+#endif
+
+
+// Labels refer to positions in the (to be) generated code.
+// There are bound, linked, and unused labels.
+//
+// Bound labels refer to known positions in the already
+// generated code. pos() is the position the label refers to.
+//
+// Linked labels refer to unknown positions in the code
+// to be generated; pos() is the position of the last
+// instruction using the label.
+
+// The link chain is terminated by a value in the instruction of -1,
+// which is an otherwise illegal value (branch -1 is inf loop).
+// The instruction 16-bit offset field addresses 32-bit words, but in
+// code is conv to an 18-bit value addressing bytes, hence the -4 value.
+
+const int kEndOfChain = -4;
+// Determines the end of the Jump chain (a subset of the label link chain).
+const int kEndOfJumpChain = 0;
+
+
+bool Assembler::IsLdih(Instr instr) {
+  int32_t opcode = GetSwOpcodeField(instr);
+
+  return opcode == op_ldih;
+}
+
+bool Assembler::IsLdi(Instr instr) {
+  int32_t opcode = GetSwOpcodeField(instr);
+
+  return opcode == op_ldi;
+}
+
+
+bool Assembler::IsBranch(Instr instr) {
+  int32_t opcode = GetSwOpcodeField(instr);
+  return opcode == op_br   || opcode == op_bsr  ||    //; unconditional branch
+         opcode == op_beq  || opcode == op_bne  ||
+         opcode == op_blt  || opcode == op_ble  ||
+         opcode == op_bgt  || opcode == op_bge  ||
+         opcode == op_blbc || opcode == op_blbs ||
+         opcode == op_fbeq || opcode == op_fbne ||
+         opcode == op_fblt || opcode == op_fble ||
+         opcode == op_fbgt || opcode == op_fbge;
+}
+
+
+bool Assembler::IsEmittedConstant(Instr instr) {
+  uint32_t label_constant = GetLabelConst(instr);
+  return label_constant == 0;  // Emitted label const in reg-exp engine.
+}
+
+
+bool Assembler::IsBeq(Instr instr) {
+  return GetSwOpcodeField(instr) == op_beq;
+}
+
+
+bool Assembler::IsBne(Instr instr) {
+  return GetSwOpcodeField(instr) == op_bne;
+}
+
+
+bool Assembler::IsAddImmediate(Instr instr) {
+  int32_t opcode =  GetSwOpcodeAndFunctionField(instr);
+  return opcode == op_addw_l || opcode == op_addl_l;
+}
+
+
+bool Assembler::IsAndImmediate(Instr instr) {
+  return GetSwOpcodeAndFunctionField(instr) == op_and_l;
+}
+
+
+int Assembler::target_at(int pos, bool is_internal) {
+  if (is_internal) {
+    int64_t* p = reinterpret_cast<int64_t*>(buffer_start_ + pos);
+    int64_t address = *p;
+    if (address == kEndOfJumpChain) {
+      return kEndOfChain;
+    } else {
+      int64_t instr_address = reinterpret_cast<int64_t>(p);
+      DCHECK(instr_address - address < INT_MAX);
+      int delta = static_cast<int>(instr_address - address);
+      DCHECK(pos > delta);
+      return pos - delta;
+    }
+  }
+  Instr instr = instr_at(pos);
+  if ((instr & ~sImm21Mask) == 0) {
+    // Emitted label constant, not part of a branch.
+    if (instr == 0) {
+       return kEndOfChain;
+     } else {
+       int32_t imm23 =((instr & static_cast<int32_t>(sImm21Mask)) << 11) >> 9;
+       return (imm23 + pos);
+     }
+  }
+  // Check we have a branch or jump instruction.
+  DCHECK(IsBranch(instr) || IsLdi(instr));
+  // Do NOT change this to <<2. We rely on arithmetic shifts here, assuming
+  // the compiler uses arithmectic shifts for signed integers.
+  if (IsBranch(instr)) {
+    int32_t imm23 = ((instr & static_cast<int32_t>(sImm21Mask)) << 11) >> 9;
+
+    if (imm23 == kEndOfChain) {
+      // EndOfChain sentinel is returned directly, not relative to pc or pos.
+      return kEndOfChain;
+    } else {
+      return pos + kBranchPCOffset + imm23;
+    }
+  } else if (IsLdi(instr)) {
+    Instr instr0_ldi = instr_at(pos + 0 * kInstrSize);
+    Instr instr2_ldih = instr_at(pos + 2 * kInstrSize);
+    Instr instr3_ldi = instr_at(pos + 3 * kInstrSize);
+    DCHECK(IsLdi(instr0_ldi));
+    DCHECK(IsLdih(instr2_ldih));
+    DCHECK(IsLdi(instr3_ldi));
+
+    // TODO(plind) create named constants for shift values.
+    int64_t imm = static_cast<int64_t>(instr0_ldi << 16) << 16;
+    imm += static_cast<int64_t>(instr2_ldih << 16);
+    imm += static_cast<int64_t>(instr3_ldi << 16) >> 16;
+
+    if (imm == kEndOfJumpChain) {
+      // EndOfChain sentinel is returned directly, not relative to pc or pos.
+      return kEndOfChain;
+    } else {
+      uint64_t instr_address = reinterpret_cast<int64_t>(buffer_start_ + pos);
+      int64_t delta = instr_address - imm;
+      DCHECK(pos > delta);
+      return (int)(pos - delta);
+    }
+  } else {
+    UNIMPLEMENTED_SW64();
+    return -1;
+  }
+}
+
+
+void Assembler::target_at_put(int pos, int target_pos, bool is_internal) {
+  if (is_internal) {
+    uint64_t imm = reinterpret_cast<uint64_t>(buffer_start_) + target_pos;
+    *reinterpret_cast<uint64_t*>(buffer_start_ + pos) = imm;
+    return;
+  }
+  Instr instr = instr_at(pos);
+  if ((instr & ~sImm21Mask) == 0) {
+    DCHECK(target_pos == kEndOfChain || target_pos >= 0);
+    // Emitted label constant, not part of a branch.
+    // Make label relative to Code pointer of generated Code object.
+    instr_at_put(pos, target_pos + (Code::kHeaderSize - kHeapObjectTag));
+    return;
+  }
+
+  DCHECK(IsBranch(instr) || IsLdi(instr));
+  if (IsBranch(instr)) {
+    int32_t imm23 = target_pos - (pos + kBranchPCOffset);
+    DCHECK((imm23 & 3) == 0);
+
+    int32_t imm21 = imm23 >> 2;
+    instr &= ~sImm21Mask;
+    DCHECK(is_int21(imm21));
+
+    instr_at_put(pos, instr | (imm21 & sImm21Mask));
+  } else if (IsLdi(instr)) {
+    Instr instr0_ldi = instr_at(pos + 0 * kInstrSize);
+    Instr instr2_ldih = instr_at(pos + 2 * kInstrSize);
+    Instr instr3_ldi = instr_at(pos + 3 * kInstrSize);
+    DCHECK(IsLdi(instr0_ldi));
+    DCHECK(IsLdih(instr2_ldih));
+    DCHECK(IsLdi(instr3_ldi));
+
+    int64_t imm = reinterpret_cast<int64_t>(buffer_start_) + target_pos;
+    DCHECK((imm & 3) == 0);
+
+    instr0_ldi &= ~kImm16Mask;
+    instr2_ldih &= ~kImm16Mask;
+    instr3_ldi &= ~kImm16Mask;
+
+    int32_t lsb32 = (int32_t) (imm);
+    int32_t msb32 = (int32_t) ((imm - lsb32) >> 32);
+    instr_at_put(pos + 0 * kInstrSize,
+                 instr0_ldi | ((int16_t)(msb32 & 0xffff) & 0xffff));
+    instr_at_put(pos + 2 * kInstrSize,
+                 instr2_ldih | (((lsb32-(int16_t)lsb32)>>16) & 0xffff));
+    instr_at_put(pos + 3 * kInstrSize,
+                 instr3_ldi | ((int16_t)(lsb32 & 0xffff) & 0xffff));
+  } else {
+    UNIMPLEMENTED_SW64();
+  }
+}
+
+void Assembler::print(const Label* L) {
+  if (L->is_unused()) {
+    PrintF("unused label\n");
+  } else if (L->is_bound()) {
+    PrintF("bound label to %d\n", L->pos());
+  } else if (L->is_linked()) {
+    Label l;
+    l.link_to(L->pos());
+    PrintF("unbound label");
+    while (l.is_linked()) {
+      PrintF("@ %d ", l.pos());
+      Instr instr = instr_at(l.pos());
+      if ((instr & ~kImm16Mask) == 0) {
+        PrintF("value\n");
+      } else {
+        PrintF("%d\n", instr);
+      }
+      next(&l, is_internal_reference(&l));
+    }
+  } else {
+    PrintF("label in inconsistent state (pos = %d)\n", L->pos_);
+  }
+}
+
+
+void Assembler::bind_to(Label* L, int pos) {
+  DCHECK(0 <= pos && pos <= pc_offset());  // Must have valid binding position.
+  int trampoline_pos = kInvalidSlotPos;
+  bool is_internal = false;
+  if (L->is_linked() && !trampoline_emitted_) {
+    unbound_labels_count_--;
+    if (!is_internal_reference(L)) {
+      next_buffer_check_ += kTrampolineSlotsSize;
+    }
+  }
+
+  while (L->is_linked()) {
+    int fixup_pos = L->pos();
+    int dist = pos - fixup_pos;
+    is_internal = is_internal_reference(L);
+    next(L, is_internal);  // Call next before overwriting link with target at
+                           // fixup_pos.
+    Instr instr = instr_at(fixup_pos);
+    if (is_internal) {
+      target_at_put(fixup_pos, pos, is_internal);
+    } else {
+      if (IsBranch(instr)) {
+        int branch_offset = BranchOffset(instr);
+        if (dist > branch_offset) {
+          if (trampoline_pos == kInvalidSlotPos) {
+            trampoline_pos = get_trampoline_entry(fixup_pos);
+            CHECK_NE(trampoline_pos, kInvalidSlotPos);
+          }
+          CHECK((trampoline_pos - fixup_pos) <= branch_offset);
+          target_at_put(fixup_pos, trampoline_pos, false);
+          fixup_pos = trampoline_pos;
+        }
+        target_at_put(fixup_pos, pos, false);
+      } else {
+        DCHECK(IsLdi(instr) || IsEmittedConstant(instr));
+        target_at_put(fixup_pos, pos, false);
+      }
+    }
+  }
+  L->bind_to(pos);
+
+  // Keep track of the last bound label so we don't eliminate any instructions
+  // before a bound label.
+  if (pos > last_bound_pos_)
+    last_bound_pos_ = pos;
+}
+
+
+void Assembler::bind(Label* L) {
+  DCHECK(!L->is_bound());  // Label can only be bound once.
+  bind_to(L, pc_offset());
+}
+
+
+void Assembler::next(Label* L, bool is_internal) {
+  DCHECK(L->is_linked());
+  int link = target_at(L->pos(), is_internal);
+  if (link == kEndOfChain) {
+    L->Unuse();
+  } else {
+    DCHECK_GE(link, 0);
+    L->link_to(link);
+  }
+}
+
+
+bool Assembler::is_near(Label* L) {
+  DCHECK(L->is_bound());
+  return pc_offset() - L->pos() < kMaxBranchOffset - 4 * kInstrSize;
+}
+
+
+bool Assembler::is_near(Label* L, OffsetSize bits) {
+  if (L == nullptr || !L->is_bound()) return true;
+  return ((pc_offset() - L->pos()) <
+          (1 << (bits + 2 - 1)) - 1 - 5 * kInstrSize);
+}
+
+
+bool Assembler::is_near_branch(Label* L) {
+  DCHECK(L->is_bound());
+  return kArchVariant == kSw64r3 ? is_near_r3(L) : is_near_pre_r3(L);
+}
+
+
+int Assembler::BranchOffset(Instr instr) {
+  int bits = OffsetSize::kOffset21;
+
+  return (1 << (bits + 2 - 1)) - 1;
+}
+
+
+// We have to use a temporary register for things that can be relocated even
+// if they can be encoded in the SW64's 16 bits of immediate-offset instruction
+// space.  There is no guarantee that the relocated location can be similarly
+// encoded.
+bool Assembler::MustUseReg(RelocInfo::Mode rmode) {
+  return !RelocInfo::IsNone(rmode);
+}
+
+
+// Returns the next free trampoline entry.
+int32_t Assembler::get_trampoline_entry(int32_t pos) {
+  int32_t trampoline_entry = kInvalidSlotPos;
+  if (!internal_trampoline_exception_) {
+    if (trampoline_.start() > pos) {
+     trampoline_entry = trampoline_.take_slot();
+    }
+
+    if (kInvalidSlotPos == trampoline_entry) {
+      internal_trampoline_exception_ = true;
+    }
+  }
+  return trampoline_entry;
+}
+
+
+uint64_t Assembler::jump_address(Label* L) {
+  int64_t target_pos;
+  if (L->is_bound()) {
+    target_pos = L->pos();
+  } else {
+    if (L->is_linked()) {
+      target_pos = L->pos();  // L's link.
+      L->link_to(pc_offset());
+    } else {
+      L->link_to(pc_offset());
+      return kEndOfJumpChain;
+    }
+  }
+  uint64_t imm = reinterpret_cast<uint64_t>(buffer_start_) + target_pos;
+  DCHECK_EQ(imm & 3, 0);
+
+  return imm;
+}
+
+uint64_t Assembler::jump_offset(Label* L) {
+  int64_t target_pos;
+  int32_t pad = IsPrevInstrCompactBranch() ? kInstrSize : 0;
+
+  if (L->is_bound()) {
+    target_pos = L->pos();
+  } else {
+    if (L->is_linked()) {
+      target_pos = L->pos();  // L's link.
+      L->link_to(pc_offset() + pad);
+    } else {
+      L->link_to(pc_offset() + pad);
+      return kEndOfJumpChain;
+    }
+  }
+  int64_t imm = target_pos - (pc_offset() + pad);
+  DCHECK_EQ(imm & 3, 0);
+
+  return static_cast<uint64_t>(imm);
+}
+
+uint64_t Assembler::branch_long_offset(Label* L) {
+  int64_t target_pos;
+
+  if (L->is_bound()) {
+    target_pos = L->pos();
+  } else {
+    if (L->is_linked()) {
+      target_pos = L->pos();  // L's link.
+      L->link_to(pc_offset());
+    } else {
+      L->link_to(pc_offset());
+      return kEndOfJumpChain;
+    }
+  }
+  int64_t offset = target_pos - (pc_offset() + kInstrSize);
+  DCHECK_EQ(offset & 3, 0);
+
+  return static_cast<uint64_t>(offset);
+}
+
+int32_t Assembler::branch_offset_helper(Label* L, OffsetSize bits) {
+  int32_t target_pos;
+  int32_t pad = IsPrevInstrCompactBranch() ? kInstrSize : 0;
+
+  if (L->is_bound()) {
+    target_pos = L->pos();
+  } else {
+    if (L->is_linked()) {
+      target_pos = L->pos();
+      L->link_to(pc_offset() + pad);
+    } else {
+      L->link_to(pc_offset() + pad);
+      if (!trampoline_emitted_) {
+        unbound_labels_count_++;
+        next_buffer_check_ -= kTrampolineSlotsSize;
+      }
+      return kEndOfChain;
+    }
+  }
+
+  int32_t offset = target_pos - (pc_offset() + kBranchPCOffset + pad);
+  DCHECK(is_intn(offset, bits + 2));
+  DCHECK_EQ(offset & 3, 0);
+
+  return offset;
+}
+
+
+void Assembler::label_at_put(Label* L, int at_offset) {
+  int target_pos;
+  if (L->is_bound()) {
+    target_pos = L->pos();
+    instr_at_put(at_offset, target_pos + (Code::kHeaderSize - kHeapObjectTag));
+  } else {
+    if (L->is_linked()) {
+      target_pos = L->pos();  // L's link.
+      int32_t imm18 = target_pos - at_offset;
+      DCHECK_EQ(imm18 & 3, 0);
+      int32_t imm16 = imm18 >> 2;
+      DCHECK(is_int16(imm16));
+      instr_at_put(at_offset, (imm16 & kImm16Mask));
+    } else {
+      target_pos = kEndOfChain;
+      instr_at_put(at_offset, 0);
+      if (!trampoline_emitted_) {
+        unbound_labels_count_++;
+        next_buffer_check_ -= kTrampolineSlotsSize;
+      }
+    }
+    L->link_to(at_offset);
+  }
+}
+
+
+//------- Branch and jump instructions --------
+
+void Assembler::br(int offset) {
+  br(zero_reg, offset);
+}
+
+
+void Assembler::bsr(int offset) {
+  bsr(ra, offset);
+}
+
+
+// ------------Memory-instructions-------------
+
+void Assembler::AdjustBaseAndOffset(MemOperand* src,
+                                    OffsetAccessType access_type,
+                                    int second_access_add_to_offset) {
+  // This method is used to adjust the base register and offset pair
+  // for a load/store when the offset doesn't fit into int16_t.
+  // It is assumed that 'base + offset' is sufficiently aligned for memory
+  // operands that are machine word in size or smaller. For doubleword-sized
+  // operands it's assumed that 'base' is a multiple of 8, while 'offset'
+  // may be a multiple of 4 (e.g. 4-byte-aligned long and double arguments
+  // and spilled variables on the stack accessed relative to the stack
+  // pointer register).
+  // We preserve the "alignment" of 'offset' by adjusting it by a multiple of 8.
+
+  bool doubleword_aligned = (src->offset() & (kDoubleSize - 1)) == 0;
+  bool two_accesses = static_cast<bool>(access_type) || !doubleword_aligned;
+  DCHECK_LE(second_access_add_to_offset, 7);  // Must be <= 7.
+
+  // is_int16 must be passed a signed value, hence the static cast below.
+  if (is_int16(src->offset()) &&
+      (!two_accesses || is_int16(static_cast<int32_t>(
+                            src->offset() + second_access_add_to_offset)))) {
+    // Nothing to do: 'offset' (and, if needed, 'offset + 4', or other specified
+    // value) fits into int16_t.
+    return;
+  }
+
+  DCHECK(src->rm() !=
+         at);  // Must not overwrite the register 'base' while loading 'offset'.
+
+#ifdef DEBUG
+  // Remember the "(mis)alignment" of 'offset', it will be checked at the end.
+  uint32_t misalignment = src->offset() & (kDoubleSize - 1);
+#endif
+
+  // Do not load the whole 32-bit 'offset' if it can be represented as
+  // a sum of two 16-bit signed offsets. This can save an instruction or two.
+  // To simplify matters, only do this for a symmetric range of offsets from
+  // about -64KB to about +64KB, allowing further addition of 4 when accessing
+  // 64-bit variables with two 32-bit accesses.
+  constexpr int32_t kMinOffsetForSimpleAdjustment =
+      0x7FF8;  // Max int16_t that's a multiple of 8.
+  constexpr int32_t kMaxOffsetForSimpleAdjustment =
+      2 * kMinOffsetForSimpleAdjustment;
+
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  if (0 <= src->offset() && src->offset() <= kMaxOffsetForSimpleAdjustment) {
+    ldi(scratch, kMinOffsetForSimpleAdjustment, src->rm());
+    src->offset_ -= kMinOffsetForSimpleAdjustment;
+  } else if (-kMaxOffsetForSimpleAdjustment <= src->offset() &&
+             src->offset() < 0) {
+    ldi(scratch, -kMinOffsetForSimpleAdjustment, src->rm());
+    src->offset_ += kMinOffsetForSimpleAdjustment;
+  } else {
+    // Do not load the whole 32-bit 'offset' if it can be represented as
+    // a sum of three 16-bit signed offsets. This can save an instruction.
+    // To simplify matters, only do this for a symmetric range of offsets from
+    // about -96KB to about +96KB, allowing further addition of 4 when accessing
+    // 64-bit variables with two 32-bit accesses.
+    constexpr int32_t kMinOffsetForMediumAdjustment =
+        2 * kMinOffsetForSimpleAdjustment;
+    constexpr int32_t kMaxOffsetForMediumAdjustment =
+        3 * kMinOffsetForSimpleAdjustment;
+    if (0 <= src->offset() && src->offset() <= kMaxOffsetForMediumAdjustment) {
+      ldi(scratch, kMinOffsetForMediumAdjustment / 2, src->rm());
+      ldi(scratch, kMinOffsetForMediumAdjustment / 2, scratch);
+      src->offset_ -= kMinOffsetForMediumAdjustment;
+    } else if (-kMaxOffsetForMediumAdjustment <= src->offset() &&
+               src->offset() < 0) {
+      ldi(scratch, -kMinOffsetForMediumAdjustment / 2, src->rm());
+      ldi(scratch, -kMinOffsetForMediumAdjustment / 2, scratch);
+      src->offset_ += kMinOffsetForMediumAdjustment;
+    } else {
+      // Now that all shorter options have been exhausted, load the full 32-bit
+      // offset.
+      int32_t loaded_offset = RoundDown(src->offset(), kDoubleSize);
+      int16_t lo_offset = static_cast<int16_t>(loaded_offset);
+      int16_t hi_offset = (loaded_offset-(int16_t)loaded_offset) >> 16; 
+      if ( ((int32_t)hi_offset == -32768) && ((int32_t)lo_offset < 0) ) {
+        // range from 0x7FFF8000 to 0x7FFFFFFF
+        ldih(scratch, 0x4000, zero_reg);
+        ldih(scratch, 0x4000, scratch);
+        if (lo_offset != 0 )
+          ldi(scratch, lo_offset, scratch);
+      } else {
+        ldih(scratch, hi_offset, zero_reg);
+        if (lo_offset != 0 )
+          ldi(scratch, lo_offset, scratch);
+      }
+      addl(scratch, src->rm(),scratch);
+      src->offset_ -= loaded_offset;
+    }
+  }
+  src->rm_ = scratch;
+
+  DCHECK(is_int16(src->offset()));
+  if (two_accesses) {
+    DCHECK(is_int16(
+        static_cast<int32_t>(src->offset() + second_access_add_to_offset)));
+  }
+  DCHECK(misalignment == (src->offset() & (kDoubleSize - 1)));
+}
+
+
+void Assembler::fmovd(FPURegister fs, FPURegister fd) {
+  fcpys(fs, fs, fd);
+}
+
+
+void Assembler::fmovs(FPURegister fs, FPURegister fd) {
+  fcpys(fs, fs, fd);
+}
+
+
+void Assembler::fnegs(FPURegister fs, FPURegister fd) {
+  fcpysn(fs, fs, fd);
+}
+
+
+void Assembler::fnegd(FPURegister fs, FPURegister fd) {
+  fcpysn(fs, fs, fd);
+}
+
+// Conversions.
+void Assembler::fcvtsw(FPURegister fs, FPURegister fd) {
+  DCHECK(fs != kScratchDoubleReg1 && fd != kScratchDoubleReg1);
+  DCHECK(fs != kScratchDoubleReg2 && fd != kScratchDoubleReg2);
+  fcvtsd(fs, kScratchDoubleReg2);
+  fcvtdl(kScratchDoubleReg2, kScratchDoubleReg1);
+  fcvtlw(kScratchDoubleReg1, fd);
+}
+
+
+void Assembler::fcvtdw(FPURegister fs, FPURegister fd) {
+  DCHECK(fs != kScratchDoubleReg1 && fd != kScratchDoubleReg1);
+  fcvtdl(fs, kScratchDoubleReg1);
+  fcvtlw(kScratchDoubleReg1, fd);
+}
+
+
+void Assembler::ftruncsw(FPURegister fs, FPURegister fd) {
+  DCHECK(fs != kScratchDoubleReg1 && fd != kScratchDoubleReg1);
+  DCHECK(fs != kScratchDoubleReg2 && fd != kScratchDoubleReg2);
+  fcvtsd(fs, kScratchDoubleReg1);
+  fcvtdl_z(kScratchDoubleReg1, kScratchDoubleReg2);
+  fcvtlw(kScratchDoubleReg2, fd);
+}
+
+
+void Assembler::ftruncdw(FPURegister fs, FPURegister fd) {
+  DCHECK(fs != kScratchDoubleReg1 && fd != kScratchDoubleReg1);
+  fcvtdl_z(fs, kScratchDoubleReg1);
+  fcvtlw(kScratchDoubleReg1, fd);
+}
+
+
+void Assembler::froundsw(FPURegister fs, FPURegister fd) {
+  DCHECK(fs != kScratchDoubleReg1 && fd != kScratchDoubleReg1);
+  DCHECK(fs != kScratchDoubleReg2 && fd != kScratchDoubleReg2);
+  fcvtsd(fs, kScratchDoubleReg1);
+  fcvtdl_g(kScratchDoubleReg1, kScratchDoubleReg2);
+  fcvtlw(kScratchDoubleReg2, fd);
+}
+
+
+void Assembler::frounddw(FPURegister fs, FPURegister fd) {
+  DCHECK(fs != kScratchDoubleReg1 && fd != kScratchDoubleReg1);
+  fcvtdl_g(fs, kScratchDoubleReg1);
+  fcvtlw(kScratchDoubleReg1, fd);
+}
+
+
+void Assembler::ffloorsw(FPURegister fs, FPURegister fd) {
+  DCHECK(fs != kScratchDoubleReg1 && fd != kScratchDoubleReg1);
+  DCHECK(fs != kScratchDoubleReg2 && fd != kScratchDoubleReg2);
+  fcvtsd(fs, kScratchDoubleReg2);
+  fcvtdl_n(kScratchDoubleReg2, kScratchDoubleReg1);
+  fcvtlw(kScratchDoubleReg1, fd);
+}
+
+
+void Assembler::ffloordw(FPURegister fs, FPURegister fd) {
+  DCHECK(fs != kScratchDoubleReg1 && fd != kScratchDoubleReg1);
+  fcvtdl_n(fs, kScratchDoubleReg1);
+  fcvtlw(kScratchDoubleReg1, fd);
+}
+
+
+void Assembler::fceilsw(FPURegister fs, FPURegister fd) {
+  DCHECK(fs != kScratchDoubleReg1 && fd != kScratchDoubleReg1);
+  DCHECK(fs != kScratchDoubleReg2 && fd != kScratchDoubleReg2);
+  fcvtsd(fs, kScratchDoubleReg2);
+  fcvtdl_p(kScratchDoubleReg2, kScratchDoubleReg1);
+  fcvtlw(kScratchDoubleReg1, fd);
+}
+
+
+void Assembler::fceildw(FPURegister fs, FPURegister fd) {
+  DCHECK(fs != kScratchDoubleReg1 && fd != kScratchDoubleReg1);
+  fcvtdl_p(fs, kScratchDoubleReg1);
+  fcvtlw(kScratchDoubleReg1, fd);
+}
+
+
+void Assembler::fcvtsl(FPURegister fs, FPURegister fd) {
+  DCHECK(fs != kScratchDoubleReg1 && fd != kScratchDoubleReg1);
+  fcvtsd(fs, kScratchDoubleReg1);
+  fcvtdl(kScratchDoubleReg1, fd);
+}
+
+
+void Assembler::ftruncsl(FPURegister fs, FPURegister fd) {
+  DCHECK(fs != kScratchDoubleReg1 && fd != kScratchDoubleReg1);
+  fcvtsd(fs, kScratchDoubleReg1);
+  fcvtdl_z(kScratchDoubleReg1, fd);
+}
+
+
+void Assembler::ftruncdl(FPURegister fs, FPURegister fd) {
+  DCHECK(fs != kScratchDoubleReg1 && fd != kScratchDoubleReg1);
+  if (fs == fd) {
+    fmovd(fs, kScratchDoubleReg1);
+    fcvtdl_z(kScratchDoubleReg1, fd);
+  }else{
+    fcvtdl_z(fs, fd);
+  }
+}
+
+
+void Assembler::froundsl(FPURegister fs, FPURegister fd) {
+  DCHECK(fs != kScratchDoubleReg1 && fd != kScratchDoubleReg1);
+  fcvtsd(fs, kScratchDoubleReg1);
+  fcvtdl_g(kScratchDoubleReg1, fd);
+}
+
+
+void Assembler::frounddl(FPURegister fs, FPURegister fd) {
+  DCHECK(fs != kScratchDoubleReg1 && fd != kScratchDoubleReg1);
+  if (fs == fd) {
+    fmovd(fs, kScratchDoubleReg1);
+    fcvtdl_g(kScratchDoubleReg1, fd);
+  } else {
+    fcvtdl_g(fs, fd);
+  }
+}
+
+
+void Assembler::ffloorsl(FPURegister fs, FPURegister fd) {
+  DCHECK(fs != kScratchDoubleReg1 && fd != kScratchDoubleReg1);
+  fcvtsd(fs, kScratchDoubleReg1);
+  fcvtdl_n(kScratchDoubleReg1, fd);
+}
+
+
+void Assembler::ffloordl(FPURegister fs, FPURegister fd) {
+  DCHECK(fs != kScratchDoubleReg1 && fd != kScratchDoubleReg1);
+  if (fs == fd) {
+    fmovd(fs, kScratchDoubleReg1);
+    fcvtdl_n(kScratchDoubleReg1, fd);
+  } else {
+    fcvtdl_n(fs, fd);
+  }
+}
+
+
+void Assembler::fceilsl(FPURegister fs, FPURegister fd) {
+  DCHECK(fs != kScratchDoubleReg1 && fd != kScratchDoubleReg1);
+  fcvtsd(fs, kScratchDoubleReg1);
+  fcvtdl_p(kScratchDoubleReg1, fd);
+}
+
+
+void Assembler::fceildl(FPURegister fs, FPURegister fd) {
+  DCHECK(fs != kScratchDoubleReg1 && fd != kScratchDoubleReg1);
+  if (fs == fd) {
+    fmovd(fs, kScratchDoubleReg1);
+    fcvtdl_p(kScratchDoubleReg1, fd);
+  } else {
+    fcvtdl_p(fs, fd);
+  }
+}
+
+
+void Assembler::fcvtws(FPURegister fs, FPURegister fd) {
+  DCHECK(fs != kScratchDoubleReg1 && fd != kScratchDoubleReg1);
+  fcvtwl(fs, kScratchDoubleReg1);
+  fcvtls(kScratchDoubleReg1, fd);
+}
+
+
+void Assembler::fcvtls_(FPURegister fs, FPURegister fd) {
+  DCHECK(fs != kScratchDoubleReg1 && fd != kScratchDoubleReg1);
+  if (fs == fd){
+    fmovd(fs, kScratchDoubleReg1);
+    fcvtls(kScratchDoubleReg1, fd);
+  }else{
+    fcvtls(fs, fd);
+  }
+}
+
+
+void Assembler::fcvtds_(FPURegister fs, FPURegister fd) {
+  DCHECK(fs != kScratchDoubleReg1 && fd != kScratchDoubleReg1);
+  if (fs == fd){
+    fmovd(fs, kScratchDoubleReg1);
+    fcvtds(kScratchDoubleReg1, fd);
+  }else{
+    fcvtds(fs, fd);
+  }
+}
+
+
+void Assembler::fcvtwd(FPURegister fs, FPURegister fd) {
+  DCHECK(fs != kScratchDoubleReg1 && fd != kScratchDoubleReg1);
+  fcvtwl(fs, kScratchDoubleReg1);
+  fcvtld(kScratchDoubleReg1, fd);
+}
+
+
+void Assembler::fcvtld_(FPURegister fs, FPURegister fd) {
+  DCHECK(fs != kScratchDoubleReg1 && fd != kScratchDoubleReg1);
+  if (fs == fd){
+    fmovd(fs, kScratchDoubleReg1);
+    fcvtld(kScratchDoubleReg1, fd);
+  }else{
+    fcvtld(fs, fd);
+  }
+}
+
+
+void Assembler::fcvtsd_(FPURegister fs, FPURegister fd) {
+  DCHECK(fs != kScratchDoubleReg1 && fd != kScratchDoubleReg1);
+  if (fs == fd){
+    fmovs(fs, kScratchDoubleReg1);
+    fcvtsd(kScratchDoubleReg1, fd);
+  }else{
+    fcvtsd(fs, fd);
+  }
+}
+
+
+// Conditions for >= SW64r3.
+void Assembler::cmp(FPUCondition cond, SecondaryField fmt,
+    FPURegister fd, FPURegister fs, FPURegister ft) {
+#ifdef SW64
+  //TODO: SecondaryField is useless!
+  DCHECK_EQ(fd, kDoubleCompareReg);
+  switch(cond) {
+    case EQ:
+      fcmpeq(fs, ft, fd);
+      break;
+    case OLT:
+      fcmplt(fs, ft, fd);
+      break;
+    case OLE:
+      fcmple(fs, ft, fd);
+      break;
+    case UN:
+      fcmpun(fs, ft, fd);
+      break;
+    default:
+      UNREACHABLE();
+  };
+#endif
+}
+
+
+#ifdef SW64  //20180904 define SW-character assembler
+
+void Assembler::GenInstrB_SW(Opcode_ops_bra opcode,
+                             Register Ra,
+                             int32_t disp) {
+  DCHECK(Ra.is_valid() && is_int21(disp));
+  Instr instr = opcode | (Ra.code() << sRaShift) | (disp & sImm21Mask);
+  emitSW(instr);
+}
+
+
+void Assembler::GenInstrFB_SW(Opcode_ops_bra opcode,
+                             FloatRegister fa,
+                             int32_t disp) {
+  DCHECK(fa.is_valid() && is_int21(disp));
+  Instr instr = opcode | (fa.code() << sRaShift) | (disp & sImm21Mask);
+  emitSW(instr);
+}
+
+
+void Assembler::GenInstrM_SW(Opcode_ops_mem opcode,
+                             Register Ra,
+                             int16_t disp,
+                             Register Rb) {
+  DCHECK(Ra.is_valid() && Rb.is_valid() && is_int16(disp));
+  Instr instr = opcode | (Ra.code() << sRaShift) | (Rb.code() << sRbShift) |
+                         (disp & sImm16Mask);
+  emitSW(instr);
+}
+
+
+void Assembler::GenInstrFM_SW(Opcode_ops_mem opcode,
+                              FloatRegister fa,
+                              int16_t disp,
+                              Register Rb) {
+  DCHECK(fa.is_valid() && Rb.is_valid() && is_int16(disp));
+  Instr instr = opcode | (fa.code() << sRaShift) | (Rb.code() << sRbShift) |
+                         (disp & sImm16Mask);
+  emitSW(instr);
+}
+
+
+void Assembler::GenInstrMWithFun_SW(Opcode_ops_atmem opcode,
+                                    Register Ra,
+                                    int16_t disp,
+                                    Register Rb) {
+  DCHECK(Ra.is_valid() && Rb.is_valid() && is_int12(disp));
+  Instr instr = opcode | (Ra.code() << sRaShift) | (Rb.code() << sRbShift) |
+                         (disp & sImm12Mask);
+  emitSW(instr);
+}
+
+
+void Assembler::GenInstrR_SW(Opcode_ops_opr opcode,
+                             Register Ra,
+                             Register Rb,
+                             Register Rc) {
+  DCHECK(Ra.is_valid() && Rb.is_valid() && Rc.is_valid());
+  Instr instr = opcode | (Ra.code() << sRaShift) | (Rb.code() << sRbShift) |
+                         (Rc.code() << sRcShift);
+  emitSW(instr);
+}
+
+
+void Assembler::GenInstrI_SW(Opcode_ops_oprl opcode,
+                             Register Ra,
+                             int16_t imm,
+                             Register Rc) {
+  DCHECK(Ra.is_valid() && is_uint8(imm) && Rc.is_valid());
+  Instr instr = opcode | (Ra.code() << sRaShift) | ( (imm << sImm8Shift) & sImm8Mask) |
+                         (Rc.code() << sRcShift);
+  emitSW(instr);
+}
+
+
+// Float-point ALU instructions.
+void Assembler::GenInstrFR_SW(Opcode_ops_fp opcode,
+                              FloatRegister fa,
+                              FloatRegister fb,
+                              FloatRegister fc) {
+  DCHECK(fa.is_valid() &&  fb.is_valid() && fc.is_valid());
+  Instr instr = opcode | (fa.code() << sRaShift) | (fb.code() << sRbShift) |
+                         (fc.code() << sRcShift);
+  emitSW(instr);
+}
+
+void Assembler::GenInstrFR_SW(Opcode_ops_fp opcode,
+                              FloatRegister fb,
+                              FloatRegister fc) {
+  DCHECK(fb.is_valid() && fc.is_valid());
+  Instr instr = opcode | (fb.code() << sRbShift) |
+                         (fc.code() << sRcShift);
+  emitSW(instr);
+}
+
+//20180914
+void Assembler::GenInstrFR_SW(Opcode_ops_fpl opcode,
+                              FloatRegister fa,
+                              int16_t imm,
+                              FloatRegister fc) {
+  DCHECK(fa.is_valid() && is_uint8(imm) && fc.is_valid());
+  Instr instr = opcode | (fa.code() << sRaShift) | ((imm << sImm8Shift) & sImm8Mask) |
+                         (fc.code() << sRcShift);
+  emitSW(instr);
+}
+
+void Assembler::GenInstrFR_SW(Opcode_ops_fpl opcode,
+                              FloatRegister fa,
+                              FloatRegister fb,
+                              int16_t fmalit,
+                              FloatRegister fc) {
+  DCHECK(fa.is_valid() && fb.is_valid() && is_uint5(fmalit) && fc.is_valid());
+  Instr instr = opcode | (fa.code() << sRaShift) | (fb.code() << sRbShift) |
+            ((fmalit << sImm5Shift) & sImm5Mask) | (fc.code() << sRcShift);
+  emitSW(instr);
+}
+
+void Assembler::GenInstrFMA_SW(Opcode_ops_fmal opcode,
+                              FloatRegister fa,
+                              FloatRegister fb,
+                              int16_t fmalit,
+                              FloatRegister fc) {
+  DCHECK(fa.is_valid() && fb.is_valid() && is_uint5(fmalit) && fc.is_valid());
+  Instr instr = opcode | (fa.code() << sRaShift) | (fb.code() << sRbShift) |
+            ((fmalit << sImm5Shift) & sImm5Mask) | (fc.code() << sRcShift);
+  emitSW(instr);
+}
+
+void Assembler::GenInstrFMA_SW(Opcode_ops_fmal opcode,
+                              FloatRegister fa,
+                              int16_t fmalit,
+                              FloatRegister fc) {
+  DCHECK(fa.is_valid() && is_uint5(fmalit) && fc.is_valid());
+  Instr instr = opcode | (fa.code() << sRaShift) |
+            ((fmalit << sImm5Shift) & sImm5Mask) | (fc.code() << sRcShift);
+  emitSW(instr);
+}
+
+void Assembler::GenInstrFMA_SW(Opcode_ops_fma opcode,
+                              FloatRegister fa,
+                              FloatRegister fb,
+                              FloatRegister fc) {
+  DCHECK(fa.is_valid() &&  fb.is_valid() && fc.is_valid());
+  Instr instr = opcode | (fa.code() << sRaShift) | (fb.code() << sRbShift) |
+                         (fc.code() << sRcShift);
+  emitSW(instr);
+}
+
+void Assembler::GenInstrSIMD_SW(Opcode_ops_atmem opcode,
+                                FloatRegister fa,
+                                int16_t atmdisp,
+                                Register Rb) {
+  DCHECK(fa.is_valid() && is_uint11(atmdisp) && Rb.is_valid());
+  Instr instr = opcode | (fa.code() << sRaShift) |
+         ((atmdisp << sImm11Shift) & sImm11Mask) | (Rb.code() << sRbShift);
+  emitSW(instr);
+}
+
+// FMA + FSEL** instructions.
+void Assembler::GenInstrFMA_SW(Opcode_ops_fma opcode,
+                               FloatRegister fa,
+                               FloatRegister fb,
+                               FloatRegister f3,
+                               FloatRegister fc) {
+  DCHECK(fa.is_valid() &&  fb.is_valid() &&  f3.is_valid() && fc.is_valid());
+  Instr instr = opcode | (fa.code() << sRaShift) | (fb.code() << sRbShift) |
+                         (f3.code() << sR3Shift) | (fc.code() << sRcShift);
+  emitSW(instr);
+}
+
+
+// SEL** instructions.
+void Assembler::GenInstrSelR_SW(Opcode_ops_sel opcode,
+                                Register Ra,
+                                Register Rb,
+                                Register R3,
+                                Register Rc) {
+  DCHECK(Ra.is_valid() &&  Rb.is_valid() &&  R3.is_valid() && Rc.is_valid());
+  Instr instr = opcode | (Ra.code() << sRaShift) | (Rb.code() << sRbShift) |
+                         (R3.code() << sR3Shift) | (Rc.code() << sRcShift);
+  emitSW(instr);
+}
+
+
+// SEL**_l instructions.
+void Assembler::GenInstrSelI_SW(Opcode_ops_sel_l opcode,
+                                Register Ra,
+                                int32_t imm,
+                                Register R3,
+                                Register Rc) {
+  DCHECK(Ra.is_valid() &&  is_int8(imm) &&  R3.is_valid() && Rc.is_valid());
+  Instr instr = opcode | (Ra.code() << sRaShift) | ((imm << sImm8Shift) & sImm8Mask) |
+                         (R3.code() << sR3Shift) | (Rc.code() << sRcShift);
+  emitSW(instr);
+}
+
+
+// All SW64 instructions
+
+void Assembler::sys_call_b(int palfn) {
+  DCHECK(is_int26(palfn));
+  Instr instr = op_sys_call |  palfn;
+  emitSW(instr);
+}
+
+
+void Assembler::sys_call(int palfn) {
+  DCHECK(is_int26(palfn));
+  Instr instr = op_sys_call | ( palfn & (( 1 << 26 ) - 1));
+  emitSW(instr);
+}
+
+
+void Assembler::call(Register Ra, Register Rb, int jmphint) {
+  // call ra, (rb), jmphint;
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  GenInstrM_SW(op_call, Ra, jmphint, Rb);
+}
+
+
+void Assembler::ret(Register Ra, Register Rb, int rethint) {
+  // ret ra, (rb), rethint;
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  GenInstrM_SW(op_ret, Ra, rethint, Rb);
+}
+
+
+void Assembler::jmp(Register Ra, Register Rb, int jmphint) {
+  // jmp ra, (rb), jmphint;
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  GenInstrM_SW(op_jmp, Ra, jmphint, Rb);
+}
+
+
+void Assembler::br(Register Ra, int bdisp) {
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  GenInstrB_SW(op_br, Ra, bdisp);
+}
+
+
+void Assembler::bsr(Register Ra, int bdisp) {
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  GenInstrB_SW(op_bsr, Ra, bdisp);
+}
+
+
+void Assembler::memb(void) {
+  Instr instr = op_memb;
+  emitSW(instr);
+}
+
+
+void Assembler::imemb(void) {
+  Instr instr = op_imemb;
+  emitSW(instr);
+}
+
+void Assembler::wmemb(void) {
+  DCHECK(kArchVariant == kSw64r3);
+  Instr instr = op_wmemb;
+  emitSW(instr);
+}
+
+
+void Assembler::rtc(Register Ra, Register Rb) {
+  DCHECK(Ra.is_valid() && Rb.is_valid());
+  Instr instr = op_rtc | Ra.code() << sRaShift | Rb.code() << sRbShift;
+  emitSW(instr);
+}
+
+
+void Assembler::rcid(Register Ra) {
+  DCHECK(Ra.is_valid());
+  Instr instr = op_rcid | Ra.code() << sRaShift;
+  emitSW(instr);
+}
+
+
+void Assembler::halt(void) {
+  Instr instr = op_halt;
+  emitSW(instr);
+}
+
+
+void Assembler::rd_f(Register Ra) {
+  DCHECK(Ra.is_valid());
+  Instr instr = op_rd_f | Ra.code() << sRaShift;
+  emitSW(instr);
+}
+
+
+void Assembler::wr_f(Register Ra) {
+  DCHECK(Ra.is_valid());
+  Instr instr = op_wr_f | Ra.code() << sRaShift;
+  emitSW(instr);
+}
+
+
+void Assembler::rtid(Register Ra) {
+  DCHECK(Ra.is_valid());
+  Instr instr = op_rtid | Ra.code() << sRaShift;
+  emitSW(instr);
+}
+
+
+void Assembler::csrrs(Register Ra, int rpiindex) {
+  DCHECK(kArchVariant == kSw64r3);
+  DCHECK(Ra.is_valid() && is_uint8(rpiindex));
+  Instr instr = op_csrrs | (Ra.code() << sRaShift) | (rpiindex & sRpiMask);
+  emitSW(instr);
+}
+
+
+void Assembler::csrrc(Register Ra, int rpiindex) {
+  DCHECK(kArchVariant == kSw64r3);
+  DCHECK(Ra.is_valid() && is_uint8(rpiindex));
+  Instr instr = op_csrrc | (Ra.code() << sRaShift) | (rpiindex & sRpiMask);
+  emitSW(instr);
+}
+
+
+void Assembler::csrr(Register Ra, int rpiindex) {
+  DCHECK(Ra.is_valid() && is_uint8(rpiindex));
+  Instr instr = op_csrr | (Ra.code() << sRaShift) | (rpiindex & sRpiMask);
+  emitSW(instr);
+}
+
+
+void Assembler::csrw(Register Ra, int rpiindex) {
+  DCHECK(Ra.is_valid() && is_uint8(rpiindex));
+  Instr instr = op_csrw | (Ra.code() << sRaShift) | (rpiindex & sRpiMask);
+  emitSW(instr);
+}
+
+
+void Assembler::pri_ret(Register Ra) {
+  DCHECK(Ra.is_valid());
+  Instr instr = op_pri_ret | Ra.code() << sRaShift;
+  emitSW(instr);
+}
+
+
+void Assembler::lldw(Register Ra, int atmdisp, Register Rb) {
+  GenInstrMWithFun_SW(op_lldw, Ra, atmdisp, Rb);
+}
+
+
+void Assembler::lldl(Register Ra, int atmdisp, Register Rb) {
+  GenInstrMWithFun_SW(op_lldl, Ra, atmdisp, Rb);
+}
+
+
+void Assembler::ldw_inc(Register Ra, int atmdisp, Register Rb) {
+  GenInstrMWithFun_SW(op_ldw_inc, Ra, atmdisp, Rb);
+}
+
+
+void Assembler::ldl_inc(Register Ra, int atmdisp, Register Rb) {
+  GenInstrMWithFun_SW(op_ldl_inc, Ra, atmdisp, Rb);
+}
+
+
+void Assembler::ldw_dec(Register Ra, int atmdisp, Register Rb) {
+  GenInstrMWithFun_SW(op_ldw_dec, Ra, atmdisp, Rb);
+}
+
+
+void Assembler::ldl_dec(Register Ra, int atmdisp, Register Rb) {
+  GenInstrMWithFun_SW(op_ldl_dec, Ra, atmdisp, Rb);
+}
+
+
+void Assembler::ldw_set(Register Ra, int atmdisp, Register Rb) {
+  GenInstrMWithFun_SW(op_ldw_set, Ra, atmdisp, Rb);
+}
+
+
+void Assembler::ldl_set(Register Ra, int atmdisp, Register Rb) {
+  GenInstrMWithFun_SW(op_ldl_set, Ra, atmdisp, Rb);
+}
+
+
+void Assembler::lstw(Register Ra, int atmdisp, Register Rb) {
+  GenInstrMWithFun_SW(op_lstw, Ra, atmdisp, Rb);
+}
+
+
+void Assembler::lstl(Register Ra, int atmdisp, Register Rb) {
+  GenInstrMWithFun_SW(op_lstl, Ra, atmdisp, Rb);
+}
+
+
+void Assembler::ldw_nc(Register Ra, int atmdisp, Register Rb) {
+  DCHECK(Ra.is_valid() && is_uint11(atmdisp) && Rb.is_valid());
+  Instr instr = op_ldw_nc | (Ra.code() << sRaShift)
+              | ((atmdisp << sImm11Shift) & sImm11Mask) | (Rb.code() << sRbShift);
+  emitSW(instr);
+}
+
+
+void Assembler::ldl_nc(Register Ra, int atmdisp, Register Rb) {
+  DCHECK(Ra.is_valid() && is_uint11(atmdisp) && Rb.is_valid());
+  Instr instr = op_ldl_nc | (Ra.code() << sRaShift)
+        | ((atmdisp << sImm11Shift) & sImm11Mask) | (Rb.code() << sRbShift);
+  emitSW(instr);
+}
+
+
+void Assembler::ldd_nc(Register Ra, int atmdisp, Register Rb) {
+  DCHECK(Ra.is_valid() && is_uint11(atmdisp) && Rb.is_valid());
+  Instr instr = op_ldd_nc | (Ra.code() << sRaShift)
+        | ((atmdisp << sImm11Shift) & sImm11Mask) | (Rb.code() << sRbShift);
+  emitSW(instr);
+}
+
+
+void Assembler::stw_nc(Register Ra, int atmdisp, Register Rb) {
+  DCHECK(Ra.is_valid() && is_uint11(atmdisp) && Rb.is_valid());
+  Instr instr = op_stw_nc | (Ra.code() << sRaShift) |
+            ((atmdisp << sImm11Shift) & sImm11Mask) | (Rb.code() << sRbShift);
+  emitSW(instr);
+}
+
+
+void Assembler::stl_nc(Register Ra, int atmdisp, Register Rb) {
+  DCHECK(Ra.is_valid() && is_uint11(atmdisp) && Rb.is_valid());
+  Instr instr = op_stl_nc | (Ra.code() << sRaShift) |
+            ((atmdisp << sImm11Shift) & sImm11Mask) | (Rb.code() << sRbShift);
+  emitSW(instr);
+}
+
+
+void Assembler::std_nc(Register Ra, int atmdisp, Register Rb) {
+  DCHECK(Ra.is_valid() && is_uint11(atmdisp) && Rb.is_valid());
+  Instr instr = op_std_nc | (Ra.code() << sRaShift) |
+            ((atmdisp << sImm11Shift) & sImm11Mask) | (Rb.code() << sRbShift);
+  emitSW(instr);
+}
+
+
+void Assembler::ldwe(FloatRegister fa, int mdisp, Register Rb) {
+  GenInstrFM_SW(op_ldwe, fa, mdisp, Rb);
+}
+
+
+void Assembler::ldse(FloatRegister fa, int mdisp, Register Rb) {
+  GenInstrFM_SW(op_ldse, fa, mdisp, Rb);
+}
+
+
+void Assembler::ldde(FloatRegister fa, int mdisp, Register Rb) {
+  GenInstrFM_SW(op_ldde, fa, mdisp, Rb);
+}
+
+
+void Assembler::vlds(FloatRegister fa, int mdisp, Register Rb) {
+  GenInstrFM_SW(op_vlds, fa, mdisp, Rb);
+}
+
+
+void Assembler::vldd(FloatRegister fa, int mdisp, Register Rb) {
+  GenInstrFM_SW(op_vldd, fa, mdisp, Rb);
+}
+
+
+void Assembler::vsts(FloatRegister fa, int mdisp, Register Rb) {
+  GenInstrFM_SW(op_vsts, fa, mdisp, Rb);
+}
+
+
+void Assembler::vstd(FloatRegister fa, int mdisp, Register Rb) {
+  GenInstrFM_SW(op_vstd, fa, mdisp, Rb);
+}
+
+
+void Assembler::addw(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_addw, Ra, Rb, Rc);
+}
+
+
+void Assembler::addw(Register Ra, int imm, Register Rc) {
+  GenInstrI_SW(op_addw_l, Ra, imm, Rc);
+}
+
+
+void Assembler::subw(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_subw, Ra, Rb, Rc);
+}
+
+
+void Assembler::subw(Register Ra, int imm, Register Rc) {
+  GenInstrI_SW(op_subw_l, Ra, imm, Rc);
+}
+
+
+void Assembler::s4addw(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_s4addw, Ra, Rb, Rc);
+}
+
+
+void Assembler::s4addw(Register Ra, int imm, Register Rc) {
+  GenInstrI_SW(op_s4addw_l, Ra, imm, Rc);
+}
+
+
+void Assembler::s4subw(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_s4subw, Ra, Rb, Rc);
+}
+
+
+void Assembler::s4subw(Register Ra, int imm, Register Rc) {
+  GenInstrI_SW(op_s4subw_l, Ra, imm, Rc);
+}
+
+
+void Assembler::s8addw(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_s8addw, Ra, Rb, Rc);
+}
+
+
+void Assembler::s8addw(Register Ra, int imm, Register Rc) {
+  GenInstrI_SW(op_s8addw_l, Ra, imm, Rc);
+}
+
+
+void Assembler::s8subw(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_s8subw, Ra, Rb, Rc);
+}
+
+
+void Assembler::s8subw(Register Ra, int imm, Register Rc) {
+  GenInstrI_SW(op_s8subw_l, Ra, imm, Rc);
+}
+
+
+void Assembler::addl(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_addl, Ra, Rb, Rc);
+}
+
+
+void Assembler::addl(Register Ra, int imm, Register Rc) {
+  GenInstrI_SW(op_addl_l, Ra, imm, Rc);
+}
+
+
+void Assembler::subl(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_subl, Ra, Rb, Rc);
+}
+
+
+void Assembler::subl(Register Ra, int imm, Register Rc) {
+  GenInstrI_SW(op_subl_l, Ra, imm, Rc);
+}
+
+
+void Assembler::s4addl(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_s4addl, Ra, Rb, Rc);
+}
+
+
+void Assembler::s4addl(Register Ra, int imm, Register Rc) {
+  GenInstrI_SW(op_s4addl_l, Ra, imm, Rc);
+}
+
+
+void Assembler::s4subl(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_s4subl, Ra, Rb, Rc);
+}
+
+
+void Assembler::s4subl(Register Ra, int imm, Register Rc) {
+  GenInstrI_SW(op_s4subl_l, Ra, imm, Rc);
+}
+
+
+void Assembler::s8addl(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_s8addl, Ra, Rb, Rc);
+}
+
+
+void Assembler::s8addl(Register Ra, int imm, Register Rc) {
+  GenInstrI_SW(op_s8addl_l, Ra, imm, Rc);
+}
+
+
+void Assembler::s8subl(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_s8subl, Ra, Rb, Rc);
+}
+
+
+void Assembler::s8subl(Register Ra, int imm, Register Rc) {
+  GenInstrI_SW(op_s8subl_l, Ra, imm, Rc);
+}
+
+
+void Assembler::mulw(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_mulw, Ra, Rb, Rc);
+}
+
+
+void Assembler::mulw(Register Ra, int imm, Register Rc) {
+  GenInstrI_SW(op_mulw_l, Ra, imm, Rc);
+}
+
+
+void Assembler::divw(Register Ra, Register Rb, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);  
+  GenInstrR_SW(op_divw, Ra, Rb, Rc);
+}
+
+
+void Assembler::udivw(Register Ra, Register Rb, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrR_SW(op_udivw, Ra, Rb, Rc);
+}
+
+
+void Assembler::remw(Register Ra, Register Rb, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);  
+  GenInstrR_SW(op_remw, Ra, Rb, Rc);
+}
+
+
+void Assembler::uremw(Register Ra, Register Rb, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);  
+  GenInstrR_SW(op_uremw, Ra, Rb, Rc);
+}
+
+
+void Assembler::mull(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_mull, Ra, Rb, Rc);
+}
+
+
+void Assembler::mull(Register Ra, int imm, Register Rc) {
+  GenInstrI_SW(op_mull_l, Ra, imm, Rc);
+}
+
+
+void Assembler::umulh(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_umulh, Ra, Rb, Rc);
+}
+
+
+void Assembler::umulh(Register Ra, int imm, Register Rc) {
+  GenInstrI_SW(op_umulh_l, Ra, imm, Rc);
+}
+
+
+void Assembler::divl(Register Ra, Register Rb, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrR_SW(op_divl, Ra, Rb, Rc);
+}
+
+
+void Assembler::udivl(Register Ra, Register Rb, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrR_SW(op_udivl, Ra, Rb, Rc);
+}
+
+
+void Assembler::reml(Register Ra, Register Rb, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrR_SW(op_reml, Ra, Rb, Rc);
+}
+
+
+void Assembler::ureml(Register Ra, Register Rb, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrR_SW(op_ureml, Ra, Rb, Rc);
+}
+
+
+void Assembler::addpi(int apint,   Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);
+  DCHECK(is_uint13(apint) && Rc.is_valid());
+  Instr instr = op_addpi | ((apint << sImm13Shift) & sImm13Mask) |
+                (Rc.code() << sRcShift);
+  emitSW(instr);
+}
+
+
+void Assembler::addpis(int apint,   Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);
+  DCHECK(is_uint13(apint) && Rc.is_valid());
+  Instr instr = op_addpis | ( (apint << sImm13Shift) & sImm13Mask) |
+                (Rc.code() << sRcShift);
+  emitSW(instr);
+}
+
+
+void Assembler::cmpeq(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_cmpeq, Ra, Rb, Rc);
+}
+
+
+void Assembler::cmpeq(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_cmpeq_l, Ra, lit, Rc);
+}
+
+
+void Assembler::cmplt(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_cmplt, Ra, Rb, Rc);
+}
+
+
+void Assembler::cmplt(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_cmplt_l, Ra, lit, Rc);
+}
+
+
+void Assembler::cmple(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_cmple, Ra, Rb, Rc);
+}
+
+
+void Assembler::cmple(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_cmple_l, Ra, lit, Rc);
+}
+
+
+void Assembler::cmpult(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_cmpult, Ra, Rb, Rc);
+}
+
+
+void Assembler::cmpult(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_cmpult_l, Ra, lit, Rc);
+}
+
+
+void Assembler::cmpule(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_cmpule, Ra, Rb, Rc);
+}
+
+
+void Assembler::cmpule(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_cmpule_l, Ra, lit, Rc);
+}
+
+
+void Assembler::sbt(Register Ra, Register Rb, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrR_SW(op_sbt, Ra, Rb, Rc);
+}
+
+
+void Assembler::sbt(Register Ra, int lit, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrI_SW(op_sbt_l, Ra, lit, Rc);
+}
+
+
+void Assembler::cbt(Register Ra, Register Rb, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrR_SW(op_cbt, Ra, Rb, Rc);
+}
+
+
+void Assembler::cbt(Register Ra, int lit, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrI_SW(op_cbt_l, Ra, lit, Rc);
+}
+
+
+void Assembler::and_ins(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_and, Ra, Rb, Rc);
+}
+
+
+void Assembler::and_ins(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_and_l, Ra, lit, Rc);
+}
+
+
+void Assembler::bic(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_bic, Ra, Rb, Rc);
+}
+
+void Assembler::bic(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_bic_l, Ra, lit, Rc);
+}
+
+void Assembler::andnot(Register Ra, Register Rb, Register Rc) {
+  bic(Ra, Rb, Rc );
+}
+
+void Assembler::andnot(Register Ra, int lit, Register Rc) {
+  bic(Ra, lit, Rc );
+}
+
+void Assembler::bis(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_bis, Ra, Rb, Rc);
+}
+
+void Assembler::bis(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_bis_l, Ra, lit, Rc);
+}
+
+void Assembler::or_ins(Register Ra, Register Rb, Register Rc) {
+  //GenInstrR_SW(op_bis, Ra, Rb, Rc);
+  bis(Ra, Rb, Rc);
+}
+
+void Assembler::or_ins(Register Ra, int lit, Register Rc) {
+  //GenInstrI_SW(op_bis_l, Ra, lit, Rc);
+  bis(Ra, lit, Rc);
+}
+
+void Assembler::ornot(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_ornot, Ra, Rb, Rc);
+}
+
+void Assembler::ornot(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_ornot_l, Ra, lit, Rc);
+}
+
+void Assembler::xor_ins(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_xor, Ra, Rb, Rc);
+}
+
+
+void Assembler::xor_ins(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_xor_l, Ra, lit, Rc);
+}
+
+
+void Assembler::eqv(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_eqv, Ra, Rb, Rc);
+}
+
+
+void Assembler::eqv(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_eqv_l, Ra, lit, Rc);
+}
+
+
+// 0x10.40-0x10.47 INS[0-7]B
+void Assembler::inslb(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_inslb, Ra, Rb, Rc);
+}
+
+
+void Assembler::inslb(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_inslb_l, Ra, lit, Rc);
+}
+
+
+void Assembler::inslh(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_inslh, Ra, Rb, Rc);
+}
+
+
+void Assembler::inslh(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_inslh_l, Ra, lit, Rc);
+}
+
+
+void Assembler::inslw(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_inslw, Ra, Rb, Rc);
+}
+
+
+void Assembler::inslw(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_inslw_l, Ra, lit, Rc);
+}
+
+
+void Assembler::insll(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_insll, Ra, Rb, Rc);
+}
+
+
+void Assembler::insll(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_insll_l, Ra, lit, Rc);
+}
+
+
+void Assembler::inshb(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_inshb, Ra, Rb, Rc);
+}
+
+
+void Assembler::inshb(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_inshb_l, Ra, lit, Rc);
+}
+
+
+void Assembler::inshh(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_inshh, Ra, Rb, Rc);
+}
+
+
+void Assembler::inshh(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_inshh_l, Ra, lit, Rc);
+}
+
+
+void Assembler::inshw(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_inshw, Ra, Rb, Rc);
+}
+
+
+void Assembler::inshw(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_inshw_l, Ra, lit, Rc);
+}
+
+
+void Assembler::inshl(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_inshl, Ra, Rb, Rc);
+}
+
+
+void Assembler::inshl(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_inshl_l, Ra, lit, Rc);
+}
+
+
+void Assembler::slll(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_slll, Ra, Rb, Rc);
+}
+
+
+void Assembler::slll(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_slll_l, Ra, lit, Rc);
+}
+
+
+void Assembler::srll(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_srll, Ra, Rb, Rc);
+}
+
+
+void Assembler::srll(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_srll_l, Ra, lit, Rc);
+}
+
+
+void Assembler::sral(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_sral, Ra, Rb, Rc);
+}
+
+
+void Assembler::sral(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_sral_l, Ra, lit, Rc);
+}
+
+
+void Assembler::roll(Register Ra, Register Rb, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrR_SW(op_roll, Ra, Rb, Rc);
+}
+
+
+void Assembler::roll(Register Ra, int lit, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrI_SW(op_roll_l, Ra, lit, Rc);
+}
+
+
+void Assembler::sllw(Register Ra, Register Rb, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrR_SW(op_sllw, Ra, Rb, Rc);
+}
+
+
+void Assembler::sllw(Register Ra, int lit, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrI_SW(op_sllw_l, Ra, lit, Rc);
+}
+
+
+void Assembler::srlw(Register Ra, Register Rb, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrR_SW(op_srlw, Ra, Rb, Rc);
+}
+
+
+void Assembler::srlw(Register Ra, int lit, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrI_SW(op_srlw_l, Ra, lit, Rc);
+}
+
+
+void Assembler::sraw(Register Ra, Register Rb, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrR_SW(op_sraw, Ra, Rb, Rc);
+}
+
+
+void Assembler::sraw(Register Ra, int lit, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrI_SW(op_sraw_l, Ra, lit, Rc);
+}
+
+
+void Assembler::rolw(Register Ra, Register Rb, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrR_SW(op_rolw, Ra, Rb, Rc);
+}
+
+
+void Assembler::rolw(Register Ra, int lit, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrI_SW(op_rolw_l, Ra, lit, Rc);
+}
+
+
+// 0x10.50-0x10.57 EXT[0-7]B
+void Assembler::extlb(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_extlb, Ra, Rb, Rc);
+}
+
+
+void Assembler::extlb(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_extlb_l, Ra, lit, Rc);
+}
+
+
+void Assembler::extlh(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_extlh, Ra, Rb, Rc);
+}
+
+
+void Assembler::extlh(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_extlh_l, Ra, lit, Rc);
+}
+
+
+void Assembler::extlw(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_extlw, Ra, Rb, Rc);
+}
+
+
+void Assembler::extlw(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_extlw_l, Ra, lit, Rc);
+}
+
+
+void Assembler::extll(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_extll, Ra, Rb, Rc);
+}
+
+
+void Assembler::extll(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_extll_l, Ra, lit, Rc);
+}
+
+
+void Assembler::exthb(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_exthb, Ra, Rb, Rc);
+}
+
+
+void Assembler::exthb(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_exthb_l, Ra, lit, Rc);
+}
+
+
+void Assembler::exthh(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_exthh, Ra, Rb, Rc);
+}
+
+
+void Assembler::exthh(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_exthh_l, Ra, lit, Rc);
+}
+
+
+void Assembler::exthw(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_exthw, Ra, Rb, Rc);
+}
+
+
+void Assembler::exthw(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_exthw_l, Ra, lit, Rc);
+}
+
+
+void Assembler::exthl(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_exthl, Ra, Rb, Rc);
+}
+
+
+void Assembler::exthl(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_exthl_l, Ra, lit, Rc);
+}
+
+
+void Assembler::ctpop(Register Rb, Register Rc) {
+  DCHECK(Rb.is_valid() && Rc.is_valid());
+  Instr instr = op_ctpop | Rb.code() << sRbShift | Rc.code() << sRcShift;
+  emitSW(instr);
+}
+
+
+void Assembler::ctlz(Register Rb, Register Rc) {
+  DCHECK(Rb.is_valid() && Rc.is_valid());
+  Instr instr = op_ctlz | Rb.code() << sRbShift | Rc.code() << sRcShift;
+  emitSW(instr);
+}
+
+
+void Assembler::cttz(Register Rb, Register Rc) {
+  DCHECK(Rb.is_valid() && Rc.is_valid());
+  Instr instr = op_cttz | Rb.code() << sRbShift | Rc.code() << sRcShift;
+  emitSW(instr);
+}
+
+
+void Assembler::revbh(Register Rb, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);
+  DCHECK(Rb.is_valid() && Rc.is_valid());
+  Instr instr = op_revbh | Rb.code() << sRbShift | Rc.code() << sRcShift;
+  emitSW(instr);
+}
+
+
+void Assembler::revbw(Register Rb, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);
+  DCHECK(Rb.is_valid() && Rc.is_valid());
+  Instr instr = op_revbw | Rb.code() << sRbShift | Rc.code() << sRcShift;
+  emitSW(instr);
+}
+
+
+void Assembler::revbl(Register Rb, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);
+  DCHECK(Rb.is_valid() && Rc.is_valid());
+  Instr instr = op_revbl | Rb.code() << sRbShift | Rc.code() << sRcShift;
+  emitSW(instr);
+}
+
+
+void Assembler::casw(Register Ra, Register Rb, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrR_SW(op_casw, Ra, Rb, Rc);
+}
+
+
+void Assembler::casl(Register Ra, Register Rb, Register Rc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrR_SW(op_casl, Ra, Rb, Rc);
+}
+
+
+// 0x10.60-0x10.67 MASK[0-7]B
+void Assembler::masklb(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_masklb, Ra, Rb, Rc);
+}
+
+
+void Assembler::masklb(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_masklb_l, Ra, lit, Rc);
+}
+
+
+void Assembler::masklh(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_masklh, Ra, Rb, Rc);
+}
+
+
+void Assembler::masklh(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_masklh_l, Ra, lit, Rc);
+}
+
+
+void Assembler::masklw(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_masklw, Ra, Rb, Rc);
+}
+
+
+void Assembler::masklw(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_masklw_l, Ra, lit, Rc);
+}
+
+
+void Assembler::maskll(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_maskll, Ra, Rb, Rc);
+}
+
+
+void Assembler::maskll(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_maskll_l, Ra, lit, Rc);
+}
+
+
+void Assembler::maskhb(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_maskhb, Ra, Rb, Rc);
+}
+
+
+void Assembler::maskhb(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_maskhb_l, Ra, lit, Rc);
+}
+
+
+void Assembler::maskhh(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_maskhh, Ra, Rb, Rc);
+}
+
+
+void Assembler::maskhh(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_maskhh_l, Ra, lit, Rc);
+}
+
+
+void Assembler::maskhw(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_maskhw, Ra, Rb, Rc);
+}
+
+
+void Assembler::maskhw(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_maskhw_l, Ra, lit, Rc);
+}
+
+
+void Assembler::maskhl(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_maskhl, Ra, Rb, Rc);
+}
+
+
+void Assembler::maskhl(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_maskhl_l, Ra, lit, Rc);
+}
+
+
+void Assembler::zap(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_zap, Ra, Rb, Rc);
+}
+
+
+void Assembler::zap(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_zap_l, Ra, lit, Rc);
+}
+
+
+void Assembler::zapnot(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_zapnot, Ra, Rb, Rc);
+}
+
+
+void Assembler::zapnot(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_zapnot_l, Ra, lit, Rc);
+}
+
+
+void Assembler::sextb(Register Rb, Register Rc) {
+  DCHECK(Rb.is_valid() && Rc.is_valid());
+  Instr instr = op_sextb | (Rb.code() << sRbShift) |
+                (Rc.code() << sRcShift);
+  emitSW(instr);
+}
+
+
+void Assembler::sextb(int lit, Register Rc) {
+  DCHECK(is_uint8(lit) && Rc.is_valid());
+  Instr instr = op_sextb_l | (lit & sImm8Mask) |
+                (Rc.code() << sRcShift);
+  emitSW(instr);
+}
+
+
+void Assembler::sexth(Register Rb, Register Rc) {
+  DCHECK(Rb.is_valid() && Rc.is_valid());
+  Instr instr = op_sexth | (Rb.code() << sRbShift) |
+                (Rc.code() << sRcShift);
+  emitSW(instr);
+}
+
+
+void Assembler::sexth(int lit, Register Rc) {
+  DCHECK(is_uint8(lit) && Rc.is_valid());
+  Instr instr = op_sexth_l | (lit & sImm8Mask) |
+                (Rc.code() << sRcShift);
+  emitSW(instr);
+}
+
+
+//0x10.6c CMPGEB
+void Assembler::cmpgeb(Register Ra, Register Rb, Register Rc) {
+  GenInstrR_SW(op_cmpgeb, Ra, Rb, Rc);
+}
+
+
+void Assembler::cmpgeb(Register Ra, int lit, Register Rc) {
+  GenInstrI_SW(op_cmpgeb_l, Ra, lit, Rc);
+}
+
+
+//void Assembler::ftois(FloatRegister fa, Register Rc )
+//void Assembler::ftoid(FloatRegister fa, Register Rc )
+void Assembler::fimovs(FloatRegister fa, Register Rc) {
+  DCHECK(fa.is_valid() && Rc.is_valid());
+  Instr instr = op_fimovs | (fa.code() << sRaShift) |
+                (Rc.code() << sRcShift);
+  emitSW(instr);
+}
+
+
+void Assembler::fimovd(FloatRegister fa, Register Rc) {
+  DCHECK(fa.is_valid() && Rc.is_valid() );
+  Instr instr = op_fimovd | (fa.code() << sRaShift) |
+                (Rc.code() << sRcShift);
+  emitSW(instr);
+}
+
+
+void Assembler::seleq(Register Ra, Register Rb, Register R3, Register Rc) {
+  GenInstrSelR_SW(op_seleq, Ra, Rb, R3, Rc);
+}
+
+
+void Assembler::seleq(Register Ra, int lit, Register R3, Register Rc) {
+  GenInstrSelI_SW(op_seleq_l, Ra, lit, R3, Rc);
+}
+
+
+void Assembler::selge(Register Ra, Register Rb, Register R3, Register Rc) {
+  GenInstrSelR_SW(op_selge, Ra, Rb, R3, Rc);
+}
+
+
+void Assembler::selge(Register Ra, int lit, Register R3, Register Rc) {
+  GenInstrSelI_SW(op_selge_l, Ra, lit, R3, Rc);
+}
+
+
+void Assembler::selgt(Register Ra, Register Rb, Register R3, Register Rc) {
+  GenInstrSelR_SW(op_selgt, Ra, Rb, R3, Rc);
+}
+
+
+void Assembler::selgt(Register Ra, int lit, Register R3, Register Rc) {
+  GenInstrSelI_SW(op_selgt_l, Ra, lit, R3, Rc);
+}
+
+
+void Assembler::selle(Register Ra, Register Rb, Register R3, Register Rc) {
+  GenInstrSelR_SW(op_selle, Ra, Rb, R3, Rc);
+}
+
+
+void Assembler::selle(Register Ra, int lit, Register R3, Register Rc) {
+  GenInstrSelI_SW(op_selle_l, Ra, lit, R3, Rc);
+}
+
+
+void Assembler::sellt(Register Ra, Register Rb, Register R3, Register Rc) {
+  GenInstrSelR_SW(op_sellt, Ra, Rb, R3, Rc);
+}
+
+
+void Assembler::sellt(Register Ra, int lit, Register R3, Register Rc) {
+  GenInstrSelI_SW(op_sellt_l, Ra, lit, R3, Rc);
+}
+
+
+void Assembler::selne(Register Ra, Register Rb, Register R3, Register Rc) {
+  GenInstrSelR_SW(op_selne, Ra, Rb, R3, Rc);
+}
+
+
+void Assembler::selne(Register Ra, int lit, Register R3, Register Rc) {
+  GenInstrSelI_SW(op_selne_l, Ra, lit, R3, Rc);
+}
+
+
+void Assembler::sellbc(Register Ra, Register Rb, Register R3, Register Rc) {
+  GenInstrSelR_SW(op_sellbc, Ra, Rb, R3, Rc);
+}
+
+
+void Assembler::sellbc(Register Ra, int lit, Register R3, Register Rc) {
+  GenInstrSelI_SW(op_sellbc_l, Ra, lit, R3, Rc);
+}
+
+
+void Assembler::sellbs(Register Ra, Register Rb, Register R3, Register Rc) {
+  GenInstrSelR_SW(op_sellbs, Ra, Rb, R3, Rc);
+}
+
+
+void Assembler::sellbs(Register Ra, int lit, Register R3, Register Rc) {
+  GenInstrSelI_SW(op_sellbs_l, Ra, lit, R3, Rc);
+}
+
+
+void Assembler::vlog(int vlog, FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  UNREACHABLE();
+}
+
+void Assembler::f_exclude_same_src_fc(Opcode_ops_fp opcode, FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  if (fa == fc || fb == fc) {
+    DCHECK(fa != kScratchDoubleReg && fb != kScratchDoubleReg);
+    GenInstrFR_SW(opcode, fa, fb, kScratchDoubleReg);
+    fmov(kScratchDoubleReg, fc);
+  } else {
+    GenInstrFR_SW(opcode, fa, fb, fc);
+  }
+}
+
+void Assembler::f_exclude_same_src_fc(Opcode_ops_fp opcode, FloatRegister fb, FloatRegister fc) {
+  if (fb == fc) {
+    DCHECK(fb != kScratchDoubleReg);
+    GenInstrFR_SW(opcode, fb, kScratchDoubleReg);
+    fmov(kScratchDoubleReg, fc);
+  } else {
+    GenInstrFR_SW(opcode, fb, fc);
+  }
+}
+
+void Assembler::vbisw(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_vbisw, fa, fb, fc);
+}
+
+void Assembler::vxorw(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_vxorw, fa, fb, fc);
+}
+
+void Assembler::vandw(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_vandw, fa, fb, fc);
+}
+
+void Assembler::veqvw(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_veqvw, fa, fb, fc);
+}
+
+void Assembler::vornotw(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_vornotw, fa, fb, fc);
+}
+
+void Assembler::vbicw(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_vbicw, fa, fb, fc);
+}
+
+void Assembler::fadds(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_fadds, fa, fb, fc);
+}
+
+void Assembler::faddd(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_faddd, fa, fb, fc);
+}
+
+void Assembler::fsubs(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_fsubs, fa, fb, fc);
+}
+
+void Assembler::fsubd(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_fsubd, fa, fb, fc);
+}
+
+void Assembler::fmuls(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_fmuls, fa, fb, fc);
+}
+
+void Assembler::fmuld(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_fmuld, fa, fb, fc);
+}
+
+void Assembler::fdivs(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_fdivs, fa, fb, fc);
+}
+
+void Assembler::fdivd(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_fdivd, fa, fb, fc);
+}
+
+void Assembler::fsqrts(FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_fsqrts, fb, fc);
+}
+
+void Assembler::fsqrtd(FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_fsqrtd, fb, fc);
+}
+
+
+void Assembler::fcmpeq(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_fcmpeq, fa, fb, fc);
+}
+
+
+void Assembler::fcmple(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_fcmple, fa, fb, fc);
+}
+
+
+void Assembler::fcmplt(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_fcmplt, fa, fb, fc);
+}
+
+
+void Assembler::fcmpun(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_fcmpun, fa, fb, fc);
+}
+
+
+void Assembler::fcvtsd(FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_fcvtsd, fb, fc);
+}
+
+
+void Assembler::fcvtds(FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_fcvtds, fb, fc);
+}
+
+
+void Assembler::fcvtdl_g(FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_fcvtdl_g, fb, fc);
+}
+
+
+void Assembler::fcvtdl_p(FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_fcvtdl_p, fb, fc);
+}
+
+
+void Assembler::fcvtdl_z(FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_fcvtdl_z, fb, fc);
+}
+
+
+void Assembler::fcvtdl_n(FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_fcvtdl_n, fb, fc);
+}
+
+
+void Assembler::fcvtdl(FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_fcvtdl, fb, fc);
+}
+
+
+void Assembler::fcvtwl(FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_fcvtwl, fb, fc);
+}
+
+
+void Assembler::fcvtlw(FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_fcvtlw, fb, fc);
+}
+
+
+void Assembler::fcvtls(FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_fcvtls, fb, fc);
+}
+
+
+void Assembler::fcvtld(FloatRegister fb, FloatRegister fc) {
+  f_exclude_same_src_fc(op_fcvtld, fb, fc);
+}
+
+
+void Assembler::fcpys(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  GenInstrFR_SW(op_fcpys, fa, fb, fc);
+}
+
+
+void Assembler::fcpyse(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  GenInstrFR_SW(op_fcpyse, fa, fb, fc);
+}
+
+
+void Assembler::fcpysn(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  GenInstrFR_SW(op_fcpysn, fa, fb, fc);
+}
+
+
+void Assembler::ifmovs(Register Ra, FloatRegister fc) {
+  DCHECK(Ra.is_valid() && fc.is_valid());
+  Instr instr = op_ifmovs | (Ra.code() << sRaShift) |
+                (fc.code() << sRcShift);
+  emitSW(instr);
+}
+
+
+void Assembler::ifmovd(Register Ra, FloatRegister fc) {
+  DCHECK(Ra.is_valid() && fc.is_valid());
+  Instr instr = op_ifmovd | (Ra.code() << sRaShift) |
+                (fc.code() << sRcShift);
+  emitSW(instr);
+}
+
+
+void Assembler::rfpcr(FloatRegister fa) {
+  DCHECK(fa.is_valid());
+  Instr instr = op_rfpcr | (fa.code() << sRaShift);
+  emitSW(instr);
+}
+
+
+void Assembler::wfpcr(FloatRegister fa) {
+  DCHECK(fa.is_valid());
+  Instr instr = op_wfpcr | (fa.code() << sRaShift);
+  emitSW(instr);
+}
+
+
+void Assembler::setfpec0() {
+//  Instr instr = op_setfpec0;
+//  emitSW(instr);
+}
+
+
+void Assembler::setfpec1() {
+//  Instr instr = op_setfpec1;
+//  emitSW(instr);
+}
+
+
+void Assembler::setfpec2() {
+//  Instr instr = op_setfpec2;
+//  emitSW(instr);
+}
+
+
+void Assembler::setfpec3() {
+//  Instr instr = op_setfpec3;
+//  emitSW(instr);
+}
+
+
+void Assembler::frecs(FloatRegister fa, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  DCHECK(fa.is_valid() &&  fc.is_valid());
+  Instr instr = op_frecs | (fa.code() << sRaShift) | (fc.code() << sRcShift);
+  emitSW(instr);
+}
+
+
+void Assembler::frecd(FloatRegister fa, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  DCHECK(fa.is_valid() &&  fc.is_valid());
+  Instr instr = op_frecd | (fa.code() << sRaShift) |  (fc.code() << sRcShift);
+  emitSW(instr);
+}
+
+
+void Assembler::fris(FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  f_exclude_same_src_fc(op_fris, fb, fc);
+}
+
+
+void Assembler::fris_g(FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  f_exclude_same_src_fc(op_fris_g, fb, fc);
+}
+
+
+void Assembler::fris_p(FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  f_exclude_same_src_fc(op_fris_p, fb, fc);
+}
+
+
+void Assembler::fris_z(FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  f_exclude_same_src_fc(op_fris_z, fb, fc);
+}
+
+
+void Assembler::fris_n(FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  f_exclude_same_src_fc(op_fris_n, fb, fc);
+}
+
+
+void Assembler::frid(FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  f_exclude_same_src_fc(op_frid, fb, fc);
+}
+
+
+void Assembler::frid_g(FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  f_exclude_same_src_fc(op_frid_g, fb, fc);
+}
+
+
+void Assembler::frid_p(FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  f_exclude_same_src_fc(op_frid_p, fb, fc);
+}
+
+
+void Assembler::frid_z(FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  f_exclude_same_src_fc(op_frid_z, fb, fc);
+}
+
+
+void Assembler::frid_n(FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  f_exclude_same_src_fc(op_frid_n, fb, fc);
+}
+
+
+void Assembler::fmas(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_fmas, fa, fb, f3, fc);
+}
+
+
+void Assembler::fmad(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_fmad, fa, fb, f3, fc);
+}
+
+
+void Assembler::fmss(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_fmss, fa, fb, f3, fc);
+}
+
+
+void Assembler::fmsd(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_fmsd, fa, fb, f3, fc);
+}
+
+
+void Assembler::fnmas(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_fnmas, fa, fb, f3, fc);
+}
+
+
+void Assembler::fnmad(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_fnmad, fa, fb, f3, fc);
+}
+
+
+void Assembler::fnmss(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_fnmss, fa, fb, f3, fc);
+}
+
+
+void Assembler::fnmsd(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_fnmsd, fa, fb, f3, fc);
+}
+
+
+void Assembler::fseleq(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_fseleq, fa, fb, f3, fc);
+}
+
+
+void Assembler::fselne(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_fselne, fa, fb, f3, fc);
+}
+
+
+void Assembler::fsellt(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_fsellt, fa, fb, f3, fc);
+}
+
+
+void Assembler::fselle(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_fselle, fa, fb, f3, fc);
+}
+
+
+void Assembler::fselgt(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_fselgt, fa, fb, f3, fc);
+}
+
+
+void Assembler::fselge(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_fselge, fa, fb, f3, fc);
+}
+
+
+void Assembler::vaddw(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  GenInstrFR_SW(op_vaddw, fa, fb, fc);
+}
+
+
+void Assembler::vaddw(FloatRegister fa, int lit, FloatRegister fc) {
+  GenInstrFR_SW(op_vaddw_l, fa, lit, fc);
+}
+
+
+void Assembler::vsubw(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  GenInstrFR_SW(op_vsubw, fa, fb, fc);
+}
+
+
+void Assembler::vsubw(FloatRegister fa, int lit, FloatRegister fc) {
+  GenInstrFR_SW(op_vsubw_l, fa, lit, fc);
+}
+
+
+void Assembler::vcmpgew(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  GenInstrFR_SW(op_vcmpgew, fa, fb, fc);
+}
+
+
+void Assembler::vcmpgew(FloatRegister fa, int lit, FloatRegister fc) {
+  GenInstrFR_SW(op_vcmpgew_l, fa, lit, fc);
+}
+
+
+void Assembler::vcmpeqw(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  GenInstrFR_SW(op_vcmpeqw, fa, fb, fc);
+}
+
+
+void Assembler::vcmpeqw(FloatRegister fa, int lit, FloatRegister fc) {
+  GenInstrFR_SW(op_vcmpeqw_l, fa, lit, fc);
+}
+
+
+void Assembler::vcmplew(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  GenInstrFR_SW(op_vcmplew, fa, fb, fc);
+}
+
+
+void Assembler::vcmplew(FloatRegister fa, int lit, FloatRegister fc) {
+  GenInstrFR_SW(op_vcmplew_l, fa, lit, fc);
+}
+
+
+void Assembler::vcmpltw(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  GenInstrFR_SW(op_vcmpltw, fa, fb, fc);
+}
+
+
+void Assembler::vcmpltw(FloatRegister fa, int lit, FloatRegister fc) {
+  GenInstrFR_SW(op_vcmpltw_l, fa, lit, fc);
+}
+
+
+void Assembler::vcmpulew(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  GenInstrFR_SW(op_vcmpulew, fa, fb, fc);
+}
+
+
+void Assembler::vcmpulew(FloatRegister fa, int lit, FloatRegister fc) {
+  GenInstrFR_SW(op_vcmpulew_l, fa, lit, fc);
+}
+
+
+void Assembler::vcmpultw(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  GenInstrFR_SW(op_vcmpultw, fa, fb, fc);
+}
+
+
+void Assembler::vcmpultw(FloatRegister fa, int lit, FloatRegister fc) {
+  GenInstrFR_SW(op_vcmpultw_l, fa, lit, fc);
+}
+
+
+void Assembler::vsllw(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  GenInstrFR_SW(op_vsllw, fa, fb, fc);
+}
+
+
+void Assembler::vsllw(FloatRegister fa, int lit, FloatRegister fc) {
+  GenInstrFR_SW(op_vsllw_l, fa, lit, fc);
+}
+
+
+void Assembler::vsrlw(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  GenInstrFR_SW(op_vsrlw, fa, fb, fc);
+}
+
+
+void Assembler::vsrlw(FloatRegister fa, int lit, FloatRegister fc) {
+  GenInstrFR_SW(op_vsrlw_l, fa, lit, fc);
+}
+
+
+void Assembler::vsraw(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  GenInstrFR_SW(op_vsraw, fa, fb, fc);
+}
+
+
+void Assembler::vsraw(FloatRegister fa, int lit, FloatRegister fc) {
+  GenInstrFR_SW(op_vsraw_l, fa, lit, fc);
+}
+
+
+void Assembler::vrolw(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  GenInstrFR_SW(op_vrolw, fa, fb, fc);
+}
+
+
+void Assembler::vrolw(FloatRegister fa, int lit, FloatRegister fc) {
+  GenInstrFR_SW(op_vrolw_l, fa, lit, fc);
+}
+
+
+void Assembler::sllow(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  GenInstrFR_SW(op_sllow, fa, fb, fc);
+}
+
+
+void Assembler::sllow(FloatRegister fa, int lit, FloatRegister fc) {
+  GenInstrFR_SW(op_sllow_l, fa, lit, fc);
+}
+
+
+void Assembler::srlow(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  GenInstrFR_SW(op_srlow, fa, fb, fc);
+}
+
+
+void Assembler::srlow(FloatRegister fa, int lit, FloatRegister fc) {
+  GenInstrFR_SW(op_srlow_l, fa, lit, fc);
+}
+
+
+void Assembler::vaddl(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  GenInstrFR_SW(op_vaddl, fa, fb, fc);
+}
+
+
+void Assembler::vaddl(FloatRegister fa, int lit, FloatRegister fc) {
+  GenInstrFR_SW(op_vaddl_l, fa, lit, fc);
+}
+
+
+void Assembler::vsubl(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  GenInstrFR_SW(op_vsubl, fa, fb, fc);
+}
+
+
+void Assembler::vsubl(FloatRegister fa, int lit, FloatRegister fc) {
+  GenInstrFR_SW(op_vsubl_l, fa, lit, fc);
+}
+
+
+void Assembler::vsllb(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vsllb, fa, fb, fc);
+}
+
+
+void Assembler::vsllb(FloatRegister fa, int lit, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vsllb_l, fa, lit, fc);
+}
+
+
+void Assembler::vsrlb(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vsrlb, fa, fb, fc);
+}
+
+
+void Assembler::vsrlb(FloatRegister fa, int lit, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vsrlb_l, fa, lit, fc);
+}
+
+
+void Assembler::vsrab(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vsrab, fa, fb, fc);
+}
+
+
+void Assembler::vsrab(FloatRegister fa, int lit, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vsrab_l, fa, lit, fc);
+}
+
+
+void Assembler::vrolb(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vrolb, fa, fb, fc);
+}
+
+
+void Assembler::vrolb(FloatRegister fa, int lit, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vrolb_l, fa, lit, fc);
+}
+
+
+void Assembler::vsllh(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vsllh, fa, fb, fc);
+}
+
+
+void Assembler::vsllh(FloatRegister fa, int lit, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vsllh_l, fa, lit, fc);
+}
+
+
+void Assembler::vsrlh(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vsrlh, fa, fb, fc);
+}
+
+
+void Assembler::vsrlh(FloatRegister fa, int lit, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vsrlh_l, fa, lit, fc);
+}
+
+
+void Assembler::vsrah(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vsrah, fa, fb, fc);
+}
+
+
+void Assembler::vsrah(FloatRegister fa, int lit, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vsrah_l, fa, lit, fc);
+}
+
+
+void Assembler::vrolh(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vrolh, fa, fb, fc);
+}
+
+
+void Assembler::vrolh(FloatRegister fa, int lit, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vrolh_l, fa, lit, fc);
+}
+
+
+void Assembler::ctpopow(FloatRegister fa, FloatRegister fc) {
+  DCHECK(fa.is_valid() &&  fc.is_valid());
+  Instr instr = op_ctpopow | (fa.code() << sRaShift) |  (fc.code() << sRcShift);
+  emitSW(instr);
+}
+
+
+void Assembler::ctlzow (FloatRegister fa, FloatRegister fc) {
+  DCHECK(fa.is_valid() &&  fc.is_valid());
+  Instr instr = op_ctlzow | (fa.code() << sRaShift) |  (fc.code() << sRcShift);
+  emitSW(instr);
+}
+
+
+void Assembler::vslll(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vslll, fa, fb, fc);
+}
+
+
+void Assembler::vslll(FloatRegister fa, int lit, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vslll_l, fa, lit, fc);
+}
+
+
+void Assembler::vsrll(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vsrll, fa, fb, fc);
+}
+
+
+void Assembler::vsrll(FloatRegister fa, int lit, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vsrll_l, fa, lit, fc);
+}
+
+
+void Assembler::vsral(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vsral, fa, fb, fc);
+}
+
+
+void Assembler::vsral(FloatRegister fa, int lit, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vsral_l, fa, lit, fc);
+}
+
+
+void Assembler::vroll(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vroll, fa, fb, fc);
+}
+
+
+void Assembler::vroll(FloatRegister fa, int lit, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vroll_l, fa, lit, fc);
+}
+
+
+void Assembler::vmaxb(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vmaxb, fa, fb, fc);
+}
+
+
+void Assembler::vminb(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vminb, fa, fb, fc);
+}
+
+
+void Assembler::vmas (FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_vmas, fa, fb, f3, fc);
+}
+
+
+void Assembler::vmad (FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_vmad, fa, fb, f3, fc);
+}
+
+
+void Assembler::vmss (FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_vmss, fa, fb, f3, fc);
+}
+
+
+void Assembler::vmsd (FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_vmsd, fa, fb, f3, fc);
+}
+
+
+void Assembler::vnmas(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_vnmas, fa, fb, f3, fc);
+}
+
+
+void Assembler::vnmad(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_vnmad, fa, fb, f3, fc);
+}
+
+
+void Assembler::vnmss(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_vnmss, fa, fb, f3, fc);
+}
+
+
+void Assembler::vnmsd(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_vnmsd, fa, fb, f3, fc);
+}
+
+
+void Assembler::vfseleq(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_vfseleq, fa, fb, f3, fc);
+}
+
+
+void Assembler::vfsellt(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_vfsellt, fa, fb, f3, fc);
+}
+
+
+void Assembler::vfselle(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_vfselle, fa, fb, f3, fc);
+}
+
+
+void Assembler::vseleqw(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_vseleqw, fa, fb, f3, fc);
+}
+
+
+void Assembler::vseleqw(FloatRegister fa, FloatRegister fb, int fmalit, FloatRegister fc) {
+  GenInstrFMA_SW(op_vseleqw_l, fa, fb, fmalit, fc);
+}
+
+
+void Assembler::vsellbcw(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_vsellbcw, fa, fb, f3, fc);
+}
+
+
+void Assembler::vsellbcw(FloatRegister fa, FloatRegister fb, int fmalit, FloatRegister fc) {
+  GenInstrFMA_SW(op_vsellbcw_l, fa, fb, fmalit, fc);
+}
+
+
+void Assembler::vselltw(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_vselltw, fa, fb, f3, fc);
+}
+
+
+void Assembler::vselltw(FloatRegister fa, FloatRegister fb, int fmalit, FloatRegister fc) {
+  GenInstrFMA_SW(op_vselltw_l, fa, fb, fmalit, fc);
+}
+
+
+void Assembler::vsellew(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_vsellew, fa, fb, f3, fc);
+}
+
+
+void Assembler::vsellew(FloatRegister fa, FloatRegister fb, int fmalit, FloatRegister fc) {
+  GenInstrFMA_SW(op_vsellew_l, fa, fb, fmalit, fc);
+}
+
+
+void Assembler::vinsw(FloatRegister fa, FloatRegister fb, int fmalit, FloatRegister fc) {
+  GenInstrFMA_SW(op_vinsw_l, fa, fb, fmalit, fc);
+}
+
+
+void Assembler::vinsf(FloatRegister fa, FloatRegister fb, int fmalit, FloatRegister fc) {
+  GenInstrFMA_SW(op_vinsf_l, fa, fb, fmalit, fc);
+}
+
+
+void Assembler::vextw(FloatRegister fa, int fmalit, FloatRegister fc) {
+  GenInstrFMA_SW(op_vextw_l, fa, fmalit, fc);
+}
+
+
+void Assembler::vextf(FloatRegister fa, int fmalit, FloatRegister fc) {
+  GenInstrFMA_SW(op_vextf_l, fa, fmalit, fc);
+}
+
+
+void Assembler::vcpyw(FloatRegister fa, FloatRegister fc) {
+  DCHECK(fa.is_valid() &&  fc.is_valid());
+  Instr instr = op_vcpyw | (fa.code() << sRaShift) |  (fc.code() << sRcShift);
+  emitSW(instr);
+}
+
+
+void Assembler::vcpyf(FloatRegister fa, FloatRegister fc) {
+  DCHECK(fa.is_valid() &&  fc.is_valid());
+  Instr instr = op_vcpyf | (fa.code() << sRaShift) |  (fc.code() << sRcShift);
+  emitSW(instr);
+}
+
+
+void Assembler::vconw(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_vconw, fa, fb, f3, fc);
+}
+
+
+void Assembler::vshfw(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_vshfw, fa, fb, f3, fc);
+}
+
+
+void Assembler::vcons(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_vcons, fa, fb, f3, fc);
+}
+
+
+void Assembler::vcond(FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc) {
+  GenInstrFMA_SW(op_vcond, fa, fb, f3, fc);
+}
+
+
+void Assembler::vinsb(FloatRegister fa, FloatRegister fb, int fmalit, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFMA_SW(op_vinsb_l, fa, fb, fmalit, fc);
+}
+
+
+void Assembler::vinsh(FloatRegister fa, FloatRegister fb, int fmalit, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFMA_SW(op_vinsh_l, fa, fb, fmalit, fc);
+}
+
+
+void Assembler::vinsectlh(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFMA_SW(op_vinsectlh, fa, fb, fc);
+}
+
+
+void Assembler::vinsectlw(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFMA_SW(op_vinsectlw, fa, fb, fc);
+}
+
+
+void Assembler::vinsectll(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFMA_SW(op_vinsectll, fa, fb, fc);
+}
+
+
+void Assembler::vinsectlb(FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFMA_SW(op_vinsectlb, fa, fb, fc);
+}
+
+
+void Assembler::vshfq(FloatRegister fa, FloatRegister fb, int fmalit, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFMA_SW(op_vshfq_l, fa, fb, fmalit, fc);
+}
+
+
+void Assembler::vshfqb (FloatRegister fa, FloatRegister fb, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFMA_SW(op_vshfqb, fa, fb, fc);
+}
+
+
+void Assembler::vcpyb(FloatRegister fa, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  DCHECK(fa.is_valid() &&  fc.is_valid());
+  Instr instr = op_vcpyb | (fa.code() << sRaShift) |  (fc.code() << sRcShift);
+  emitSW(instr);
+}
+
+
+void Assembler::vcpyh(FloatRegister fa, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  DCHECK(fa.is_valid() &&  fc.is_valid());
+  Instr instr = op_vcpyh | (fa.code() << sRaShift) |  (fc.code() << sRcShift);
+  emitSW(instr);
+}
+
+
+void Assembler::vsm3r(FloatRegister fa, FloatRegister fb, int fmalit, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFMA_SW(op_vsm3r_l, fa, fb, fmalit, fc);
+}
+
+
+void Assembler::vfcvtsh(FloatRegister fa, FloatRegister fb, int fmalit, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vfcvtsh_l, fa, fb, fmalit, fc);
+}
+
+
+void Assembler::vfcvths(FloatRegister fa, FloatRegister fb, int fmalit, FloatRegister fc) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrFR_SW(op_vfcvths_l, fa, fb, fmalit, fc);
+}
+
+
+void Assembler::vldw_u(FloatRegister fa, int atmdisp, Register Rb) {
+  GenInstrSIMD_SW (op_vldw_u, fa, atmdisp, Rb );
+}
+
+
+void Assembler::vstw_u(FloatRegister fa, int atmdisp, Register Rb) {
+  GenInstrSIMD_SW (op_vstw_u, fa, atmdisp, Rb );
+}
+
+
+void Assembler::vlds_u(FloatRegister fa, int atmdisp, Register Rb) {
+  GenInstrSIMD_SW (op_vlds_u, fa, atmdisp, Rb );
+}
+
+
+void Assembler::vsts_u(FloatRegister fa, int atmdisp, Register Rb) {
+  GenInstrSIMD_SW (op_vsts_u, fa, atmdisp, Rb );
+}
+
+
+void Assembler::vldd_u(FloatRegister fa, int atmdisp, Register Rb) {
+  GenInstrSIMD_SW (op_vldd_u, fa, atmdisp, Rb );
+}
+
+
+void Assembler::vstd_u(FloatRegister fa, int atmdisp, Register Rb) {
+  GenInstrSIMD_SW (op_vstd_u, fa, atmdisp, Rb );
+}
+
+
+void Assembler::vstw_ul(FloatRegister fa, int atmdisp, Register Rb) {
+  GenInstrSIMD_SW (op_vstw_ul, fa, atmdisp, Rb );
+}
+
+
+void Assembler::vstw_uh(FloatRegister fa, int atmdisp, Register Rb) {
+  GenInstrSIMD_SW (op_vstw_uh, fa, atmdisp, Rb );
+}
+
+
+void Assembler::vsts_ul(FloatRegister fa, int atmdisp, Register Rb) {
+  GenInstrSIMD_SW (op_vsts_ul, fa, atmdisp, Rb );
+}
+
+
+void Assembler::vsts_uh(FloatRegister fa, int atmdisp, Register Rb) {
+  GenInstrSIMD_SW (op_vsts_uh, fa, atmdisp, Rb );
+}
+
+
+void Assembler::vstd_ul(FloatRegister fa, int atmdisp, Register Rb) {
+  GenInstrSIMD_SW (op_vstd_ul, fa, atmdisp, Rb );
+}
+
+
+void Assembler::vstd_uh(FloatRegister fa, int atmdisp, Register Rb) {
+  GenInstrSIMD_SW (op_vstd_uh, fa, atmdisp, Rb );
+}
+
+
+void Assembler::lbr(int palfn) {
+  DCHECK(kArchVariant == kSw64r3);
+  DCHECK(is_int26(palfn));
+  Instr instr = op_sys_call | ( palfn & (( 1 << 26 ) - 1));
+  emitSW(instr);
+}
+
+
+void Assembler::ldbu_a(Register Ra, int atmdisp, Register Rb) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrMWithFun_SW(op_ldbu_a, Ra, atmdisp, Rb);
+}
+
+
+void Assembler::ldhu_a(Register Ra, int atmdisp, Register Rb) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrMWithFun_SW(op_ldhu_a, Ra, atmdisp, Rb);
+}
+
+
+void Assembler::ldw_a(Register Ra, int atmdisp, Register Rb) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrMWithFun_SW(op_ldw_a, Ra, atmdisp, Rb);
+}
+
+
+void Assembler::ldl_a(Register Ra, int atmdisp, Register Rb) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrMWithFun_SW(op_ldl_a, Ra, atmdisp, Rb);
+}
+
+
+void Assembler::stb_a(Register Ra, int atmdisp, Register Rb) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrMWithFun_SW(op_stb_a, Ra, atmdisp, Rb);
+}
+
+
+void Assembler::sth_a(Register Ra, int atmdisp, Register Rb) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrMWithFun_SW(op_sth_a, Ra, atmdisp, Rb);
+}
+
+
+void Assembler::stw_a(Register Ra, int atmdisp, Register Rb) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrMWithFun_SW(op_stw_a, Ra, atmdisp, Rb);
+}
+
+
+void Assembler::stl_a(Register Ra, int atmdisp, Register Rb) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrMWithFun_SW(op_stl_a, Ra, atmdisp, Rb);
+}
+
+
+void Assembler::flds_a(FloatRegister fa, int atmdisp, Register Rb) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrSIMD_SW (op_flds_a, fa, atmdisp, Rb );
+}
+
+
+void Assembler::fldd_a(FloatRegister fa, int atmdisp, Register Rb) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrSIMD_SW (op_fldd_a, fa, atmdisp, Rb );
+}
+
+
+void Assembler::fsts_a(FloatRegister fa, int atmdisp, Register Rb) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrSIMD_SW (op_fsts_a, fa, atmdisp, Rb );
+}
+
+
+void Assembler::fstd_a(FloatRegister fa, int atmdisp, Register Rb) {
+  DCHECK(kArchVariant == kSw64r3);
+  GenInstrSIMD_SW (op_fstd_a, fa, atmdisp, Rb );
+}
+
+
+void Assembler::dpfhr(int th, int atmdisp, Register Rb) {
+  DCHECK(kArchVariant == kSw64r3);
+  DCHECK( is_uint5(th) && is_uint11(atmdisp)  && Rb.is_valid());
+  Instr instr = op_dpfhr | ((th << sRaShift) & sRaFieldMask) | ((atmdisp << sImm11Shift) & sImm11Mask)
+              | (Rb.code() << sRbShift);
+  emitSW(instr);
+}
+
+
+void Assembler::dpfhw(int th, int atmdisp, Register Rb) {
+  DCHECK(kArchVariant == kSw64r3);
+  DCHECK( is_uint5(th) && is_uint11(atmdisp)  && Rb.is_valid());
+  Instr instr = op_dpfhw | ((th << sRaShift) & sRaFieldMask) | ((atmdisp << sImm11Shift) & sImm11Mask)
+              | (Rb.code() << sRbShift);
+  emitSW(instr);
+}
+
+
+//0x1A.00-0x1c.E  SIMD instructions.
+
+
+void Assembler::ldbu(Register Ra, int mdisp, Register Rb) {
+  GenInstrM_SW(op_ldbu, Ra, mdisp, Rb);
+}
+
+
+void Assembler::ldhu(Register Ra, int mdisp, Register Rb) {
+  GenInstrM_SW(op_ldhu, Ra, mdisp, Rb);
+}
+
+
+void Assembler::ldw(Register Ra, int mdisp, Register Rb) {
+  GenInstrM_SW(op_ldw, Ra, mdisp, Rb);
+}
+
+
+void Assembler::ldl(Register Ra, int mdisp, Register Rb) {
+  GenInstrM_SW(op_ldl, Ra, mdisp, Rb);
+}
+
+
+void Assembler::ldl_u(Register Ra, int mdisp, Register Rb) {
+  GenInstrM_SW(op_ldl_u, Ra, mdisp, Rb);
+}
+
+
+void Assembler::pri_ld(Register Ra, int ev6hwdisp, Register Rb) {
+  UNREACHABLE();
+}
+
+
+void Assembler::flds(FloatRegister fa, int mdisp, Register Rb) {
+  GenInstrFM_SW(op_flds, fa, mdisp, Rb);
+}
+
+
+void Assembler::fldd(FloatRegister fa, int mdisp, Register Rb) {
+  GenInstrFM_SW(op_fldd, fa, mdisp, Rb);
+}
+
+
+void Assembler::stb(Register Ra, int mdisp, Register Rb) {
+  GenInstrM_SW(op_stb, Ra, mdisp, Rb);
+}
+
+
+void Assembler::sth(Register Ra, int mdisp, Register Rb) {
+  GenInstrM_SW(op_sth, Ra, mdisp, Rb);
+}
+
+
+void Assembler::stw(Register Ra, int mdisp, Register Rb) {
+  GenInstrM_SW(op_stw, Ra, mdisp, Rb);
+}
+
+
+void Assembler::stl(Register Ra, int mdisp, Register Rb) {
+  GenInstrM_SW(op_stl, Ra, mdisp, Rb);
+}
+
+
+void Assembler::stl_u(Register Ra, int mdisp, Register Rb) {
+  GenInstrM_SW(op_stl_u, Ra, mdisp, Rb);
+}
+
+
+void Assembler::pri_st(Register Ra, int ev6hwdisp, Register Rb) {
+  UNREACHABLE();
+}
+
+
+void Assembler::fsts(FloatRegister fa, int mdisp, Register Rb) {
+  GenInstrFM_SW(op_fsts, fa, mdisp, Rb);
+}
+
+
+void Assembler::fstd(FloatRegister fa, int mdisp, Register Rb) {
+  GenInstrFM_SW(op_fstd, fa, mdisp, Rb);
+}
+
+
+void Assembler::beq(Register Ra, int bdisp) {
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  GenInstrB_SW(op_beq, Ra, bdisp);
+}
+
+
+void Assembler::bne(Register Ra, int bdisp) {
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  GenInstrB_SW(op_bne, Ra, bdisp);
+}
+
+
+void Assembler::blt(Register Ra, int bdisp) {
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  GenInstrB_SW(op_blt, Ra, bdisp);
+}
+
+
+void Assembler::ble(Register Ra, int bdisp) {
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  GenInstrB_SW(op_ble, Ra, bdisp);
+}
+
+
+void Assembler::bgt(Register Ra, int bdisp) {
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  GenInstrB_SW(op_bgt, Ra, bdisp);
+}
+
+
+void Assembler::bge(Register Ra, int bdisp) {
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  GenInstrB_SW(op_bge, Ra, bdisp);
+}
+
+
+void Assembler::blbc(Register Ra, int bdisp) {
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  GenInstrB_SW(op_blbc, Ra, bdisp);
+}
+
+
+void Assembler::blbs(Register Ra, int bdisp) {
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  GenInstrB_SW(op_blbs, Ra, bdisp);
+}
+
+
+void Assembler::fbeq(FloatRegister fa, int bdisp) {
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  GenInstrFB_SW(op_fbeq, fa, bdisp);
+}
+
+
+void Assembler::fbne(FloatRegister fa, int bdisp) {
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  GenInstrFB_SW(op_fbne, fa, bdisp);
+}
+
+
+void Assembler::fblt(FloatRegister fa, int bdisp) {
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  GenInstrFB_SW(op_fblt, fa, bdisp);
+}
+
+
+void Assembler::fble(FloatRegister fa, int bdisp) {
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  GenInstrFB_SW(op_fble, fa, bdisp);
+}
+
+
+void Assembler::fbgt(FloatRegister fa, int bdisp) {
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  GenInstrFB_SW(op_fbgt, fa, bdisp);
+}
+
+
+void Assembler::fbge(FloatRegister fa, int bdisp) {
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  GenInstrFB_SW(op_fbge, fa, bdisp);
+}
+
+
+void Assembler::ldi(Register Ra, int mdisp, Register Rb) {
+  GenInstrM_SW(op_ldi, Ra, mdisp, Rb);
+}
+
+
+void Assembler::ldih(Register Ra, int mdisp, Register Rb) {
+  GenInstrM_SW(op_ldih, Ra, mdisp, Rb);
+}
+
+
+// cache control instruction
+void Assembler::s_fillcs(int mdisp, Register Rb) {
+  ldw(zero_reg, mdisp, Rb);
+}
+
+
+void Assembler::s_fillde(int mdisp, Register Rb) {
+  ldl(zero_reg, mdisp, Rb);
+}
+
+
+void Assembler::fillde(int mdisp, Register Rb) {
+  flds(f31, mdisp, Rb);
+}
+
+
+void Assembler::fillde_e(int mdisp, Register Rb) {
+  fldd(f31, mdisp, Rb);
+}
+
+
+void Assembler::fillcs(int mdisp, Register Rb) {
+  ldwe(f31, mdisp, Rb);
+}
+
+
+void Assembler::fillcs_e(int mdisp, Register Rb) {
+  ldde(f31, mdisp, Rb);
+}
+
+
+void Assembler::e_fillcs(int mdisp, Register Rb) {
+  ldse(f31, mdisp, Rb);
+}
+
+
+void Assembler::e_fillde(int mdisp, Register Rb) {
+  vlds(f31/*V31*/, mdisp, Rb);
+}
+
+
+void Assembler::flushd(int mdisp, Register Rb) {
+  ldbu(zero_reg, mdisp, Rb);
+}
+
+
+void Assembler::evictdl(int mdisp, Register Rb) {
+  ldl_u(zero_reg, mdisp, Rb);
+}
+
+
+void Assembler::evictdg(int mdisp, Register Rb) {
+  ldhu(zero_reg, mdisp, Rb);
+}
+
+void Assembler::ldb(Register Ra, const MemOperand& rs) {  // sw add
+  ldbu(Ra, rs);
+  sextb(Ra, Ra);
+}
+
+
+// Helper for base-reg + offset, when offset is larger than int16.
+void Assembler::SwLoadRegPlusOffsetToAt(const MemOperand& src) {
+  DCHECK(src.rm() != at);
+  DCHECK(is_int32(src.offset_));
+
+  int16_t lo_offset = static_cast<int16_t>(src.offset_);
+  int16_t hi_offset = (src.offset_-(int16_t)src.offset_) >> 16; 
+  if ( ((int32_t)hi_offset == -32768) && ((int32_t)lo_offset < 0) ) {
+    // range from 0x7FFF8000 to 0x7FFFFFFF
+    ldih(at, 0x4000, zero_reg);
+    ldih(at, 0x4000, at);
+    if (lo_offset != 0 )
+      ldi(at, lo_offset, at);
+  } else {
+    ldih(at, hi_offset, zero_reg);
+    if (lo_offset != 0 )
+      ldi(at, lo_offset, at);
+  }
+  addl(src.rm(), at, at);  // Add base register.
+}
+
+
+void Assembler::ldbu(Register Ra, const MemOperand& rs) {
+  if (is_int16(rs.offset_)) {
+    GenInstrM_SW(op_ldbu, Ra, rs.offset_, rs.rm());
+  } else {  // Offset > 16 bits, use multiple instructions to load.
+    SwLoadRegPlusOffsetToAt(rs);
+    GenInstrM_SW(op_ldbu, Ra, 0, at);
+  }
+}
+
+
+void Assembler::ldh(Register Ra, const MemOperand& rs) {  // sw add
+  ldhu(Ra, rs);
+  sexth(Ra, Ra);
+}
+
+
+void Assembler::ldhu(Register Ra, const MemOperand& rs) {
+  if (is_int16(rs.offset_)) {
+    GenInstrM_SW(op_ldhu, Ra, rs.offset_, rs.rm());
+  } else {  // Offset > 16 bits, use multiple instructions to load.
+    SwLoadRegPlusOffsetToAt(rs);
+    GenInstrM_SW(op_ldhu, Ra, 0, at);
+  }
+}
+
+
+void Assembler::ldw(Register Ra, const MemOperand& rs) {
+  if (is_int16(rs.offset_)) {
+    GenInstrM_SW(op_ldw, Ra, rs.offset_, rs.rm());
+  } else {  // Offset > 16 bits, use multiple instructions to load.
+    SwLoadRegPlusOffsetToAt(rs);
+    GenInstrM_SW(op_ldw, Ra, 0, at);  // Equiv to ldw(rd, MemOperand(at, 0));
+  }
+}
+
+
+void Assembler::ldwu(Register Ra, const MemOperand& rs) {  // sw add
+  ldw(Ra, rs);
+  zapnot(Ra, 0xf, Ra);
+}
+
+
+void Assembler::ldl(Register Ra, const MemOperand& rs) {
+  if (is_int16(rs.offset_)) {
+    GenInstrM_SW(op_ldl, Ra, rs.offset_, rs.rm());
+  } else {  // Offset > 16 bits, use multiple instructions to load.
+    SwLoadRegPlusOffsetToAt(rs);
+    GenInstrM_SW(op_ldl, Ra, 0, at);
+  }
+}
+
+
+void Assembler::flds(FloatRegister fa, const MemOperand& rs) {
+  if (is_int16(rs.offset_)) {
+    GenInstrFM_SW(op_flds, fa, rs.offset_, rs.rm());
+  } else {  // Offset > 16 bits, use multiple instructions to load.
+    SwLoadRegPlusOffsetToAt(rs);
+    GenInstrFM_SW(op_flds, fa, 0, at);
+  }
+}
+
+
+void Assembler::fldd(FloatRegister fa, const MemOperand& rs) {
+  if (is_int16(rs.offset_)) {
+    GenInstrFM_SW(op_fldd, fa, rs.offset_, rs.rm());
+  } else {  // Offset > 16 bits, use multiple instructions to load.
+    SwLoadRegPlusOffsetToAt(rs);
+    GenInstrFM_SW(op_fldd, fa, 0, at);
+  }
+}
+
+
+
+void Assembler::stb(Register Ra, const MemOperand& rs) {
+  if (is_int16(rs.offset_)) {
+    GenInstrM_SW(op_stb, Ra, rs.offset_, rs.rm());
+  } else {  // Offset > 16 bits, use multiple instructions to load.
+    SwLoadRegPlusOffsetToAt(rs);
+    GenInstrM_SW(op_stb, Ra, 0, at);
+  }
+}
+
+
+void Assembler::sth(Register Ra, const MemOperand& rs) {
+  if (is_int16(rs.offset_)) {
+    GenInstrM_SW(op_sth, Ra, rs.offset_, rs.rm());
+  } else {  // Offset > 16 bits, use multiple instructions to load.
+    SwLoadRegPlusOffsetToAt(rs);
+    GenInstrM_SW(op_sth, Ra, 0, at);
+  }
+}
+
+
+void Assembler::stw(Register Ra, const MemOperand& rs) {
+  if (is_int16(rs.offset_)) {
+    GenInstrM_SW(op_stw, Ra, rs.offset_, rs.rm());
+  } else {  // Offset > 16 bits, use multiple instructions to load.
+    SwLoadRegPlusOffsetToAt(rs);
+    GenInstrM_SW(op_stw, Ra, 0, at);
+  }
+}
+
+
+void Assembler::stl(Register Ra, const MemOperand& rs) {
+  if (is_int16(rs.offset_)) {
+    GenInstrM_SW(op_stl, Ra, rs.offset_, rs.rm());
+  } else {  // Offset > 16 bits, use multiple instructions to load.
+    SwLoadRegPlusOffsetToAt(rs);
+    GenInstrM_SW(op_stl, Ra, 0, at);
+  }
+}
+
+
+void Assembler::fsts(FloatRegister fa, const MemOperand& rs) {
+  if (is_int16(rs.offset_)) {
+    GenInstrFM_SW(op_fsts, fa, rs.offset_, rs.rm());
+  } else {  // Offset > 16 bits, use multiple instructions to load.
+    SwLoadRegPlusOffsetToAt(rs);
+    GenInstrFM_SW(op_fsts, fa, 0, at);
+  }
+}
+
+
+void Assembler::fstd(FloatRegister fa, const MemOperand& rs) {
+  if (is_int16(rs.offset_)) {
+    GenInstrFM_SW(op_fstd, fa, rs.offset_, rs.rm());
+  } else {  // Offset > 16 bits, use multiple instructions to load.
+    SwLoadRegPlusOffsetToAt(rs);
+    GenInstrFM_SW(op_fstd, fa, 0, at);
+  }
+}
+#endif
+
+
+int Assembler::RelocateInternalReference(RelocInfo::Mode rmode, Address pc,
+                                         intptr_t pc_delta) {
+  if (RelocInfo::IsInternalReference(rmode)) {
+    int64_t* p = reinterpret_cast<int64_t*>(pc);
+    if (*p == kEndOfJumpChain) {
+      return 0;  // Number of instructions patched.
+    }
+    *p += pc_delta;
+    return 2;  // Number of instructions patched.
+  }
+  Instr instr = instr_at(pc);
+  DCHECK(RelocInfo::IsInternalReferenceEncoded(rmode));
+#ifdef SW64
+  // br in BranchLong
+  DCHECK(IsBranch(instr));
+  return 0;
+#else
+  if (IsLui(instr)) {
+    Instr instr_lui = instr_at(pc + 0 * kInstrSize);
+    Instr instr_ori = instr_at(pc + 1 * kInstrSize);
+    Instr instr_ori2 = instr_at(pc + 3 * kInstrSize);
+    DCHECK(IsOri(instr_ori));
+    DCHECK(IsOri(instr_ori2));
+    // TODO(plind): symbolic names for the shifts.
+    int64_t imm = (instr_lui & static_cast<int64_t>(kImm16Mask)) << 48;
+    imm |= (instr_ori & static_cast<int64_t>(kImm16Mask)) << 32;
+    imm |= (instr_ori2 & static_cast<int64_t>(kImm16Mask)) << 16;
+    // Sign extend address.
+    imm >>= 16;
+
+    if (imm == kEndOfJumpChain) {
+      return 0;  // Number of instructions patched.
+    }
+    imm += pc_delta;
+    DCHECK_EQ(imm & 3, 0);
+
+    instr_lui &= ~kImm16Mask;
+    instr_ori &= ~kImm16Mask;
+    instr_ori2 &= ~kImm16Mask;
+
+    instr_at_put(pc + 0 * kInstrSize,
+                 instr_lui | ((imm >> 32) & kImm16Mask));
+    instr_at_put(pc + 1 * kInstrSize,
+                 instr_ori | (imm >> 16 & kImm16Mask));
+    instr_at_put(pc + 3 * kInstrSize,
+                 instr_ori2 | (imm & kImm16Mask));
+    return 4;  // Number of instructions patched.
+  } else if (IsJ(instr) || IsJal(instr)) {
+    // Regular j/jal relocation.
+    uint32_t imm28 = (instr & static_cast<int32_t>(kImm26Mask)) << 2;
+    imm28 += pc_delta;
+    imm28 &= kImm28Mask;
+    instr &= ~kImm26Mask;
+    DCHECK_EQ(imm28 & 3, 0);
+    uint32_t imm26 = static_cast<uint32_t>(imm28 >> 2);
+    instr_at_put(pc, instr | (imm26 & kImm26Mask));
+    return 1;  // Number of instructions patched.
+  } else {
+    DCHECK(((instr & kJumpRawMask) == kJRawMark) ||
+           ((instr & kJumpRawMask) == kJalRawMark));
+    // Unbox raw offset and emit j/jal.
+    int32_t imm28 = (instr & static_cast<int32_t>(kImm26Mask)) << 2;
+    // Sign extend 28-bit offset to 32-bit.
+    imm28 = (imm28 << 4) >> 4;
+    uint64_t target =
+        static_cast<int64_t>(imm28) + reinterpret_cast<uint64_t>(pc);
+    target &= kImm28Mask;
+    DCHECK_EQ(imm28 & 3, 0);
+    uint32_t imm26 = static_cast<uint32_t>(target >> 2);
+    // Check markings whether to emit j or jal.
+    uint32_t unbox = (instr & kJRawMark) ? J : JAL;
+    instr_at_put(pc, unbox | (imm26 & kImm26Mask));
+    return 1;  // Number of instructions patched.
+  }
+#endif
+}
+
+
+void Assembler::GrowBuffer() {
+  // Compute new buffer size.
+  int old_size = buffer_->size();
+  int new_size = std::min(2 * old_size, old_size + 1 * MB);
+
+  // Some internal data structures overflow for very large buffers,
+  // they must ensure that kMaximalBufferSize is not too large.
+  if (new_size > kMaximalBufferSize) {
+    V8::FatalProcessOutOfMemory(nullptr, "Assembler::GrowBuffer");
+  }
+
+  // Set up new buffer.
+  std::unique_ptr<AssemblerBuffer> new_buffer = buffer_->Grow(new_size);
+  DCHECK_EQ(new_size, new_buffer->size());
+  byte* new_start = new_buffer->start();
+
+  // Copy the data.
+  intptr_t pc_delta = new_start - buffer_start_;
+  intptr_t rc_delta = (new_start + new_size) - (buffer_start_ + old_size);
+  size_t reloc_size = (buffer_start_ + old_size) - reloc_info_writer.pos();
+  MemMove(new_start, buffer_start_, pc_offset());
+  MemMove(reloc_info_writer.pos() + rc_delta, reloc_info_writer.pos(),
+          reloc_size);
+
+  // Switch buffers.
+  buffer_ = std::move(new_buffer);
+  buffer_start_ = new_start;
+  pc_ += pc_delta;
+  reloc_info_writer.Reposition(reloc_info_writer.pos() + rc_delta,
+                               reloc_info_writer.last_pc() + pc_delta);
+
+  // Relocate runtime entries.
+  Vector<byte> instructions{buffer_start_, pc_offset()};
+  Vector<const byte> reloc_info{reloc_info_writer.pos(), reloc_size};
+  for (RelocIterator it(instructions, reloc_info, 0); !it.done(); it.next()) {
+    RelocInfo::Mode rmode = it.rinfo()->rmode();
+    if (rmode == RelocInfo::INTERNAL_REFERENCE) {
+      RelocateInternalReference(rmode, it.rinfo()->pc(), pc_delta);
+    }
+  }
+  DCHECK(!overflow());
+}
+
+
+void Assembler::db(uint8_t data) {
+  CheckForEmitInForbiddenSlot();
+  *reinterpret_cast<uint8_t*>(pc_) = data;
+  pc_ += sizeof(uint8_t);
+}
+
+void Assembler::dd(uint32_t data) {
+  CheckForEmitInForbiddenSlot();
+  *reinterpret_cast<uint32_t*>(pc_) = data;
+  pc_ += sizeof(uint32_t);
+}
+
+void Assembler::dq(uint64_t data) {
+  CheckForEmitInForbiddenSlot();
+  *reinterpret_cast<uint64_t*>(pc_) = data;
+  pc_ += sizeof(uint64_t);
+}
+
+void Assembler::dd(Label* label) {
+  uint64_t data;
+  CheckForEmitInForbiddenSlot();
+  if (label->is_bound()) {
+    data = reinterpret_cast<uint64_t>(buffer_start_ + label->pos());
+  } else {
+    data = jump_address(label);
+    unbound_labels_count_++;
+    internal_reference_positions_.insert(label->pos());
+  }
+  RecordRelocInfo(RelocInfo::INTERNAL_REFERENCE);
+  EmitHelper(data);
+}
+
+
+void Assembler::RecordRelocInfo(RelocInfo::Mode rmode, intptr_t data) {
+  if (!ShouldRecordRelocInfo(rmode)) return;
+  // We do not try to reuse pool constants.
+  RelocInfo rinfo(reinterpret_cast<Address>(pc_), rmode, data, Code());
+  DCHECK_GE(buffer_space(), kMaxRelocSize);  // Too late to grow buffer here.
+  reloc_info_writer.Write(&rinfo);
+}
+
+
+void Assembler::BlockTrampolinePoolFor(int instructions) {
+  CheckTrampolinePoolQuick(instructions);
+  BlockTrampolinePoolBefore(pc_offset() + instructions * kInstrSize);
+}
+
+
+void Assembler::CheckTrampolinePool() {
+  // Some small sequences of instructions must not be broken up by the
+  // insertion of a trampoline pool; such sequences are protected by setting
+  // either trampoline_pool_blocked_nesting_ or no_trampoline_pool_before_,
+  // which are both checked here. Also, recursive calls to CheckTrampolinePool
+  // are blocked by trampoline_pool_blocked_nesting_.
+  if ((trampoline_pool_blocked_nesting_ > 0) ||
+      (pc_offset() < no_trampoline_pool_before_)) {
+    // Emission is currently blocked; make sure we try again as soon as
+    // possible.
+    if (trampoline_pool_blocked_nesting_ > 0) {
+      next_buffer_check_ = pc_offset() + kInstrSize;
+    } else {
+      next_buffer_check_ = no_trampoline_pool_before_;
+    }
+    return;
+  }
+
+  DCHECK(!trampoline_emitted_);
+  DCHECK_GE(unbound_labels_count_, 0);
+  if (unbound_labels_count_ > 0) {
+    // First we emit jump (2 instructions), then we emit trampoline pool.
+    { BlockTrampolinePoolScope block_trampoline_pool(this);
+      Label after_pool;
+      br(&after_pool);
+      nop();
+
+      int pool_start = pc_offset();
+      for (int i = 0; i < unbound_labels_count_; i++) {
+        { BlockGrowBufferScope block_buf_growth(this);
+          // Buffer growth (and relocation) must be blocked for internal
+          // references until associated instructions are emitted and available
+          // to be patched.
+          RecordRelocInfo(RelocInfo::INTERNAL_REFERENCE_ENCODED);
+          br(&after_pool);
+        }
+        nop();
+      }
+      // If unbound_labels_count_ is big enough, label after_pool will
+      // need a trampoline too, so we must create the trampoline before
+      // the bind operation to make sure function 'bind' can get this
+      // information.
+      trampoline_ = Trampoline(pool_start, unbound_labels_count_);
+      bind(&after_pool);
+
+      trampoline_emitted_ = true;
+      // As we are only going to emit trampoline once, we need to prevent any
+      // further emission.
+      next_buffer_check_ = kMaxInt;
+    }
+  } else {
+    // Number of branches to unbound label at this point is zero, so we can
+    // move next buffer check to maximum.
+    next_buffer_check_ = pc_offset() +
+        kMaxBranchOffset - kTrampolineSlotsSize * 16;
+  }
+  return;
+}
+
+
+Address Assembler::target_address_at(Address pc) {
+  Instr instr0 = instr_at(pc);
+ #ifdef DEBUG
+  Instr instr1 = instr_at(pc + kInstrSize);
+ #endif
+  Instr instr2 = instr_at(pc + 2 * kInstrSize);
+  Instr instr3 = instr_at(pc + 3 * kInstrSize);
+
+  DCHECK(GetSwOpcodeField(instr0) == op_ldi);
+  DCHECK(GetSwOpcodeAndFunctionField(instr1) == op_slll_l);
+  DCHECK(GetSwOpcodeField(instr2) == op_ldih);
+  DCHECK(GetSwOpcodeField(instr3) == op_ldi);
+
+  // Interpret 4 instructions generated by set
+  uintptr_t addr;
+  addr  = (instr0 << 16) >> 16;
+  addr  = addr << 32;
+  addr += (instr2 << 16) + ((instr3 << 16) >> 16);
+
+  return static_cast<Address>(addr);
+
+  // We should never get here, force a bad address if we do.
+  UNREACHABLE();
+}
+
+// On Sw64, a target address is stored in a 4-instruction sequence:
+//    0: lui(rd, (j.imm64_ >> 32) & kImm16Mask);
+//    1: ori(rd, rd, (j.imm64_ >> 16) & kImm16Mask);
+//    2: dsll(rd, rd, 16);
+//    3: ori(rd, rd, j.imm32_ & kImm16Mask);
+//
+// Patching the address must replace all the lui & ori instructions,
+// and flush the i-cache.
+//
+// There is an optimization below, which emits a nop when the address
+// fits in just 16 bits. This is unlikely to help, and should be benchmarked,
+// and possibly removed.
+void Assembler::set_target_value_at(Address pc, uint64_t target,
+                                    ICacheFlushMode icache_flush_mode) {
+  // There is an optimization where only 4 instructions are used to load address
+  // in code on MIP64 because only 48-bits of address is effectively used.
+  // It relies on fact the upper [63:48] bits are not used for virtual address
+  // translation and they have to be set according to value of bit 47 in order
+  // get canonical address.
+  uint32_t* p = reinterpret_cast<uint32_t*>(pc);
+  uint64_t itarget = reinterpret_cast<uintptr_t>(target);
+
+#ifdef DEBUG
+  // Check we have the result from a li macro-instruction, using instr pair.
+  Instr instr0 = instr_at(pc);
+  Instr instr1 = instr_at(pc + kInstrSize);
+  Instr instr2 = instr_at(pc + 2 * kInstrSize);
+  Instr instr3 = instr_at(pc + 3 * kInstrSize);
+  CHECK(GetSwOpcodeField(instr0) == op_ldi);
+  CHECK(GetSwOpcodeAndFunctionField(instr1) == op_slll_l);
+  CHECK(GetSwOpcodeField(instr2) == op_ldih);
+  CHECK(GetSwOpcodeField(instr3) == op_ldi);
+#endif
+
+  // Must use 4 instructions to insure patchable code.
+  int32_t lsb32 = (int32_t) (itarget);
+  int32_t msb32 = (int32_t) ((itarget - lsb32) >> 32);
+
+  // Maybe value to "|" is negative, so need set it to 16-bits.
+  *(p+0) = ( *(p+0) & 0xffff0000) | ((int16_t)(msb32 & 0xffff) & 0xffff);
+  *(p+2) = ( *(p+2) & 0xffff0000) | (((lsb32-(int16_t)lsb32)>>16) & 0xffff);
+  *(p+3) = ( *(p+3) & 0xffff0000) | ((int16_t)(lsb32 & 0xffff) & 0xffff);
+
+  if (icache_flush_mode != SKIP_ICACHE_FLUSH) {
+    FlushInstructionCache(pc, 4 * kInstrSize);
+  }
+}
+
+UseScratchRegisterScope::UseScratchRegisterScope(Assembler* assembler)
+    : available_(assembler->GetScratchRegisterList()),
+      old_available_(*available_) {}
+
+UseScratchRegisterScope::~UseScratchRegisterScope() {
+  *available_ = old_available_;
+}
+
+Register UseScratchRegisterScope::Acquire() {
+  DCHECK_NOT_NULL(available_);
+  DCHECK_NE(*available_, 0);
+  int index = static_cast<int>(base::bits::CountTrailingZeros32(*available_));
+  *available_ &= ~(1UL << index);
+
+  return Register::from_code(index);
+}
+
+bool UseScratchRegisterScope::hasAvailable() const { return *available_ != 0; }
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_TARGET_ARCH_SW64
diff --git a/src/3rdparty/chromium/v8/src/codegen/sw64/assembler-sw64.h b/src/3rdparty/chromium/v8/src/codegen/sw64/assembler-sw64.h
new file mode 100755
index 0000000000..02e6bdce53
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/codegen/sw64/assembler-sw64.h
@@ -0,0 +1,1636 @@
+// Copyright (c) 1994-2006 Sun Microsystems Inc.
+// All Rights Reserved.
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are
+// met:
+//
+// - Redistributions of source code must retain the above copyright notice,
+// this list of conditions and the following disclaimer.
+//
+// - Redistribution in binary form must reproduce the above copyright
+// notice, this list of conditions and the following disclaimer in the
+// documentation and/or other materials provided with the distribution.
+//
+// - Neither the name of Sun Microsystems or the names of contributors may
+// be used to endorse or promote products derived from this software without
+// specific prior written permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
+// IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
+// THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+// PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+// CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+// EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+// The original source code covered by the above license above has been
+// modified significantly by Google Inc.
+// Copyright 2012 the V8 project authors. All rights reserved.
+
+#ifndef V8_CODEGEN_SW64_ASSEMBLER_SW64_H_
+#define V8_CODEGEN_SW64_ASSEMBLER_SW64_H_
+
+#include <stdio.h>
+#include <memory>
+#include <set>
+
+#include "src/codegen/assembler.h"
+#include "src/codegen/external-reference.h"
+#include "src/codegen/label.h"
+#include "src/codegen/sw64/constants-sw64.h"
+#include "src/codegen/sw64/register-sw64.h"
+#include "src/objects/contexts.h"
+#include "src/objects/smi.h"
+
+namespace v8 {
+namespace internal {
+
+class SafepointTableBuilder;
+
+// -----------------------------------------------------------------------------
+// Machine instruction Operands.
+constexpr int kSmiShift = kSmiTagSize + kSmiShiftSize;
+constexpr uint64_t kSmiShiftMask = (1UL << kSmiShift) - 1;
+// Class Operand represents a shifter operand in data processing instructions.
+class Operand {
+ public:
+  // Immediate.
+  V8_INLINE explicit Operand(int64_t immediate,
+                             RelocInfo::Mode rmode = RelocInfo::NONE)
+      : rm_(no_reg), rmode_(rmode) {
+    value_.immediate = immediate;
+  }
+  V8_INLINE static Operand Zero() { return Operand(static_cast<intptr_t>(0)); }
+  V8_INLINE explicit Operand(const ExternalReference& f)
+      : rm_(no_reg), rmode_(RelocInfo::EXTERNAL_REFERENCE) {
+    value_.immediate = static_cast<int64_t>(f.address());
+  }
+  V8_INLINE explicit Operand(const char* s);
+  explicit Operand(Handle<HeapObject> handle);
+  V8_INLINE explicit Operand(Smi value) : rm_(no_reg), rmode_(RelocInfo::NONE) {
+    value_.immediate = static_cast<intptr_t>(value.ptr());
+  }
+
+  static Operand EmbeddedNumber(double number);  // Smi or HeapNumber.
+  static Operand EmbeddedStringConstant(const StringConstantBase* str);
+
+  // Register.
+  V8_INLINE explicit Operand(Register rm) : rm_(rm) {}
+
+  // Return true if this is a register operand.
+  V8_INLINE bool is_reg() const;
+
+  inline int64_t immediate() const;
+
+  bool IsImmediate() const { return !rm_.is_valid(); }
+
+  HeapObjectRequest heap_object_request() const {
+    DCHECK(IsHeapObjectRequest());
+    return value_.heap_object_request;
+  }
+
+  bool IsHeapObjectRequest() const {
+    DCHECK_IMPLIES(is_heap_object_request_, IsImmediate());
+    DCHECK_IMPLIES(is_heap_object_request_,
+                   rmode_ == RelocInfo::FULL_EMBEDDED_OBJECT ||
+                       rmode_ == RelocInfo::CODE_TARGET);
+    return is_heap_object_request_;
+  }
+
+  Register rm() const { return rm_; }
+
+  RelocInfo::Mode rmode() const { return rmode_; }
+
+ private:
+  Register rm_;
+  union Value {
+    Value() {}
+    HeapObjectRequest heap_object_request;  // if is_heap_object_request_
+    int64_t immediate;                      // otherwise
+  } value_;                                 // valid if rm_ == no_reg
+  bool is_heap_object_request_ = false;
+  RelocInfo::Mode rmode_;
+
+  friend class Assembler;
+  friend class MacroAssembler;
+};
+
+
+// On SW64 we have only one addressing mode with base_reg + offset.
+// Class MemOperand represents a memory operand in load and store instructions.
+class V8_EXPORT_PRIVATE  MemOperand : public Operand {
+ public:
+  // Immediate value attached to offset.
+  enum OffsetAddend {
+    offset_minus_one = -1,
+    offset_zero = 0
+  };
+
+  explicit MemOperand(Register rn, int32_t offset = 0);
+  explicit MemOperand(Register rn, int32_t unit, int32_t multiplier,
+                      OffsetAddend offset_addend = offset_zero);
+  int32_t offset() const { return offset_; }
+
+  bool OffsetIsInt16Encodable() const {
+    return is_int16(offset_);
+  }
+
+ private:
+  int32_t offset_;
+
+  friend class Assembler;
+};
+
+
+class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
+ public:
+  // Create an assembler. Instructions and relocation information are emitted
+  // into a buffer, with the instructions starting from the beginning and the
+  // relocation information starting from the end of the buffer. See CodeDesc
+  // for a detailed comment on the layout (globals.h).
+  //
+  // If the provided buffer is nullptr, the assembler allocates and grows its
+  // own buffer. Otherwise it takes ownership of the provided buffer.
+  explicit Assembler(const AssemblerOptions&,
+                     std::unique_ptr<AssemblerBuffer> = {});
+
+  virtual ~Assembler() { }
+
+  // GetCode emits any pending (non-emitted) code and fills the descriptor desc.
+  static constexpr int kNoHandlerTable = 0;
+  static constexpr SafepointTableBuilder* kNoSafepointTable = nullptr;
+  void GetCode(Isolate* isolate, CodeDesc* desc,
+               SafepointTableBuilder* safepoint_table_builder,
+               int handler_table_offset);
+
+  // Convenience wrapper for code without safepoint or handler tables.
+  void GetCode(Isolate* isolate, CodeDesc* desc) {
+    GetCode(isolate, desc, kNoSafepointTable, kNoHandlerTable);
+  }
+
+  // Unused on this architecture.
+  void MaybeEmitOutOfLineConstantPool() {}
+
+  // Sw64 uses BlockTrampolinePool to prevent generating trampoline inside a
+  // continuous instruction block. For Call instruction, it prevents generating
+  // trampoline between jalr and delay slot instruction. In the destructor of
+  // BlockTrampolinePool, it must check if it needs to generate trampoline
+  // immediately, if it does not do this, the branch range will go beyond the
+  // max branch offset, that means the pc_offset after call CheckTrampolinePool
+  // may be not the Call instruction's location. So we use last_call_pc here for
+  // safepoint record.
+  int pc_offset_for_safepoint() {
+//SKTODO
+#if 0
+#ifdef DEBUG
+    Instr instr1 =
+        instr_at(static_cast<int>(last_call_pc_ - buffer_start_ - kInstrSize));
+    Instr instr2 = instr_at(
+        static_cast<int>(last_call_pc_ - buffer_start_ - kInstrSize * 2));
+    if (GetOpcodeField(instr1) != SPECIAL) {  // instr1 == jialc.
+      DCHECK(GetOpcodeField(instr1) == POP76 &&
+             GetRs(instr1) == 0);
+    } else {
+      if (GetFunctionField(instr1) == SLL) {  // instr1 == nop, instr2 == jalr.
+        DCHECK(GetOpcodeField(instr2) == SPECIAL &&
+               GetFunctionField(instr2) == JALR);
+      } else {  // instr1 == jalr.
+        DCHECK(GetFunctionField(instr1) == JALR);
+      }
+    }
+#endif
+#endif
+    return static_cast<int>(last_call_pc_ - buffer_start_);
+  }
+
+  // Label operations & relative jumps (PPUM Appendix D).
+  //
+  // Takes a branch opcode (cc) and a label (L) and generates
+  // either a backward branch or a forward branch and links it
+  // to the label fixup chain. Usage:
+  //
+  // Label L;    // unbound label
+  // j(cc, &L);  // forward branch to unbound label
+  // bind(&L);   // bind label to the current pc
+  // j(cc, &L);  // backward branch to bound label
+  // bind(&L);   // illegal: a label may be bound only once
+  //
+  // Note: The same Label can be used for forward and backward branches
+  // but it may be bound only once.
+  void bind(Label* L);  // Binds an unbound label L to current code position.
+
+  enum OffsetSize : int { kOffset26 = 26, kOffset21 = 21, kOffset16 = 16 };
+
+  // Determines if Label is bound and near enough so that branch instruction
+  // can be used to reach it, instead of jump instruction.
+  bool is_near(Label* L);
+  bool is_near(Label* L, OffsetSize bits);
+  bool is_near_branch(Label* L);
+  inline bool is_near_pre_r3(Label* L) {
+    DCHECK(!(kArchVariant == kSw64r3));
+    return pc_offset() - L->pos() < kMaxBranchOffset - 4 * kInstrSize;
+  }
+  inline bool is_near_r3(Label* L) {
+    DCHECK_EQ(kArchVariant, kSw64r3);
+    return pc_offset() - L->pos() < kMaxCompactBranchOffset - 4 * kInstrSize;
+  }
+
+  int BranchOffset(Instr instr);
+
+  // Returns the branch offset to the given label from the current code
+  // position. Links the label to the current position if it is still unbound.
+  // Manages the jump elimination optimization if the second parameter is true.
+  int32_t branch_offset_helper(Label* L, OffsetSize bits);
+  inline int32_t branch_offset(Label* L) {
+    return branch_offset_helper(L, OffsetSize::kOffset21);  // change kOffset21 as default.
+  }
+  inline int32_t branch_offset21(Label* L) {
+    return branch_offset_helper(L, OffsetSize::kOffset21);
+  }
+  inline int32_t branch_offset26(Label* L) {
+    return branch_offset_helper(L, OffsetSize::kOffset26);
+  }
+  inline int32_t shifted_branch_offset(Label* L) {
+    return branch_offset(L) >> 2;
+  }
+  inline int32_t shifted_branch_offset21(Label* L) {
+    return branch_offset21(L) >> 2;
+  }
+  inline int32_t shifted_branch_offset26(Label* L) {
+    return branch_offset26(L) >> 2;
+  }
+  uint64_t jump_address(Label* L);
+  uint64_t jump_offset(Label* L);
+  uint64_t branch_long_offset(Label* L);
+
+  // Puts a labels target address at the given position.
+  // The high 8 bits are set to zero.
+  void label_at_put(Label* L, int at_offset);
+
+  // Read/Modify the code target address in the branch/call instruction at pc.
+  // The isolate argument is unused (and may be nullptr) when skipping flushing.
+  static Address target_address_at(Address pc);
+  V8_INLINE static void set_target_address_at(
+      Address pc, Address target,
+      ICacheFlushMode icache_flush_mode = FLUSH_ICACHE_IF_NEEDED) {
+    set_target_value_at(pc, target, icache_flush_mode);
+  }
+  // On SW64 there is no Constant Pool so we skip that parameter.
+  V8_INLINE static Address target_address_at(Address pc,
+                                             Address constant_pool) {
+    return target_address_at(pc);
+  }
+  V8_INLINE static void set_target_address_at(
+      Address pc, Address constant_pool, Address target,
+      ICacheFlushMode icache_flush_mode = FLUSH_ICACHE_IF_NEEDED) {
+    set_target_address_at(pc, target, icache_flush_mode);
+  }
+
+  static void set_target_value_at(
+      Address pc, uint64_t target,
+      ICacheFlushMode icache_flush_mode = FLUSH_ICACHE_IF_NEEDED);
+
+  static void JumpLabelToJumpRegister(Address pc);
+
+  // This sets the branch destination (which gets loaded at the call address).
+  // This is for calls and branches within generated code.  The serializer
+  // has already deserialized the lui/ori instructions etc.
+  inline static void deserialization_set_special_target_at(
+      Address instruction_payload, Code code, Address target);
+
+  // Get the size of the special target encoded at 'instruction_payload'.
+  inline static int deserialization_special_target_size(
+      Address instruction_payload);
+
+  // This sets the internal reference at the pc.
+  inline static void deserialization_set_target_internal_reference_at(
+      Address pc, Address target,
+      RelocInfo::Mode mode = RelocInfo::INTERNAL_REFERENCE);
+
+  // Difference between address of current opcode and target address offset.
+  static constexpr int kBranchPCOffset = kInstrSize;
+
+  // Difference between address of current opcode and target address offset,
+  // when we are generatinga sequence of instructions for long relative PC
+  // branches
+  //static constexpr int kLongBranchPCOffset = 3 * kInstrSize;
+
+  // Adjust ra register in branch delay slot of bal instruction so to skip
+  // instructions not needed after optimization of PIC in
+  // TurboAssembler::BranchAndLink method.
+
+  //static constexpr int kOptimizedBranchAndLinkLongReturnOffset = 4 * kInstrSize;
+
+  // Here we are patching the address in the LUI/ORI instruction pair.
+  // These values are used in the serialization process and must be zero for
+  // SW64 platform, as Code, Embedded Object or External-reference pointers
+  // are split across two consecutive instructions and don't exist separately
+  // in the code, so the serializer should not step forwards in memory after
+  // a target is resolved and written.
+  static constexpr int kSpecialTargetSize = 0;
+
+  // Number of consecutive instructions used to store 32bit/64bit constant.
+  // This constant was used in RelocInfo::target_address_address() function
+  // to tell serializer address of the instruction that follows
+  // LUI/ORI instruction pair.
+  static constexpr int kInstructionsFor64BitConstant = 4;
+
+  // Difference between address of current opcode and value read from pc
+  // register.
+  static constexpr int kPcLoadDelta = 4;
+
+  // Max offset for instructions with 21-bit offset field
+  static constexpr int kMaxBranchOffset = (1 << (23 - 1)) - 1;
+
+  // Max offset for compact branch instructions with 26-bit offset field
+  static constexpr int kMaxCompactBranchOffset = (1 << (23 - 1)) - 1;
+
+  static constexpr int kTrampolineSlotsSize = 2 * kInstrSize;
+
+  RegList* GetScratchRegisterList() { return &scratch_register_list_; }
+
+  // ---------------------------------------------------------------------------
+  // Code generation.
+
+  // Insert the smallest number of nop instructions
+  // possible to align the pc offset to a multiple
+  // of m. m must be a power of 2 (>= 4).
+  void Align(int m);
+  // Insert the smallest number of zero bytes possible to align the pc offset
+  // to a mulitple of m. m must be a power of 2 (>= 2).
+  void DataAlign(int m);
+  // Aligns code to something that's optimal for a jump target for the platform.
+  void CodeTargetAlign();
+
+  // Different nop operations are used by the code generator to detect certain
+  // states of the generated code.
+  enum NopMarkerTypes {
+    NON_MARKING_NOP = 0,
+    DEBUG_BREAK_NOP,
+    // IC markers.
+    PROPERTY_ACCESS_INLINED,
+    PROPERTY_ACCESS_INLINED_CONTEXT,
+    PROPERTY_ACCESS_INLINED_CONTEXT_DONT_DELETE,
+    // Helper values.
+    LAST_CODE_MARKER,
+    FIRST_IC_MARKER = PROPERTY_ACCESS_INLINED,
+  };
+
+  // Type == 0 is the default non-marking nop. For sw64 this is a
+  // sll(zero_reg, zero_reg, 0). We use rt_reg == at for non-zero
+  // marking, to avoid conflict with ssnop and ehb instructions.
+  void nop(unsigned int type = 0) {
+    DCHECK_EQ(type, 0);
+    ldi(zero_reg, 0, zero_reg);
+  }
+
+
+  // --------Branch-and-jump-instructions----------
+  // We don't use likely variant of instructions.
+  void br(int32_t offset);
+  inline void br(Label* L) { br(shifted_branch_offset(L)); }
+  void bsr(int32_t offset);
+  inline void bsr(Label* L) { bsr(shifted_branch_offset(L)); }
+
+  void fmovd(FPURegister fs, FPURegister fd);
+  void fmovs(FPURegister fs, FPURegister fd);
+  void fnegs(FPURegister fs, FPURegister fd);
+  void fnegd(FPURegister fs, FPURegister fd);
+
+  // Conversion.
+  void fcvtsw(FPURegister fs, FPURegister fd);
+  void fcvtdw(FPURegister fs, FPURegister fd);
+  void ftruncsw(FPURegister fs, FPURegister fd);
+  void ftruncdw(FPURegister fs, FPURegister fd);
+  void froundsw(FPURegister fs, FPURegister fd);
+  void frounddw(FPURegister fs, FPURegister fd);
+  void ffloorsw(FPURegister fs, FPURegister fd);
+  void ffloordw(FPURegister fs, FPURegister fd);
+  void fceilsw(FPURegister fs, FPURegister fd);
+  void fceildw(FPURegister fs, FPURegister fd);
+
+
+  void fcvtsl(FPURegister fs, FPURegister fd);
+  void ftruncsl(FPURegister fs, FPURegister fd);
+  void ftruncdl(FPURegister fs, FPURegister fd);
+  void froundsl(FPURegister fs, FPURegister fd);
+  void frounddl(FPURegister fs, FPURegister fd);
+  void ffloorsl(FPURegister fs, FPURegister fd);
+  void ffloordl(FPURegister fs, FPURegister fd);
+  void fceilsl(FPURegister fs, FPURegister fd);
+  void fceildl(FPURegister fs, FPURegister fd);
+
+  void fcvtws(FPURegister fs, FPURegister fd);
+  void fcvtls_(FPURegister fs, FPURegister fd);
+  void fcvtds_(FPURegister fs, FPURegister fd);
+
+  void fcvtwd(FPURegister fs, FPURegister fd);
+  void fcvtld_(FPURegister fs, FPURegister fd);
+  void fcvtsd_(FPURegister fs, FPURegister fd);
+
+  // Conditions and branches for SW64r3.
+  void cmp(FPUCondition cond, SecondaryField fmt,
+         FPURegister fd, FPURegister ft, FPURegister fs);
+
+  void sld_b(MSARegister wd, MSARegister ws, Register rt);
+  void sld_h(MSARegister wd, MSARegister ws, Register rt);
+  void sld_w(MSARegister wd, MSARegister ws, Register rt);
+  void sld_d(MSARegister wd, MSARegister ws, Register rt);
+  void splat_b(MSARegister wd, MSARegister ws, Register rt);
+  void splat_h(MSARegister wd, MSARegister ws, Register rt);
+  void splat_w(MSARegister wd, MSARegister ws, Register rt);
+  void splat_d(MSARegister wd, MSARegister ws, Register rt);
+
+#ifdef SW64
+  void sys_call_b( int palfn );
+  void sys_call  ( int palfn );
+
+  void call      ( Register Ra, Register Rb, int jmphint );
+  void ret       ( Register Ra, Register Rb, int rethint );
+  void jmp       ( Register Ra, Register Rb, int jmphint );
+
+  void br        ( Register Ra, int bdisp );
+  void bsr       ( Register Ra, int bdisp );
+
+  void memb      ( void );
+  void imemb     ( void );
+  void wmemb     ( void );     //SW6B
+//  void rtc       ( Register Ra );
+  void rtc       ( Register Ra, Register Rb );
+  void rcid      ( Register Ra);
+  void halt      ( void);
+
+  void rd_f      ( Register Ra );       //SW2F
+  void wr_f      ( Register Ra );       //SW2F
+
+  void rtid      ( Register Ra);
+  void csrrs     ( Register Ra, int rpiindex );          //SW6B
+  void csrrc     ( Register Ra, int rpiindex );          //SW6B
+  void csrr      ( Register Ra, int rpiindex );
+  void csrw      ( Register Ra, int rpiindex );
+//  void pri_rcsr  ( Register Ra, int rpiindex );
+//  void pri_wcsr  ( Register Ra, int rpiindex );
+  void pri_ret   ( Register Ra );
+//  void pri_ret_b ( Register Ra );
+
+  void lldw      ( Register Ra, int atmdisp, Register Rb );
+  void lldl      ( Register Ra, int atmdisp, Register Rb );
+  void ldw_inc   ( Register Ra, int atmdisp, Register Rb );     //SW2F
+  void ldl_inc   ( Register Ra, int atmdisp, Register Rb );     //SW2F
+  void ldw_dec   ( Register Ra, int atmdisp, Register Rb );     //SW2F
+  void ldl_dec   ( Register Ra, int atmdisp, Register Rb );     //SW2F
+  void ldw_set   ( Register Ra, int atmdisp, Register Rb );     //SW2F
+  void ldl_set   ( Register Ra, int atmdisp, Register Rb );     //SW2F
+  void lstw      ( Register Ra, int atmdisp, Register Rb );
+  void lstl      ( Register Ra, int atmdisp, Register Rb );
+  void ldw_nc    ( Register Ra, int atmdisp, Register Rb );
+  void ldl_nc    ( Register Ra, int atmdisp, Register Rb );
+  void ldd_nc    ( Register Ra, int atmdisp, Register Rb );
+  void stw_nc    ( Register Ra, int atmdisp, Register Rb );
+  void stl_nc    ( Register Ra, int atmdisp, Register Rb );
+  void std_nc    ( Register Ra, int atmdisp, Register Rb );
+
+  //TODO:  0x8.a-0x8.f *_NC instructions
+
+  // --------Load/Store-instructions-----------------
+  void ldwe      ( FloatRegister fa, int mdisp, Register Rb );
+  void ldse      ( FloatRegister fa, int mdisp, Register Rb );
+  void ldde      ( FloatRegister fa, int mdisp, Register Rb );
+  void vlds      ( FloatRegister fa, int mdisp, Register Rb );
+  void vldd      ( FloatRegister fa, int mdisp, Register Rb );
+  void vsts      ( FloatRegister fa, int mdisp, Register Rb );
+  void vstd      ( FloatRegister fa, int mdisp, Register Rb );
+
+  // --------ALU-instructions-----------------
+  void addw      ( Register Ra, Register Rb, Register Rc );
+  void addw      ( Register Ra, int lit,     Register Rc );
+  void subw      ( Register Ra, Register Rb, Register Rc );
+  void subw      ( Register Ra, int lit,     Register Rc );
+  void s4addw    ( Register Ra, Register Rb, Register Rc );
+  void s4addw    ( Register Ra, int lit,     Register Rc );
+  void s4subw    ( Register Ra, Register Rb, Register Rc );
+  void s4subw    ( Register Ra, int lit,     Register Rc );
+  void s8addw    ( Register Ra, Register Rb, Register Rc );
+  void s8addw    ( Register Ra, int lit,     Register Rc );
+  void s8subw    ( Register Ra, Register Rb, Register Rc );
+  void s8subw    ( Register Ra, int lit,     Register Rc );
+  void addl      ( Register Ra, Register Rb, Register Rc );
+  void addl      ( Register Ra, int lit,     Register Rc );
+  void subl      ( Register Ra, Register Rb, Register Rc );
+  void subl      ( Register Ra, int lit,     Register Rc );
+  void s4addl    ( Register Ra, Register Rb, Register Rc );
+  void s4addl    ( Register Ra, int lit,     Register Rc );
+  void s4subl    ( Register Ra, Register Rb, Register Rc );
+  void s4subl    ( Register Ra, int lit,     Register Rc );
+  void s8addl    ( Register Ra, Register Rb, Register Rc );
+  void s8addl    ( Register Ra, int lit,     Register Rc );
+  void s8subl    ( Register Ra, Register Rb, Register Rc );
+  void s8subl    ( Register Ra, int lit,     Register Rc );
+  void mulw      ( Register Ra, Register Rb, Register Rc );
+  void mulw      ( Register Ra, int lit,     Register Rc );
+  void divw      ( Register Ra, Register Rb, Register Rc );  //SW6B
+  void udivw     ( Register Ra, Register Rb, Register Rc );  //SW6B
+  void remw      ( Register Ra, Register Rb, Register Rc );  //SW6B
+  void uremw     ( Register Ra, Register Rb, Register Rc );  //SW6B
+  void mull      ( Register Ra, Register Rb, Register Rc );
+  void mull      ( Register Ra, int lit,     Register Rc );
+  void umulh     ( Register Ra, Register Rb, Register Rc );
+  void umulh     ( Register Ra, int lit,     Register Rc );
+  void divl      ( Register Ra, Register Rb, Register Rc );  //SW6B
+  void udivl     ( Register Ra, Register Rb, Register Rc );  //SW6B
+  void reml      ( Register Ra, Register Rb, Register Rc );  //SW6B
+  void ureml     ( Register Ra, Register Rb, Register Rc );  //SW6B
+  void addpi     ( int apint,   Register Rc );  //SW6B
+  void addpis    ( int apint,   Register Rc );  //SW6B
+
+  void cmpeq     ( Register Ra, Register Rb, Register Rc );
+  void cmpeq     ( Register Ra, int lit,     Register Rc );
+  void cmplt     ( Register Ra, Register Rb, Register Rc );
+  void cmplt     ( Register Ra, int lit,     Register Rc );
+  void cmple     ( Register Ra, Register Rb, Register Rc );
+  void cmple     ( Register Ra, int lit,     Register Rc );
+  void cmpult    ( Register Ra, Register Rb, Register Rc );
+  void cmpult    ( Register Ra, int lit,     Register Rc );
+  void cmpule    ( Register Ra, Register Rb, Register Rc );
+  void cmpule    ( Register Ra, int lit,     Register Rc );
+  void sbt       ( Register Ra, Register Rb, Register Rc );     //SW6B
+  void sbt       ( Register Ra, int lit,     Register Rc );     //SW6B
+  void cbt       ( Register Ra, Register Rb, Register Rc );     //SW6B
+  void cbt       ( Register Ra, int lit,     Register Rc );     //SW6B
+
+  void and_ins   ( Register Ra, Register Rb, Register Rc );
+  void and_ins   ( Register Ra, int lit,     Register Rc );
+  void bic       ( Register Ra, Register Rb, Register Rc );
+    void andnot  ( Register Ra, Register Rb, Register Rc ); // bic
+  void bic       ( Register Ra, int lit,     Register Rc );
+    void andnot  ( Register Ra, int lit,     Register Rc ); // bic
+  void bis       ( Register Ra, Register Rb, Register Rc );
+    void or_ins  ( Register Ra, Register Rb, Register Rc );
+  void bis       ( Register Ra, int lit,     Register Rc );
+    void or_ins  ( Register Ra, int lit,     Register Rc );
+  void ornot     ( Register Ra, Register Rb, Register Rc );
+  void ornot     ( Register Ra, int lit,     Register Rc );
+  void xor_ins   ( Register Ra, Register Rb, Register Rc );
+  void xor_ins   ( Register Ra, int lit,     Register Rc );
+  void eqv       ( Register Ra, Register Rb, Register Rc );
+  void eqv       ( Register Ra, int lit,     Register Rc );
+
+  // 0x10.40-0x10.47 INS[0-7]B
+  void inslb     ( Register Ra, Register Rb, Register Rc );
+  void inslb     ( Register Ra, int lit,     Register Rc );
+  void inslh     ( Register Ra, Register Rb, Register Rc );
+  void inslh     ( Register Ra, int lit,     Register Rc );
+  void inslw     ( Register Ra, Register Rb, Register Rc );
+  void inslw     ( Register Ra, int lit,     Register Rc );
+  void insll     ( Register Ra, Register Rb, Register Rc );
+  void insll     ( Register Ra, int lit,     Register Rc );
+  void inshb     ( Register Ra, Register Rb, Register Rc );
+  void inshb     ( Register Ra, int lit,     Register Rc );
+  void inshh     ( Register Ra, Register Rb, Register Rc );
+  void inshh     ( Register Ra, int lit,     Register Rc );
+  void inshw     ( Register Ra, Register Rb, Register Rc );
+  void inshw     ( Register Ra, int lit,     Register Rc );
+  void inshl     ( Register Ra, Register Rb, Register Rc );
+  void inshl     ( Register Ra, int lit,     Register Rc );
+
+  void slll      ( Register Ra, Register Rb, Register Rc );
+  void slll      ( Register Ra, int lit,     Register Rc );
+  void srll      ( Register Ra, Register Rb, Register Rc );
+  void srll      ( Register Ra, int lit,     Register Rc );
+  void sral      ( Register Ra, Register Rb, Register Rc );
+  void sral      ( Register Ra, int lit,     Register Rc );
+  void roll      ( Register Ra, Register Rb, Register Rc );      //SW6B
+  void roll      ( Register Ra, int lit,     Register Rc );      //SW6B
+  void sllw      ( Register Ra, Register Rb, Register Rc );      //SW6B
+  void sllw      ( Register Ra, int lit,     Register Rc );      //SW6B
+  void srlw      ( Register Ra, Register Rb, Register Rc );      //SW6B
+  void srlw      ( Register Ra, int lit,     Register Rc );      //SW6B
+  void sraw      ( Register Ra, Register Rb, Register Rc );      //SW6B
+  void sraw      ( Register Ra, int lit,     Register Rc );      //SW6B
+  void rolw      ( Register Ra, Register Rb, Register Rc );      //SW6B
+  void rolw      ( Register Ra, int lit,     Register Rc );      //SW6B
+
+  // 0x10.50-0x10.57 EXT[0-7]B
+  void extlb     ( Register Ra, Register Rb, Register Rc );
+  void extlb     ( Register Ra, int lit,     Register Rc );
+  void extlh     ( Register Ra, Register Rb, Register Rc );
+  void extlh     ( Register Ra, int lit,     Register Rc );
+  void extlw     ( Register Ra, Register Rb, Register Rc );
+  void extlw     ( Register Ra, int lit,     Register Rc );
+  void extll     ( Register Ra, Register Rb, Register Rc );
+  void extll     ( Register Ra, int lit,     Register Rc );
+  void exthb     ( Register Ra, Register Rb, Register Rc );
+  void exthb     ( Register Ra, int lit,     Register Rc );
+  void exthh     ( Register Ra, Register Rb, Register Rc );
+  void exthh     ( Register Ra, int lit,     Register Rc );
+  void exthw     ( Register Ra, Register Rb, Register Rc );
+  void exthw     ( Register Ra, int lit,     Register Rc );
+  void exthl     ( Register Ra, Register Rb, Register Rc );
+  void exthl     ( Register Ra, int lit,     Register Rc );
+
+  void ctpop     (              Register Rb, Register Rc );
+  void ctlz      (              Register Rb, Register Rc );
+  void cttz      (              Register Rb, Register Rc );
+  void revbh     (              Register Rb, Register Rc );      //SW6B
+  void revbw     (              Register Rb, Register Rc );      //SW6B
+  void revbl     (              Register Rb, Register Rc );      //SW6B
+  void casw      ( Register Ra, Register Rb, Register Rc );      //SW6B
+  void casl      ( Register Ra, Register Rb, Register Rc );      //SW6B
+
+  // 0x10.60-0x10.67 MASK[0-7]B
+  void masklb    ( Register Ra, Register Rb, Register Rc );
+  void masklb    ( Register Ra, int lit,     Register Rc );
+  void masklh    ( Register Ra, Register Rb, Register Rc );
+  void masklh    ( Register Ra, int lit,     Register Rc );
+  void masklw    ( Register Ra, Register Rb, Register Rc );
+  void masklw    ( Register Ra, int lit,     Register Rc );
+  void maskll    ( Register Ra, Register Rb, Register Rc );
+  void maskll    ( Register Ra, int lit,     Register Rc );
+  void maskhb    ( Register Ra, Register Rb, Register Rc );
+  void maskhb    ( Register Ra, int lit,     Register Rc );
+  void maskhh    ( Register Ra, Register Rb, Register Rc );
+  void maskhh    ( Register Ra, int lit,     Register Rc );
+  void maskhw    ( Register Ra, Register Rb, Register Rc );
+  void maskhw    ( Register Ra, int lit,     Register Rc );
+  void maskhl    ( Register Ra, Register Rb, Register Rc );
+  void maskhl    ( Register Ra, int lit,     Register Rc );
+
+  void zap       ( Register Ra, Register Rb, Register Rc );
+  void zap       ( Register Ra, int lit,     Register Rc );
+  void zapnot    ( Register Ra, Register Rb, Register Rc );
+  void zapnot    ( Register Ra, int lit,     Register Rc );
+  void sextb     (              Register Rb, Register Rc);
+  void sextb     (              int lit,     Register Rc );
+  void sexth     (              Register Rb, Register Rc );
+  void sexth     (              int lit,     Register Rc );
+  //0x10.6c CMPGEB
+  void cmpgeb    ( Register Ra, Register Rb, Register Rc );
+  void cmpgeb    ( Register Ra, int lit,     Register Rc );
+
+  void fimovs    ( FloatRegister fa,     Register Rc );
+  void fimovd    ( FloatRegister fa,     Register Rc );
+
+  void seleq     ( Register Ra, Register Rb,Register R3, Register Rc );
+  void seleq     ( Register Ra, int lit,    Register R3,Register Rc );
+  void selge     ( Register Ra, Register Rb,Register R3, Register Rc );
+  void selge     ( Register Ra, int lit,    Register R3,Register Rc );
+  void selgt     ( Register Ra, Register Rb,Register R3, Register Rc );
+  void selgt     ( Register Ra, int lit,    Register R3,Register Rc );
+  void selle     ( Register Ra, Register Rb,Register R3, Register Rc );
+  void selle     ( Register Ra, int lit,    Register R3,Register Rc );
+  void sellt     ( Register Ra, Register Rb,Register R3, Register Rc );
+  void sellt     ( Register Ra, int lit,    Register R3,Register Rc );
+  void selne     ( Register Ra, Register Rb,Register R3, Register Rc );
+  void selne     ( Register Ra, int lit,    Register R3,Register Rc );
+  void sellbc    ( Register Ra, Register Rb,Register R3, Register Rc );
+  void sellbc    ( Register Ra, int lit,    Register R3,Register Rc );
+  void sellbs    ( Register Ra, Register Rb,Register R3, Register Rc );
+  void sellbs    ( Register Ra, int lit,    Register R3,Register Rc );
+
+  void vlog      ( int vlog, FloatRegister fa,FloatRegister fb,FloatRegister f3, FloatRegister fc);
+
+  void f_exclude_same_src_fc(Opcode_ops_fp opcode, FloatRegister fa, FloatRegister fb, FloatRegister fc);
+  void f_exclude_same_src_fc(Opcode_ops_fp opcode, FloatRegister fb, FloatRegister fc);
+  void vbisw     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vxorw     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vandw     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void veqvw     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vornotw   ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vbicw     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void fadds     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void faddd     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void fsubs     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void fsubd     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void fmuls     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void fmuld     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void fdivs     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void fdivd     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void fsqrts    (                   FloatRegister fb, FloatRegister fc );
+  void fsqrtd    (                   FloatRegister fb, FloatRegister fc );
+  void fcmpeq    ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void fcmple    ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void fcmplt    ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void fcmpun    ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void fcvtsd    (                   FloatRegister fb, FloatRegister fc );
+  void fcvtds    (                   FloatRegister fb, FloatRegister fc );
+  void fcvtdl_g  (                   FloatRegister fb, FloatRegister fc );
+  void fcvtdl_p  (                   FloatRegister fb, FloatRegister fc );
+  void fcvtdl_z  (                   FloatRegister fb, FloatRegister fc );
+  void fcvtdl_n  (                   FloatRegister fb, FloatRegister fc );
+  void fcvtdl    (                   FloatRegister fb, FloatRegister fc );
+  void fcvtwl    (                   FloatRegister fb, FloatRegister fc );
+  void fcvtlw    (                   FloatRegister fb, FloatRegister fc );
+  void fcvtls    (                   FloatRegister fb, FloatRegister fc );
+  void fcvtld    (                   FloatRegister fb, FloatRegister fc );
+  void fcpys     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void fcpyse    ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void fcpysn    ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void ifmovs    ( Register Ra,                        FloatRegister fc );
+  void ifmovd    ( Register Ra,                        FloatRegister fc );
+  void rfpcr     ( FloatRegister fa);
+  void wfpcr     ( FloatRegister fa);
+  void setfpec0  ();
+  void setfpec1  ();
+  void setfpec2  ();
+  void setfpec3  ();
+
+  void frecs     (                   FloatRegister fa, FloatRegister fc );     //SW6B
+  void frecd     (                   FloatRegister fa, FloatRegister fc );     //SW6B
+  void fris      (                   FloatRegister fb, FloatRegister fc );     //SW6B
+  void fris_g    (                   FloatRegister fb, FloatRegister fc );     //SW6B
+  void fris_p    (                   FloatRegister fb, FloatRegister fc );     //SW6B
+  void fris_z    (                   FloatRegister fb, FloatRegister fc );     //SW6B
+  void fris_n    (                   FloatRegister fb, FloatRegister fc );     //SW6B
+  void frid      (                   FloatRegister fb, FloatRegister fc );     //SW6B
+  void frid_g    (                   FloatRegister fb, FloatRegister fc );     //SW6B
+  void frid_p    (                   FloatRegister fb, FloatRegister fc );     //SW6B
+  void frid_z    (                   FloatRegister fb, FloatRegister fc );     //SW6B
+  void frid_n    (                   FloatRegister fb, FloatRegister fc );     //SW6B
+  void fmas      ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void fmad      ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void fmss      ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void fmsd      ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void fnmas     ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void fnmad     ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void fnmss     ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void fnmsd     ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+
+  void fseleq    ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void fselne    ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void fsellt    ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void fselle    ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void fselgt    ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void fselge    ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+
+  void vaddw     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vaddw     ( FloatRegister fa, int lit,          FloatRegister fc );
+  void vsubw     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vsubw     ( FloatRegister fa, int lit,          FloatRegister fc );
+  void vcmpgew   ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vcmpgew   ( FloatRegister fa, int lit,          FloatRegister fc );
+  void vcmpeqw   ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vcmpeqw   ( FloatRegister fa, int lit,          FloatRegister fc );
+  void vcmplew   ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vcmplew   ( FloatRegister fa, int lit,          FloatRegister fc );
+  void vcmpltw   ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vcmpltw   ( FloatRegister fa, int lit,          FloatRegister fc );
+  void vcmpulew  ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vcmpulew  ( FloatRegister fa, int lit,          FloatRegister fc );
+  void vcmpultw  ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vcmpultw  ( FloatRegister fa, int lit,          FloatRegister fc );
+  void vsllw     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vsllw     ( FloatRegister fa, int lit,          FloatRegister fc );
+  void vsrlw     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vsrlw     ( FloatRegister fa, int lit,          FloatRegister fc );
+  void vsraw     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vsraw     ( FloatRegister fa, int lit,          FloatRegister fc );
+  void vrolw     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vrolw     ( FloatRegister fa, int lit,          FloatRegister fc );
+  void sllow     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void sllow     ( FloatRegister fa, int lit,          FloatRegister fc );
+  void srlow     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void srlow     ( FloatRegister fa, int lit,          FloatRegister fc );
+  void vaddl     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vaddl     ( FloatRegister fa, int lit,          FloatRegister fc );
+  void vsubl     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vsubl     ( FloatRegister fa, int lit,          FloatRegister fc );
+  void vsllb     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vsllb     ( FloatRegister fa, int lit,          FloatRegister fc );      //SW6B
+  void vsrlb     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vsrlb     ( FloatRegister fa, int lit,          FloatRegister fc );      //SW6B
+  void vsrab     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vsrab     ( FloatRegister fa, int lit,          FloatRegister fc );      //SW6B
+  void vrolb     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vrolb     ( FloatRegister fa, int lit,          FloatRegister fc );      //SW6B
+  void vsllh     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vsllh     ( FloatRegister fa, int lit,          FloatRegister fc );      //SW6B
+  void vsrlh     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vsrlh     ( FloatRegister fa, int lit,          FloatRegister fc );      //SW6B
+  void vsrah     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vsrah     ( FloatRegister fa, int lit,          FloatRegister fc );      //SW6B
+  void vrolh     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vrolh     ( FloatRegister fa, int lit,          FloatRegister fc );      //SW6B
+  void ctpopow   ( FloatRegister fa,                   FloatRegister fc );
+  void ctlzow    ( FloatRegister fa,                   FloatRegister fc );
+  void vslll     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vslll     ( FloatRegister fa, int lit,          FloatRegister fc );      //SW6B
+  void vsrll     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vsrll     ( FloatRegister fa, int lit,          FloatRegister fc );      //SW6B
+  void vsral     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vsral     ( FloatRegister fa, int lit,          FloatRegister fc );      //SW6B
+  void vroll     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vroll     ( FloatRegister fa, int lit,          FloatRegister fc );      //SW6B
+  void vmaxb     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vminb     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+
+  // some unimplemented SIMD
+  void vucaddw   ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vucaddw   ( FloatRegister fa, int lit,          FloatRegister fc );
+  void vucsubw   ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vucsubw   ( FloatRegister fa, int lit,          FloatRegister fc );
+  void vucaddh   ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vucaddh   ( FloatRegister fa, int lit,          FloatRegister fc );
+  void vucsubh   ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vucsubh   ( FloatRegister fa, int lit,          FloatRegister fc );
+  void vucaddb   ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vucaddb   ( FloatRegister fa, int lit,          FloatRegister fc );
+  void vucsubb   ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vucsubb   ( FloatRegister fa, int lit,          FloatRegister fc );
+  void sraow     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void sraow     ( FloatRegister fa, int lit,          FloatRegister fc );      //SW6B
+  void vsumw     ( FloatRegister fa,                   FloatRegister fc );      //SW6B
+  void vsuml     ( FloatRegister fa,                   FloatRegister fc );      //SW6B
+  void vcmpueqb  ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vcmpueqb  ( FloatRegister fa, int lit,          FloatRegister fc );      //SW6B
+  void vcmpugtb  ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vcmpugtb  ( FloatRegister fa, int lit,          FloatRegister fc );      //SW6B
+  void vmaxh     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vminh     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vmaxw     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vminw     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vmaxl     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vminl     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vumaxb    ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vuminb    ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vumaxh    ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vuminh    ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vumaxw    ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vuminw    ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vumaxl    ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vuminl    ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+
+  void vsm3msw   ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vsm4key   ( FloatRegister fa, int lit,          FloatRegister fc );      //SW6B
+  void vsm4r     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vbinvw    (                   FloatRegister fb, FloatRegister fc );      //SW6B
+
+  void vadds     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vaddd     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vsubs     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vsubd     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vmuls     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vmuld     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vdivs     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vdivd     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vsqrts    (                   FloatRegister fb, FloatRegister fc );
+  void vsqrtd    (                   FloatRegister fb, FloatRegister fc );
+  void vfcmpeq   ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vfcmple   ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vfcmplt   ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vfcmpun   ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vcpys     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vfmov     ( FloatRegister fa,                   FloatRegister fc );
+  void vcpyse    ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vcpysn    ( FloatRegister fa, FloatRegister fb, FloatRegister fc );
+  void vsums     ( FloatRegister fa,                   FloatRegister fc );      //SW6B
+  void vsumd     ( FloatRegister fa,                   FloatRegister fc );      //SW6B
+  void vfrecs    ( FloatRegister fa,                   FloatRegister fc );      //SW6B
+  void vfrecd    ( FloatRegister fa,                   FloatRegister fc );      //SW6B
+  void vfcvtsd   (                   FloatRegister fb, FloatRegister fc );      //SW6B
+  void vfcvtds   (                   FloatRegister fb, FloatRegister fc );      //SW6B
+  void vfcvtls   (                   FloatRegister fb, FloatRegister fc );      //SW6B
+  void vfcvtld   (                   FloatRegister fb, FloatRegister fc );      //SW6B
+  void vfcvtdl   (                   FloatRegister fb, FloatRegister fc );      //SW6B
+  void vfcvtdl_g (                   FloatRegister fb, FloatRegister fc );      //SW6B
+  void vfcvtdl_p (                   FloatRegister fb, FloatRegister fc );      //SW6B
+  void vfcvtdl_z (                   FloatRegister fb, FloatRegister fc );      //SW6B
+  void vfcvtdl_n (                   FloatRegister fb, FloatRegister fc );      //SW6B
+  void vfris     (                   FloatRegister fb, FloatRegister fc );      //SW6B
+  void vfris_g   (                   FloatRegister fb, FloatRegister fc );      //SW6B
+  void vfris_p   (                   FloatRegister fb, FloatRegister fc );      //SW6B
+  void vfris_z   (                   FloatRegister fb, FloatRegister fc );      //SW6B
+  void vfris_n   (                   FloatRegister fb, FloatRegister fc );      //SW6B
+  void vfrid     (                   FloatRegister fb, FloatRegister fc );      //SW6B
+  void vfrid_g   (                   FloatRegister fb, FloatRegister fc );      //SW6B
+  void vfrid_p   (                   FloatRegister fb, FloatRegister fc );      //SW6B
+  void vfrid_z   (                   FloatRegister fb, FloatRegister fc );      //SW6B
+  void vfrid_n   (                   FloatRegister fb, FloatRegister fc );      //SW6B
+  void vmaxs     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vmins     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vmaxd     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  void vmind     ( FloatRegister fa, FloatRegister fb, FloatRegister fc );      //SW6B
+  // end of unimplemented SIMD
+
+  void vmas      ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void vmad      ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void vmss      ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void vmsd      ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void vnmas     ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void vnmad     ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void vnmss     ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void vnmsd     ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void vfseleq   ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void vfsellt   ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void vfselle   ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void vseleqw   ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void vseleqw   ( FloatRegister fa, FloatRegister fb, int fmalit,       FloatRegister fc );
+  void vsellbcw  ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void vsellbcw  ( FloatRegister fa, FloatRegister fb, int fmalit,       FloatRegister fc );
+  void vselltw   ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void vselltw   ( FloatRegister fa, FloatRegister fb, int fmalit,       FloatRegister fc );
+  void vsellew   ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void vsellew   ( FloatRegister fa, FloatRegister fb, int fmalit,       FloatRegister fc );
+  void vinsw     ( FloatRegister fa, FloatRegister fb, int fmalit,       FloatRegister fc );
+  void vinsf     ( FloatRegister fa, FloatRegister fb, int fmalit,       FloatRegister fc );
+  void vextw     ( FloatRegister fa, int fmalit, FloatRegister fc);
+  void vextf     ( FloatRegister fa, int fmalit, FloatRegister fc);
+  void vcpyw     ( FloatRegister fa,             FloatRegister fc);
+  void vcpyf     ( FloatRegister fa,             FloatRegister fc);
+  void vconw     ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void vshfw     ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void vcons     ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void vcond     ( FloatRegister fa, FloatRegister fb, FloatRegister f3, FloatRegister fc );
+  void vinsb     ( FloatRegister fa, FloatRegister fb, int fmalit,       FloatRegister fc );        //SW6B
+  void vinsh     ( FloatRegister fa, FloatRegister fb, int fmalit,       FloatRegister fc );        //SW6B
+  void vinsectlh ( FloatRegister fa, FloatRegister fb, FloatRegister fc );                          //SW6B
+  void vinsectlw ( FloatRegister fa, FloatRegister fb, FloatRegister fc );                          //SW6B
+  void vinsectll ( FloatRegister fa, FloatRegister fb, FloatRegister fc );                          //SW6B
+  void vinsectlb ( FloatRegister fa, FloatRegister fb, FloatRegister fc );                          //SW6B
+  void vshfq     ( FloatRegister fa, FloatRegister fb, int fmalit,       FloatRegister fc );        //SW6B
+  void vshfqb    ( FloatRegister fa, FloatRegister fb, FloatRegister fc );                          //SW6B
+  void vcpyb     ( FloatRegister fa,                   FloatRegister fc );                          //SW6B
+  void vcpyh     ( FloatRegister fa,                   FloatRegister fc );                          //SW6B
+  void vsm3r     ( FloatRegister fa, FloatRegister fb, int fmalit,       FloatRegister fc );        //SW6B
+  void vfcvtsh   ( FloatRegister fa, FloatRegister fb, int fmalit,       FloatRegister fc );        //SW6B
+  void vfcvths   ( FloatRegister fa, FloatRegister fb, int fmalit,       FloatRegister fc );        //SW6B
+
+  void vldw_u    ( FloatRegister fa, int atmdisp, Register Rb );
+  void vstw_u    ( FloatRegister fa, int atmdisp, Register Rb );
+  void vlds_u    ( FloatRegister fa, int atmdisp, Register Rb );
+  void vsts_u    ( FloatRegister fa, int atmdisp, Register Rb );
+  void vldd_u    ( FloatRegister fa, int atmdisp, Register Rb );
+  void vstd_u    ( FloatRegister fa, int atmdisp, Register Rb );
+  void vstw_ul   ( FloatRegister fa, int atmdisp, Register Rb );
+  void vstw_uh   ( FloatRegister fa, int atmdisp, Register Rb );
+  void vsts_ul   ( FloatRegister fa, int atmdisp, Register Rb );
+  void vsts_uh   ( FloatRegister fa, int atmdisp, Register Rb );
+  void vstd_ul   ( FloatRegister fa, int atmdisp, Register Rb );
+  void vstd_uh   ( FloatRegister fa, int atmdisp, Register Rb );
+  void lbr       ( int palfn );                                 //SW6B
+  void ldbu_a    ( Register Ra, int atmdisp, Register Rb );     //SW6B
+  void ldhu_a    ( Register Ra, int atmdisp, Register Rb );     //SW6B
+  void ldw_a     ( Register Ra, int atmdisp, Register Rb );     //SW6B
+  void ldl_a     ( Register Ra, int atmdisp, Register Rb );     //SW6B
+  void stb_a     ( Register Ra, int atmdisp, Register Rb );     //SW6B
+  void sth_a     ( Register Ra, int atmdisp, Register Rb );     //SW6B
+  void stw_a     ( Register Ra, int atmdisp, Register Rb );     //SW6B
+  void stl_a     ( Register Ra, int atmdisp, Register Rb );     //SW6B
+  void flds_a    ( FloatRegister fa, int atmdisp, Register Rb );        //SW6B
+  void fldd_a    ( FloatRegister fa, int atmdisp, Register Rb );        //SW6B
+  void fsts_a    ( FloatRegister fa, int atmdisp, Register Rb );        //SW6B
+  void fstd_a    ( FloatRegister fa, int atmdisp, Register Rb );        //SW6B
+  void dpfhr     ( int th, int atmdisp, Register Rb );      //SW6B
+  void dpfhw     ( int th, int atmdisp, Register Rb );      //SW6B
+
+  //TODO: 0x1A.00-0x1c.E  SIMD instructions.
+
+
+  void ldbu      ( Register Ra, int mdisp, Register Rb );
+  void ldhu      ( Register Ra, int mdisp, Register Rb );
+  void ldw       ( Register Ra, int mdisp, Register Rb );
+  void ldl       ( Register Ra, int mdisp, Register Rb );
+  void ldl_u     ( Register Ra, int mdisp, Register Rb );
+  void pri_ld    ( Register Ra, int ev6hwdisp, Register Rb );
+  void flds      ( FloatRegister fa, int mdisp, Register Rb );
+  void fldd      ( FloatRegister fa, int mdisp, Register Rb );
+
+  void stb       ( Register Ra, int mdisp, Register Rb );
+  void sth       ( Register Ra, int mdisp, Register Rb );
+  void stw       ( Register Ra, int mdisp, Register Rb );
+  void stl       ( Register Ra, int mdisp, Register Rb );
+  void stl_u     ( Register Ra, int mdisp, Register Rb );
+  void pri_st    ( Register Ra, int ev6hwdisp, Register Rb );
+  void fsts      ( FloatRegister fa, int mdisp, Register Rb );
+  void fstd      ( FloatRegister fa, int mdisp, Register Rb );
+
+  // Branch Instructions.
+  void beq       ( Register Ra, int bdisp );
+  void bne       ( Register Ra, int bdisp );
+  void blt       ( Register Ra, int bdisp );
+  void ble       ( Register Ra, int bdisp );
+  void bgt       ( Register Ra, int bdisp );
+  void bge       ( Register Ra, int bdisp );
+  void blbc      ( Register Ra, int bdisp );
+  void blbs      ( Register Ra, int bdisp );
+  void fbeq      ( FloatRegister fa, int bdisp );
+  void fbne      ( FloatRegister fa, int bdisp );
+  void fblt      ( FloatRegister fa, int bdisp );
+  void fble      ( FloatRegister fa, int bdisp );
+  void fbgt      ( FloatRegister fa, int bdisp );
+  void fbge      ( FloatRegister fa, int bdisp );
+  void ldi       ( Register Ra, int mdisp, Register Rb );
+  void ldih      ( Register Ra, int mdisp, Register Rb );
+
+  // cache control instruction
+  void s_fillcs  (               int mdisp, Register Rb );
+  void s_fillde  (               int mdisp, Register Rb );
+  void fillde    (               int mdisp, Register Rb );
+  void fillde_e  (               int mdisp, Register Rb );
+  void fillcs    (               int mdisp, Register Rb );
+  void fillcs_e  (               int mdisp, Register Rb );
+  void e_fillcs  (               int mdisp, Register Rb );
+  void e_fillde  (               int mdisp, Register Rb );
+  void flushd    (               int mdisp, Register Rb );
+  void evictdl   (               int mdisp, Register Rb );
+  void evictdg   (               int mdisp, Register Rb );
+
+#ifdef SW64
+  // ------------Memory-instructions-------------
+  void ldb       ( Register Ra, const MemOperand& rs );  // sw add
+  void ldbu      ( Register Ra, const MemOperand& rs );
+  void ldh       ( Register Ra, const MemOperand& rs );  // sw add
+  void ldhu      ( Register Ra, const MemOperand& rs );
+  void ldw       ( Register Ra, const MemOperand& rs );
+  void ldwu      ( Register Ra, const MemOperand& rs );  // sw add
+  void ldl       ( Register Ra, const MemOperand& rs );
+  void flds      ( FloatRegister fa, const MemOperand& rs );
+  void fldd      ( FloatRegister fa, const MemOperand& rs );
+
+  void stb       ( Register Ra, const MemOperand& rs );
+  void sth       ( Register Ra, const MemOperand& rs );
+  void stw       ( Register Ra, const MemOperand& rs );
+  void stl       ( Register Ra, const MemOperand& rs );
+  void fsts      ( FloatRegister fa, const MemOperand& rs );
+  void fstd      ( FloatRegister fa, const MemOperand& rs );
+
+  void beq(Register Ra, Label* L) {
+    beq(Ra, branch_offset(L) >> 2);
+  }
+  void bne(Register Ra, Label* L) {
+    bne(Ra, branch_offset(L) >> 2);
+  }
+  void blt(Register Ra, Label* L) {
+    blt(Ra, branch_offset(L) >> 2);
+  }
+  void ble(Register Ra, Label* L) {
+    ble(Ra, branch_offset(L) >> 2);
+  }
+  void bgt(Register Ra, Label* L) {
+    bgt(Ra, branch_offset(L) >> 2);
+  }
+  void bge(Register Ra, Label* L) {
+    bge(Ra, branch_offset(L) >> 2);
+  }
+
+  void fbr(Label* L) {
+    fbeq(f31, branch_offset(L) >> 2);
+  }
+  void fbeq(FloatRegister fa, Label* L) {
+    fbeq(fa, branch_offset(L) >> 2);
+  }
+  void fbne(FloatRegister fa, Label* L) {
+    fbne(fa, branch_offset(L) >> 2);
+  }
+  void fblt(FloatRegister fa, Label* L) {
+    fblt(fa, branch_offset(L) >> 2);
+  }
+  void fble(FloatRegister fa, Label* L) {
+    fble(fa, branch_offset(L) >> 2);
+  }
+  void fbgt(FloatRegister fa, Label* L) {
+    fbgt(fa, branch_offset(L) >> 2);
+  }
+  void fbge(FloatRegister fa, Label* L) {
+    fbge(fa, branch_offset(L) >> 2);
+  }
+  void fmov(FPURegister rs, FPURegister rd) {
+    fcpys(rs, rs, rd);
+  }
+#endif
+#endif
+
+  // Check the code size generated from label to here.
+  int SizeOfCodeGeneratedSince(Label* label) {
+    return pc_offset() - label->pos();
+  }
+
+  // Check the number of instructions generated from label to here.
+  int InstructionsGeneratedSince(Label* label) {
+    return SizeOfCodeGeneratedSince(label) / kInstrSize;
+  }
+
+  // Class for scoping postponing the trampoline pool generation.
+  class BlockTrampolinePoolScope {
+   public:
+    explicit BlockTrampolinePoolScope(Assembler* assem) : assem_(assem) {
+      assem_->StartBlockTrampolinePool();
+    }
+    ~BlockTrampolinePoolScope() {
+      assem_->EndBlockTrampolinePool();
+    }
+
+   private:
+    Assembler* assem_;
+
+    DISALLOW_IMPLICIT_CONSTRUCTORS(BlockTrampolinePoolScope);
+  };
+
+  // Class for postponing the assembly buffer growth. Typically used for
+  // sequences of instructions that must be emitted as a unit, before
+  // buffer growth (and relocation) can occur.
+  // This blocking scope is not nestable.
+  class BlockGrowBufferScope {
+   public:
+    explicit BlockGrowBufferScope(Assembler* assem) : assem_(assem) {
+      assem_->StartBlockGrowBuffer();
+    }
+    ~BlockGrowBufferScope() {
+      assem_->EndBlockGrowBuffer();
+    }
+
+   private:
+    Assembler* assem_;
+
+    DISALLOW_IMPLICIT_CONSTRUCTORS(BlockGrowBufferScope);
+  };
+
+  // Record a deoptimization reason that can be used by a log or cpu profiler.
+  // Use --trace-deopt to enable.
+  void RecordDeoptReason(DeoptimizeReason reason, SourcePosition position,
+                         int id);
+
+  static int RelocateInternalReference(RelocInfo::Mode rmode, Address pc,
+                                       intptr_t pc_delta);
+
+  // Writes a single byte or word of data in the code stream.  Used for
+  // inline tables, e.g., jump-tables.
+  void db(uint8_t data);
+  void dd(uint32_t data);
+  void dq(uint64_t data);
+  void dp(uintptr_t data) { dq(data); }
+  void dd(Label* label);
+
+  // Postpone the generation of the trampoline pool for the specified number of
+  // instructions.
+  void BlockTrampolinePoolFor(int instructions);
+
+  // Check if there is less than kGap bytes available in the buffer.
+  // If this is the case, we need to grow the buffer before emitting
+  // an instruction or relocation information.
+  inline bool overflow() const { return pc_ >= reloc_info_writer.pos() - kGap; }
+
+  // Get the number of bytes available in the buffer.
+  inline intptr_t available_space() const {
+    return reloc_info_writer.pos() - pc_;
+  }
+
+  // Read/patch instructions.
+  static Instr instr_at(Address pc) { return *reinterpret_cast<Instr*>(pc); }
+  static void instr_at_put(Address pc, Instr instr) {
+    *reinterpret_cast<Instr*>(pc) = instr;
+  }
+  Instr instr_at(int pos) {
+    return *reinterpret_cast<Instr*>(buffer_start_ + pos);
+  }
+  void instr_at_put(int pos, Instr instr) {
+    *reinterpret_cast<Instr*>(buffer_start_ + pos) = instr;
+  }
+
+  // Check if an instruction is a branch of some kind.
+#ifdef SW64
+  static bool IsLdih(Instr instr);
+  static bool IsLdi(Instr instr);
+#endif
+  static bool IsBranch(Instr instr);
+
+  static bool IsBeq(Instr instr);
+  static bool IsBne(Instr instr);
+
+  static uint32_t GetLabelConst(Instr instr);
+
+#ifdef SW64
+  static uint32_t GetSwRa(Instr instr);
+  static uint32_t GetSwRb(Instr instr);
+  static uint32_t GetSwRc(Instr instr);
+
+  static int32_t GetSwOpcodeField(Instr instr);
+  static int32_t GetSwOpcodeAndFunctionField(Instr instr);
+  static uint32_t GetSwImmediate8(Instr instr);
+  static uint32_t GetSwImmediate16(Instr instr);
+#endif
+
+  static bool IsAddImmediate(Instr instr);
+
+  static bool IsAndImmediate(Instr instr);
+  static bool IsEmittedConstant(Instr instr);
+
+  void CheckTrampolinePool();
+
+  bool IsPrevInstrCompactBranch() { return prev_instr_compact_branch_; }
+  static bool IsCompactBranchSupported() { return kArchVariant == kSw64r3; }
+
+  inline int UnboundLabelsCount() { return unbound_labels_count_; }
+
+ protected:
+  // Readable constants for base and offset adjustment helper, these indicate if
+  // aside from offset, another value like offset + 4 should fit into int16.
+  enum class OffsetAccessType : bool {
+    SINGLE_ACCESS = false,
+    TWO_ACCESSES = true
+  };
+
+  // Helper function for memory load/store using base register and offset.
+  void AdjustBaseAndOffset(
+      MemOperand* src,
+      OffsetAccessType access_type = OffsetAccessType::SINGLE_ACCESS,
+      int second_access_add_to_offset = 4);
+
+  inline static void set_target_internal_reference_encoded_at(Address pc,
+                                                              Address target);
+
+  int64_t buffer_space() const { return reloc_info_writer.pos() - pc_; }
+
+  // Decode branch instruction at pos and return branch target pos.
+  int target_at(int pos, bool is_internal);
+
+  // Patch branch instruction at pos to branch to given branch target pos.
+  void target_at_put(int pos, int target_pos, bool is_internal);
+
+  // Say if we need to relocate with this mode.
+  bool MustUseReg(RelocInfo::Mode rmode);
+
+  // Record reloc info for current pc_.
+  void RecordRelocInfo(RelocInfo::Mode rmode, intptr_t data = 0);
+
+  // Block the emission of the trampoline pool before pc_offset.
+  void BlockTrampolinePoolBefore(int pc_offset) {
+    if (no_trampoline_pool_before_ < pc_offset)
+      no_trampoline_pool_before_ = pc_offset;
+  }
+
+  void StartBlockTrampolinePool() {
+    trampoline_pool_blocked_nesting_++;
+  }
+
+  void EndBlockTrampolinePool() {
+    trampoline_pool_blocked_nesting_--;
+    if (trampoline_pool_blocked_nesting_ == 0) {
+      CheckTrampolinePoolQuick(1);
+    }
+  }
+
+  bool is_trampoline_pool_blocked() const {
+    return trampoline_pool_blocked_nesting_ > 0;
+  }
+
+  bool has_exception() const {
+    return internal_trampoline_exception_;
+  }
+
+  bool is_trampoline_emitted() const {
+    return trampoline_emitted_;
+  }
+
+  // Temporarily block automatic assembly buffer growth.
+  void StartBlockGrowBuffer() {
+    DCHECK(!block_buffer_growth_);
+    block_buffer_growth_ = true;
+  }
+
+  void EndBlockGrowBuffer() {
+    DCHECK(block_buffer_growth_);
+    block_buffer_growth_ = false;
+  }
+
+  bool is_buffer_growth_blocked() const {
+    return block_buffer_growth_;
+  }
+
+  void EmitForbiddenSlotInstruction() {
+    if (IsPrevInstrCompactBranch()) {
+      nop();
+    }
+  }
+
+  void CheckTrampolinePoolQuick(int extra_instructions = 0) {
+    if (pc_offset() >= next_buffer_check_ - extra_instructions * kInstrSize) {
+      CheckTrampolinePool();
+    }
+  }
+
+  void set_last_call_pc_(byte* pc) { last_call_pc_ = pc; }
+
+ private:
+  // Avoid overflows for displacements etc.
+  static const int kMaximalBufferSize = 512 * MB;
+
+  // Buffer size and constant pool distance are checked together at regular
+  // intervals of kBufferCheckInterval emitted bytes.
+  static constexpr int kBufferCheckInterval = 1 * KB / 2;
+
+  // Code generation.
+  // The relocation writer's position is at least kGap bytes below the end of
+  // the generated instructions. This is so that multi-instruction sequences do
+  // not have to check for overflow. The same is true for writes of large
+  // relocation info entries.
+  static constexpr int kGap = 64;
+  STATIC_ASSERT(AssemblerBase::kMinimalBufferSize >= 2 * kGap);
+
+  // Repeated checking whether the trampoline pool should be emitted is rather
+  // expensive. By default we only check again once a number of instructions
+  // has been generated.
+  static constexpr int kCheckConstIntervalInst = 32;
+  static constexpr int kCheckConstInterval =
+      kCheckConstIntervalInst * kInstrSize;
+
+  int next_buffer_check_;  // pc offset of next buffer check.
+
+  // Emission of the trampoline pool may be blocked in some code sequences.
+  int trampoline_pool_blocked_nesting_;  // Block emission if this is not zero.
+  int no_trampoline_pool_before_;  // Block emission before this pc offset.
+
+  // Keep track of the last emitted pool to guarantee a maximal distance.
+  int last_trampoline_pool_end_;  // pc offset of the end of the last pool.
+
+  // Automatic growth of the assembly buffer may be blocked for some sequences.
+  bool block_buffer_growth_;  // Block growth when true.
+
+  // Relocation information generation.
+  // Each relocation is encoded as a variable size value.
+  static constexpr int kMaxRelocSize = RelocInfoWriter::kMaxSize;
+  RelocInfoWriter reloc_info_writer;
+
+  // The bound position, before this we cannot do instruction elimination.
+  int last_bound_pos_;
+
+  // Readable constants for compact branch handling in emit()
+  enum class CompactBranchType : bool { NO = false, COMPACT_BRANCH = true };
+
+  // Code emission.
+  inline void CheckBuffer();
+  void GrowBuffer();
+  inline void emit(Instr x,
+                   CompactBranchType is_compact_branch = CompactBranchType::NO);
+  inline void emit(uint64_t x);
+  inline void CheckForEmitInForbiddenSlot();
+  template <typename T>
+  inline void EmitHelper(T x);
+  inline void EmitHelper(Instr x, CompactBranchType is_compact_branch);
+
+#ifdef SW64
+  inline void emitSW(Instr x);
+  inline void emitSW(uint64_t x);
+#endif
+
+  // Instruction generation.
+  // We have 3 different kind of encoding layout on SW64.
+  // However due to many different types of objects encoded in the same fields
+  // we have quite a few aliases for each mode.
+  // Using the same structure to refer to Register and FPURegister would spare a
+  // few aliases, but mixing both does not look clean to me.
+  // Anyway we could surely implement this differently.
+
+  inline bool is_valid_msa_df_m(SecondaryField bit_df, uint32_t m) {
+    switch (bit_df) {
+      case BIT_DF_b:
+        return is_uint3(m);
+      case BIT_DF_h:
+        return is_uint4(m);
+      case BIT_DF_w:
+        return is_uint5(m);
+      case BIT_DF_d:
+        return is_uint6(m);
+      default:
+        return false;
+    }
+  }
+
+  inline bool is_valid_msa_df_n(SecondaryField elm_df, uint32_t n) {
+    switch (elm_df) {
+      case ELM_DF_B:
+        return is_uint4(n);
+      case ELM_DF_H:
+        return is_uint3(n);
+      case ELM_DF_W:
+        return is_uint2(n);
+      case ELM_DF_D:
+        return is_uint1(n);
+      default:
+        return false;
+    }
+  }
+
+#ifdef SW64
+  void GenInstrB_SW(Opcode_ops_bra opcode,
+                    Register Ra,
+                    int32_t disp);
+
+  void GenInstrFB_SW(Opcode_ops_bra opcode,
+                     FloatRegister fa,
+                     int32_t disp);
+
+  void GenInstrM_SW(Opcode_ops_mem opcode,
+                    Register Ra,
+                    int16_t disp,
+                    Register Rb);
+
+  void GenInstrFM_SW(Opcode_ops_mem opcode,
+                     FloatRegister fa,
+                     int16_t disp,
+                     Register Rb);
+
+  void GenInstrMWithFun_SW(Opcode_ops_atmem opcode,
+                           Register Ra,
+                           int16_t disp,
+                           Register Rb);
+
+  void GenInstrR_SW(Opcode_ops_opr opcode,
+                    Register Ra,
+                    Register Rb,
+                    Register Rc);
+
+  void GenInstrI_SW(Opcode_ops_oprl opcode,
+                    Register Ra,
+                    int16_t imm,
+                    Register Rc);
+
+  void GenInstrFR_SW(Opcode_ops_fp opcode,
+                     FloatRegister fa,
+                     FloatRegister fb,
+                     FloatRegister fc);
+
+  void GenInstrFR_SW(Opcode_ops_fp opcode,
+                     FloatRegister fb,
+                     FloatRegister fc);
+
+  void GenInstrFR_SW(Opcode_ops_fpl opcode,
+                     FloatRegister fa,
+                     int16_t imm,
+                     FloatRegister fc);
+
+  void GenInstrFR_SW(Opcode_ops_fpl opcode,
+                     FloatRegister fa,
+                     FloatRegister fb,
+                     int16_t fmalit,
+                     FloatRegister fc);
+
+  void GenInstrFMA_SW(Opcode_ops_fmal opcode,
+                      FloatRegister fa,
+                      FloatRegister fb,
+                      int16_t fmalit,
+                      FloatRegister fc);
+
+  void GenInstrFMA_SW(Opcode_ops_fmal opcode,
+                      FloatRegister fa,
+                      int16_t fmalit,
+                      FloatRegister fc);
+
+  void GenInstrFMA_SW(Opcode_ops_fma opcode,
+                      FloatRegister fa,
+                      FloatRegister fb,
+                      FloatRegister f3,
+                      FloatRegister fc);
+
+  void GenInstrFMA_SW(Opcode_ops_fma opcode,
+                      FloatRegister fa,
+                      FloatRegister fb,
+                      FloatRegister fc);
+
+  void GenInstrSIMD_SW(Opcode_ops_atmem opcode,
+                       FloatRegister fa,
+                       int16_t atmdisp,
+                       Register Rb );
+
+  void GenInstrSelR_SW(Opcode_ops_sel opcode,
+                       Register Ra,
+                       Register Rb,
+                       Register R3,
+                       Register Rc);
+
+  void GenInstrSelI_SW(Opcode_ops_sel_l opcode,
+                       Register Ra,
+                       int32_t imm,
+                       Register R3,
+                       Register Rc);
+
+  // Helpers.
+  void SwLoadRegPlusOffsetToAt(const MemOperand& src);
+#endif
+
+  // Labels.
+  void print(const Label* L);
+  void bind_to(Label* L, int pos);
+  void next(Label* L, bool is_internal);
+
+  // One trampoline consists of:
+  // - space for trampoline slots,
+  // - space for labels.
+  //
+  // Space for trampoline slots is equal to slot_count * 2 * kInstrSize.
+  // Space for trampoline slots precedes space for labels. Each label is of one
+  // instruction size, so total amount for labels is equal to
+  // label_count *  kInstrSize.
+  class Trampoline {
+   public:
+    Trampoline() {
+      start_ = 0;
+      next_slot_ = 0;
+      free_slot_count_ = 0;
+      end_ = 0;
+    }
+    Trampoline(int start, int slot_count) {
+      start_ = start;
+      next_slot_ = start;
+      free_slot_count_ = slot_count;
+      end_ = start + slot_count * kTrampolineSlotsSize;
+    }
+    int start() {
+      return start_;
+    }
+    int end() {
+      return end_;
+    }
+    int take_slot() {
+      int trampoline_slot = kInvalidSlotPos;
+      if (free_slot_count_ <= 0) {
+        // We have run out of space on trampolines.
+        // Make sure we fail in debug mode, so we become aware of each case
+        // when this happens.
+        DCHECK(0);
+        // Internal exception will be caught.
+      } else {
+        trampoline_slot = next_slot_;
+        free_slot_count_--;
+        next_slot_ += kTrampolineSlotsSize;
+      }
+      return trampoline_slot;
+    }
+
+   private:
+    int start_;
+    int end_;
+    int next_slot_;
+    int free_slot_count_;
+  };
+
+  int32_t get_trampoline_entry(int32_t pos);
+  int unbound_labels_count_;
+  // After trampoline is emitted, long branches are used in generated code for
+  // the forward branches whose target offsets could be beyond reach of branch
+  // instruction. We use this information to trigger different mode of
+  // branch instruction generation, where we use jump instructions rather
+  // than regular branch instructions.
+  bool trampoline_emitted_;
+  static constexpr int kInvalidSlotPos = -1;
+
+  // Internal reference positions, required for unbounded internal reference
+  // labels.
+  std::set<int64_t> internal_reference_positions_;
+  bool is_internal_reference(Label* L) {
+    return internal_reference_positions_.find(L->pos()) !=
+           internal_reference_positions_.end();
+  }
+
+  void EmittedCompactBranchInstruction() { prev_instr_compact_branch_ = true; }
+  void ClearCompactBranchState() { prev_instr_compact_branch_ = false; }
+  bool prev_instr_compact_branch_ = false;
+
+  Trampoline trampoline_;
+  bool internal_trampoline_exception_;
+
+  // Keep track of the last Call's position to ensure that safepoint can get the
+  // correct information even if there is a trampoline immediately after the
+  // Call.
+  byte* last_call_pc_;
+
+  RegList scratch_register_list_;
+
+ private:
+  void AllocateAndInstallRequestedHeapObjects(Isolate* isolate);
+
+  int WriteCodeComments();
+
+  friend class RegExpMacroAssemblerSW64;
+  friend class RelocInfo;
+  friend class BlockTrampolinePoolScope;
+  friend class EnsureSpace;
+};
+
+
+class EnsureSpace {
+ public:
+  explicit inline EnsureSpace(Assembler* assembler);
+};
+
+class V8_EXPORT_PRIVATE UseScratchRegisterScope {
+ public:
+  explicit UseScratchRegisterScope(Assembler* assembler);
+  ~UseScratchRegisterScope();
+
+  Register Acquire();
+  bool hasAvailable() const;
+
+ private:
+  RegList* available_;
+  RegList old_available_;
+};
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_CODEGEN_SW64_ASSEMBLER_SW64_H_
diff --git a/src/3rdparty/chromium/v8/src/codegen/sw64/constants-sw64.cc b/src/3rdparty/chromium/v8/src/codegen/sw64/constants-sw64.cc
new file mode 100755
index 0000000000..76c5be76f5
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/codegen/sw64/constants-sw64.cc
@@ -0,0 +1,160 @@
+// Copyright 2011 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#if V8_TARGET_ARCH_SW64
+
+#include "src/codegen/sw64/constants-sw64.h"
+
+namespace v8 {
+namespace internal {
+
+
+// -----------------------------------------------------------------------------
+// Registers.
+
+
+// These register names are defined in a way to match the native disassembler
+// formatting. See for example the command "objdump -d <binary file>".
+const char* Registers::names_[kNumRegisters] = {
+  "v0",
+  "t0", "t1", "t2", "t3", "t4", "t5", "t6", "t7",
+  "s0", "s1", "s2", "s3", "s4", "s5", "fp",
+  "a0", "a1", "a2", "a3", "a4", "a5",
+  "t8", "t9", "t10", "t11",
+  "ra",
+  "t12",
+  "at",
+  "gp",
+  "sp",
+  "zero_reg"
+};
+
+
+// List of alias names which can be used when referring to SW64 registers.
+const Registers::RegisterAlias Registers::aliases_[] = {
+    {14, "cp"},
+    {15, "s6"},
+    {15, "s6_fp"},
+    {31, "zero_reg"},
+    {kInvalidRegister, nullptr}};
+
+const char* Registers::Name(int reg) {
+  const char* result;
+  if ((0 <= reg) && (reg < kNumRegisters)) {
+    result = names_[reg];
+  } else {
+    result = "noreg";
+  }
+  return result;
+}
+
+
+int Registers::Number(const char* name) {
+  // Look through the canonical names.
+  for (int i = 0; i < kNumRegisters; i++) {
+    if (strcmp(names_[i], name) == 0) {
+      return i;
+    }
+  }
+
+  // Look through the alias names.
+  int i = 0;
+  while (aliases_[i].reg != kInvalidRegister) {
+    if (strcmp(aliases_[i].name, name) == 0) {
+      return aliases_[i].reg;
+    }
+    i++;
+  }
+
+  // No register with the reguested name found.
+  return kInvalidRegister;
+}
+
+
+const char* FPURegisters::names_[kNumFPURegisters] = {
+  "f0", "f1", "f2", "f3", "f4", "f5", "f6", "f7", "f8", "f9", "f10", "f11",
+  "f12", "f13", "f14", "f15", "f16", "f17", "f18", "f19", "f20", "f21",
+  "f22", "f23", "f24", "f25", "f26", "f27", "f28", "f29", "f30", "f31"
+};
+
+
+// List of alias names which can be used when referring to SW64 registers.
+const FPURegisters::RegisterAlias FPURegisters::aliases_[] = {
+    {kInvalidRegister, nullptr}};
+
+const char* FPURegisters::Name(int creg) {
+  const char* result;
+  if ((0 <= creg) && (creg < kNumFPURegisters)) {
+    result = names_[creg];
+  } else {
+    result = "nocreg";
+  }
+  return result;
+}
+
+
+int FPURegisters::Number(const char* name) {
+  // Look through the canonical names.
+  for (int i = 0; i < kNumFPURegisters; i++) {
+    if (strcmp(names_[i], name) == 0) {
+      return i;
+    }
+  }
+
+  // Look through the alias names.
+  int i = 0;
+  while (aliases_[i].creg != kInvalidRegister) {
+    if (strcmp(aliases_[i].name, name) == 0) {
+      return aliases_[i].creg;
+    }
+    i++;
+  }
+
+  // No Cregister with the reguested name found.
+  return kInvalidFPURegister;
+}
+
+const char* MSARegisters::names_[kNumMSARegisters] = {
+    "v0",  "v1",  "v2",  "v3",  "v4",  "v5",  "v6",  "v7",  "v8",  "v9",  "v10",
+    "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21",
+    "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v30", "v31"};
+
+const MSARegisters::RegisterAlias MSARegisters::aliases_[] = {
+    {kInvalidRegister, nullptr}};
+
+const char* MSARegisters::Name(int creg) {
+  const char* result;
+  if ((0 <= creg) && (creg < kNumMSARegisters)) {
+    result = names_[creg];
+  } else {
+    result = "nocreg";
+  }
+  return result;
+}
+
+int MSARegisters::Number(const char* name) {
+  // Look through the canonical names.
+  for (int i = 0; i < kNumMSARegisters; i++) {
+    if (strcmp(names_[i], name) == 0) {
+      return i;
+    }
+  }
+
+  // Look through the alias names.
+  int i = 0;
+  while (aliases_[i].creg != kInvalidRegister) {
+    if (strcmp(aliases_[i].name, name) == 0) {
+      return aliases_[i].creg;
+    }
+    i++;
+  }
+
+  // No Cregister with the reguested name found.
+  return kInvalidMSARegister;
+}
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_TARGET_ARCH_SW64
diff --git a/src/3rdparty/chromium/v8/src/codegen/sw64/constants-sw64.h b/src/3rdparty/chromium/v8/src/codegen/sw64/constants-sw64.h
new file mode 100755
index 0000000000..7bfe7ea959
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/codegen/sw64/constants-sw64.h
@@ -0,0 +1,2756 @@
+// Copyright 2012 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef V8_CODEGEN_SW64_CONSTANTS_SW64_H_
+#define V8_CODEGEN_SW64_CONSTANTS_SW64_H_
+
+#include "src/base/logging.h"
+#include "src/base/macros.h"
+#include "src/common/globals.h"
+
+// UNIMPLEMENTED_ macro for SW64.
+#ifdef DEBUG
+#define UNIMPLEMENTED_SW64()                                                  \
+  v8::internal::PrintF("%s, \tline %d: \tfunction %s not implemented. \n",    \
+                       __FILE__, __LINE__, __func__)
+#else
+#define UNIMPLEMENTED_SW64()
+#endif
+
+#define UNSUPPORTED_SW64() v8::internal::PrintF("Unsupported instruction.\n")
+
+enum ArchVariants {
+  kSw64r2,
+  kSw64r3
+};
+
+
+#ifdef _SW64_ARCH_SW64R2
+  static const ArchVariants kArchVariant = kSw64r2;
+#elif  _SW64_ARCH_SW64R3
+  static const ArchVariants kArchVariant = kSw64r3;
+#else
+  static const ArchVariants kArchVariant = kSw64r2;
+#endif
+
+
+  enum Endianness { kLittle, kBig };
+
+#if defined(V8_TARGET_LITTLE_ENDIAN)
+  static const Endianness kArchEndian = kLittle;
+#elif defined(V8_TARGET_BIG_ENDIAN)
+  static const Endianness kArchEndian = kBig;
+#else
+#error Unknown endianness
+#endif
+
+// TODO(plind): consider renaming these ...
+#if(defined(__sw64_hard_float) && __sw64_hard_float != 0)
+// Use floating-point coprocessor instructions. This flag is raised when
+// -mhard-float is passed to the compiler.
+const bool IsSw64SoftFloatABI = false;
+#elif(defined(__sw64_soft_float) && __sw64_soft_float != 0)
+// This flag is raised when -msoft-float is passed to the compiler.
+// Although FPU is a base requirement for v8, soft-float ABI is used
+// on soft-float systems with FPU kernel emulation.
+const bool IsSw64SoftFloatABI = true;
+#else
+const bool IsSw64SoftFloatABI = true;
+#endif
+
+#if defined(V8_TARGET_LITTLE_ENDIAN)
+const uint32_t kSwLwrOffset = 0;
+const uint32_t kSwLwlOffset = 3;
+const uint32_t kSwSwrOffset = 0;
+const uint32_t kSwSwlOffset = 3;
+const uint32_t kSwLdrOffset = 0;
+const uint32_t kSwLdlOffset = 7;
+const uint32_t kSwSdrOffset = 0;
+const uint32_t kSwSdlOffset = 7;
+#else
+#error Unknown endianness
+#endif
+
+#if defined(V8_TARGET_LITTLE_ENDIAN)
+const uint32_t kLeastSignificantByteInInt32Offset = 0;
+const uint32_t kLessSignificantWordInDoublewordOffset = 0;
+#else
+#error Unknown endianness
+#endif
+
+#ifndef __STDC_FORMAT_MACROS
+#define __STDC_FORMAT_MACROS
+#endif
+#include <inttypes.h>
+
+
+// Defines constants and accessor classes to assemble, disassemble
+//
+
+namespace v8 {
+namespace internal {
+
+// TODO(sigurds): Change this value once we use relative jumps.
+constexpr size_t kMaxPCRelativeCodeRangeInMB = 0;
+
+// -----------------------------------------------------------------------------
+// Registers and FPURegisters.
+
+// Number of general purpose registers.
+const int kNumRegisters = 32;
+const int kInvalidRegister = -1;
+
+// Number of registers with HI, LO, and pc.
+const int kNumSimuRegisters = 35;
+
+// In the simulator, the PC register is simulated as the 34th register.
+const int kPCRegister = 34;
+
+// Number coprocessor registers.
+const int kNumFPURegisters = 32;
+const int kInvalidFPURegister = -1;
+
+// Number of MSA registers
+const int kNumMSARegisters = 32;
+const int kInvalidMSARegister = -1;
+
+const int kInvalidMSAControlRegister = -1;
+const int kMSAIRRegister = 0;
+const int kMSACSRRegister = 1;
+const int kMSARegSize = 128;
+const int kMSALanesByte = kMSARegSize / 8;
+const int kMSALanesHalf = kMSARegSize / 16;
+const int kMSALanesWord = kMSARegSize / 32;
+const int kMSALanesDword = kMSARegSize / 64;
+
+// FPU (coprocessor 1) control registers. Currently only FCSR is implemented.
+const int kFCSRRegister = 31;
+const int kInvalidFPUControlRegister = -1;
+const uint32_t kFPUInvalidResult = static_cast<uint32_t>(1u << 31) - 1;
+const int32_t kFPUInvalidResultNegative = static_cast<int32_t>(1u << 31);
+const uint64_t kFPU64InvalidResult =
+    static_cast<uint64_t>(static_cast<uint64_t>(1) << 63) - 1;
+const int64_t kFPU64InvalidResultNegative =
+    static_cast<int64_t>(static_cast<uint64_t>(1) << 63);
+
+// FCSR constants.
+const uint32_t kFCSRInexactFlagBit = 2;
+const uint32_t kFCSRUnderflowFlagBit = 3;
+const uint32_t kFCSROverflowFlagBit = 4;
+const uint32_t kFCSRDivideByZeroFlagBit = 5;
+const uint32_t kFCSRInvalidOpFlagBit = 6;
+const uint32_t kFCSRNaN2008FlagBit = 18;
+
+const uint32_t kFCSRInexactFlagMask = 1 << kFCSRInexactFlagBit;
+const uint32_t kFCSRUnderflowFlagMask = 1 << kFCSRUnderflowFlagBit;
+const uint32_t kFCSROverflowFlagMask = 1 << kFCSROverflowFlagBit;
+const uint32_t kFCSRDivideByZeroFlagMask = 1 << kFCSRDivideByZeroFlagBit;
+const uint32_t kFCSRInvalidOpFlagMask = 1 << kFCSRInvalidOpFlagBit;
+const uint32_t kFCSRNaN2008FlagMask = 1 << kFCSRNaN2008FlagBit;
+
+const uint32_t kFCSRFlagMask =
+    kFCSRInexactFlagMask |
+    kFCSRUnderflowFlagMask |
+    kFCSROverflowFlagMask |
+    kFCSRDivideByZeroFlagMask |
+    kFCSRInvalidOpFlagMask;
+
+const uint32_t kFCSRExceptionFlagMask = kFCSRFlagMask ^ kFCSRInexactFlagMask;
+
+#ifdef SW64
+// SW64 FCSR constants.
+const uint64_t sFCSROverflowIntegerFlagBit = 57;
+const uint64_t sFCSRInexactFlagBit = 56;
+const uint64_t sFCSRUnderflowFlagBit = 55;
+const uint64_t sFCSROverflowFlagBit = 54;
+const uint64_t sFCSRDivideByZeroFlagBit = 53;
+const uint64_t sFCSRInvalidOpFlagBit = 52;
+
+const uint64_t sFCSRInexactControlBit       = 62;
+const uint64_t sFCSRUnderflowControlBit     = 61;
+const uint64_t sFCSROverflowControlBit      = 51;
+const uint64_t sFCSRDivideByZeroControlBit  = 50;
+const uint64_t sFCSRInvalidOpControlBit     = 49;
+
+const uint64_t sFCSRRound0Bit     = 58;
+const uint64_t sFCSRRound1Bit     = 59;
+
+const uint64_t sFCSRControlMask =   (0x1UL << sFCSRInexactControlBit)     |
+                                    (0x1UL << sFCSRUnderflowControlBit)   |
+                                    (0x1UL << sFCSROverflowControlBit)    |
+                                    (0x1UL << sFCSRDivideByZeroControlBit)|
+                                    (0x1UL << sFCSRInvalidOpControlBit);
+
+const uint64_t sFCSRRound0Mask =   0x1UL << sFCSRRound0Bit;
+const uint64_t sFCSRRound1Mask =   0x1UL << sFCSRRound1Bit;
+
+const uint64_t sFCSROverflowIntegerFlagMask = 0x1UL << sFCSROverflowIntegerFlagBit;
+const uint64_t sFCSRInexactFlagMask = 0x1UL << sFCSRInexactFlagBit;
+const uint64_t sFCSRUnderflowFlagMask = 0x1UL << sFCSRUnderflowFlagBit;
+const uint64_t sFCSROverflowFlagMask = 0x1UL << sFCSROverflowFlagBit;
+const uint64_t sFCSRDivideByZeroFlagMask = 0x1UL << sFCSRDivideByZeroFlagBit;
+const uint64_t sFCSRInvalidOpFlagMask = 0x1UL << sFCSRInvalidOpFlagBit;
+
+const uint64_t sFCSRFlagMask =
+    sFCSROverflowIntegerFlagMask |
+    sFCSRInexactFlagMask |
+    sFCSRUnderflowFlagMask |
+    sFCSROverflowFlagMask |
+    sFCSRDivideByZeroFlagMask |
+    sFCSRInvalidOpFlagMask;
+
+const uint64_t sFCSRExceptionFlagMask = sFCSRFlagMask ^ sFCSRInexactFlagMask;
+#endif
+
+// 'pref' instruction hints
+const int32_t kPrefHintLoad = 0;
+const int32_t kPrefHintStore = 1;
+const int32_t kPrefHintLoadStreamed = 4;
+const int32_t kPrefHintStoreStreamed = 5;
+const int32_t kPrefHintLoadRetained = 6;
+const int32_t kPrefHintStoreRetained = 7;
+const int32_t kPrefHintWritebackInvalidate = 25;
+const int32_t kPrefHintPrepareForStore = 30;
+
+// Actual value of root register is offset from the root array's start
+// to take advantage of negative displacement values.
+// TODO(sigurds): Choose best value.
+constexpr int kRootRegisterBias = 256;
+
+// Helper functions for converting between register numbers and names.
+class Registers {
+ public:
+  // Return the name of the register.
+  static const char* Name(int reg);
+
+  // Lookup the register number for the name provided.
+  static int Number(const char* name);
+
+  struct RegisterAlias {
+    int reg;
+    const char* name;
+  };
+
+  static const int64_t kMaxValue = 0x7fffffffffffffffl;
+  static const int64_t kMinValue = 0x8000000000000000l;
+
+ private:
+  static const char* names_[kNumRegisters];
+  static const RegisterAlias aliases_[];
+};
+
+// Helper functions for converting between register numbers and names.
+class FPURegisters {
+ public:
+  // Return the name of the register.
+  static const char* Name(int reg);
+
+  // Lookup the register number for the name provided.
+  static int Number(const char* name);
+
+  struct RegisterAlias {
+    int creg;
+    const char* name;
+  };
+
+ private:
+  static const char* names_[kNumFPURegisters];
+  static const RegisterAlias aliases_[];
+};
+
+// Helper functions for converting between register numbers and names.
+class MSARegisters {
+ public:
+  // Return the name of the register.
+  static const char* Name(int reg);
+
+  // Lookup the register number for the name provided.
+  static int Number(const char* name);
+
+  struct RegisterAlias {
+    int creg;
+    const char* name;
+  };
+
+ private:
+  static const char* names_[kNumMSARegisters];
+  static const RegisterAlias aliases_[];
+};
+
+// -----------------------------------------------------------------------------
+// Instructions encoding constants.
+
+// On SW64 all instructions are 32 bits.
+using Instr = int32_t;
+
+// Special Software Interrupt codes when used in the presence of the SW64
+// simulator.
+enum SoftwareInterruptCodes {
+  // Transition to C code.
+  call_rt_redirected = 0xfffff
+};
+
+// On SW64 Simulator breakpoints can have different codes:
+// - Breaks between 0 and kMaxWatchpointCode are treated as simple watchpoints,
+//   the simulator will run through them and print the registers.
+// - Breaks between kMaxWatchpointCode and kMaxStopCode are treated as stop()
+//   instructions (see Assembler::stop()).
+// - Breaks larger than kMaxStopCode are simple breaks, dropping you into the
+//   debugger.
+const uint32_t kMaxWatchpointCode = 31;
+const uint32_t kMaxStopCode = 127;
+STATIC_ASSERT(kMaxWatchpointCode < kMaxStopCode);
+
+
+// ----- Fields offset and length.
+const int kOpcodeShift   = 26;
+const int kOpcodeBits    = 6;
+const int kRsShift       = 21;
+const int kRsBits        = 5;
+const int kRtShift       = 16;
+const int kRtBits        = 5;
+const int kRdShift       = 11;
+const int kRdBits        = 5;
+const int kSaShift       = 6;
+const int kSaBits        = 5;
+const int kLsaSaBits = 2;
+const int kFunctionShift = 0;
+const int kFunctionBits  = 6;
+const int kLuiShift      = 16;
+const int kBp2Shift = 6;
+const int kBp2Bits = 2;
+const int kBp3Shift = 6;
+const int kBp3Bits = 3;
+const int kBaseShift = 21;
+const int kBaseBits = 5;
+const int kBit6Shift = 6;
+const int kBit6Bits = 1;
+
+const int kImm9Shift = 7;
+const int kImm9Bits = 9;
+const int kImm16Shift = 0;
+const int kImm16Bits = 16;
+const int kImm18Shift = 0;
+const int kImm18Bits = 18;
+const int kImm19Shift = 0;
+const int kImm19Bits = 19;
+const int kImm21Shift = 0;
+const int kImm21Bits = 21;
+const int kImm26Shift = 0;
+const int kImm26Bits = 26;
+const int kImm28Shift = 0;
+const int kImm28Bits = 28;
+const int kImm32Shift = 0;
+const int kImm32Bits = 32;
+const int kMsaImm8Shift = 16;
+const int kMsaImm8Bits = 8;
+const int kMsaImm5Shift = 16;
+const int kMsaImm5Bits = 5;
+const int kMsaImm10Shift = 11;
+const int kMsaImm10Bits = 10;
+const int kMsaImmMI10Shift = 16;
+const int kMsaImmMI10Bits = 10;
+
+// In branches and jumps immediate fields point to words, not bytes,
+// and are therefore shifted by 2.
+const int kImmFieldShift = 2;
+
+const int kFrBits        = 5;
+const int kFrShift       = 21;
+const int kFsShift       = 11;
+const int kFsBits        = 5;
+const int kFtShift       = 16;
+const int kFtBits        = 5;
+const int kFdShift       = 6;
+const int kFdBits        = 5;
+const int kFCccShift     = 8;
+const int kFCccBits      = 3;
+const int kFBccShift     = 18;
+const int kFBccBits      = 3;
+const int kFBtrueShift   = 16;
+const int kFBtrueBits    = 1;
+const int kWtBits = 5;
+const int kWtShift = 16;
+const int kWsBits = 5;
+const int kWsShift = 11;
+const int kWdBits = 5;
+const int kWdShift = 6;
+
+// ----- Miscellaneous useful masks.
+// Instruction bit masks.
+const int kOpcodeMask = ((1 << kOpcodeBits) - 1) << kOpcodeShift;
+const int kImm9Mask = ((1 << kImm9Bits) - 1) << kImm9Shift;
+const int kImm16Mask = ((1 << kImm16Bits) - 1) << kImm16Shift;
+const int kImm18Mask = ((1 << kImm18Bits) - 1) << kImm18Shift;
+const int kImm19Mask = ((1 << kImm19Bits) - 1) << kImm19Shift;
+const int kImm21Mask = ((1 << kImm21Bits) - 1) << kImm21Shift;
+const int kImm26Mask = ((1 << kImm26Bits) - 1) << kImm26Shift;
+const int kImm28Mask = ((1 << kImm28Bits) - 1) << kImm28Shift;
+const int kImm5Mask = ((1 << 5) - 1);
+const int kImm8Mask = ((1 << 8) - 1);
+const int kImm10Mask = ((1 << 10) - 1);
+const int kMsaI5I10Mask = ((7U << 23) | ((1 << 6) - 1));
+const int kMsaI8Mask = ((3U << 24) | ((1 << 6) - 1));
+const int kMsaI5Mask = ((7U << 23) | ((1 << 6) - 1));
+const int kMsaMI10Mask = (15U << 2);
+const int kMsaBITMask = ((7U << 23) | ((1 << 6) - 1));
+const int kMsaELMMask = (15U << 22);
+const int kMsaLongerELMMask = kMsaELMMask | (63U << 16);
+const int kMsa3RMask = ((7U << 23) | ((1 << 6) - 1));
+const int kMsa3RFMask = ((15U << 22) | ((1 << 6) - 1));
+const int kMsaVECMask = (23U << 21);
+const int kMsa2RMask = (7U << 18);
+const int kMsa2RFMask = (15U << 17);
+const int kRsFieldMask = ((1 << kRsBits) - 1) << kRsShift;
+const int kRtFieldMask = ((1 << kRtBits) - 1) << kRtShift;
+const int kRdFieldMask = ((1 << kRdBits) - 1) << kRdShift;
+const int kSaFieldMask = ((1 << kSaBits) - 1) << kSaShift;
+const int kFunctionFieldMask = ((1 << kFunctionBits) - 1) << kFunctionShift;
+// Misc masks.
+const int kHiMaskOf32 = 0xffff << 16;  // Only to be used with 32-bit values
+const int kLoMaskOf32 = 0xffff;
+const int kSignMaskOf32 = 0x80000000;  // Only to be used with 32-bit values
+const int kJumpAddrMask = (1 << (kImm26Bits + kImmFieldShift)) - 1;
+const int64_t kTop16MaskOf64 = (int64_t)0xffff << 48;
+const int64_t kHigher16MaskOf64 = (int64_t)0xffff << 32;
+const int64_t kUpper16MaskOf64 = (int64_t)0xffff << 16;
+const int32_t kJalRawMark = 0x00000000;
+const int32_t kJRawMark = 0xf0000000;
+const int32_t kJumpRawMask = 0xf0000000;
+
+#ifdef SW64 //20180904 ,use the opcode form in hotspot
+
+// ----- Fields offset and length for sw64
+const int sOpcodeShift   = 26; //31-26
+const int sOpcodeBits    = 6;
+const int sRaShift       = 21; //25-21
+const int sRaBits        = 5;
+const int sRbShift       = 16; //20-16
+const int sRbBits        = 5;
+const int sFunctionShift = 5;  //12- 5
+const int sFunctionBits  = 8;
+const int sRcShift       = 0;  // 4- 0
+const int sRcBits        = 5;
+const int sR3Shift       = 5;  // 9- 5
+const int sR3Bits        = 5;
+const int sRdShift       = 0; //jzy 20150317
+const int sRdBits        = 5;
+
+// ----- 21-bits disp(20-0) for SYS_CALL
+const int sImm21Shift = 0;
+const int sImm21Bits  = 21;
+
+// ----- 16-bits disp(15-0) for M
+const int sImm16Shift = 0;
+const int sImm16Bits  = 16;
+
+// ----- 12-bits disp(11-0) for MWithFun
+const int sImm12Shift = 0;
+const int sImm12Bits  = 12;
+
+// ----- 8-bits(20-13) imm for ALU_I & complex interger ALU
+const int sImm8Shift = 13;
+const int sImm8Bits  = 8;
+
+// ----- 5-bits(9-5) imm for complex-float ALU
+const int sImm5Shift = 5;
+const int sImm5Bits  = 5;
+
+//----- 13-bits(25-13) imm for ALU
+const int sImm13Shift = 13;
+const int sImm13Bits = 13;
+
+//----- 11-bits disp(10-0) for SIMD
+const int sImm11Shift = 0;
+const int sImm11Bits = 11;
+
+//----- 8-bits disp(7-0) for CSR
+const int RpiShift = 0;
+const int RpiBits = 8;
+
+// Instruction bit masks.
+const int sOpcodeMask   = ((1 << sOpcodeBits) - 1)   << sOpcodeShift;
+const int sFunctionMask = ((1 << sFunctionBits) - 1) << sFunctionShift;
+const int sImm5Mask     = ((1 << sImm5Bits) - 1)  << sImm5Shift;
+const int sImm8Mask     = ((1 << sImm8Bits) - 1)  << sImm8Shift;
+const int sImm12Mask    = ((1 << sImm12Bits) - 1) << sImm12Shift;
+const int sImm16Mask    = ((1 << sImm16Bits) - 1) << sImm16Shift;
+const int sImm21Mask    = ((1 << sImm21Bits) - 1) << sImm21Shift;
+const int sRaFieldMask  = ((1 << sRaBits) - 1) << sRaShift;
+const int sRbFieldMask  = ((1 << sRbBits) - 1) << sRbShift;
+const int sRcFieldMask  = ((1 << sRcBits) - 1) << sRcShift;
+const int sR3FieldMask  = ((1 << sR3Bits) - 1) << sR3Shift;
+const int sRdFieldMask  = ((1 << sRdBits) - 1) << sRdShift;
+const int sImm13Mask    = ((1 << sImm13Bits) - 1) << sImm13Shift;
+const int sImm11Mask    = ((1 << sImm11Bits) - 1) << sImm11Shift;
+const int sRpiMask      = ((1 << RpiBits) - 1) << RpiShift;
+
+#define OP(x)           (((x) & 0x3F) << 26)
+#define PCD(oo)         (OP(oo))
+#define OPMEM(oo)       (OP(oo))
+#define BRA(oo)         (OP(oo))
+
+#define OFP(oo,ff)      (OP(oo) | (((ff) & 0xFF) << 5))
+#define FMA(oo,ff)      (OP(oo) | (((ff) & 0x3F) << 10))
+#define MFC(oo,ff)      (OP(oo) | ((ff) & 0xFFFF))
+#define MBR(oo,h)       (OP(oo) | (((h) & 3) << 14))
+#define OPR(oo,ff)      (OP(oo) | (((ff) & 0xFF) << 5))
+#define OPRL(oo,ff)     (OP(oo) | (((ff) & 0xFF) << 5))
+#define TOPR(oo,ff)     (OP(oo) | (((ff) & 0x07) << 10))
+#define TOPRL(oo,ff)    (OP(oo) | (((ff) & 0x07) << 10))
+
+#define ATMEM(oo,h)     (OP(oo) | (((h) & 0xF) << 12))
+#define PRIRET(oo,h)    (OP(oo) | (((h) & 0x1) << 20))
+#define SPCD(oo,ff)     (OP(oo) | ((ff) & 0x3FFFFFF))
+#define EV6HWMEM(oo,ff) (OP(oo) | (((ff) & 0xF) << 12))
+#define CSR(oo,ff)      (OP(oo) | (((ff) & 0xFF) << 8))
+
+#define LOGX(oo,ff)     (OP(oo) | (((ff) & 0x3F) << 10))
+#define PSE_LOGX(oo,ff) (OP(oo) | (((ff) & 0x3F) << 10) | (((ff) >> 0x6) << 26 ) | 0x3E0 )
+
+enum OpcodeSW : uint32_t {
+};
+
+
+enum Opcode_ops_mem {
+  op_call     = OPMEM(0x01),
+  op_ret      = OPMEM(0x02),
+  op_jmp      = OPMEM(0x03),
+  op_ldwe     = OPMEM(0x09),  op_fillcs   = op_ldwe,
+  op_ldse     = OPMEM(0x0A),  op_e_fillcs = op_ldse,
+  op_ldde     = OPMEM(0x0B),  op_fillcs_e = op_ldde,
+  op_vlds     = OPMEM(0x0C),  op_e_fillde = op_vlds,
+  op_vldd     = OPMEM(0x0D),
+  op_vsts     = OPMEM(0x0E),
+  op_vstd     = OPMEM(0x0F),
+  op_ldbu     = OPMEM(0x20),  op_flushd   = op_ldbu,
+  op_ldhu     = OPMEM(0x21),  op_evictdg  = op_ldhu,
+  op_ldw      = OPMEM(0x22),  op_s_fillcs = op_ldw,
+  op_ldl      = OPMEM(0x23),  op_s_fillde = op_ldl,
+  op_ldl_u    = OPMEM(0x24),  op_evictdl  = op_ldl_u,
+  op_flds     = OPMEM(0x26),  op_fillde   = op_flds,
+  op_fldd     = OPMEM(0x27),  op_fillde_e = op_fldd,
+  op_stb      = OPMEM(0x28),
+  op_sth      = OPMEM(0x29),
+  op_stw      = OPMEM(0x2A),
+  op_stl      = OPMEM(0x2B),
+  op_stl_u    = OPMEM(0x2C),
+  op_fsts     = OPMEM(0x2E),
+  op_fstd     = OPMEM(0x2F),
+  op_ldi      = OPMEM(0x3E),
+  op_ldih     = OPMEM(0x3F)
+//  unop        = OPMEM(0x3F) | (30 << 16),
+};
+
+
+enum Opcode_ops_atmem {
+  op_lldw     = ATMEM(0x08, 0x0),
+  op_lldl     = ATMEM(0x08, 0x1),
+  op_ldw_inc  = ATMEM(0x08, 0x2),  //SW2F
+  op_ldl_inc  = ATMEM(0x08, 0x3),  //SW2F
+  op_ldw_dec  = ATMEM(0x08, 0x4),  //SW2F
+  op_ldl_dec  = ATMEM(0x08, 0x5),  //SW2F
+  op_ldw_set  = ATMEM(0x08, 0x6),  //SW2F
+  op_ldl_set  = ATMEM(0x08, 0x7),  //SW2F
+  op_lstw     = ATMEM(0x08, 0x8),
+  op_lstl     = ATMEM(0x08, 0x9),
+  op_ldw_nc   = ATMEM(0x08, 0xA),
+  op_ldl_nc   = ATMEM(0x08, 0xB),
+  op_ldd_nc   = ATMEM(0x08, 0xC),
+  op_stw_nc   = ATMEM(0x08, 0xD),
+  op_stl_nc   = ATMEM(0x08, 0xE),
+  op_std_nc   = ATMEM(0x08, 0xF),
+  op_vldw_u   = ATMEM(0x1C, 0x0),
+  op_vstw_u   = ATMEM(0x1C, 0x1),
+  op_vlds_u   = ATMEM(0x1C, 0x2),
+  op_vsts_u   = ATMEM(0x1C, 0x3),
+  op_vldd_u   = ATMEM(0x1C, 0x4),
+  op_vstd_u   = ATMEM(0x1C, 0x5),
+  op_vstw_ul  = ATMEM(0x1C, 0x8),
+  op_vstw_uh  = ATMEM(0x1C, 0x9),
+  op_vsts_ul  = ATMEM(0x1C, 0xA),
+  op_vsts_uh  = ATMEM(0x1C, 0xB),
+  op_vstd_ul  = ATMEM(0x1C, 0xC),
+  op_vstd_uh  = ATMEM(0x1C, 0xD),
+  op_vldd_nc  = ATMEM(0x1C, 0xE),
+  op_vstd_nc  = ATMEM(0x1C, 0xF),
+  op_ldbu_a   = ATMEM(0x1E, 0x0),  //SW6B
+  op_ldhu_a   = ATMEM(0x1E, 0x1),  //SW6B
+  op_ldw_a    = ATMEM(0x1E, 0x2),  //SW6B
+  op_ldl_a    = ATMEM(0x1E, 0x3),  //SW6B
+  op_flds_a   = ATMEM(0x1E, 0x4),  //SW6B
+  op_fldd_a   = ATMEM(0x1E, 0x5),  //SW6B
+  op_stb_a    = ATMEM(0x1E, 0x6),  //SW6B
+  op_sth_a    = ATMEM(0x1E, 0x7),  //SW6B
+  op_stw_a    = ATMEM(0x1E, 0x8),  //SW6B
+  op_stl_a    = ATMEM(0x1E, 0x9),  //SW6B
+  op_fsts_a   = ATMEM(0x1E, 0xA),  //SW6B
+  op_fstd_a   = ATMEM(0x1E, 0xB)   //SW6B
+};
+
+enum Opcode_ops_ev6hwmem {
+  op_pri_ld   = EV6HWMEM(0x25, 0x0),
+  op_pri_st   = EV6HWMEM(0x2D, 0x0),
+};
+
+enum Opcode_ops_opr {
+  op_addw     = OPR(0x10, 0x00),
+  op_subw     = OPR(0x10, 0x01),
+  op_s4addw   = OPR(0x10, 0x02),
+  op_s4subw   = OPR(0x10, 0x03),
+  op_s8addw   = OPR(0x10, 0x04),
+  op_s8subw   = OPR(0x10, 0x05),
+  op_addl     = OPR(0x10, 0x08),
+  op_subl     = OPR(0x10, 0x09),
+  op_s4addl   = OPR(0x10, 0x0A),
+  op_s4subl   = OPR(0x10, 0x0B),
+  op_s8addl   = OPR(0x10, 0x0C),
+  op_s8subl   = OPR(0x10, 0x0D),
+  op_mulw     = OPR(0x10, 0x10),
+  op_divw     = OPR(0x10, 0x11),  //SW6B
+  op_udivw    = OPR(0x10, 0x12),  //SW6B
+  op_remw     = OPR(0x10, 0x13),  //SW6B
+  op_uremw    = OPR(0x10, 0x14),  //SW6B
+  op_mull     = OPR(0x10, 0x18),
+  op_umulh    = OPR(0x10, 0x19),
+  op_divl     = OPR(0x10, 0x1A),  //SW6B
+  op_udivl    = OPR(0x10, 0x1B),  //SW6B
+  op_reml     = OPR(0x10, 0x1C),  //SW6B
+  op_ureml    = OPR(0x10, 0x1D),  //SW6B
+  op_addpi    = OPR(0x10, 0x1E),  //SW6B
+  op_addpis   = OPR(0x10, 0x1F),  //SW6B
+  op_cmpeq    = OPR(0x10, 0x28),
+  op_cmplt    = OPR(0x10, 0x29),
+  op_cmple    = OPR(0x10, 0x2A),
+  op_cmpult   = OPR(0x10, 0x2B),
+  op_cmpule   = OPR(0x10, 0x2C),
+  op_sbt      = OPR(0x10, 0x2D),  //SW6B
+  op_cbt      = OPR(0x10, 0x2E),  //SW6B
+  op_and      = OPR(0x10, 0x38),
+  op_bic      = OPR(0x10, 0x39),
+  op_bis      = OPR(0x10, 0x3A),
+  op_ornot    = OPR(0x10, 0x3B),
+  op_xor      = OPR(0x10, 0x3C),
+  op_eqv      = OPR(0x10, 0x3D),
+  op_inslb    = OPR(0x10, 0x40),  //0x10.40~0x10.47
+  op_inslh    = OPR(0x10, 0x41),
+  op_inslw    = OPR(0x10, 0x42),
+  op_insll    = OPR(0x10, 0x43),
+  op_inshb    = OPR(0x10, 0x44),
+  op_inshh    = OPR(0x10, 0x45),
+  op_inshw    = OPR(0x10, 0x46),
+  op_inshl    = OPR(0x10, 0x47),
+  op_slll     = OPR(0x10, 0x48),
+  op_srll     = OPR(0x10, 0x49),
+  op_sral     = OPR(0x10, 0x4A),
+  op_roll     = OPR(0x10, 0x4B),  //SW6B
+  op_sllw     = OPR(0x10, 0x4C),  //SW6B
+  op_srlw     = OPR(0x10, 0x4D),  //SW6B
+  op_sraw     = OPR(0x10, 0x4E),  //SW6B
+  op_rolw     = OPR(0x10, 0x4F),  //SW6B
+  op_extlb    = OPR(0x10, 0x50),  //0x10.50~0x10.57
+  op_extlh    = OPR(0x10, 0x51),
+  op_extlw    = OPR(0x10, 0x52),
+  op_extll    = OPR(0x10, 0x53),
+  op_exthb    = OPR(0x10, 0x54),
+  op_exthh    = OPR(0x10, 0x55),
+  op_exthw    = OPR(0x10, 0x56),
+  op_exthl    = OPR(0x10, 0x57),
+  op_ctpop    = OPR(0x10, 0x58),
+  op_ctlz     = OPR(0x10, 0x59),
+  op_cttz     = OPR(0x10, 0x5A),
+  op_revbh    = OPR(0x10, 0x5B),  //SW6B
+  op_revbw    = OPR(0x10, 0x5C),  //SW6B
+  op_revbl    = OPR(0x10, 0x5D),  //SW6B
+  op_casw     = OPR(0x10, 0x5E),  //SW6B
+  op_casl     = OPR(0x10, 0x5F),  //SW6B
+  op_masklb   = OPR(0x10, 0x60),  //0x10.60~0x10.67
+  op_masklh   = OPR(0x10, 0x61),
+  op_masklw   = OPR(0x10, 0x62),
+  op_maskll   = OPR(0x10, 0x63),
+  op_maskhb   = OPR(0x10, 0x64),
+  op_maskhh   = OPR(0x10, 0x65),
+  op_maskhw   = OPR(0x10, 0x66),
+  op_maskhl   = OPR(0x10, 0x67),
+  op_zap      = OPR(0x10, 0x68),
+  op_zapnot   = OPR(0x10, 0x69),
+  op_sextb    = OPR(0x10, 0x6A),
+  op_sexth    = OPR(0x10, 0x6B),
+  op_cmpgeb   = OPR(0x10, 0x6C),  //0x10.6C
+  op_fimovs   = OPR(0x10, 0x70),
+  op_fimovd   = OPR(0x10, 0x78),
+};
+
+enum Opcode_ops_sel {
+  op_seleq  = TOPR(0x11, 0x0),
+  op_selge  = TOPR(0x11, 0x1),
+  op_selgt  = TOPR(0x11, 0x2),
+  op_selle  = TOPR(0x11, 0x3),
+  op_sellt  = TOPR(0x11, 0x4),
+  op_selne  = TOPR(0x11, 0x5),
+  op_sellbc = TOPR(0x11, 0x6),
+  op_sellbs = TOPR(0x11, 0x7)
+};
+
+enum Opcode_ops_oprl {
+  op_addw_l   = OPRL(0x12, 0x00),
+  op_subw_l   = OPRL(0x12, 0x01),
+  op_s4addw_l = OPRL(0x12, 0x02),
+  op_s4subw_l = OPRL(0x12, 0x03),
+  op_s8addw_l = OPRL(0x12, 0x04),
+  op_s8subw_l = OPRL(0x12, 0x05),
+  op_addl_l   = OPRL(0x12, 0x08),
+  op_subl_l   = OPRL(0x12, 0x09),
+  op_s4addl_l = OPRL(0x12, 0x0A),
+  op_s4subl_l = OPRL(0x12, 0x0B),
+  op_s8addl_l = OPRL(0x12, 0x0C),
+  op_s8subl_l = OPRL(0x12, 0x0D),
+  op_mulw_l   = OPRL(0x12, 0x10),
+  op_mull_l   = OPRL(0x12, 0x18),
+  op_umulh_l  = OPRL(0x12, 0x19),
+  op_cmpeq_l  = OPRL(0x12, 0x28),
+  op_cmplt_l  = OPRL(0x12, 0x29),
+  op_cmple_l  = OPRL(0x12, 0x2A),
+  op_cmpult_l = OPRL(0x12, 0x2B),
+  op_cmpule_l = OPRL(0x12, 0x2C),
+  op_sbt_l    = OPRL(0x12, 0x2D),  //SW6B
+  op_cbt_l    = OPRL(0x12, 0x2E),  //SW6B
+  op_and_l    = OPRL(0x12, 0x38),
+  op_bic_l    = OPRL(0x12, 0x39),
+  op_bis_l    = OPRL(0x12, 0x3A),
+  op_ornot_l  = OPRL(0x12, 0x3B),
+  op_xor_l    = OPRL(0x12, 0x3C),
+  op_eqv_l    = OPRL(0x12, 0x3D),
+  op_inslb_l  = OPRL(0x12, 0x40),  //0x12.40~0x12.47
+  op_inslh_l  = OPRL(0x12, 0x41),
+  op_inslw_l  = OPRL(0x12, 0x42),
+  op_insll_l  = OPRL(0x12, 0x43),
+  op_inshb_l  = OPRL(0x12, 0x44),
+  op_inshh_l  = OPRL(0x12, 0x45),
+  op_inshw_l  = OPRL(0x12, 0x46),
+  op_inshl_l  = OPRL(0x12, 0x47),
+  op_slll_l   = OPRL(0x12, 0x48),
+  op_srll_l   = OPRL(0x12, 0x49),
+  op_sral_l   = OPRL(0x12, 0x4A),
+  op_roll_l   = OPRL(0x12, 0x4B),  //SW6B
+  op_sllw_l   = OPRL(0x12, 0x4C),
+  op_srlw_l   = OPRL(0x12, 0x4D),
+  op_sraw_l   = OPRL(0x12, 0x4E),
+  op_rolw_l   = OPRL(0x12, 0x4F),  //SW6B
+  op_extlb_l  = OPRL(0x12, 0x50),  //0x12.50~0x12.57
+  op_extlh_l  = OPRL(0x12, 0x51),
+  op_extlw_l  = OPRL(0x12, 0x52),
+  op_extll_l  = OPRL(0x12, 0x53),
+  op_exthb_l  = OPRL(0x12, 0x54),
+  op_exthh_l  = OPRL(0x12, 0x55),
+  op_exthw_l  = OPRL(0x12, 0x56),
+  op_exthl_l  = OPRL(0x12, 0x57),
+  op_masklb_l = OPRL(0x12, 0x60),  //0x12.60~0x12.67
+  op_masklh_l = OPRL(0x12, 0x61),
+  op_masklw_l = OPRL(0x12, 0x62),
+  op_maskll_l = OPRL(0x12, 0x63),
+  op_maskhb_l = OPRL(0x12, 0x64),
+  op_maskhh_l = OPRL(0x12, 0x65),
+  op_maskhw_l = OPRL(0x12, 0x66),
+  op_maskhl_l = OPRL(0x12, 0x67),
+  op_zap_l    = OPRL(0x12, 0x68),
+  op_zapnot_l = OPRL(0x12, 0x69),
+  op_sextb_l  = OPRL(0x12, 0x6A),
+  op_sexth_l  = OPRL(0x12, 0x6B),
+  op_cmpgeb_l = OPRL(0x12, 0x6C),  //0x12.6C
+};
+
+enum Opcode_ops_sel_l {
+  op_seleq_l  = TOPRL(0x13, 0x0),
+  op_selge_l  = TOPRL(0x13, 0x1),
+  op_selgt_l  = TOPRL(0x13, 0x2),
+  op_selle_l  = TOPRL(0x13, 0x3),
+  op_sellt_l  = TOPRL(0x13, 0x4),
+  op_selne_l  = TOPRL(0x13, 0x5),
+  op_sellbc_l = TOPRL(0x13, 0x6),
+  op_sellbs_l = TOPRL(0x13, 0x7)
+};
+
+enum Opcode_ops_bra {
+  op_br   = BRA(0x04),
+  op_bsr  = BRA(0x05),
+  op_beq  = BRA(0x30),
+  op_bne  = BRA(0x31),
+  op_blt  = BRA(0x32),
+  op_ble  = BRA(0x33),
+  op_bgt  = BRA(0x34),
+  op_bge  = BRA(0x35),
+  op_blbc = BRA(0x36),
+  op_blbs = BRA(0x37),
+  op_fbeq = BRA(0x38),
+  op_fbne = BRA(0x39),
+  op_fblt = BRA(0x3A),
+  op_fble = BRA(0x3B),
+  op_fbgt = BRA(0x3C),
+  op_fbge = BRA(0x3D)
+};
+
+enum Opcode_ops_fp {
+  op_fadds    = OFP(0x18, 0x00),
+  op_faddd    = OFP(0x18, 0x01),
+  op_fsubs    = OFP(0x18, 0x02),
+  op_fsubd    = OFP(0x18, 0x03),
+  op_fmuls    = OFP(0x18, 0x04),
+  op_fmuld    = OFP(0x18, 0x05),
+  op_fdivs    = OFP(0x18, 0x06),
+  op_fdivd    = OFP(0x18, 0x07),
+  op_fsqrts   = OFP(0x18, 0x08),
+  op_fsqrtd   = OFP(0x18, 0x09),
+  op_fcmpeq   = OFP(0x18, 0x10),
+  op_fcmple   = OFP(0x18, 0x11),
+  op_fcmplt   = OFP(0x18, 0x12),
+  op_fcmpun   = OFP(0x18, 0x13),
+  op_fcvtsd   = OFP(0x18, 0x20),
+  op_fcvtds   = OFP(0x18, 0x21),
+  op_fcvtdl_g = OFP(0x18, 0x22),  //lx_fcvtdl
+  op_fcvtdl_p = OFP(0x18, 0x23),
+  op_fcvtdl_z = OFP(0x18, 0x24),
+  op_fcvtdl_n = OFP(0x18, 0x25),  //lx_fcvtdl
+  op_fcvtdl   = OFP(0x18, 0x27),
+  op_fcvtwl   = OFP(0x18, 0x28),
+  op_fcvtlw   = OFP(0x18, 0x29),
+  op_fcvtls   = OFP(0x18, 0x2D),
+  op_fcvtld   = OFP(0x18, 0x2F),
+  op_fcpys    = OFP(0x18, 0x30),
+  op_fcpyse   = OFP(0x18, 0x31),
+  op_fcpysn   = OFP(0x18, 0x32),
+  op_ifmovs   = OFP(0x18, 0x40),
+  op_ifmovd   = OFP(0x18, 0x41),
+  op_rfpcr    = OFP(0x18, 0x50),
+  op_wfpcr    = OFP(0x18, 0x51),
+  op_setfpec0 = OFP(0x18, 0x54),
+  op_setfpec1 = OFP(0x18, 0x55),
+  op_setfpec2 = OFP(0x18, 0x56),
+  op_setfpec3 = OFP(0x18, 0x57),
+  op_frecs    = OFP(0x18, 0x58),  //SW6B
+  op_frecd    = OFP(0x18, 0x59),  //SW6B
+  op_fris     = OFP(0x18, 0x5A),  //SW6B
+  op_fris_g   = OFP(0x18, 0x5B),  //SW6B
+  op_fris_p   = OFP(0x18, 0x5C),  //SW6B
+  op_fris_z   = OFP(0x18, 0x5D),  //SW6B
+  op_fris_n   = OFP(0x18, 0x5F),  //SW6B
+  op_frid     = OFP(0x18, 0x60),  //SW6B
+  op_frid_g   = OFP(0x18, 0x61),  //SW6B
+  op_frid_p   = OFP(0x18, 0x62),  //SW6B
+  op_frid_z   = OFP(0x18, 0x63),  //SW6B
+  op_frid_n   = OFP(0x18, 0x64),  //SW6B
+  op_vaddw    = OFP(0x1A, 0x00),
+  op_vsubw    = OFP(0x1A, 0x01),
+  op_vcmpgew  = OFP(0x1A, 0x02),
+  op_vcmpeqw  = OFP(0x1A, 0x03),
+  op_vcmplew  = OFP(0x1A, 0x04),
+  op_vcmpltw  = OFP(0x1A, 0x05),
+  op_vcmpulew = OFP(0x1A, 0x06),
+  op_vcmpultw = OFP(0x1A, 0x07),
+  op_vsllw    = OFP(0x1A, 0x08),
+  op_vsrlw    = OFP(0x1A, 0x09),
+  op_vsraw    = OFP(0x1A, 0x0A),
+  op_vrolw    = OFP(0x1A, 0x0B),
+  op_sllow    = OFP(0x1A, 0x0C),
+  op_srlow    = OFP(0x1A, 0x0D),
+  op_vaddl    = OFP(0x1A, 0x0E),
+  op_vsubl    = OFP(0x1A, 0x0F),
+  op_vsllb    = OFP(0x1A, 0x10),  //SW6B
+  op_vsrlb    = OFP(0x1A, 0x11),  //SW6B
+  op_vsrab    = OFP(0x1A, 0x12),  //SW6B
+  op_vrolb    = OFP(0x1A, 0x13),  //SW6B
+  op_vsllh    = OFP(0x1A, 0x14),  //SW6B
+  op_vsrlh    = OFP(0x1A, 0x15),  //SW6B
+  op_vsrah    = OFP(0x1A, 0x16),  //SW6B
+  op_vrolh    = OFP(0x1A, 0x17),  //SW6B
+  op_ctpopow  = OFP(0x1A, 0x18),
+  op_ctlzow   = OFP(0x1A, 0x19),
+  op_vslll    = OFP(0x1A, 0x1A),  //SW6B
+  op_vsrll    = OFP(0x1A, 0x1B),  //SW6B
+  op_vsral    = OFP(0x1A, 0x1C),  //SW6B
+  op_vroll    = OFP(0x1A, 0x1D),  //SW6B
+  op_vmaxb    = OFP(0x1A, 0x1E),  //SW6B
+  op_vminb    = OFP(0x1A, 0x1F),  //SW6B
+  op_vucaddw  = OFP(0x1A, 0x40),
+  op_vucsubw  = OFP(0x1A, 0x41),
+  op_vucaddh  = OFP(0x1A, 0x42),
+  op_vucsubh  = OFP(0x1A, 0x43),
+  op_vucaddb  = OFP(0x1A, 0x44),
+  op_vucsubb  = OFP(0x1A, 0x45),
+  op_sraow    = OFP(0x1A, 0x46),  //SW6B
+  op_vsumw    = OFP(0x1A, 0x47),  //SW6B
+  op_vsuml    = OFP(0x1A, 0x48),  //SW6B
+  op_vcmpueqb = OFP(0x1A, 0x4B),  //SW6B
+  op_vcmpugtb = OFP(0x1A, 0x4C),  //SW6B
+  op_vmaxh    = OFP(0x1A, 0x50),  //SW6B
+  op_vminh    = OFP(0x1A, 0x51),  //SW6B
+  op_vmaxw    = OFP(0x1A, 0x52),  //SW6B
+  op_vminw    = OFP(0x1A, 0x53),  //SW6B
+  op_vmaxl    = OFP(0x1A, 0x54),  //SW6B
+  op_vminl    = OFP(0x1A, 0x55),  //SW6B
+  op_vumaxb   = OFP(0x1A, 0x56),  //SW6B
+  op_vuminb   = OFP(0x1A, 0x57),  //SW6B
+  op_vumaxh   = OFP(0x1A, 0x58),  //SW6B
+  op_vuminh   = OFP(0x1A, 0x59),  //SW6B
+  op_vumaxw   = OFP(0x1A, 0x5A),  //SW6B
+  op_vuminw   = OFP(0x1A, 0x5B),  //SW6B
+  op_vumaxl   = OFP(0x1A, 0x5C),  //SW6B
+  op_vuminl   = OFP(0x1A, 0x5D),  //SW6B
+  op_vsm3msw  = OFP(0x1A, 0x67),  //SW6B, ENCRYPT
+  op_vsm4r    = OFP(0x1A, 0x69),  //SW6B, ENCRYPT
+  op_vbinvw   = OFP(0x1A, 0x6A),  //SW6B, ENCRYPT
+  op_vadds    = OFP(0x1A, 0x80),
+  op_vaddd    = OFP(0x1A, 0x81),
+  op_vsubs    = OFP(0x1A, 0x82),
+  op_vsubd    = OFP(0x1A, 0x83),
+  op_vmuls    = OFP(0x1A, 0x84),
+  op_vmuld    = OFP(0x1A, 0x85),
+  op_vdivs    = OFP(0x1A, 0x86),
+  op_vdivd    = OFP(0x1A, 0x87),
+  op_vsqrts   = OFP(0x1A, 0x88),
+  op_vsqrtd   = OFP(0x1A, 0x89),
+  op_vfcmpeq  = OFP(0x1A, 0x8C),
+  op_vfcmple  = OFP(0x1A, 0x8D),
+  op_vfcmplt  = OFP(0x1A, 0x8E),
+  op_vfcmpun  = OFP(0x1A, 0x8F),
+  op_vcpys    = OFP(0x1A, 0x90),
+  op_vcpyse   = OFP(0x1A, 0x91),
+  op_vcpysn   = OFP(0x1A, 0x92),
+  op_vsums    = OFP(0x1A, 0x93),  //SW6B
+  op_vsumd    = OFP(0x1A, 0x94),  //SW6B
+  op_vfcvtsd  = OFP(0x1A, 0x95),  //SW6B
+  op_vfcvtds  = OFP(0x1A, 0x96),  //SW6B
+  op_vfcvtls  = OFP(0x1A, 0x99),  //SW6B
+  op_vfcvtld  = OFP(0x1A, 0x9A),  //SW6B
+  op_vfcvtdl  = OFP(0x1A, 0x9B),  //SW6B
+  op_vfcvtdl_g    = OFP(0x1A, 0x9C),  //SW6B
+  op_vfcvtdl_p    = OFP(0x1A, 0x9D),  //SW6B
+  op_vfcvtdl_z    = OFP(0x1A, 0x9E),  //SW6B
+  op_vfcvtdl_n    = OFP(0x1A, 0x9F),  //SW6B
+  op_vfris    = OFP(0x1A, 0xA0),  //SW6B
+  op_vfris_g  = OFP(0x1A, 0xA1),  //SW6B
+  op_vfris_p  = OFP(0x1A, 0xA2),  //SW6B
+  op_vfris_z  = OFP(0x1A, 0xA3),  //SW6B
+  op_vfris_n  = OFP(0x1A, 0xA4),  //SW6B
+  op_vfrid    = OFP(0x1A, 0xA5),  //SW6B
+  op_vfrid_g  = OFP(0x1A, 0xA6),  //SW6B
+  op_vfrid_p  = OFP(0x1A, 0xA7),  //SW6B
+  op_vfrid_z  = OFP(0x1A, 0xA8),  //SW6B
+  op_vfrid_n  = OFP(0x1A, 0xA9),  //SW6B
+  op_vfrecs   = OFP(0x1A, 0xAA),  //SW6B
+  op_vfrecd   = OFP(0x1A, 0xAB),  //SW6B
+  op_vmaxs    = OFP(0x1A, 0xAC),  //SW6B
+  op_vmins    = OFP(0x1A, 0xAD),  //SW6B
+  op_vmaxd    = OFP(0x1A, 0xAE),  //SW6B
+  op_vmind    = OFP(0x1A, 0xAF),  //SW6B
+
+  op_vbisw    = PSE_LOGX(0x14, 0x30),
+  op_vxorw    = PSE_LOGX(0x14, 0x3c),
+  op_vandw    = PSE_LOGX(0x14, 0xc0),
+  op_veqvw    = PSE_LOGX(0x14, 0xc3),
+  op_vornotw  = PSE_LOGX(0x14, 0xf3),
+  op_vbicw    = PSE_LOGX(0x14, 0xfc)
+};
+
+enum Opcode_ops_fpl {
+  op_vaddw_l      = OFP(0x1A, 0x20),
+  op_vsubw_l      = OFP(0x1A, 0x21),
+  op_vcmpgew_l    = OFP(0x1A, 0x22),
+  op_vcmpeqw_l    = OFP(0x1A, 0x23),
+  op_vcmplew_l    = OFP(0x1A, 0x24),
+  op_vcmpltw_l    = OFP(0x1A, 0x25),
+  op_vcmpulew_l   = OFP(0x1A, 0x26),
+  op_vcmpultw_l   = OFP(0x1A, 0x27),
+  op_vsllw_l      = OFP(0x1A, 0x28),
+  op_vsrlw_l      = OFP(0x1A, 0x29),
+  op_vsraw_l      = OFP(0x1A, 0x2A),
+  op_vrolw_l      = OFP(0x1A, 0x2B),
+  op_sllow_l      = OFP(0x1A, 0x2C),
+  op_srlow_l      = OFP(0x1A, 0x2D),
+  op_vaddl_l      = OFP(0x1A, 0x2E),
+  op_vsubl_l      = OFP(0x1A, 0x2F),
+  op_vsllb_l      = OFP(0x1A, 0x30),  //SW6B
+  op_vsrlb_l      = OFP(0x1A, 0x31),  //SW6B
+  op_vsrab_l      = OFP(0x1A, 0x32),  //SW6B
+  op_vrolb_l      = OFP(0x1A, 0x33),  //SW6B
+  op_vsllh_l      = OFP(0x1A, 0x34),  //SW6B
+  op_vsrlh_l      = OFP(0x1A, 0x35),  //SW6B
+  op_vsrah_l      = OFP(0x1A, 0x36),  //SW6B
+  op_vrolh_l      = OFP(0x1A, 0x37),  //SW6B
+  op_vslll_l      = OFP(0x1A, 0x3A),  //SW6B
+  op_vsrll_l      = OFP(0x1A, 0x3B),  //SW6B
+  op_vsral_l      = OFP(0x1A, 0x3C),  //SW6B
+  op_vroll_l      = OFP(0x1A, 0x3D),  //SW6B
+  op_vucaddw_l    = OFP(0x1A, 0x60),
+  op_vucsubw_l    = OFP(0x1A, 0x61),
+  op_vucaddh_l    = OFP(0x1A, 0x62),
+  op_vucsubh_l    = OFP(0x1A, 0x63),
+  op_vucaddb_l    = OFP(0x1A, 0x64),
+  op_vucsubb_l    = OFP(0x1A, 0x65),
+  op_sraow_l      = OFP(0x1A, 0x66),  //SW6B
+  op_vsm4key_l    = OFP(0x1A, 0x68),  //SW6B, ENCRYPT
+  op_vcmpueqb_l   = OFP(0x1A, 0x6B),  //SW6B
+  op_vcmpugtb_l   = OFP(0x1A, 0x6C),  //SW6B
+  op_vfcvtsh_l    = OFP(0x1B, 0x35),  //SW6B
+  op_vfcvths_l    = OFP(0x1B, 0x36)   //SW6B
+};
+
+enum Opcode_ops_fma {
+  op_fmas     = FMA(0x19, 0x00),
+  op_fmad     = FMA(0x19, 0x01),
+  op_fmss     = FMA(0x19, 0x02),
+  op_fmsd     = FMA(0x19, 0x03),
+  op_fnmas    = FMA(0x19, 0x04),
+  op_fnmad    = FMA(0x19, 0x05),
+  op_fnmss    = FMA(0x19, 0x06),
+  op_fnmsd    = FMA(0x19, 0x07),
+  op_fseleq   = FMA(0x19, 0x10),
+  op_fselne   = FMA(0x19, 0x11),
+  op_fsellt   = FMA(0x19, 0x12),
+  op_fselle   = FMA(0x19, 0x13),
+  op_fselgt   = FMA(0x19, 0x14),
+  op_fselge   = FMA(0x19, 0x15),
+  op_vmas     = FMA(0x1B, 0x00),
+  op_vmad     = FMA(0x1B, 0x01),
+  op_vmss     = FMA(0x1B, 0x02),
+  op_vmsd     = FMA(0x1B, 0x03),
+  op_vnmas    = FMA(0x1B, 0x04),
+  op_vnmad    = FMA(0x1B, 0x05),
+  op_vnmss    = FMA(0x1B, 0x06),
+  op_vnmsd    = FMA(0x1B, 0x07),
+  op_vfseleq  = FMA(0x1B, 0x10),
+  op_vfsellt  = FMA(0x1B, 0x12),
+  op_vfselle  = FMA(0x1B, 0x13),
+  op_vseleqw  = FMA(0x1B, 0x18),
+  op_vsellbcw = FMA(0x1B, 0x19),
+  op_vselltw  = FMA(0x1B, 0x1A),
+  op_vsellew  = FMA(0x1B, 0x1B),
+  op_vcpyw    = FMA(0x1B, 0x24),
+  op_vcpyf    = FMA(0x1B, 0x25),
+  op_vconw    = FMA(0x1B, 0x26),
+  op_vshfw    = FMA(0x1B, 0x27),
+  op_vcons    = FMA(0x1B, 0x28),
+  op_vcond    = FMA(0x1B, 0x29),
+  op_vinsectlh    = FMA(0x1B, 0x2C),  //SW6B
+  op_vinsectlw    = FMA(0x1B, 0x2D),  //SW6B
+  op_vinsectll    = FMA(0x1B, 0x2E),  //SW6B
+  op_vinsectlb    = FMA(0x1B, 0x2F),  //SW6B
+  op_vshfqb   = FMA(0x1B, 0x31),  //SW6B
+  op_vcpyh    = FMA(0x1B, 0x32),  //SW6B
+  op_vcpyb    = FMA(0x1B, 0x33)   //SW6B
+};
+
+enum Opcode_ops_fmal {
+  op_vinsw_l      = FMA(0x1B, 0x20),
+  op_vinsf_l      = FMA(0x1B, 0x21),
+  op_vextw_l      = FMA(0x1B, 0x22),
+  op_vextf_l      = FMA(0x1B, 0x23),
+  op_vinsb_l      = FMA(0x1B, 0x2A),  //SW6B
+  op_vinsh_l      = FMA(0x1B, 0x2B),  //SW6B
+  op_vshfq_l      = FMA(0x1B, 0x30),  //SW6B
+  op_vsm3r_l      = FMA(0x1B, 0x34),  //SW6B, ENCRYPT
+  op_vseleqw_l    = FMA(0x1B, 0x38),
+  op_vsellbcw_l   = FMA(0x1B, 0x39),
+  op_vselltw_l    = FMA(0x1B, 0x3A),
+  op_vsellew_l    = FMA(0x1B, 0x3B)
+};
+
+enum Opcode_ops_extra {
+  op_sys_call     = PCD(0x00),
+  op_memb         = MFC(0x06, 0x0000),
+  op_imemb        = MFC(0x06, 0x0001),  //SW6B
+  op_wmemb        = MFC(0x06, 0x0002),  //SW6B
+  op_rtc          = MFC(0x06, 0x0020),
+  op_rcid         = MFC(0x06, 0x0040),
+  op_halt         = MFC(0x06, 0x0080),
+  op_rd_f         = MFC(0x06, 0x1000),  //SW2F
+  op_wr_f         = MFC(0x06, 0x1020),  //SW2F
+  op_rtid         = MFC(0x06, 0x1040),
+  op_csrrs        = CSR(0x06, 0xFC),    //SW6B
+  op_csrrc        = CSR(0x06, 0xFD),    //SW6B
+  op_csrr         = CSR(0x06, 0xFE),
+  op_csrw         = CSR(0x06, 0xFF),
+  op_pri_ret      = PRIRET(0x07, 0x0),
+  op_vlog         = LOGX(0x14, 0x00),
+  op_lbr          = PCD(0x1D),           //SW6B
+  op_dpfhr        = ATMEM(0x1E, 0xE),    //SW6B
+  op_dpfhw        = ATMEM(0x1E, 0xF),    //SW6B
+};
+
+enum Opcode_ops_simulator {
+  op_trap         = PCD(0x1F)
+};
+
+enum TrapCode : uint32_t {
+  BREAK = 0,
+  REDIRECT = 1
+};
+
+#endif
+
+// ----- XXXX64 Opcodes and Function Fields.
+// We use this presentation to stay close to the table representation in
+// SW32 Architecture For Programmers, Volume II: The SW32 Instruction Set.
+enum Opcode : uint32_t {
+  SPECIAL = 0U << kOpcodeShift,
+  REGIMM = 1U << kOpcodeShift,
+
+  J = ((0U << 3) + 2) << kOpcodeShift,
+  JAL = ((0U << 3) + 3) << kOpcodeShift,
+  BEQ = ((0U << 3) + 4) << kOpcodeShift,
+  BNE = ((0U << 3) + 5) << kOpcodeShift,
+  BLEZ = ((0U << 3) + 6) << kOpcodeShift,
+  BGTZ = ((0U << 3) + 7) << kOpcodeShift,
+
+  ADDI = ((1U << 3) + 0) << kOpcodeShift,
+  ADDIU = ((1U << 3) + 1) << kOpcodeShift,
+  SLTI = ((1U << 3) + 2) << kOpcodeShift,
+  SLTIU = ((1U << 3) + 3) << kOpcodeShift,
+  ANDI = ((1U << 3) + 4) << kOpcodeShift,
+  ORI = ((1U << 3) + 5) << kOpcodeShift,
+  XORI = ((1U << 3) + 6) << kOpcodeShift,
+  LUI = ((1U << 3) + 7) << kOpcodeShift,  // LUI/AUI family.
+  DAUI = ((3U << 3) + 5) << kOpcodeShift,
+
+  BEQC = ((2U << 3) + 0) << kOpcodeShift,
+  COP1 = ((2U << 3) + 1) << kOpcodeShift,  // Coprocessor 1 class.
+  BEQL = ((2U << 3) + 4) << kOpcodeShift,
+  BNEL = ((2U << 3) + 5) << kOpcodeShift,
+  BLEZL = ((2U << 3) + 6) << kOpcodeShift,
+  BGTZL = ((2U << 3) + 7) << kOpcodeShift,
+
+  DADDI = ((3U << 3) + 0) << kOpcodeShift,  // This is also BNEC.
+//  DADDIU = ((3U << 3) + 1) << kOpcodeShift,
+  LDL = ((3U << 3) + 2) << kOpcodeShift,
+  LDR = ((3U << 3) + 3) << kOpcodeShift,
+  SPECIAL2 = ((3U << 3) + 4) << kOpcodeShift,
+  MSA = ((3U << 3) + 6) << kOpcodeShift,
+  SPECIAL3 = ((3U << 3) + 7) << kOpcodeShift,
+
+  LB = ((4U << 3) + 0) << kOpcodeShift,
+  LH = ((4U << 3) + 1) << kOpcodeShift,
+  LWL = ((4U << 3) + 2) << kOpcodeShift,
+  LW = ((4U << 3) + 3) << kOpcodeShift,
+  LBU = ((4U << 3) + 4) << kOpcodeShift,
+  LHU = ((4U << 3) + 5) << kOpcodeShift,
+  LWR = ((4U << 3) + 6) << kOpcodeShift,
+  LWU = ((4U << 3) + 7) << kOpcodeShift,
+
+  SB = ((5U << 3) + 0) << kOpcodeShift,
+  SH = ((5U << 3) + 1) << kOpcodeShift,
+  SWL = ((5U << 3) + 2) << kOpcodeShift,
+  SW = ((5U << 3) + 3) << kOpcodeShift,
+  SDL = ((5U << 3) + 4) << kOpcodeShift,
+  SDR = ((5U << 3) + 5) << kOpcodeShift,
+  SWR = ((5U << 3) + 6) << kOpcodeShift,
+
+  LL = ((6U << 3) + 0) << kOpcodeShift,
+  LWC1 = ((6U << 3) + 1) << kOpcodeShift,
+  BC = ((6U << 3) + 2) << kOpcodeShift,
+  LLD = ((6U << 3) + 4) << kOpcodeShift,
+  LDC1 = ((6U << 3) + 5) << kOpcodeShift,
+  POP66 = ((6U << 3) + 6) << kOpcodeShift,
+  LD = ((6U << 3) + 7) << kOpcodeShift,
+
+  PREF = ((6U << 3) + 3) << kOpcodeShift,
+
+  SC = ((7U << 3) + 0) << kOpcodeShift,
+  SWC1 = ((7U << 3) + 1) << kOpcodeShift,
+  BALC = ((7U << 3) + 2) << kOpcodeShift,
+  PCREL = ((7U << 3) + 3) << kOpcodeShift,
+  SCD = ((7U << 3) + 4) << kOpcodeShift,
+  SDC1 = ((7U << 3) + 5) << kOpcodeShift,
+  POP76 = ((7U << 3) + 6) << kOpcodeShift,
+  SD = ((7U << 3) + 7) << kOpcodeShift,
+
+  COP1X = ((1U << 4) + 3) << kOpcodeShift,
+
+  // New r6 instruction.
+  POP06 = BLEZ,   // bgeuc/bleuc, blezalc, bgezalc
+  POP07 = BGTZ,   // bltuc/bgtuc, bgtzalc, bltzalc
+  POP10 = ADDI,   // beqzalc, bovc, beqc
+  POP26 = BLEZL,  // bgezc, blezc, bgec/blec
+  POP27 = BGTZL,  // bgtzc, bltzc, bltc/bgtc
+  POP30 = DADDI,  // bnezalc, bnvc, bnec
+};
+
+enum SecondaryField : uint32_t {
+  // SPECIAL Encoding of Function Field.
+  SLL = ((0U << 3) + 0),
+  MOVCI = ((0U << 3) + 1),
+  SRL = ((0U << 3) + 2),
+  SRA = ((0U << 3) + 3),
+  SLLV = ((0U << 3) + 4),
+  LSA = ((0U << 3) + 5),
+  SRLV = ((0U << 3) + 6),
+  SRAV = ((0U << 3) + 7),
+
+  JR = ((1U << 3) + 0),
+  JALR = ((1U << 3) + 1),
+  //MOVZ = ((1U << 3) + 2),
+  MOVN = ((1U << 3) + 3),
+  //BREAK = ((1U << 3) + 5),
+  SYNC = ((1U << 3) + 7),
+
+  MFHI = ((2U << 3) + 0),
+  CLZ_R6 = ((2U << 3) + 0),
+  CLO_R6 = ((2U << 3) + 1),
+  MFLO = ((2U << 3) + 2),
+  DCLZ_R6 = ((2U << 3) + 2),
+  DCLO_R6 = ((2U << 3) + 3),
+//  DSLLV = ((2U << 3) + 4),
+  DLSA = ((2U << 3) + 5),
+//  DSRLV = ((2U << 3) + 6),
+//  DSRAV = ((2U << 3) + 7),
+
+  MULT = ((3U << 3) + 0),
+  MULTU = ((3U << 3) + 1),
+//  DIV = ((3U << 3) + 2),
+//  DIVU = ((3U << 3) + 3),
+  DMULT = ((3U << 3) + 4),
+  DMULTU = ((3U << 3) + 5),
+//  DDIV = ((3U << 3) + 6),
+//  DDIVU = ((3U << 3) + 7),
+
+  ADD = ((4U << 3) + 0),
+//  ADDU = ((4U << 3) + 1),
+  SUB = ((4U << 3) + 2),
+//  SUBU = ((4U << 3) + 3),
+  AND = ((4U << 3) + 4),
+  OR = ((4U << 3) + 5),
+  XOR = ((4U << 3) + 6),
+  NOR = ((4U << 3) + 7),
+
+  SLT = ((5U << 3) + 2),
+  SLTU = ((5U << 3) + 3),
+  DADD = ((5U << 3) + 4),
+//  DADDU = ((5U << 3) + 5),
+  DSUB = ((5U << 3) + 6),
+//  DSUBU = ((5U << 3) + 7),
+
+  TGE = ((6U << 3) + 0),
+  TGEU = ((6U << 3) + 1),
+  TLT = ((6U << 3) + 2),
+  TLTU = ((6U << 3) + 3),
+  TEQ = ((6U << 3) + 4),
+  SELEQZ_S = ((6U << 3) + 5),
+  TNE = ((6U << 3) + 6),
+  SELNEZ_S = ((6U << 3) + 7),
+
+//  DSLL = ((7U << 3) + 0),
+//  DSRL = ((7U << 3) + 2),
+//  DSRA = ((7U << 3) + 3),
+//  DSLL32 = ((7U << 3) + 4),
+//  DSRL32 = ((7U << 3) + 6),
+//  DSRA32 = ((7U << 3) + 7),
+
+  // Multiply integers in r6.
+  MUL_MUH = ((3U << 3) + 0),      // MUL, MUH.
+  MUL_MUH_U = ((3U << 3) + 1),    // MUL_U, MUH_U.
+  D_MUL_MUH = ((7U << 2) + 0),    // DMUL, DMUH.
+  D_MUL_MUH_U = ((7U << 2) + 1),  // DMUL_U, DMUH_U.
+  RINT = ((3U << 3) + 2),
+
+  MUL_OP = ((0U << 3) + 2),
+  MUH_OP = ((0U << 3) + 3),
+  DIV_OP = ((0U << 3) + 2),
+  MOD_OP = ((0U << 3) + 3),
+
+  DIV_MOD = ((3U << 3) + 2),
+  DIV_MOD_U = ((3U << 3) + 3),
+  D_DIV_MOD = ((3U << 3) + 6),
+  D_DIV_MOD_U = ((3U << 3) + 7),
+
+  // drotr in special4?
+
+  // SPECIAL2 Encoding of Function Field.
+  MUL = ((0U << 3) + 2),
+  CLZ = ((4U << 3) + 0),
+  CLO = ((4U << 3) + 1),
+  DCLZ = ((4U << 3) + 4),
+  DCLO = ((4U << 3) + 5),
+
+  // SPECIAL3 Encoding of Function Field.
+  EXT = ((0U << 3) + 0),
+  DEXTM = ((0U << 3) + 1),
+  DEXTU = ((0U << 3) + 2),
+  DEXT = ((0U << 3) + 3),
+  INS = ((0U << 3) + 4),
+  DINSM = ((0U << 3) + 5),
+  DINSU = ((0U << 3) + 6),
+  DINS = ((0U << 3) + 7),
+
+  BSHFL = ((4U << 3) + 0),
+  DBSHFL = ((4U << 3) + 4),
+  SC_R6 = ((4U << 3) + 6),
+  SCD_R6 = ((4U << 3) + 7),
+  LL_R6 = ((6U << 3) + 6),
+  LLD_R6 = ((6U << 3) + 7),
+
+  // SPECIAL3 Encoding of sa Field.
+  BITSWAP = ((0U << 3) + 0),
+  ALIGN = ((0U << 3) + 2),
+  WSBH = ((0U << 3) + 2),
+  SEB = ((2U << 3) + 0),
+  SEH = ((3U << 3) + 0),
+
+  DBITSWAP = ((0U << 3) + 0),
+  DALIGN = ((0U << 3) + 1),
+  DBITSWAP_SA = ((0U << 3) + 0) << kSaShift,
+  DSBH = ((0U << 3) + 2),
+  DSHD = ((0U << 3) + 5),
+
+  // REGIMM  encoding of rt Field.
+  BLTZ = ((0U << 3) + 0) << 16,
+  BGEZ = ((0U << 3) + 1) << 16,
+  BLTZAL = ((2U << 3) + 0) << 16,
+  BGEZAL = ((2U << 3) + 1) << 16,
+  BGEZALL = ((2U << 3) + 3) << 16,
+  DAHI = ((0U << 3) + 6) << 16,
+  DATI = ((3U << 3) + 6) << 16,
+
+  // COP1 Encoding of rs Field.
+  MFC1 = ((0U << 3) + 0) << 21,
+  DMFC1 = ((0U << 3) + 1) << 21,
+  CFC1 = ((0U << 3) + 2) << 21,
+  MFHC1 = ((0U << 3) + 3) << 21,
+  MTC1 = ((0U << 3) + 4) << 21,
+  DMTC1 = ((0U << 3) + 5) << 21,
+  CTC1 = ((0U << 3) + 6) << 21,
+  MTHC1 = ((0U << 3) + 7) << 21,
+  BC1 = ((1U << 3) + 0) << 21,
+  S = ((2U << 3) + 0) << 21,
+  D = ((2U << 3) + 1) << 21,
+  W = ((2U << 3) + 4) << 21,
+  L = ((2U << 3) + 5) << 21,
+  PS = ((2U << 3) + 6) << 21,
+  // COP1 Encoding of Function Field When rs=S.
+
+  ADD_S = ((0U << 3) + 0),
+  SUB_S = ((0U << 3) + 1),
+  MUL_S = ((0U << 3) + 2),
+  DIV_S = ((0U << 3) + 3),
+  ABS_S = ((0U << 3) + 5),
+  SQRT_S = ((0U << 3) + 4),
+  MOV_S = ((0U << 3) + 6),
+  NEG_S = ((0U << 3) + 7),
+  ROUND_L_S = ((1U << 3) + 0),
+  TRUNC_L_S = ((1U << 3) + 1),
+  CEIL_L_S = ((1U << 3) + 2),
+  FLOOR_L_S = ((1U << 3) + 3),
+  ROUND_W_S = ((1U << 3) + 4),
+  TRUNC_W_S = ((1U << 3) + 5),
+  CEIL_W_S = ((1U << 3) + 6),
+  FLOOR_W_S = ((1U << 3) + 7),
+  RECIP_S = ((2U << 3) + 5),
+  RSQRT_S = ((2U << 3) + 6),
+  MADDF_S = ((3U << 3) + 0),
+  MSUBF_S = ((3U << 3) + 1),
+  CLASS_S = ((3U << 3) + 3),
+  CVT_D_S = ((4U << 3) + 1),
+  CVT_W_S = ((4U << 3) + 4),
+  CVT_L_S = ((4U << 3) + 5),
+  CVT_PS_S = ((4U << 3) + 6),
+  // COP1 Encoding of Function Field When rs=D.
+  ADD_D = ((0U << 3) + 0),
+  SUB_D = ((0U << 3) + 1),
+  MUL_D = ((0U << 3) + 2),
+  DIV_D = ((0U << 3) + 3),
+  SQRT_D = ((0U << 3) + 4),
+  ABS_D = ((0U << 3) + 5),
+  MOV_D = ((0U << 3) + 6),
+  NEG_D = ((0U << 3) + 7),
+  ROUND_L_D = ((1U << 3) + 0),
+  TRUNC_L_D = ((1U << 3) + 1),
+  CEIL_L_D = ((1U << 3) + 2),
+  FLOOR_L_D = ((1U << 3) + 3),
+  ROUND_W_D = ((1U << 3) + 4),
+  TRUNC_W_D = ((1U << 3) + 5),
+  CEIL_W_D = ((1U << 3) + 6),
+  FLOOR_W_D = ((1U << 3) + 7),
+  RECIP_D = ((2U << 3) + 5),
+  RSQRT_D = ((2U << 3) + 6),
+  MADDF_D = ((3U << 3) + 0),
+  MSUBF_D = ((3U << 3) + 1),
+  CLASS_D = ((3U << 3) + 3),
+  MIN = ((3U << 3) + 4),
+  MINA = ((3U << 3) + 5),
+  MAX = ((3U << 3) + 6),
+  MAXA = ((3U << 3) + 7),
+  CVT_S_D = ((4U << 3) + 0),
+  CVT_W_D = ((4U << 3) + 4),
+  CVT_L_D = ((4U << 3) + 5),
+  C_F_D = ((6U << 3) + 0),
+  C_UN_D = ((6U << 3) + 1),
+  C_EQ_D = ((6U << 3) + 2),
+  C_UEQ_D = ((6U << 3) + 3),
+  C_OLT_D = ((6U << 3) + 4),
+  C_ULT_D = ((6U << 3) + 5),
+  C_OLE_D = ((6U << 3) + 6),
+  C_ULE_D = ((6U << 3) + 7),
+
+  // COP1 Encoding of Function Field When rs=W or L.
+  CVT_S_W = ((4U << 3) + 0),
+  CVT_D_W = ((4U << 3) + 1),
+  CVT_S_L = ((4U << 3) + 0),
+  CVT_D_L = ((4U << 3) + 1),
+  BC1EQZ = ((2U << 2) + 1) << 21,
+  BC1NEZ = ((3U << 2) + 1) << 21,
+  // COP1 CMP positive predicates Bit 5..4 = 00.
+  CMP_AF = ((0U << 3) + 0),
+  CMP_UN = ((0U << 3) + 1),
+  CMP_EQ = ((0U << 3) + 2),
+  CMP_UEQ = ((0U << 3) + 3),
+  CMP_LT = ((0U << 3) + 4),
+  CMP_ULT = ((0U << 3) + 5),
+  CMP_LE = ((0U << 3) + 6),
+  CMP_ULE = ((0U << 3) + 7),
+  CMP_SAF = ((1U << 3) + 0),
+  CMP_SUN = ((1U << 3) + 1),
+  CMP_SEQ = ((1U << 3) + 2),
+  CMP_SUEQ = ((1U << 3) + 3),
+  CMP_SSLT = ((1U << 3) + 4),
+  CMP_SSULT = ((1U << 3) + 5),
+  CMP_SLE = ((1U << 3) + 6),
+  CMP_SULE = ((1U << 3) + 7),
+  // COP1 CMP negative predicates Bit 5..4 = 01.
+  CMP_AT = ((2U << 3) + 0),  // Reserved, not implemented.
+  CMP_OR = ((2U << 3) + 1),
+  CMP_UNE = ((2U << 3) + 2),
+  CMP_NE = ((2U << 3) + 3),
+  CMP_UGE = ((2U << 3) + 4),  // Reserved, not implemented.
+  CMP_OGE = ((2U << 3) + 5),  // Reserved, not implemented.
+  CMP_UGT = ((2U << 3) + 6),  // Reserved, not implemented.
+  CMP_OGT = ((2U << 3) + 7),  // Reserved, not implemented.
+  CMP_SAT = ((3U << 3) + 0),  // Reserved, not implemented.
+  CMP_SOR = ((3U << 3) + 1),
+  CMP_SUNE = ((3U << 3) + 2),
+  CMP_SNE = ((3U << 3) + 3),
+  CMP_SUGE = ((3U << 3) + 4),  // Reserved, not implemented.
+  CMP_SOGE = ((3U << 3) + 5),  // Reserved, not implemented.
+  CMP_SUGT = ((3U << 3) + 6),  // Reserved, not implemented.
+  CMP_SOGT = ((3U << 3) + 7),  // Reserved, not implemented.
+
+  SEL = ((2U << 3) + 0),
+  MOVF = ((2U << 3) + 1),      // Function field for MOVT.fmt and MOVF.fmt
+  MOVZ_C = ((2U << 3) + 2),    // COP1 on FPR registers.
+  MOVN_C = ((2U << 3) + 3),    // COP1 on FPR registers.
+  SELEQZ_C = ((2U << 3) + 4),  // COP1 on FPR registers.
+  SELNEZ_C = ((2U << 3) + 7),  // COP1 on FPR registers.
+
+  // COP1 Encoding of Function Field When rs=PS.
+
+  // COP1X Encoding of Function Field.
+  MADD_S = ((4U << 3) + 0),
+  MADD_D = ((4U << 3) + 1),
+  MSUB_S = ((5U << 3) + 0),
+  MSUB_D = ((5U << 3) + 1),
+
+  // PCREL Encoding of rt Field.
+  ADDIUPC = ((0U << 2) + 0),
+  LWPC = ((0U << 2) + 1),
+  LWUPC = ((0U << 2) + 2),
+  LDPC = ((0U << 3) + 6),
+  // reserved ((1U << 3) + 6),
+  AUIPC = ((3U << 3) + 6),
+  ALUIPC = ((3U << 3) + 7),
+
+  // POP66 Encoding of rs Field.
+  JIC = ((0U << 5) + 0),
+
+  // POP76 Encoding of rs Field.
+  JIALC = ((0U << 5) + 0),
+
+  // COP1 Encoding of rs Field for MSA Branch Instructions
+  BZ_V = (((1U << 3) + 3) << kRsShift),
+  BNZ_V = (((1U << 3) + 7) << kRsShift),
+  BZ_B = (((3U << 3) + 0) << kRsShift),
+  BZ_H = (((3U << 3) + 1) << kRsShift),
+  BZ_W = (((3U << 3) + 2) << kRsShift),
+  BZ_D = (((3U << 3) + 3) << kRsShift),
+  BNZ_B = (((3U << 3) + 4) << kRsShift),
+  BNZ_H = (((3U << 3) + 5) << kRsShift),
+  BNZ_W = (((3U << 3) + 6) << kRsShift),
+  BNZ_D = (((3U << 3) + 7) << kRsShift),
+
+  // MSA: Operation Field for MI10 Instruction Formats
+  MSA_LD = (8U << 2),
+  MSA_ST = (9U << 2),
+  LD_B = ((8U << 2) + 0),
+  LD_H = ((8U << 2) + 1),
+  LD_W = ((8U << 2) + 2),
+  LD_D = ((8U << 2) + 3),
+  ST_B = ((9U << 2) + 0),
+  ST_H = ((9U << 2) + 1),
+  ST_W = ((9U << 2) + 2),
+  ST_D = ((9U << 2) + 3),
+
+  // MSA: Operation Field for I5 Instruction Format
+  ADDVI = ((0U << 23) + 6),
+  SUBVI = ((1U << 23) + 6),
+  MAXI_S = ((2U << 23) + 6),
+  MAXI_U = ((3U << 23) + 6),
+  MINI_S = ((4U << 23) + 6),
+  MINI_U = ((5U << 23) + 6),
+  CEQI = ((0U << 23) + 7),
+  CLTI_S = ((2U << 23) + 7),
+  CLTI_U = ((3U << 23) + 7),
+  CLEI_S = ((4U << 23) + 7),
+  CLEI_U = ((5U << 23) + 7),
+  LDI = ((6U << 23) + 7),  // I10 instruction format
+  I5_DF_b = (0U << 21),
+  I5_DF_h = (1U << 21),
+  I5_DF_w = (2U << 21),
+  I5_DF_d = (3U << 21),
+
+  // MSA: Operation Field for I8 Instruction Format
+  ANDI_B = ((0U << 24) + 0),
+  ORI_B = ((1U << 24) + 0),
+  NORI_B = ((2U << 24) + 0),
+  XORI_B = ((3U << 24) + 0),
+  BMNZI_B = ((0U << 24) + 1),
+  BMZI_B = ((1U << 24) + 1),
+  BSELI_B = ((2U << 24) + 1),
+  SHF_B = ((0U << 24) + 2),
+  SHF_H = ((1U << 24) + 2),
+  SHF_W = ((2U << 24) + 2),
+
+  MSA_VEC_2R_2RF_MINOR = ((3U << 3) + 6),
+
+  // MSA: Operation Field for VEC Instruction Formats
+  AND_V = (((0U << 2) + 0) << 21),
+  OR_V = (((0U << 2) + 1) << 21),
+  NOR_V = (((0U << 2) + 2) << 21),
+  XOR_V = (((0U << 2) + 3) << 21),
+  BMNZ_V = (((1U << 2) + 0) << 21),
+  BMZ_V = (((1U << 2) + 1) << 21),
+  BSEL_V = (((1U << 2) + 2) << 21),
+
+  // MSA: Operation Field for 2R Instruction Formats
+  MSA_2R_FORMAT = (((6U << 2) + 0) << 21),
+  FILL = (0U << 18),
+  PCNT = (1U << 18),
+  NLOC = (2U << 18),
+  NLZC = (3U << 18),
+  MSA_2R_DF_b = (0U << 16),
+  MSA_2R_DF_h = (1U << 16),
+  MSA_2R_DF_w = (2U << 16),
+  MSA_2R_DF_d = (3U << 16),
+
+  // MSA: Operation Field for 2RF Instruction Formats
+  MSA_2RF_FORMAT = (((6U << 2) + 1) << 21),
+  FCLASS = (0U << 17),
+  FTRUNC_S = (1U << 17),
+  FTRUNC_U = (2U << 17),
+  FSQRT = (3U << 17),
+  FRSQRT = (4U << 17),
+  FRCP = (5U << 17),
+  FRINT = (6U << 17),
+  FLOG2 = (7U << 17),
+  FEXUPL = (8U << 17),
+  FEXUPR = (9U << 17),
+  FFQL = (10U << 17),
+  FFQR = (11U << 17),
+  FTINT_S = (12U << 17),
+  FTINT_U = (13U << 17),
+  FFINT_S = (14U << 17),
+  FFINT_U = (15U << 17),
+  MSA_2RF_DF_w = (0U << 16),
+  MSA_2RF_DF_d = (1U << 16),
+
+  // MSA: Operation Field for 3R Instruction Format
+  SLL_MSA = ((0U << 23) + 13),
+  SRA_MSA = ((1U << 23) + 13),
+  SRL_MSA = ((2U << 23) + 13),
+  BCLR = ((3U << 23) + 13),
+  BSET = ((4U << 23) + 13),
+  BNEG = ((5U << 23) + 13),
+  BINSL = ((6U << 23) + 13),
+  BINSR = ((7U << 23) + 13),
+  ADDV = ((0U << 23) + 14),
+  SUBV = ((1U << 23) + 14),
+  MAX_S = ((2U << 23) + 14),
+  MAX_U = ((3U << 23) + 14),
+  MIN_S = ((4U << 23) + 14),
+  MIN_U = ((5U << 23) + 14),
+  MAX_A = ((6U << 23) + 14),
+  MIN_A = ((7U << 23) + 14),
+  CEQ = ((0U << 23) + 15),
+  CLT_S = ((2U << 23) + 15),
+  CLT_U = ((3U << 23) + 15),
+  CLE_S = ((4U << 23) + 15),
+  CLE_U = ((5U << 23) + 15),
+  ADD_A = ((0U << 23) + 16),
+  ADDS_A = ((1U << 23) + 16),
+  ADDS_S = ((2U << 23) + 16),
+  ADDS_U = ((3U << 23) + 16),
+  AVE_S = ((4U << 23) + 16),
+  AVE_U = ((5U << 23) + 16),
+  AVER_S = ((6U << 23) + 16),
+  AVER_U = ((7U << 23) + 16),
+  SUBS_S = ((0U << 23) + 17),
+  SUBS_U = ((1U << 23) + 17),
+  SUBSUS_U = ((2U << 23) + 17),
+  SUBSUU_S = ((3U << 23) + 17),
+  ASUB_S = ((4U << 23) + 17),
+  ASUB_U = ((5U << 23) + 17),
+  MULV = ((0U << 23) + 18),
+  MADDV = ((1U << 23) + 18),
+  MSUBV = ((2U << 23) + 18),
+  DIV_S_MSA = ((4U << 23) + 18),
+  DIV_U = ((5U << 23) + 18),
+  MOD_S = ((6U << 23) + 18),
+  MOD_U = ((7U << 23) + 18),
+  DOTP_S = ((0U << 23) + 19),
+  DOTP_U = ((1U << 23) + 19),
+  DPADD_S = ((2U << 23) + 19),
+  DPADD_U = ((3U << 23) + 19),
+  DPSUB_S = ((4U << 23) + 19),
+  DPSUB_U = ((5U << 23) + 19),
+  SLD = ((0U << 23) + 20),
+  SPLAT = ((1U << 23) + 20),
+  PCKEV = ((2U << 23) + 20),
+  PCKOD = ((3U << 23) + 20),
+  ILVL = ((4U << 23) + 20),
+  ILVR = ((5U << 23) + 20),
+  ILVEV = ((6U << 23) + 20),
+  ILVOD = ((7U << 23) + 20),
+  VSHF = ((0U << 23) + 21),
+  SRAR = ((1U << 23) + 21),
+  SRLR = ((2U << 23) + 21),
+  HADD_S = ((4U << 23) + 21),
+  HADD_U = ((5U << 23) + 21),
+  HSUB_S = ((6U << 23) + 21),
+  HSUB_U = ((7U << 23) + 21),
+  MSA_3R_DF_b = (0U << 21),
+  MSA_3R_DF_h = (1U << 21),
+  MSA_3R_DF_w = (2U << 21),
+  MSA_3R_DF_d = (3U << 21),
+
+  // MSA: Operation Field for 3RF Instruction Format
+  FCAF = ((0U << 22) + 26),
+  FCUN = ((1U << 22) + 26),
+  FCEQ = ((2U << 22) + 26),
+  FCUEQ = ((3U << 22) + 26),
+  FCLT = ((4U << 22) + 26),
+  FCULT = ((5U << 22) + 26),
+  FCLE = ((6U << 22) + 26),
+  FCULE = ((7U << 22) + 26),
+  FSAF = ((8U << 22) + 26),
+  FSUN = ((9U << 22) + 26),
+  FSEQ = ((10U << 22) + 26),
+  FSUEQ = ((11U << 22) + 26),
+  FSLT = ((12U << 22) + 26),
+  FSULT = ((13U << 22) + 26),
+  FSLE = ((14U << 22) + 26),
+  FSULE = ((15U << 22) + 26),
+  FADD = ((0U << 22) + 27),
+  FSUB = ((1U << 22) + 27),
+  FMUL = ((2U << 22) + 27),
+  FDIV = ((3U << 22) + 27),
+  FMADD = ((4U << 22) + 27),
+  FMSUB = ((5U << 22) + 27),
+  FEXP2 = ((7U << 22) + 27),
+  FEXDO = ((8U << 22) + 27),
+  FTQ = ((10U << 22) + 27),
+  FMIN = ((12U << 22) + 27),
+  FMIN_A = ((13U << 22) + 27),
+  FMAX = ((14U << 22) + 27),
+  FMAX_A = ((15U << 22) + 27),
+  FCOR = ((1U << 22) + 28),
+  FCUNE = ((2U << 22) + 28),
+  FCNE = ((3U << 22) + 28),
+  MUL_Q = ((4U << 22) + 28),
+  MADD_Q = ((5U << 22) + 28),
+  MSUB_Q = ((6U << 22) + 28),
+  FSOR = ((9U << 22) + 28),
+  FSUNE = ((10U << 22) + 28),
+  FSNE = ((11U << 22) + 28),
+  MULR_Q = ((12U << 22) + 28),
+  MADDR_Q = ((13U << 22) + 28),
+  MSUBR_Q = ((14U << 22) + 28),
+
+  // MSA: Operation Field for ELM Instruction Format
+  MSA_ELM_MINOR = ((3U << 3) + 1),
+  SLDI = (0U << 22),
+  CTCMSA = ((0U << 22) | (62U << 16)),
+  SPLATI = (1U << 22),
+  CFCMSA = ((1U << 22) | (62U << 16)),
+  COPY_S = (2U << 22),
+  MOVE_V = ((2U << 22) | (62U << 16)),
+  COPY_U = (3U << 22),
+  INSERT = (4U << 22),
+  INSVE = (5U << 22),
+  ELM_DF_B = ((0U << 4) << 16),
+  ELM_DF_H = ((4U << 3) << 16),
+  ELM_DF_W = ((12U << 2) << 16),
+  ELM_DF_D = ((28U << 1) << 16),
+
+  // MSA: Operation Field for BIT Instruction Format
+  SLLI = ((0U << 23) + 9),
+  SRAI = ((1U << 23) + 9),
+  SRLI = ((2U << 23) + 9),
+  BCLRI = ((3U << 23) + 9),
+  BSETI = ((4U << 23) + 9),
+  BNEGI = ((5U << 23) + 9),
+  BINSLI = ((6U << 23) + 9),
+  BINSRI = ((7U << 23) + 9),
+  SAT_S = ((0U << 23) + 10),
+  SAT_U = ((1U << 23) + 10),
+  SRARI = ((2U << 23) + 10),
+  SRLRI = ((3U << 23) + 10),
+  BIT_DF_b = ((14U << 3) << 16),
+  BIT_DF_h = ((6U << 4) << 16),
+  BIT_DF_w = ((2U << 5) << 16),
+  BIT_DF_d = ((0U << 6) << 16),
+
+  nullptrSF = 0U
+};
+
+enum MSAMinorOpcode : uint32_t {
+  kMsaMinorUndefined = 0,
+  kMsaMinorI8,
+  kMsaMinorI5,
+  kMsaMinorI10,
+  kMsaMinorBIT,
+  kMsaMinor3R,
+  kMsaMinor3RF,
+  kMsaMinorELM,
+  kMsaMinorVEC,
+  kMsaMinor2R,
+  kMsaMinor2RF,
+  kMsaMinorMI10
+};
+
+// ----- Emulated conditions.
+// On XXXX64 we use this enum to abstract from conditional branch instructions.
+// The 'U' prefix is used to specify unsigned comparisons.
+// Opposite conditions must be paired as odd/even numbers
+// because 'NegateCondition' function flips LSB to negate condition.
+enum Condition {
+  // Any value < 0 is considered no_condition.
+  kNoCondition = -1,
+  overflow = 0,
+  no_overflow = 1,
+  Uless = 2,
+  Ugreater_equal = 3,
+  Uless_equal = 4,
+  Ugreater = 5,
+  equal = 6,
+  not_equal = 7,  // Unordered or Not Equal.
+  negative = 8,
+  positive = 9,
+  parity_even = 10,
+  parity_odd = 11,
+  less = 12,
+  greater_equal = 13,
+  less_equal = 14,
+  greater = 15,
+  ueq = 16,  // Unordered or Equal.
+  ogl = 17,  // Ordered and Not Equal.
+  cc_always = 18,
+
+  // Aliases.
+  carry = Uless,
+  not_carry = Ugreater_equal,
+  zero = equal,
+  eq = equal,
+  not_zero = not_equal,
+  ne = not_equal,
+  nz = not_equal,
+  sign = negative,
+  not_sign = positive,
+  mi = negative,
+  pl = positive,
+  hi = Ugreater,
+  ls = Uless_equal,
+  ge = greater_equal,
+  lt = less,
+  gt = greater,
+  le = less_equal,
+  hs = Ugreater_equal,
+  lo = Uless,
+  al = cc_always,
+  ult = Uless,
+  uge = Ugreater_equal,
+  ule = Uless_equal,
+  ugt = Ugreater,
+  cc_default = kNoCondition
+};
+
+
+// Returns the equivalent of !cc.
+// Negation of the default kNoCondition (-1) results in a non-default
+// no_condition value (-2). As long as tests for no_condition check
+// for condition < 0, this will work as expected.
+inline Condition NegateCondition(Condition cc) {
+  DCHECK(cc != cc_always);
+  return static_cast<Condition>(cc ^ 1);
+}
+
+
+inline Condition NegateFpuCondition(Condition cc) {
+  DCHECK(cc != cc_always);
+  switch (cc) {
+    case ult:
+      return ge;
+    case ugt:
+      return le;
+    case uge:
+      return lt;
+    case ule:
+      return gt;
+    case lt:
+      return uge;
+    case gt:
+      return ule;
+    case ge:
+      return ult;
+    case le:
+      return ugt;
+    case eq:
+      return ne;
+    case ne:
+      return eq;
+    case ueq:
+      return ogl;
+    case ogl:
+      return ueq;
+    default:
+      return cc;
+  }
+}
+
+enum MSABranchCondition {
+  all_not_zero = 0,   // Branch If All Elements Are Not Zero
+  one_elem_not_zero,  // Branch If At Least One Element of Any Format Is Not
+                      // Zero
+  one_elem_zero,      // Branch If At Least One Element Is Zero
+  all_zero            // Branch If All Elements of Any Format Are Zero
+};
+
+inline MSABranchCondition NegateMSABranchCondition(MSABranchCondition cond) {
+  switch (cond) {
+    case all_not_zero:
+      return one_elem_zero;
+    case one_elem_not_zero:
+      return all_zero;
+    case one_elem_zero:
+      return all_not_zero;
+    case all_zero:
+      return one_elem_not_zero;
+    default:
+      return cond;
+  }
+}
+
+enum MSABranchDF {
+  MSA_BRANCH_B = 0,
+  MSA_BRANCH_H,
+  MSA_BRANCH_W,
+  MSA_BRANCH_D,
+  MSA_BRANCH_V
+};
+
+
+// ----- Coprocessor conditions.
+enum FPUCondition {
+  kNoFPUCondition = -1,
+
+  F = 0x00,    // False.
+  UN = 0x01,   // Unordered.
+  EQ = 0x02,   // Equal.
+  UEQ = 0x03,  // Unordered or Equal.
+  OLT = 0x04,  // Ordered or Less Than, on Sw64 release < 3.
+  LT = 0x04,   // Ordered or Less Than, on Sw64 release >= 3.
+  ULT = 0x05,  // Unordered or Less Than.
+  OLE = 0x06,  // Ordered or Less Than or Equal, on Sw64 release < 3.
+  LE = 0x06,   // Ordered or Less Than or Equal, on Sw64 release >= 3.
+  ULE = 0x07,  // Unordered or Less Than or Equal.
+
+  // Following constants are available on Sw64 release >= 3 only.
+  ORD = 0x11,  // Ordered, on Sw64 release >= 3.
+  UNE = 0x12,  // Not equal, on Sw64 release >= 3.
+  NE = 0x13,   // Ordered Greater Than or Less Than. on Sw64 >= 3 only.
+};
+
+
+// FPU rounding modes.
+enum FPURoundingMode {
+  RN = 0 << 0,  // Round to Nearest.
+  RZ = 1 << 0,  // Round towards zero.
+  RP = 2 << 0,  // Round towards Plus Infinity.
+  RM = 3 << 0,  // Round towards Minus Infinity.
+
+  // Aliases.
+  kRoundToNearest = RN,
+  kRoundToZero = RZ,
+  kRoundToPlusInf = RP,
+  kRoundToMinusInf = RM,
+
+  mode_round = RN,
+  mode_ceil = RP,
+  mode_floor = RM,
+  mode_trunc = RZ
+};
+
+const uint32_t kFPURoundingModeMask = 3 << 0;
+
+enum CheckForInexactConversion {
+  kCheckForInexactConversion,
+  kDontCheckForInexactConversion
+};
+
+enum class MaxMinKind : int { kMin = 0, kMax = 1 };
+
+// -----------------------------------------------------------------------------
+// Hints.
+
+// Branch hints are not used on the XXXX64.  They are defined so that they can
+// appear in shared function signatures, but will be ignored in XXXX64
+// implementations.
+enum Hint {
+  no_hint = 0
+};
+
+
+inline Hint NegateHint(Hint hint) {
+  return no_hint;
+}
+
+
+// -----------------------------------------------------------------------------
+// Specific instructions, constants, and masks.
+// These constants are declared in assembler-sw64.cc, as they use named
+// registers and other constants.
+
+// addiu(sp, sp, 4) aka Pop() operation or part of Pop(r)
+// operations as post-increment of sp.
+//extern const Instr kPopInstruction;
+// addiu(sp, sp, -4) part of Push(r) operation as pre-decrement of sp.
+//extern const Instr kPushInstruction;
+// Stw(r, MemOperand(sp, 0))
+//extern const Instr kPushRegPattern;
+// Ldw(r, MemOperand(sp, 0))
+//extern const Instr kPopRegPattern;
+//extern const Instr kLwRegFpOffsetPattern;
+//extern const Instr kSwRegFpOffsetPattern;
+//extern const Instr kLwRegFpNegOffsetPattern;
+//extern const Instr kSwRegFpNegOffsetPattern;
+// A mask for the Rt register for push, pop, lw, sw instructions.
+extern const Instr kRtMask;
+extern const Instr kLwSwInstrTypeMask;
+extern const Instr kLwSwInstrArgumentMask;
+extern const Instr kLwSwOffsetMask;
+
+// simulator custom instruction
+const Instr rtCallRedirInstr = op_trap | REDIRECT;
+// A nop instruction. (Encoding of sll 0 0 0).
+const Instr nopInstr = 0;
+
+static constexpr uint64_t OpcodeToBitNumber(Opcode opcode) {
+  return 1ULL << (static_cast<uint32_t>(opcode) >> kOpcodeShift);
+}
+
+constexpr uint8_t kInstrSize = 4;
+constexpr uint8_t kInstrSizeLog2 = 2;
+
+class InstructionBase {
+ public:
+  enum {
+    // On XXXX64 PC cannot actually be directly accessed. We behave as if PC was
+    // always the value of the current instruction being executed.
+    kPCReadOffset = 0
+  };
+
+  // Instruction type.
+  enum Type { kRegisterType, kImmediateType, kJumpType, 
+#ifdef SW64  //jzy 20150213
+    kSwSyscallType,
+    kSwTransferanceType, 
+    kSwStorageType,     
+    kSwSimpleCalculationType,   
+    kSwCompositeCalculationType,
+    kSwExtendType,
+    kSwSimulatorTrap,
+#endif
+    kUnsupported = -1 };
+
+  // Get the raw instruction bits.
+  inline Instr InstructionBits() const {
+    return *reinterpret_cast<const Instr*>(this);
+  }
+
+  // Set the raw instruction bits to value.
+  inline void SetInstructionBits(Instr value) {
+    *reinterpret_cast<Instr*>(this) = value;
+  }
+
+  // Read one particular bit out of the instruction bits.
+  inline int Bit(int nr) const {
+    return (InstructionBits() >> nr) & 1;
+  }
+
+  // Read a bit field out of the instruction bits.
+  inline int Bits(int hi, int lo) const {
+    return (InstructionBits() >> lo) & ((2U << (hi - lo)) - 1);
+  }
+
+  static constexpr uint64_t kOpcodeImmediateTypeMask =
+      OpcodeToBitNumber(REGIMM) | OpcodeToBitNumber(BEQ) |
+      OpcodeToBitNumber(BNE) | OpcodeToBitNumber(BLEZ) |
+      OpcodeToBitNumber(BGTZ) | OpcodeToBitNumber(ADDI) |
+      OpcodeToBitNumber(DADDI) | OpcodeToBitNumber(ADDIU) |
+      /*OpcodeToBitNumber(DADDIU) |*/ OpcodeToBitNumber(SLTI) |
+      OpcodeToBitNumber(SLTIU) | OpcodeToBitNumber(ANDI) |
+      OpcodeToBitNumber(ORI) | OpcodeToBitNumber(XORI) |
+      OpcodeToBitNumber(LUI) | OpcodeToBitNumber(BEQL) |
+      OpcodeToBitNumber(BNEL) | OpcodeToBitNumber(BLEZL) |
+      OpcodeToBitNumber(BGTZL) | OpcodeToBitNumber(POP66) |
+      OpcodeToBitNumber(POP76) | OpcodeToBitNumber(LB) | OpcodeToBitNumber(LH) |
+      OpcodeToBitNumber(LWL) | OpcodeToBitNumber(LW) | OpcodeToBitNumber(LWU) |
+      OpcodeToBitNumber(LD) | OpcodeToBitNumber(LBU) | OpcodeToBitNumber(LHU) |
+      OpcodeToBitNumber(LDL) | OpcodeToBitNumber(LDR) | OpcodeToBitNumber(LWR) |
+      OpcodeToBitNumber(SDL) | OpcodeToBitNumber(SB) | OpcodeToBitNumber(SH) |
+      OpcodeToBitNumber(SWL) | OpcodeToBitNumber(SW) | OpcodeToBitNumber(SD) |
+      OpcodeToBitNumber(SWR) | OpcodeToBitNumber(SDR) |
+      OpcodeToBitNumber(LWC1) | OpcodeToBitNumber(LDC1) |
+      OpcodeToBitNumber(SWC1) | OpcodeToBitNumber(SDC1) |
+      OpcodeToBitNumber(PCREL) | OpcodeToBitNumber(DAUI) |
+      OpcodeToBitNumber(BC) | OpcodeToBitNumber(BALC);
+
+#define FunctionFieldToBitNumber(function) (1ULL << function)
+
+  // On r6, DCLZ_R6 aliases to existing MFLO.
+  static const uint64_t kFunctionFieldRegisterTypeMask =
+      FunctionFieldToBitNumber(JR) | FunctionFieldToBitNumber(JALR) |
+      FunctionFieldToBitNumber(BREAK) | FunctionFieldToBitNumber(SLL) |
+    /*  FunctionFieldToBitNumber(DSLL) | FunctionFieldToBitNumber(DSLL32) |*/
+      FunctionFieldToBitNumber(SRL) |/* FunctionFieldToBitNumber(DSRL) |
+      FunctionFieldToBitNumber(DSRL32) |*/ FunctionFieldToBitNumber(SRA) |
+     /* FunctionFieldToBitNumber(DSRA) | FunctionFieldToBitNumber(DSRA32) |*/
+      FunctionFieldToBitNumber(SLLV) |/* FunctionFieldToBitNumber(DSLLV) |*/
+      FunctionFieldToBitNumber(SRLV) |/* FunctionFieldToBitNumber(DSRLV) |*/
+      FunctionFieldToBitNumber(SRAV) |/* FunctionFieldToBitNumber(DSRAV) |*/
+      FunctionFieldToBitNumber(LSA) | FunctionFieldToBitNumber(DLSA) |
+      FunctionFieldToBitNumber(MFHI) | FunctionFieldToBitNumber(MFLO) |
+      FunctionFieldToBitNumber(MULT) | FunctionFieldToBitNumber(DMULT) |
+      FunctionFieldToBitNumber(MULTU) | FunctionFieldToBitNumber(DMULTU) |
+     /* FunctionFieldToBitNumber(DIV) | FunctionFieldToBitNumber(DDIV) |
+      FunctionFieldToBitNumber(DIVU) | FunctionFieldToBitNumber(DDIVU) |*/
+      FunctionFieldToBitNumber(ADD) | FunctionFieldToBitNumber(DADD) |
+     /* FunctionFieldToBitNumber(ADDU) | FunctionFieldToBitNumber(DADDU) |*/
+      FunctionFieldToBitNumber(SUB) | FunctionFieldToBitNumber(DSUB) |
+     /* FunctionFieldToBitNumber(SUBU) | FunctionFieldToBitNumber(DSUBU) |*/
+      FunctionFieldToBitNumber(AND) | FunctionFieldToBitNumber(OR) |
+      FunctionFieldToBitNumber(XOR) | FunctionFieldToBitNumber(NOR) |
+      FunctionFieldToBitNumber(SLT) | FunctionFieldToBitNumber(SLTU) |
+      FunctionFieldToBitNumber(TGE) | FunctionFieldToBitNumber(TGEU) |
+      FunctionFieldToBitNumber(TLT) | FunctionFieldToBitNumber(TLTU) |
+      FunctionFieldToBitNumber(TEQ) | FunctionFieldToBitNumber(TNE) |
+     /* FunctionFieldToBitNumber(MOVZ) |*/ FunctionFieldToBitNumber(MOVN) |
+      FunctionFieldToBitNumber(MOVCI) | FunctionFieldToBitNumber(SELEQZ_S) |
+      FunctionFieldToBitNumber(SELNEZ_S) | FunctionFieldToBitNumber(SYNC);
+
+
+  // Accessors for the different named fields used in the XXXX64 encoding.
+  inline Opcode OpcodeValue() const {
+    return static_cast<Opcode>(
+        Bits(kOpcodeShift + kOpcodeBits - 1, kOpcodeShift));
+  }
+
+  inline int FunctionFieldRaw() const {
+    return InstructionBits() & kFunctionFieldMask;
+  }
+
+  // Return the fields at their original place in the instruction encoding.
+  inline Opcode OpcodeFieldRaw() const {
+    return static_cast<Opcode>(InstructionBits() & kOpcodeMask);
+  }
+
+  inline int OpcodeFieldValue() const {
+    return static_cast<int>(InstructionBits() & kOpcodeMask);
+  }
+
+  // Safe to call within InstructionType().
+  inline int RsFieldRawNoAssert() const {
+    return InstructionBits() & kRsFieldMask;
+  }
+
+  inline int SaFieldRaw() const { return InstructionBits() & kSaFieldMask; }
+
+  // Get the encoding type of the instruction.
+  inline Type InstructionType() const;
+
+  inline MSAMinorOpcode MSAMinorOpcodeField() const {
+    int op = this->FunctionFieldRaw();
+    switch (op) {
+      case 0:
+      case 1:
+      case 2:
+        return kMsaMinorI8;
+      case 6:
+        return kMsaMinorI5;
+      case 7:
+        return (((this->InstructionBits() & kMsaI5I10Mask) == LDI)
+                    ? kMsaMinorI10
+                    : kMsaMinorI5);
+      case 9:
+      case 10:
+        return kMsaMinorBIT;
+      case 13:
+      case 14:
+      case 15:
+      case 16:
+      case 17:
+      case 18:
+      case 19:
+      case 20:
+      case 21:
+        return kMsaMinor3R;
+      case 25:
+        return kMsaMinorELM;
+      case 26:
+      case 27:
+      case 28:
+        return kMsaMinor3RF;
+      case 30:
+        switch (this->RsFieldRawNoAssert()) {
+          case MSA_2R_FORMAT:
+            return kMsaMinor2R;
+          case MSA_2RF_FORMAT:
+            return kMsaMinor2RF;
+          default:
+            return kMsaMinorVEC;
+        }
+        break;
+      case 32:
+      case 33:
+      case 34:
+      case 35:
+      case 36:
+      case 37:
+      case 38:
+      case 39:
+        return kMsaMinorMI10;
+      default:
+        return kMsaMinorUndefined;
+    }
+  }
+
+ protected:
+  InstructionBase() {}
+};
+
+template <class T>
+class InstructionGetters : public T {
+ public:
+  inline int RsValue() const {
+    return this->Bits(kRsShift + kRsBits - 1, kRsShift);
+  }
+
+  inline int RtValue() const {
+    return this->Bits(kRtShift + kRtBits - 1, kRtShift);
+  }
+
+  inline int RdValue() const {
+    return this->Bits(kRdShift + kRdBits - 1, kRdShift);
+  }
+
+  inline int BaseValue() const {
+    DCHECK_EQ(this->InstructionType(), InstructionBase::kImmediateType);
+    return this->Bits(kBaseShift + kBaseBits - 1, kBaseShift);
+  }
+
+  inline int SaValue() const {
+    DCHECK_EQ(this->InstructionType(), InstructionBase::kRegisterType);
+    return this->Bits(kSaShift + kSaBits - 1, kSaShift);
+  }
+
+  inline int LsaSaValue() const {
+    DCHECK_EQ(this->InstructionType(), InstructionBase::kRegisterType);
+    return this->Bits(kSaShift + kLsaSaBits - 1, kSaShift);
+  }
+
+  inline int FunctionValue() const {
+    DCHECK(this->InstructionType() == InstructionBase::kRegisterType ||
+           this->InstructionType() == InstructionBase::kImmediateType);
+    return this->Bits(kFunctionShift + kFunctionBits - 1, kFunctionShift);
+  }
+
+  inline int FdValue() const {
+    return this->Bits(kFdShift + kFdBits - 1, kFdShift);
+  }
+
+  inline int FsValue() const {
+    return this->Bits(kFsShift + kFsBits - 1, kFsShift);
+  }
+
+  inline int FtValue() const {
+    return this->Bits(kFtShift + kFtBits - 1, kFtShift);
+  }
+
+  inline int FrValue() const {
+    return this->Bits(kFrShift + kFrBits - 1, kFrShift);
+  }
+
+  inline int WdValue() const {
+    return this->Bits(kWdShift + kWdBits - 1, kWdShift);
+  }
+
+  inline int WsValue() const {
+    return this->Bits(kWsShift + kWsBits - 1, kWsShift);
+  }
+
+  inline int WtValue() const {
+    return this->Bits(kWtShift + kWtBits - 1, kWtShift);
+  }
+
+  inline int Bp2Value() const {
+    DCHECK_EQ(this->InstructionType(), InstructionBase::kRegisterType);
+    return this->Bits(kBp2Shift + kBp2Bits - 1, kBp2Shift);
+  }
+
+  inline int Bp3Value() const {
+    DCHECK_EQ(this->InstructionType(), InstructionBase::kRegisterType);
+    return this->Bits(kBp3Shift + kBp3Bits - 1, kBp3Shift);
+  }
+
+  // Float Compare condition code instruction bits.
+  inline int FCccValue() const {
+    return this->Bits(kFCccShift + kFCccBits - 1, kFCccShift);
+  }
+
+  // Float Branch condition code instruction bits.
+  inline int FBccValue() const {
+    return this->Bits(kFBccShift + kFBccBits - 1, kFBccShift);
+  }
+
+  // Float Branch true/false instruction bit.
+  inline int FBtrueValue() const {
+    return this->Bits(kFBtrueShift + kFBtrueBits - 1, kFBtrueShift);
+  }
+
+  // Return the fields at their original place in the instruction encoding.
+  inline Opcode OpcodeFieldRaw() const {
+    return static_cast<Opcode>(this->InstructionBits() & kOpcodeMask);
+  }
+
+  inline int OpcodeFieldValue() const {
+      return static_cast<int>(this->InstructionBits() & kOpcodeMask);
+  }
+
+  inline int RsFieldRaw() const {
+    DCHECK(this->InstructionType() == InstructionBase::kRegisterType ||
+           this->InstructionType() == InstructionBase::kImmediateType);
+    return this->InstructionBits() & kRsFieldMask;
+  }
+
+  // Same as above function, but safe to call within InstructionType().
+  inline int RsFieldRawNoAssert() const {
+    return this->InstructionBits() & kRsFieldMask;
+  }
+
+  inline int RtFieldRaw() const {
+    DCHECK(this->InstructionType() == InstructionBase::kRegisterType ||
+           this->InstructionType() == InstructionBase::kImmediateType);
+    return this->InstructionBits() & kRtFieldMask;
+  }
+
+  inline int RdFieldRaw() const {
+    DCHECK_EQ(this->InstructionType(), InstructionBase::kRegisterType);
+    return this->InstructionBits() & kRdFieldMask;
+  }
+
+  inline int SaFieldRaw() const {
+    return this->InstructionBits() & kSaFieldMask;
+  }
+
+  inline int FunctionFieldRaw() const {
+    return this->InstructionBits() & kFunctionFieldMask;
+  }
+
+#ifdef SW64 //jzy 20150213
+  inline int SwGetMask(int hi, int lo) const {
+    int mask = 2 << (hi-lo);
+    mask -= 1;
+    return mask << lo;
+  }
+ 
+  //the position of instruction function_code in SW is different in different type of instructions
+  inline int SwFunctionFieldRaw(int hi, int lo) const {
+    return this->InstructionBits() & SwGetMask(hi, lo);
+  }
+  inline int SwFunctionFieldValue(int hi, int lo) const {
+    return this->InstructionBits() & SwGetMask(hi, lo);
+  }
+
+  inline int SwImmOrDispFieldRaw(int hi, int lo) const {
+    return this->InstructionBits() & SwGetMask(hi, lo);
+  }
+  inline int SwImmOrDispFieldValue(int hi, int lo) const {
+    int shift_len = 32 - (hi-lo+1);
+    return (((this->InstructionBits() & SwGetMask(hi, lo)) >> lo) << shift_len) >> shift_len;
+  }
+
+  inline int SwRaFieldRaw() const {
+    return this->InstructionBits() & sRaFieldMask; //SwGetMask(25,21); 
+  }
+  inline int SwRbFieldRaw() const {
+    return this->InstructionBits() & sRbFieldMask; //SwGetMask(20,16); 
+  }
+  inline int SwRcFieldRaw(int hi, int lo) const {
+    return this->InstructionBits() & SwGetMask(hi,lo); 
+  }
+  inline int SwRdFieldRaw() const {
+    return this->InstructionBits() & sRdFieldMask; //SwGetMask(4,0); 
+  }
+
+  inline int SwFaFieldRaw() const {
+    return this->InstructionBits() & sRaFieldMask; //SwGetMask(25,21); 
+  }
+  inline int SwFbFieldRaw() const {
+    return this->InstructionBits() & sRbFieldMask; //SwGetMask(20,16); 
+  }
+  inline int SwFcFieldRaw(int hi, int lo) const {
+    return this->InstructionBits() & SwGetMask(hi,lo); 
+  }
+  inline int SwFdFieldRaw() const {
+    return this->InstructionBits() & sRdFieldMask; //SwGetMask(4,0); 
+  }
+
+  inline int SwRaValue() const {
+    return this->Bits(sRaShift+sRaBits-1, sRaShift); //Bits(25,21);
+  }
+  inline int SwRbValue() const {
+    return this->Bits(sRbShift+sRbBits-1, sRbShift); //Bits(20,16);
+  }
+  //the position of rc register in SW is different in different type of instructions
+  inline int SwRcValue(int hi, int lo) const {
+    return this->Bits(hi, lo);
+  }
+  inline int SwRdValue() const {
+    return this->Bits(sRdShift+sRdBits-1, sRdShift); //Bits(4,0);
+  }
+  inline int SwFaValue() const {
+    return this->Bits(sRaShift+sRaBits-1, sRaShift); //Bits(25,21);
+  }
+  inline int SwFbValue() const {
+    return this->Bits(sRbShift+sRbBits-1, sRbShift); //Bits(20,16);
+  }
+  //the position of rc register in SW is different in different type of instructions
+  inline int SwFcValue(int hi, int lo) const {
+    return this->Bits(hi, lo);
+  }
+  inline int SwFdValue() const {
+    return this->Bits(sRdShift+sRdBits-1, sRdShift); //Bits(4,0);
+  }
+#endif
+
+  // Get the secondary field according to the opcode.
+  inline int SecondaryValue() const {
+    Opcode op = this->OpcodeFieldRaw();
+    switch (op) {
+      case SPECIAL:
+      case SPECIAL2:
+        return FunctionValue();
+      case COP1:
+        return RsValue();
+      case REGIMM:
+        return RtValue();
+      default:
+        return nullptrSF;
+    }
+  }
+
+  inline int32_t ImmValue(int bits) const {
+    DCHECK_EQ(this->InstructionType(), InstructionBase::kImmediateType);
+    return this->Bits(bits - 1, 0);
+  }
+
+  inline int32_t Imm9Value() const {
+    DCHECK_EQ(this->InstructionType(), InstructionBase::kImmediateType);
+    return this->Bits(kImm9Shift + kImm9Bits - 1, kImm9Shift);
+  }
+
+  inline int32_t Imm16Value() const {
+    DCHECK_EQ(this->InstructionType(), InstructionBase::kImmediateType);
+    return this->Bits(kImm16Shift + kImm16Bits - 1, kImm16Shift);
+  }
+
+  inline int32_t Imm18Value() const {
+    return this->Bits(kImm18Shift + kImm18Bits - 1, kImm18Shift);
+  }
+
+  inline int32_t Imm19Value() const {
+    DCHECK_EQ(this->InstructionType(), InstructionBase::kImmediateType);
+    return this->Bits(kImm19Shift + kImm19Bits - 1, kImm19Shift);
+  }
+
+  inline int32_t Imm21Value() const {
+    DCHECK_EQ(this->InstructionType(), InstructionBase::kImmediateType);
+    return this->Bits(kImm21Shift + kImm21Bits - 1, kImm21Shift);
+  }
+
+  inline int32_t Imm26Value() const {
+    DCHECK((this->InstructionType() == InstructionBase::kJumpType) ||
+           (this->InstructionType() == InstructionBase::kImmediateType));
+    return this->Bits(kImm26Shift + kImm26Bits - 1, kImm26Shift);
+  }
+
+  inline int32_t MsaImm8Value() const {
+    DCHECK_EQ(this->InstructionType(), InstructionBase::kImmediateType);
+    return this->Bits(kMsaImm8Shift + kMsaImm8Bits - 1, kMsaImm8Shift);
+  }
+
+  inline int32_t MsaImm5Value() const {
+    DCHECK_EQ(this->InstructionType(), InstructionBase::kImmediateType);
+    return this->Bits(kMsaImm5Shift + kMsaImm5Bits - 1, kMsaImm5Shift);
+  }
+
+  inline int32_t MsaImm10Value() const {
+    DCHECK_EQ(this->InstructionType(), InstructionBase::kImmediateType);
+    return this->Bits(kMsaImm10Shift + kMsaImm10Bits - 1, kMsaImm10Shift);
+  }
+
+  inline int32_t MsaImmMI10Value() const {
+    DCHECK_EQ(this->InstructionType(), InstructionBase::kImmediateType);
+    return this->Bits(kMsaImmMI10Shift + kMsaImmMI10Bits - 1, kMsaImmMI10Shift);
+  }
+
+  inline int32_t MsaBitDf() const {
+    DCHECK_EQ(this->InstructionType(), InstructionBase::kImmediateType);
+    int32_t df_m = this->Bits(22, 16);
+    if (((df_m >> 6) & 1U) == 0) {
+      return 3;
+    } else if (((df_m >> 5) & 3U) == 2) {
+      return 2;
+    } else if (((df_m >> 4) & 7U) == 6) {
+      return 1;
+    } else if (((df_m >> 3) & 15U) == 14) {
+      return 0;
+    } else {
+      return -1;
+    }
+  }
+
+  inline int32_t MsaBitMValue() const {
+    DCHECK_EQ(this->InstructionType(), InstructionBase::kImmediateType);
+    return this->Bits(16 + this->MsaBitDf() + 3, 16);
+  }
+
+  inline int32_t MsaElmDf() const {
+    DCHECK(this->InstructionType() == InstructionBase::kRegisterType ||
+           this->InstructionType() == InstructionBase::kImmediateType);
+    int32_t df_n = this->Bits(21, 16);
+    if (((df_n >> 4) & 3U) == 0) {
+      return 0;
+    } else if (((df_n >> 3) & 7U) == 4) {
+      return 1;
+    } else if (((df_n >> 2) & 15U) == 12) {
+      return 2;
+    } else if (((df_n >> 1) & 31U) == 28) {
+      return 3;
+    } else {
+      return -1;
+    }
+  }
+
+  inline int32_t MsaElmNValue() const {
+    DCHECK(this->InstructionType() == InstructionBase::kRegisterType ||
+           this->InstructionType() == InstructionBase::kImmediateType);
+    return this->Bits(16 + 4 - this->MsaElmDf(), 16);
+  }
+
+  static bool IsForbiddenAfterBranchInstr(Instr instr);
+
+  // Say if the instruction should not be used in a branch delay slot or
+  // immediately after a compact branch.
+  inline bool IsForbiddenAfterBranch() const {
+    return IsForbiddenAfterBranchInstr(this->InstructionBits());
+  }
+
+  inline bool IsForbiddenInBranchDelay() const {
+    return IsForbiddenAfterBranch();
+  }
+
+  // Say if the instruction 'links'. e.g. jal, bal.
+  bool IsLinkingInstruction() const;
+  // Say if the instruction is a break or a trap.
+  bool IsTrap() const;
+
+  inline bool IsMSABranchInstr() const {
+    if (this->OpcodeFieldRaw() == COP1) {
+      switch (this->RsFieldRaw()) {
+        case BZ_V:
+        case BZ_B:
+        case BZ_H:
+        case BZ_W:
+        case BZ_D:
+        case BNZ_V:
+        case BNZ_B:
+        case BNZ_H:
+        case BNZ_W:
+        case BNZ_D:
+          return true;
+        default:
+          return false;
+      }
+    }
+    return false;
+  }
+
+  inline bool IsMSAInstr() const {
+    if (this->IsMSABranchInstr() || (this->OpcodeFieldRaw() == MSA))
+      return true;
+    return false;
+  }
+};
+
+class Instruction : public InstructionGetters<InstructionBase> {
+ public:
+  // Instructions are read of out a code stream. The only way to get a
+  // reference to an instruction is to convert a pointer. There is no way
+  // to allocate or create instances of class Instruction.
+  // Use the At(pc) function to create references to Instruction.
+  static Instruction* At(byte* pc) {
+    return reinterpret_cast<Instruction*>(pc);
+  }
+
+ private:
+  // We need to prevent the creation of instances of class Instruction.
+  DISALLOW_IMPLICIT_CONSTRUCTORS(Instruction);
+};
+
+
+// -----------------------------------------------------------------------------
+// XXXX64 assembly various constants.
+
+// C/C++ argument slots size.
+const int kCArgSlotCount = 0;
+
+// TODO(plind): below should be based on kPointerSize
+// TODO(plind): find all usages and remove the needless instructions for n64.
+const int kCArgsSlotsSize = kCArgSlotCount * kInstrSize * 2;
+
+const int kInvalidStackOffset = -1;
+const int kBranchReturnOffset = 1 * kInstrSize;
+
+static const int kNegOffset = 0x00008000;
+
+InstructionBase::Type InstructionBase::InstructionType() const {
+  switch (OpcodeFieldValue()) {
+    //cjq 20150317: add instruction 'sys_call' to do
+    case op_sys_call:
+        return kSwSyscallType;
+
+    case op_call:
+    case op_ret:
+    case op_jmp:
+    case op_ldwe:
+    case op_ldse:
+    case op_ldde:
+    case op_vlds:
+    case op_vldd:
+    case op_vsts:
+    case op_vstd:
+    case op_ldbu:
+    case op_ldhu:
+    case op_ldw:
+    case op_ldl:
+    case op_ldl_u:
+    case op_flds:
+    case op_fldd:
+    case op_stb:
+    case op_sth:
+    case op_stw:
+    case op_stl:
+    case op_stl_u:
+    case op_fsts:
+    case op_fstd:
+    case op_ldi:
+    case op_ldih:
+        return kSwStorageType;
+    case OP(0x08):
+        return kSwStorageType;
+    case OP(0x06):
+        return kSwStorageType;//jzy 20150213:TODO
+
+    case op_br:
+    case op_bsr:
+    case op_beq:
+    case op_bne:
+    case op_blt:
+    case op_ble:
+    case op_bgt:
+    case op_bge:
+    case op_blbc:
+    case op_blbs:
+    case op_fbeq:
+    case op_fbne:
+    case op_fblt:
+    case op_fble:
+    case op_fbgt:
+    case op_fbge:
+        return kSwTransferanceType;//ld 20150319
+
+    case OP(0x10): 
+    case OP(0x12):
+    case OP(0x18):
+        return kSwSimpleCalculationType;
+
+    case OP(0x11):
+    case OP(0x13):
+    case OP(0x19):
+        return kSwCompositeCalculationType;
+
+    case op_trap:
+        return kSwSimulatorTrap;
+
+    default:
+      return kImmediateType;
+  }
+
+  return kUnsupported;
+}
+#undef OpcodeToBitNumber
+#undef FunctionFieldToBitNumber
+
+// -----------------------------------------------------------------------------
+// Instructions.
+
+template <class P>
+bool InstructionGetters<P>::IsLinkingInstruction() const {
+  switch (OpcodeFieldRaw()) {
+    case JAL:
+      return true;
+    case POP76:
+      if (RsFieldRawNoAssert() == JIALC)
+        return true;  // JIALC
+      else
+        return false;  // BNEZC
+    case REGIMM:
+      switch (RtFieldRaw()) {
+        case BGEZAL:
+        case BLTZAL:
+          return true;
+        default:
+          return false;
+      }
+    case SPECIAL:
+      switch (FunctionFieldRaw()) {
+        case JALR:
+          return true;
+        default:
+          return false;
+      }
+    default:
+      return false;
+  }
+}
+
+template <class P>
+bool InstructionGetters<P>::IsTrap() const {
+    return OpcodeFieldValue() == op_trap;
+}
+
+// static
+template <class T>
+bool InstructionGetters<T>::IsForbiddenAfterBranchInstr(Instr instr) {
+  Opcode opcode = static_cast<Opcode>(instr & kOpcodeMask);
+  switch (opcode) {
+    case J:
+    case JAL:
+    case BEQ:
+    case BNE:
+    case BLEZ:  // POP06 bgeuc/bleuc, blezalc, bgezalc
+    case BGTZ:  // POP07 bltuc/bgtuc, bgtzalc, bltzalc
+    case BEQL:
+    case BNEL:
+    case BLEZL:  // POP26 bgezc, blezc, bgec/blec
+    case BGTZL:  // POP27 bgtzc, bltzc, bltc/bgtc
+    case BC:
+    case BALC:
+    case POP10:  // beqzalc, bovc, beqc
+    case POP30:  // bnezalc, bnvc, bnec
+    case POP66:  // beqzc, jic
+    case POP76:  // bnezc, jialc
+      return true;
+    case REGIMM:
+      switch (instr & kRtFieldMask) {
+        case BLTZ:
+        case BGEZ:
+        case BLTZAL:
+        case BGEZAL:
+          return true;
+        default:
+          return false;
+      }
+      break;
+    case SPECIAL:
+      switch (instr & kFunctionFieldMask) {
+        case JR:
+        case JALR:
+          return true;
+        default:
+          return false;
+      }
+      break;
+    case COP1:
+      switch (instr & kRsFieldMask) {
+        case BC1:
+        case BC1EQZ:
+        case BC1NEZ:
+        case BZ_V:
+        case BZ_B:
+        case BZ_H:
+        case BZ_W:
+        case BZ_D:
+        case BNZ_V:
+        case BNZ_B:
+        case BNZ_H:
+        case BNZ_W:
+        case BNZ_D:
+          return true;
+          break;
+        default:
+          return false;
+      }
+      break;
+    default:
+      return false;
+  }
+}
+
+#ifdef SW64
+
+#undef  OP
+#undef  PCD
+#undef  OPMEM
+#undef  BRA
+
+#undef  OFP
+#undef  FMA
+#undef  MFC
+#undef  MBR
+#undef  OPR
+#undef  OPRL
+#undef  TOPR
+#undef  TOPRL
+
+#undef  ATMEM
+#undef  PRIRET
+#undef  SPCD
+#undef  EV6HWMEM
+#undef  CSR
+
+#undef  LOGX
+#undef  PSE_LOGX
+
+#endif
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_CODEGEN_SW64_CONSTANTS_SW64_H_
diff --git a/src/3rdparty/chromium/v8/src/codegen/sw64/cpu-sw64.cc b/src/3rdparty/chromium/v8/src/codegen/sw64/cpu-sw64.cc
new file mode 100755
index 0000000000..bfd8138a8b
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/codegen/sw64/cpu-sw64.cc
@@ -0,0 +1,40 @@
+// Copyright 2012 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+// CPU specific code for arm independent of OS goes here.
+
+#include <sys/syscall.h>
+#include <unistd.h>
+
+#if V8_TARGET_ARCH_SW64
+
+#include "src/codegen/assembler.h"
+#include "src/codegen/macro-assembler.h"
+
+#include "src/execution/simulator.h"  // For cache flushing.
+#include "src/codegen/cpu-features.h"
+
+namespace v8 {
+namespace internal {
+
+
+void CpuFeatures::FlushICache(void* start, size_t size) {
+  // Nothing to do, flushing no instructions.
+  if (size == 0) {
+    return;
+  }
+
+#if defined(__sw_64__)
+  asm volatile("ldi  $0, 266($31)\n"
+               "sys_call 0x83\n");
+#else
+#error "Target Architecture are not supported in CpuFeatures::FlushICache"
+#endif
+
+}
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_TARGET_ARCH_SW64
diff --git a/src/3rdparty/chromium/v8/src/codegen/sw64/interface-descriptors-sw64.cc b/src/3rdparty/chromium/v8/src/codegen/sw64/interface-descriptors-sw64.cc
new file mode 100755
index 0000000000..8edc3a06f7
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/codegen/sw64/interface-descriptors-sw64.cc
@@ -0,0 +1,351 @@
+// Copyright 2012 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#if V8_TARGET_ARCH_SW64
+
+#include "src/codegen/interface-descriptors.h"
+
+#include "src/execution/frames.h"
+
+namespace v8 {
+namespace internal {
+
+const Register CallInterfaceDescriptor::ContextRegister() { return cp; }
+
+void CallInterfaceDescriptor::DefaultInitializePlatformSpecific(
+    CallInterfaceDescriptorData* data, int register_parameter_count) {
+  const Register default_stub_registers[] = {a0, a1, a2, a3, a4};
+  CHECK_LE(static_cast<size_t>(register_parameter_count),
+           arraysize(default_stub_registers));
+  data->InitializePlatformSpecific(register_parameter_count,
+                                   default_stub_registers);
+}
+
+void WasmI32AtomicWait32Descriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  const Register default_stub_registers[] = {a0, a1, a2, a3};
+  CHECK_EQ(static_cast<size_t>(kParameterCount),
+           arraysize(default_stub_registers));
+  data->InitializePlatformSpecific(kParameterCount, default_stub_registers);
+}
+
+void WasmI64AtomicWait32Descriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  const Register default_stub_registers[] = {a0, a1, a2, a3, a4};
+  CHECK_EQ(static_cast<size_t>(kParameterCount - kStackArgumentsCount),
+           arraysize(default_stub_registers));
+  data->InitializePlatformSpecific(kParameterCount - kStackArgumentsCount,
+                                   default_stub_registers);
+}
+
+void RecordWriteDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  const Register default_stub_registers[] = {a0, a1, a2, a3, kReturnRegister0};
+
+  data->RestrictAllocatableRegisters(default_stub_registers,
+                                     arraysize(default_stub_registers));
+
+  CHECK_LE(static_cast<size_t>(kParameterCount),
+           arraysize(default_stub_registers));
+  data->InitializePlatformSpecific(kParameterCount, default_stub_registers);
+}
+
+void EphemeronKeyBarrierDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  const Register default_stub_registers[] = {a0, a1, a2, a3, kReturnRegister0};
+
+  data->RestrictAllocatableRegisters(default_stub_registers,
+                                     arraysize(default_stub_registers));
+
+  CHECK_LE(static_cast<size_t>(kParameterCount),
+           arraysize(default_stub_registers));
+  data->InitializePlatformSpecific(kParameterCount, default_stub_registers);
+}
+
+const Register LoadDescriptor::ReceiverRegister() { return a1; }
+const Register LoadDescriptor::NameRegister() { return a2; }
+const Register LoadDescriptor::SlotRegister() { return a0; }
+
+const Register LoadWithVectorDescriptor::VectorRegister() { return a3; }
+
+const Register
+LoadWithReceiverAndVectorDescriptor::LookupStartObjectRegister() {
+  return a4;
+}
+
+const Register StoreDescriptor::ReceiverRegister() { return a1; }
+const Register StoreDescriptor::NameRegister() { return a2; }
+const Register StoreDescriptor::ValueRegister() { return a0; }
+const Register StoreDescriptor::SlotRegister() { return a4; }
+
+const Register StoreWithVectorDescriptor::VectorRegister() { return a3; }
+
+const Register StoreTransitionDescriptor::SlotRegister() { return a4; }
+const Register StoreTransitionDescriptor::VectorRegister() { return a3; }
+const Register StoreTransitionDescriptor::MapRegister() { return a5; }
+
+const Register ApiGetterDescriptor::HolderRegister() { return a0; }
+const Register ApiGetterDescriptor::CallbackRegister() { return a3; }
+
+const Register GrowArrayElementsDescriptor::ObjectRegister() { return a0; }
+const Register GrowArrayElementsDescriptor::KeyRegister() { return a3; }
+
+
+// static
+const Register TypeConversionDescriptor::ArgumentRegister() { return a0; }
+
+void TypeofDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  Register registers[] = {a3};
+  data->InitializePlatformSpecific(arraysize(registers), registers);
+}
+
+void CallTrampolineDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  // a1: target
+  // a0: number of arguments
+  Register registers[] = {a1, a0};
+  data->InitializePlatformSpecific(arraysize(registers), registers);
+}
+
+void CallVarargsDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  // a0 : number of arguments (on the stack, not including receiver)
+  // a1 : the target to call
+  // a4 : arguments list length (untagged)
+  // a2 : arguments list (FixedArray)
+  Register registers[] = {a1, a0, a4, a2};
+  data->InitializePlatformSpecific(arraysize(registers), registers);
+}
+
+void CallForwardVarargsDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  // a1: the target to call
+  // a0: number of arguments
+  // a2: start index (to support rest parameters)
+  Register registers[] = {a1, a0, a2};
+  data->InitializePlatformSpecific(arraysize(registers), registers);
+}
+
+void CallFunctionTemplateDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  // a1 : function template info
+  // a0 : number of arguments (on the stack, not including receiver)
+  Register registers[] = {a1, a0};
+  data->InitializePlatformSpecific(arraysize(registers), registers);
+}
+
+void CallWithSpreadDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  // a0 : number of arguments (on the stack, not including receiver)
+  // a1 : the target to call
+  // a2 : the object to spread
+  Register registers[] = {a1, a0, a2};
+  data->InitializePlatformSpecific(arraysize(registers), registers);
+}
+
+void CallWithArrayLikeDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  // a1 : the target to call
+  // a2 : the arguments list
+  Register registers[] = {a1, a2};
+  data->InitializePlatformSpecific(arraysize(registers), registers);
+}
+
+void ConstructVarargsDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  // a0 : number of arguments (on the stack, not including receiver)
+  // a1 : the target to call
+  // a3 : the new target
+  // a4 : arguments list length (untagged)
+  // a2 : arguments list (FixedArray)
+  Register registers[] = {a1, a3, a0, a4, a2};
+  data->InitializePlatformSpecific(arraysize(registers), registers);
+}
+
+void ConstructForwardVarargsDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  // a1: the target to call
+  // a3: new target
+  // a0: number of arguments
+  // a2: start index (to support rest parameters)
+  Register registers[] = {a1, a3, a0, a2};
+  data->InitializePlatformSpecific(arraysize(registers), registers);
+}
+
+void ConstructWithSpreadDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  // a0 : number of arguments (on the stack, not including receiver)
+  // a1 : the target to call
+  // a3 : the new target
+  // a2 : the object to spread
+  Register registers[] = {a1, a3, a0, a2};
+  data->InitializePlatformSpecific(arraysize(registers), registers);
+}
+
+void ConstructWithArrayLikeDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  // a1 : the target to call
+  // a3 : the new target
+  // a2 : the arguments list
+  Register registers[] = {a1, a3, a2};
+  data->InitializePlatformSpecific(arraysize(registers), registers);
+}
+
+void ConstructStubDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  // a1: target
+  // a3: new target
+  // a0: number of arguments
+  // a2: allocation site or undefined
+  Register registers[] = {a1, a3, a0, a2};
+  data->InitializePlatformSpecific(arraysize(registers), registers);
+}
+
+void AbortDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  Register registers[] = {a0};
+  data->InitializePlatformSpecific(arraysize(registers), registers);
+}
+
+void CompareDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  Register registers[] = {a1, a0};
+  data->InitializePlatformSpecific(arraysize(registers), registers);
+}
+
+
+void BinaryOpDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  Register registers[] = {a1, a0};
+  data->InitializePlatformSpecific(arraysize(registers), registers);
+}
+
+void ArgumentsAdaptorDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  Register registers[] = {
+      a1,  // JSFunction
+      a3,  // the new target
+      a0,  // actual number of arguments
+      a2,  // expected number of arguments
+  };
+  data->InitializePlatformSpecific(arraysize(registers), registers);
+}
+
+void ApiCallbackDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  Register registers[] = {
+      a1,                                   // kApiFunctionAddress
+      a2,                                   // kArgc
+      a3,  // kCallData
+      a0,  // kHolder
+  };
+  data->InitializePlatformSpecific(arraysize(registers), registers);
+}
+
+void InterpreterDispatchDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  Register registers[] = {
+      kInterpreterAccumulatorRegister, kInterpreterBytecodeOffsetRegister,
+      kInterpreterBytecodeArrayRegister, kInterpreterDispatchTableRegister};
+  data->InitializePlatformSpecific(arraysize(registers), registers);
+}
+
+void InterpreterPushArgsThenCallDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  Register registers[] = {
+      a0,  // argument count (not including receiver)
+      a2,  // address of first argument
+      a1   // the target callable to be call
+  };
+  data->InitializePlatformSpecific(arraysize(registers), registers);
+}
+
+void InterpreterPushArgsThenConstructDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  Register registers[] = {
+      a0,  // argument count (not including receiver)
+      a4,  // address of the first argument
+      a1,  // constructor to call
+      a3,  // new target
+      a2,  // allocation site feedback if available, undefined otherwise
+  };
+  data->InitializePlatformSpecific(arraysize(registers), registers);
+}
+
+void ResumeGeneratorDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  Register registers[] = {
+      v0,  // the value to pass to the generator
+      a1   // the JSGeneratorObject to resume
+  };
+  data->InitializePlatformSpecific(arraysize(registers), registers);
+}
+
+void FrameDropperTrampolineDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  Register registers[] = {
+      a1,  // loaded new FP
+  };
+  data->InitializePlatformSpecific(arraysize(registers), registers);
+}
+
+void RunMicrotasksEntryDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  Register registers[] = {a0, a1};
+  data->InitializePlatformSpecific(arraysize(registers), registers);
+}
+
+void BinaryOp_WithFeedbackDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  // TODO(v8:8888): Implement on this platform.
+  DefaultInitializePlatformSpecific(data, 4);
+}
+
+void CallTrampoline_WithFeedbackDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  // TODO(v8:8888): Implement on this platform.
+  DefaultInitializePlatformSpecific(data, 4);
+}
+
+void CallWithArrayLike_WithFeedbackDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  // TODO(v8:8888): Implement on this platform.
+  DefaultInitializePlatformSpecific(data, 4);
+}
+
+void CallWithSpread_WithFeedbackDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  // TODO(v8:8888): Implement on this platform.
+  DefaultInitializePlatformSpecific(data, 4);
+}
+
+void ConstructWithArrayLike_WithFeedbackDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  // TODO(v8:8888): Implement on this platform.
+  DefaultInitializePlatformSpecific(data, 4);
+}
+
+void ConstructWithSpread_WithFeedbackDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  // TODO(v8:8888): Implement on this platform.
+  DefaultInitializePlatformSpecific(data, 4);
+}
+
+void Compare_WithFeedbackDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  // TODO(v8:8888): Implement on this platform.
+  DefaultInitializePlatformSpecific(data, 4);
+}
+
+void UnaryOp_WithFeedbackDescriptor::InitializePlatformSpecific(
+    CallInterfaceDescriptorData* data) {
+  // TODO(v8:8888): Implement on this platform.
+  DefaultInitializePlatformSpecific(data, 3);
+}
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_TARGET_ARCH_SW64
diff --git a/src/3rdparty/chromium/v8/src/codegen/sw64/macro-assembler-sw64.cc b/src/3rdparty/chromium/v8/src/codegen/sw64/macro-assembler-sw64.cc
new file mode 100755
index 0000000000..a678af1406
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/codegen/sw64/macro-assembler-sw64.cc
@@ -0,0 +1,5089 @@
+// Copyright 2012 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#include <limits.h>  // For LONG_MIN, LONG_MAX.
+
+#if V8_TARGET_ARCH_SW64
+
+#include "src/base/bits.h"
+#include "src/base/division-by-constant.h"
+#include "src/codegen/assembler-inl.h"
+#include "src/codegen/callable.h"
+#include "src/codegen/code-factory.h"
+#include "src/codegen/external-reference-table.h"
+#include "src/codegen/macro-assembler.h"
+#include "src/codegen/register-configuration.h"
+#include "src/debug/debug.h"
+#include "src/execution/frames-inl.h"
+#include "src/heap/memory-chunk.h"
+#include "src/init/bootstrapper.h"
+#include "src/logging/counters.h"
+#include "src/objects/heap-number.h"
+#include "src/runtime/runtime.h"
+#include "src/snapshot/embedded/embedded-data.h"
+#include "src/snapshot/snapshot.h"
+#include "src/wasm/wasm-code-manager.h"
+
+namespace v8 {
+namespace internal {
+
+static inline bool IsZero(const Operand& rt) {
+  if (rt.is_reg()) {
+    return rt.rm() == zero_reg;
+  } else {
+    return rt.immediate() == 0;
+  }
+}
+
+int TurboAssembler::RequiredStackSizeForCallerSaved(SaveFPRegsMode fp_mode,
+                                                    Register exclusion1,
+                                                    Register exclusion2,
+                                                    Register exclusion3) const {
+  int bytes = 0;
+  RegList exclusions = 0;
+  if (exclusion1 != no_reg) {
+    exclusions |= exclusion1.bit();
+    if (exclusion2 != no_reg) {
+      exclusions |= exclusion2.bit();
+      if (exclusion3 != no_reg) {
+        exclusions |= exclusion3.bit();
+      }
+    }
+  }
+
+  RegList list = kJSCallerSaved & ~exclusions;
+  bytes += NumRegs(list) * kPointerSize;
+
+  if (fp_mode == kSaveFPRegs) {
+    bytes += NumRegs(kCallerSavedFPU) * kDoubleSize;
+  }
+
+  return bytes;
+}
+
+int TurboAssembler::PushCallerSaved(SaveFPRegsMode fp_mode, Register exclusion1,
+                                    Register exclusion2, Register exclusion3) {
+  int bytes = 0;
+  RegList exclusions = 0;
+  if (exclusion1 != no_reg) {
+    exclusions |= exclusion1.bit();
+    if (exclusion2 != no_reg) {
+      exclusions |= exclusion2.bit();
+      if (exclusion3 != no_reg) {
+        exclusions |= exclusion3.bit();
+      }
+    }
+  }
+
+  RegList list = kJSCallerSaved & ~exclusions;
+  MultiPush(list);
+  bytes += NumRegs(list) * kPointerSize;
+
+  if (fp_mode == kSaveFPRegs) {
+    MultiPushFPU(kCallerSavedFPU);
+    bytes += NumRegs(kCallerSavedFPU) * kDoubleSize;
+  }
+
+  return bytes;
+}
+
+int TurboAssembler::PopCallerSaved(SaveFPRegsMode fp_mode, Register exclusion1,
+                                   Register exclusion2, Register exclusion3) {
+  int bytes = 0;
+  if (fp_mode == kSaveFPRegs) {
+    MultiPopFPU(kCallerSavedFPU);
+    bytes += NumRegs(kCallerSavedFPU) * kDoubleSize;
+  }
+
+  RegList exclusions = 0;
+  if (exclusion1 != no_reg) {
+    exclusions |= exclusion1.bit();
+    if (exclusion2 != no_reg) {
+      exclusions |= exclusion2.bit();
+      if (exclusion3 != no_reg) {
+        exclusions |= exclusion3.bit();
+      }
+    }
+  }
+
+  RegList list = kJSCallerSaved & ~exclusions;
+  MultiPop(list);
+  bytes += NumRegs(list) * kPointerSize;
+
+  return bytes;
+}
+
+void TurboAssembler::LoadRoot(Register destination, RootIndex index) {SCOPEMARK_NAME(TurboAssembler::LoadRoot, this);
+  Ldl(destination, MemOperand(s4, RootRegisterOffsetForRootIndex(index)));
+}
+
+void TurboAssembler::LoadRoot(Register destination, RootIndex index,
+                              Condition cond, Register src1,
+                              const Operand& src2) {SCOPEMARK_NAME(TurboAssembler::LoadRoot, this);
+  Branch(2, NegateCondition(cond), src1, src2);
+  Ldl(destination, MemOperand(s4, RootRegisterOffsetForRootIndex(index)));
+}
+
+
+void TurboAssembler::PushCommonFrame(Register marker_reg) {SCOPEMARK_NAME(TurboAssembler::PushCommonFrame, this);
+  if (marker_reg.is_valid()) {
+    Push(ra, fp, marker_reg);
+    Addl(fp, sp, Operand(kPointerSize));
+  } else {
+    Push(ra, fp);
+    mov(fp, sp);
+  }
+}
+
+void TurboAssembler::PushStandardFrame(Register function_reg) {SCOPEMARK_NAME(TurboAssembler::PushStandardFrame, this);
+  int offset = -StandardFrameConstants::kContextOffset;
+  if (function_reg.is_valid()) {
+    Push(ra, fp, cp, function_reg, kJavaScriptCallArgCountRegister);
+    offset += 2 * kPointerSize;
+  } else {
+    Push(ra, fp, cp, kJavaScriptCallArgCountRegister);
+    offset += kPointerSize;
+  }
+  Addl(fp, sp, Operand(offset));
+}
+
+int MacroAssembler::SafepointRegisterStackIndex(int reg_code) {
+  // The registers are pushed starting with the highest encoding,
+  // which means that lowest encodings are closest to the stack pointer.
+  return kSafepointRegisterStackIndexMap[reg_code];
+}
+
+
+// Clobbers object, dst, value, and ra, if (ra_status == kRAHasBeenSaved)
+// The register 'object' contains a heap object pointer.  The heap object
+// tag is shifted away.
+void MacroAssembler::RecordWriteField(Register object, int offset,
+                                      Register value, Register dst,
+                                      RAStatus ra_status,
+                                      SaveFPRegsMode save_fp,
+                                      RememberedSetAction remembered_set_action,
+                                      SmiCheck smi_check) {SCOPEMARK_NAME(MacroAssembler::RecordWriteField, this);
+  DCHECK(!AreAliased(value, dst, t11, object));
+  // First, check if a write barrier is even needed. The tests below
+  // catch stores of Smis.
+  Label done;
+
+  // Skip barrier if writing a smi.
+  if (smi_check == INLINE_SMI_CHECK) {
+    JumpIfSmi(value, &done);
+  }
+
+  // Although the object register is tagged, the offset is relative to the start
+  // of the object, so so offset must be a multiple of kPointerSize.
+  DCHECK(IsAligned(offset, kPointerSize));
+
+  Addl(dst, object, Operand(offset - kHeapObjectTag));
+  if (emit_debug_code()) {
+    BlockTrampolinePoolScope block_trampoline_pool(this);
+    Label ok;
+    And(t11, dst, Operand(kPointerSize - 1));
+    Branch(&ok, eq, t11, Operand(zero_reg));
+    halt();//stop("Unaligned cell in write barrier");
+    bind(&ok);
+  }
+
+  RecordWrite(object, dst, value, ra_status, save_fp, remembered_set_action,
+              OMIT_SMI_CHECK);
+
+  bind(&done);
+
+  // Clobber clobbered input registers when running with the debug-code flag
+  // turned on to provoke errors.
+  if (emit_debug_code()) {
+    li(value, Operand(bit_cast<int64_t>(kZapValue + 4)));
+    li(dst, Operand(bit_cast<int64_t>(kZapValue + 8)));
+  }
+}
+
+void TurboAssembler::SaveRegisters(RegList registers) {SCOPEMARK_NAME(TurboAssembler::SaveRegisters, this);
+  DCHECK_GT(NumRegs(registers), 0);
+  RegList regs = 0;
+  for (int i = 0; i < Register::kNumRegisters; ++i) {
+    if ((registers >> i) & 1u) {
+      regs |= Register::from_code(i).bit();
+    }
+  }
+  MultiPush(regs);
+}
+
+void TurboAssembler::RestoreRegisters(RegList registers) {SCOPEMARK_NAME(TurboAssembler::RestoreRegisters, this);
+  DCHECK_GT(NumRegs(registers), 0);
+  RegList regs = 0;
+  for (int i = 0; i < Register::kNumRegisters; ++i) {
+    if ((registers >> i) & 1u) {
+      regs |= Register::from_code(i).bit();
+    }
+  }
+  MultiPop(regs);
+}
+
+//SKTODO
+void TurboAssembler::CallEphemeronKeyBarrier(Register object, Register address,
+                                             SaveFPRegsMode fp_mode) {
+  EphemeronKeyBarrierDescriptor descriptor;
+  RegList registers = descriptor.allocatable_registers();
+
+  SaveRegisters(registers);
+
+  Register object_parameter(
+      descriptor.GetRegisterParameter(EphemeronKeyBarrierDescriptor::kObject));
+  Register slot_parameter(descriptor.GetRegisterParameter(
+      EphemeronKeyBarrierDescriptor::kSlotAddress));
+  Register fp_mode_parameter(
+      descriptor.GetRegisterParameter(EphemeronKeyBarrierDescriptor::kFPMode));
+
+  Push(object);
+  Push(address);
+
+  Pop(slot_parameter);
+  Pop(object_parameter);
+
+  Move(fp_mode_parameter, Smi::FromEnum(fp_mode));
+  Call(isolate()->builtins()->builtin_handle(Builtins::kEphemeronKeyBarrier),
+       RelocInfo::CODE_TARGET);
+  RestoreRegisters(registers);
+}
+
+void TurboAssembler::CallRecordWriteStub(
+    Register object, Register address,
+    RememberedSetAction remembered_set_action, SaveFPRegsMode fp_mode) {SCOPEMARK_NAME(TurboAssembler::CallRecordWriteStub, this);
+  CallRecordWriteStub(
+      object, address, remembered_set_action, fp_mode,
+      isolate()->builtins()->builtin_handle(Builtins::kRecordWrite),
+      kNullAddress);
+}
+
+void TurboAssembler::CallRecordWriteStub(
+    Register object, Register address,
+    RememberedSetAction remembered_set_action, SaveFPRegsMode fp_mode,
+    Address wasm_target) {SCOPEMARK_NAME(TurboAssembler::CallRecordWriteStub, this);
+  CallRecordWriteStub(object, address, remembered_set_action, fp_mode,
+                      Handle<Code>::null(), wasm_target);
+}
+
+void TurboAssembler::CallRecordWriteStub(
+    Register object, Register address,
+    RememberedSetAction remembered_set_action, SaveFPRegsMode fp_mode,
+    Handle<Code> code_target, Address wasm_target) {
+  DCHECK_NE(code_target.is_null(), wasm_target == kNullAddress);
+  // TODO(albertnetymk): For now we ignore remembered_set_action and fp_mode,
+  // i.e. always emit remember set and save FP registers in RecordWriteStub. If
+  // large performance regression is observed, we should use these values to
+  // avoid unnecessary work.
+
+  RecordWriteDescriptor descriptor;
+  RegList registers = descriptor.allocatable_registers();
+
+  SaveRegisters(registers);
+  Register object_parameter(
+      descriptor.GetRegisterParameter(RecordWriteDescriptor::kObject));
+  Register slot_parameter(
+      descriptor.GetRegisterParameter(RecordWriteDescriptor::kSlot));
+  Register remembered_set_parameter(
+      descriptor.GetRegisterParameter(RecordWriteDescriptor::kRememberedSet));
+  Register fp_mode_parameter(
+      descriptor.GetRegisterParameter(RecordWriteDescriptor::kFPMode));
+
+  Push(object);
+  Push(address);
+
+  Pop(slot_parameter);
+  Pop(object_parameter);
+
+  Move(remembered_set_parameter, Smi::FromEnum(remembered_set_action));
+  Move(fp_mode_parameter, Smi::FromEnum(fp_mode));
+  if (code_target.is_null()) {
+    Call(wasm_target, RelocInfo::WASM_STUB_CALL);
+  } else {
+    Call(code_target, RelocInfo::CODE_TARGET);
+  }
+
+  RestoreRegisters(registers);
+}
+
+// Clobbers object, address, value, and ra, if (ra_status == kRAHasBeenSaved)
+// The register 'object' contains a heap object pointer.  The heap object
+// tag is shifted away.
+void MacroAssembler::RecordWrite(Register object, Register address,
+                                 Register value, RAStatus ra_status,
+                                 SaveFPRegsMode fp_mode,
+                                 RememberedSetAction remembered_set_action,
+                                 SmiCheck smi_check) {SCOPEMARK_NAME(MacroAssembler::RecordWrite, this);
+  DCHECK(!AreAliased(object, address, value, t11));
+  DCHECK(!AreAliased(object, address, value, t12));
+
+  if (emit_debug_code()) {
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    Ldl(scratch, MemOperand(address));
+    Assert(eq, AbortReason::kWrongAddressOrValuePassedToRecordWrite, scratch,
+           Operand(value));
+  }
+
+  if ((remembered_set_action == OMIT_REMEMBERED_SET &&
+       !FLAG_incremental_marking) ||
+      FLAG_disable_write_barriers) {
+    return;
+  }
+
+  // First, check if a write barrier is even needed. The tests below
+  // catch stores of smis and stores into the young generation.
+  Label done;
+
+  if (smi_check == INLINE_SMI_CHECK) {
+    DCHECK_EQ(0, kSmiTag);
+    JumpIfSmi(value, &done);
+  }
+
+  CheckPageFlag(value,
+                value,  // Used as scratch.
+                MemoryChunk::kPointersToHereAreInterestingMask, eq, &done);
+  CheckPageFlag(object,
+                value,  // Used as scratch.
+                MemoryChunk::kPointersFromHereAreInterestingMask,
+                eq,
+                &done);
+
+  // Record the actual write.
+  if (ra_status == kRAHasNotBeenSaved) {
+    push(ra);
+  }
+  CallRecordWriteStub(object, address, remembered_set_action, fp_mode);
+  if (ra_status == kRAHasNotBeenSaved) {
+    pop(ra);
+  }
+
+  bind(&done);
+
+  // Clobber clobbered registers when running with the debug-code flag
+  // turned on to provoke errors.
+  if (emit_debug_code()) {
+    li(address, Operand(bit_cast<int64_t>(kZapValue + 12)));
+    li(value, Operand(bit_cast<int64_t>(kZapValue + 16)));
+  }
+}
+
+// ---------------------------------------------------------------------------
+// Instruction macros.
+
+void TurboAssembler::Addw(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Addw, this);
+  if (rt.is_reg()) {
+    addw(rs, rt.rm(), rd);
+  } else {
+    if (is_uint8(rt.immediate()) && !MustUseReg(rt.rmode())) {
+      addw(rs, (int)rt.immediate(), rd);
+    } else if (is_uint8(-rt.immediate()) && !MustUseReg(rt.rmode())){
+      subw(rs, (int)-rt.immediate(), rd);
+    } else {
+      // li handles the relocation.
+      UseScratchRegisterScope temps(this);
+      Register scratch = temps.Acquire();
+      DCHECK(rs != scratch);
+      li(scratch, rt);
+      addw(rs, scratch, rd);
+    }
+  }
+}
+
+void TurboAssembler::Addl(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Addl, this);
+  if (rt.is_reg()) {
+    addl(rs, rt.rm(), rd);
+  } else {
+    if (is_uint8(rt.immediate()) && !MustUseReg(rt.rmode())) {
+      addl(rs, (int)rt.immediate(), rd);
+    } else if (is_uint8(-rt.immediate()) && !MustUseReg(rt.rmode())){
+      subl(rs, (int)-rt.immediate(), rd);
+    } else {
+      // li handles the relocation.
+      UseScratchRegisterScope temps(this);
+      Register scratch = temps.Acquire();
+      DCHECK(rs != scratch);
+      li(scratch, rt);
+      addl(rs, scratch, rd);
+    }
+  }
+}
+
+void TurboAssembler::Subw(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Subw, this);
+  if (rt.is_reg()) {
+    subw(rs, rt.rm(), rd);
+  } else {
+    if (is_uint8(-rt.immediate()) && !MustUseReg(rt.rmode())) {
+      addw(rs, (int)-rt.immediate(), rd);
+    } else if (is_uint8(rt.immediate()) && !MustUseReg(rt.rmode())){
+      subw(rs, (int)rt.immediate(), rd);
+    } else {
+      UseScratchRegisterScope temps(this);
+      Register scratch = temps.Acquire();
+      DCHECK(rs != scratch);
+      li(scratch, rt);
+      subw(rs, scratch, rd);
+    }
+  }
+}
+
+void TurboAssembler::Subl(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Subl, this);
+  if (rt.is_reg()) {
+    subl(rs, rt.rm(), rd);
+  } else if (is_uint8(-rt.immediate()) && !MustUseReg(rt.rmode())) {
+    addl(rs, (int)-rt.immediate(), rd);
+  } else if (is_uint8(rt.immediate()) && !MustUseReg(rt.rmode())) {
+    subl(rs, (int)rt.immediate(), rd);
+  } else {
+    DCHECK(rs != at);
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    li(scratch, rt);
+    subl(rs, scratch, rd);
+  }
+}
+
+void TurboAssembler::Mulw(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Mulw, this);
+  if (rt.is_reg()) {
+    mulw(rs, rt.rm(), rd);
+  } else {
+    if (is_uint8(rt.immediate()) && !MustUseReg(rt.rmode()) ){
+      mulw(rs, (int)rt.immediate(), rd);
+    } else {
+      // li handles the relocation.
+      UseScratchRegisterScope temps(this);
+      Register scratch = temps.Acquire();
+      DCHECK(rs != scratch);
+      li(scratch, rt);
+      mulw(rs, scratch, rd);      
+    }
+  }
+}
+
+void TurboAssembler::Mulwh(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Mulwh, this);
+  if (rt.is_reg()) {
+    mull(rs, rt.rm(), rd);
+  } else {
+    if (is_uint8(rt.immediate()) && !MustUseReg(rt.rmode())) {
+      mull(rs, (int)rt.immediate(), rd);
+    } else {
+      // li handles the relocation.
+      UseScratchRegisterScope temps(this);
+      Register scratch = temps.Acquire();
+      DCHECK(rs != scratch);
+      li(scratch, rt);
+      mull(rs, scratch, rd);
+    }
+  }
+  sral(rd, 32, rd);
+}
+
+void TurboAssembler::Mulhu(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Mulhu, this);
+  if (rt.is_reg()) {
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    DCHECK(rs != scratch);
+    DCHECK(rt.rm() != scratch);
+    zapnot(rs, 0xf, scratch);
+    zapnot(rt.rm(), 0xf, rd);
+    mull(scratch, rd, rd);
+  } else {
+    if (is_uint8(rt.immediate()) && !MustUseReg(rt.rmode())) {
+      UseScratchRegisterScope temps(this);
+      Register scratch = temps.Acquire();
+      DCHECK(rs != scratch);
+      zapnot(rs, 0xf, scratch);
+      mull(scratch, (int)rt.immediate(), rd);
+    } else {
+      // li handles the relocation.
+      UseScratchRegisterScope temps(this);
+      Register scratch = temps.Acquire();
+      DCHECK(rs != scratch);
+      li(scratch, rt);
+      zapnot(scratch, 0xf, scratch);
+      zapnot(rs, 0xf, rd);
+      mull(rd, scratch, rd);
+    }
+  }
+  sral(rd, 32, rd);
+}
+
+void TurboAssembler::Mull(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Mull, this);
+  if (rt.is_reg()) {
+    mull(rs, rt.rm(), rd);
+  } else {
+    if (is_uint8(rt.immediate()) && !MustUseReg(rt.rmode())) {
+      mull(rs, (int)rt.immediate(), rd);
+    } else {
+      // li handles the relocation.
+      UseScratchRegisterScope temps(this);
+      Register scratch = temps.Acquire();
+      DCHECK(rs != scratch);
+      li(scratch, rt);
+      mull(rs, scratch, rd);
+    }
+  }
+}
+
+void TurboAssembler::Dmulh(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Dmulh, this);
+  if (rt.is_reg()) {
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    DCHECK(rs != scratch);
+    DCHECK(rt.rm() != scratch);
+    umulh(rs, rt.rm(), rd);
+    srll(rs, 63, scratch);
+    mull(scratch, rt.rm(), scratch);
+    subl(rd, scratch, rd);
+    srll(rt.rm(), 63, scratch);
+    mull(scratch, rs, scratch);
+    subl(rd, scratch, rd);
+  } else {
+    // li handles the relocation.
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    DCHECK(rs != scratch);
+    DCHECK(a5 != scratch);
+    li(scratch, rt);
+    umulh(rs, scratch, rd);
+    srll(rs, 63, a5);
+    mull(a5, scratch, a5);
+    subl(rd, a5, rd);
+    srll(scratch, 63, a5);
+    mull(a5, rs, a5);
+    subl(rd, a5, rd);
+  }
+}
+
+void TurboAssembler::Divw(Register rd, Register rs, const Operand& rt) {
+  FPURegister fsrc1 = f22;
+  FPURegister fsrc2 = f23;
+  FPURegister fdest = f24;
+    
+  if (rt.is_reg()) {
+    ifmovd(rs, fsrc1);
+    ifmovd(rt.rm(), fsrc2);
+    fcvtld(fsrc1, fsrc1);
+    fcvtld(fsrc2, fsrc2);
+    fdivd(fsrc1, fsrc2, fdest);
+    fcvtdl_z(fdest, fdest);
+    fimovd(fdest, rd);
+  } else {
+    // li handles the relocation.
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    DCHECK(rs != scratch);
+    li(scratch, rt);
+    ifmovd(rs, fsrc1);
+    ifmovd(scratch, fsrc2);
+    fcvtld(fsrc1, fsrc1);
+    fcvtld(fsrc2, fsrc2);
+    fdivd(fsrc1, fsrc2, fdest);
+    fcvtdl_z(fdest, fdest);
+    fimovd(fdest, rd);
+  }
+}
+
+void TurboAssembler::Divwu(Register rd, Register rs, const Operand& rt) {
+  FPURegister fsrc1 = f22;
+  FPURegister fsrc2 = f23;
+  FPURegister fdest = f24;
+  zapnot(rs, 0xf, rs);
+    
+  if (rt.is_reg()) {
+    zapnot(rt.rm(), 0xf, rt.rm());
+    ifmovd(rs, fsrc1);
+    ifmovd(rt.rm(), fsrc2);
+    fcvtld(fsrc1, fsrc1);
+    fcvtld(fsrc2, fsrc2);
+    fdivd(fsrc1, fsrc2, fdest);
+    fcvtdl_z(fdest, fdest);
+    fimovd(fdest, rd);
+  } else {
+    // li handles the relocation.
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    DCHECK(rs != scratch);
+    li(scratch, rt);
+    zapnot(scratch, 0xf, scratch);
+    ifmovd(rs, fsrc1);
+    ifmovd(scratch, fsrc2);
+    fcvtld(fsrc1, fsrc1);
+    fcvtld(fsrc2, fsrc2);
+    fdivd(fsrc1, fsrc2, fdest);
+    fcvtdl_z(fdest, fdest);
+    fimovd(fdest, rd);
+  }
+}
+
+void TurboAssembler::Divl(Register rd, Register rs, const Operand& rt) {
+  FPURegister fsrc1 = f22;
+  FPURegister fsrc2 = f23;
+  FPURegister fdest = f24;
+    
+  if (rt.is_reg()) {
+    ifmovd(rs, fsrc1);
+    ifmovd(rt.rm(), fsrc2);
+    fcvtld(fsrc1, fsrc1);
+    fcvtld(fsrc2, fsrc2);
+    fdivd(fsrc1, fsrc2, fdest);
+    fcvtdl_z(fdest, fdest);
+    fimovd(fdest, rd);
+  } else {
+    // li handles the relocation.
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    DCHECK(rs != scratch);
+    li(scratch, rt);
+    ifmovd(rs, fsrc1);
+    ifmovd(scratch, fsrc2);
+    fcvtld(fsrc1, fsrc1);
+    fcvtld(fsrc2, fsrc2);
+    fdivd(fsrc1, fsrc2, fdest);
+    fcvtdl_z(fdest, fdest);
+    fimovd(fdest, rd);
+  }
+}
+
+void TurboAssembler::Divlu(Register rd, Register rs, const Operand& rt) {
+  FPURegister fsrc1 = f22;
+  FPURegister fsrc2 = f23;
+  FPURegister fdest = f24;
+    
+  if (rt.is_reg()) {
+    ifmovd(rs, fsrc1);
+    ifmovd(rt.rm(), fsrc2);
+    fcvtld(fsrc1, fsrc1);
+    fcvtld(fsrc2, fsrc2);
+    fdivd(fsrc1, fsrc2, fdest);
+    fcvtdl_z(fdest, fdest);
+    fimovd(fdest, rd);
+  } else {
+    // li handles the relocation.
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    DCHECK(rs != scratch);
+    li(scratch, rt);
+    ifmovd(rs, fsrc1);
+    ifmovd(scratch, fsrc2);
+    fcvtld(fsrc1, fsrc1);
+    fcvtld(fsrc2, fsrc2);
+    fdivd(fsrc1, fsrc2, fdest);
+    fcvtdl_z(fdest, fdest);
+    fimovd(fdest, rd);
+  }
+}
+
+void TurboAssembler::Modw(Register rd, Register rs, const Operand& rt) {
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  DCHECK(scratch != rs);
+  
+  FPURegister fsrc1 = f22;
+  FPURegister fsrc2 = f23;
+  FPURegister fdest = f24;
+
+  if (rt.is_reg()) {
+    ifmovd(rs, fsrc1);
+    ifmovd(rt.rm(), fsrc2);
+    fcvtld(fsrc1, fsrc1);
+    fcvtld(fsrc2, fsrc2);
+    fdivd(fsrc1, fsrc2, fdest);
+    fcvtdl_z(fdest, fdest);
+    fimovd(fdest, scratch);
+    mulw(scratch, rt.rm(), scratch);
+    subw(rs, scratch, rd);
+  } else {
+    Register scratch2 = t12;
+    DCHECK(scratch2 != rs);
+    // li handles the relocation.
+    li(scratch, rt);
+    ifmovd(rs, fsrc1);
+    ifmovd(scratch, fsrc2);
+    fcvtld(fsrc1, fsrc1);
+    fcvtld(fsrc2, fsrc2);
+    fdivd(fsrc1, fsrc2, fdest);
+    fcvtdl_z(fdest, fdest);
+    fimovd(fdest, scratch2);
+    mulw(scratch, scratch2, scratch);
+    subw(rs, scratch, rd);
+  }
+}
+
+void TurboAssembler::Modwu(Register rd, Register rs, const Operand& rt) {
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  DCHECK(scratch != rs);
+  zapnot(rs, 0xf, rs);
+  
+  FPURegister fsrc1 = f22;
+  FPURegister fsrc2 = f23;
+  FPURegister fdest = f24;
+
+  if (rt.is_reg()) {
+    zapnot(rt.rm(), 0xf, rt.rm());
+    ifmovd(rs, fsrc1);
+    ifmovd(rt.rm(), fsrc2);
+    fcvtld(fsrc1, fsrc1);
+    fcvtld(fsrc2, fsrc2);
+    fdivd(fsrc1, fsrc2, fdest);
+    fcvtdl_z(fdest, fdest);
+    fimovd(fdest, scratch);
+    mulw(scratch, rt.rm(), scratch);
+    subw(rs, scratch, rd);
+  } else {
+    Register scratch2 = t12;
+    DCHECK(scratch2 != rs);
+    // li handles the relocation.
+    li(scratch, rt);
+    zapnot(scratch, 0xf, scratch);
+    ifmovd(rs, fsrc1);
+    ifmovd(scratch, fsrc2);
+    fcvtld(fsrc1, fsrc1);
+    fcvtld(fsrc2, fsrc2);
+    fdivd(fsrc1, fsrc2, fdest);
+    fcvtdl_z(fdest, fdest);
+    fimovd(fdest, scratch2);
+    mulw(scratch, scratch2, scratch);
+    subw(rs, scratch, rd);
+  }
+}
+
+void TurboAssembler::Modl(Register rd, Register rs, const Operand& rt) {
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  DCHECK(scratch != rs);
+  
+  FPURegister fsrc1 = f22;
+  FPURegister fsrc2 = f23;
+  FPURegister fdest = f24;
+
+  if (rt.is_reg()) {
+    ifmovd(rs, fsrc1);
+    ifmovd(rt.rm(), fsrc2);
+    fcvtld(fsrc1, fsrc1);
+    fcvtld(fsrc2, fsrc2);
+    fdivd(fsrc1, fsrc2, fdest);
+    fcvtdl_z(fdest, fdest);
+    fimovd(fdest, scratch);
+    mull(scratch, rt.rm(), scratch);
+    subl(rs, scratch, rd);
+  } else {
+    Register scratch2 = t12;
+    DCHECK(scratch2 != rs);
+    // li handles the relocation.
+    li(scratch, rt);
+    ifmovd(rs, fsrc1);
+    ifmovd(scratch, fsrc2);
+    fcvtld(fsrc1, fsrc1);
+    fcvtld(fsrc2, fsrc2);
+    fdivd(fsrc1, fsrc2, fdest);
+    fcvtdl_z(fdest, fdest);
+    fimovd(fdest, scratch2);
+    mull(scratch, scratch2, scratch);
+    subl(rs, scratch, rd);
+  }
+}
+
+void TurboAssembler::Modlu(Register rd, Register rs, const Operand& rt) {
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  DCHECK(scratch != rs);
+  
+  FPURegister fsrc1 = f22;
+  FPURegister fsrc2 = f23;
+  FPURegister fdest = f24;
+
+  if (rt.is_reg()) {
+    ifmovd(rs, fsrc1);
+    ifmovd(rt.rm(), fsrc2);
+    fcvtld(fsrc1, fsrc1);
+    fcvtld(fsrc2, fsrc2);
+    fdivd(fsrc1, fsrc2, fdest);
+    fcvtdl_z(fdest, fdest);
+    fimovd(fdest, scratch);
+    mull(scratch, rt.rm(), scratch);
+    subl(rs, scratch, rd);
+  } else {
+    Register scratch2 = t12;
+    DCHECK(scratch2 != rs);
+    // li handles the relocation.
+    li(scratch, rt);
+    ifmovd(rs, fsrc1);
+    ifmovd(scratch, fsrc2);
+    fcvtld(fsrc1, fsrc1);
+    fcvtld(fsrc2, fsrc2);
+    fdivd(fsrc1, fsrc2, fdest);
+    fcvtdl_z(fdest, fdest);
+    fimovd(fdest, scratch2);
+    mull(scratch, scratch2, scratch);
+    subl(rs, scratch, rd);
+  }
+}
+
+void TurboAssembler::Dmodu(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Dmodu, this);
+  UNREACHABLE();
+}
+
+void TurboAssembler::Abs_sw(FPURegister fd, FPURegister fs) {SCOPEMARK_NAME(TurboAssembler::Abs, this);
+  fcpys(f31,fs,fd);
+}
+
+void TurboAssembler::Sllw(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Sllw, this);
+  if (rt.is_reg()) {
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    DCHECK(rs != scratch);
+    and_ins(rt.rm(), 0x1f, scratch);
+    slll(rs, scratch, rd);
+    addw(rd, 0x0, rd);
+  } else {
+    slll(rs, ((int)rt.immediate()) & 0x1f, rd);
+    addw(rd, 0x0, rd);
+  }
+}
+
+void TurboAssembler::Srlw(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Srlw, this);
+  if (rt.is_reg()) {
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    DCHECK(rs != scratch);
+    and_ins(rt.rm(), 0x1f, scratch);
+    zapnot(rs, 0xf, rd);
+    srll(rd, scratch, rd);
+    addw(rd, 0x0, rd);
+  } else {
+    zapnot(rs, 0xf, rd);
+    srll(rd, ((int)rt.immediate()) & 0x1f, rd);
+    addw(rd, 0x0, rd);
+  }
+}
+
+void TurboAssembler::Sraw(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Sraw, this);
+  if (rt.is_reg()) {
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    DCHECK(rs != scratch);
+    and_ins(rt.rm(), 0x1f, scratch);
+    addw(rs, 0x0, rd);
+    sral(rd, scratch, rd);
+  } else {
+    addw(rs, 0x0, rd);
+    sral(rd, ((int)rt.immediate()) & 0x1f, rd);
+  }
+}
+
+void TurboAssembler::And(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::And, this);
+  if (rt.is_reg()) {
+    and_ins(rs, rt.rm(), rd);
+  } else {
+    if (is_uint8(rt.immediate()) && !MustUseReg(rt.rmode())) {//20181121  is_uint16
+      and_ins(rs, (int)rt.immediate(), rd);
+    } else {
+      // li handles the relocation.
+      UseScratchRegisterScope temps(this);
+      Register scratch = temps.Acquire();
+      DCHECK(rs != scratch);
+      li(scratch, rt);
+      and_ins(rs, scratch, rd);
+    }
+  }
+}
+
+void TurboAssembler::Or(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Or, this);
+  if (rt.is_reg()) {
+    or_ins(rs, rt.rm(), rd);
+  } else {
+    if (is_uint8(rt.immediate()) && !MustUseReg(rt.rmode())) {//20181121  is_uint16
+      or_ins(rs, (int)rt.immediate(), rd);
+    } else {
+      // li handles the relocation.
+      UseScratchRegisterScope temps(this);
+      Register scratch = temps.Acquire();
+      DCHECK(rs != scratch);
+      li(scratch, rt);
+      or_ins(rs, scratch, rd);
+    }
+  }
+}
+
+void TurboAssembler::Xor(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Xor, this);
+  if (rt.is_reg()) {
+    xor_ins(rs, rt.rm(), rd);
+  } else {
+    if (is_uint8(rt.immediate()) && !MustUseReg(rt.rmode())) {
+      xor_ins(rs, (int)rt.immediate(), rd);
+    } else {
+      // li handles the relocation.
+      UseScratchRegisterScope temps(this);
+      Register scratch = temps.Acquire();
+      DCHECK(rs != scratch);
+      li(scratch, rt);
+      xor_ins(rs, scratch, rd);
+    }
+  }
+}
+
+void TurboAssembler::Nor(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Nor, this);
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  if (rt.is_reg()) {
+    bis(rs, rt.rm(), scratch);
+    ornot(zero_reg, scratch, rd);
+  } else {
+    if (is_uint8(rt.immediate()) && !MustUseReg(rt.rmode())) {
+      bis(rs, (int)rt.immediate(), rd);
+      ornot(zero_reg, rd, rd);
+    } else {
+      // li handles the relocation.
+      DCHECK(rs != scratch);    
+      li(scratch, rt);
+      bis(rs, scratch, scratch);
+      ornot(zero_reg, scratch, rd);
+    }
+  }
+}  
+
+void TurboAssembler::Neg(Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Neg, this);
+  subl(zero_reg, rt.rm(),rs);
+}
+
+void TurboAssembler::Cmplt(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Cmplt, this);
+  if (rt.is_reg()) {
+    cmplt(rs, rt.rm(), rd);
+  } else {
+    if (is_uint8(rt.immediate()) && !MustUseReg(rt.rmode())) {
+      cmplt(rs, (int)rt.immediate(), rd);
+    } else {
+      // li handles the relocation.
+      UseScratchRegisterScope temps(this);
+      BlockTrampolinePoolScope block_trampoline_pool(this);
+      Register scratch = temps.hasAvailable() ? temps.Acquire() : t11;
+      DCHECK(rs != scratch);
+      li(scratch, rt);
+      cmplt(rs, scratch, rd);
+    }
+  }
+}
+
+void TurboAssembler::Cmpult(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Cmpult, this);
+  if (rt.is_reg()) {
+    cmpult(rs, rt.rm(), rd);
+  } else {
+    if (is_uint8(rt.immediate()) && !MustUseReg(rt.rmode())) {
+      cmpult(rs, (int)rt.immediate(), rd);
+    } else {
+      // li handles the relocation.
+      UseScratchRegisterScope temps(this);
+      BlockTrampolinePoolScope block_trampoline_pool(this);
+      Register scratch = temps.hasAvailable() ? temps.Acquire() : t11;
+      DCHECK(rs != scratch);
+      li(scratch, rt);
+      cmpult(rs, scratch, rd);
+    }
+  }
+}
+
+void TurboAssembler::Cmple(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Cmple, this);
+  if (rt.is_reg()) {
+    cmple(rs, rt.rm(), rd);
+  } else {
+    if (is_uint8(rt.immediate()) && !MustUseReg(rt.rmode())) {
+      cmple(rs, (int)rt.immediate(), rd);
+    } else {
+      // li handles the relocation.
+      UseScratchRegisterScope temps(this);
+      BlockTrampolinePoolScope block_trampoline_pool(this);
+      Register scratch = temps.hasAvailable() ? temps.Acquire() : t11;
+      DCHECK(rs != scratch);
+      li(scratch, rt);
+      cmple(rs, scratch, rd);
+    }
+  }
+}
+
+void TurboAssembler::Cmpule(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Cmpule, this);
+  if (rt.is_reg()) {
+    cmpule(rs, rt.rm(), rd);
+  } else {
+    if (is_uint8(rt.immediate()) && !MustUseReg(rt.rmode())) {
+      cmpule(rs, (int)rt.immediate(), rd);
+    } else {
+    // li handles the relocation.
+      UseScratchRegisterScope temps(this);
+      Register scratch = temps.hasAvailable() ? temps.Acquire() : t11;
+      DCHECK(rs != scratch);
+      li(scratch, rt);
+      cmpule(rs, scratch, rd);
+    }
+  }
+}
+
+void TurboAssembler::Cmpge(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Cmpge, this);
+  if (rt.is_reg()) {
+    cmple(rt.rm(), rs, rd);
+  } else {
+    // li handles the relocation.
+      UseScratchRegisterScope temps(this);
+      Register scratch = temps.hasAvailable() ? temps.Acquire() : t11;
+      DCHECK(rs != scratch);
+      li(scratch, rt);
+      cmple(scratch, rs, rd);
+    }
+  }
+
+void TurboAssembler::Cmpuge(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Cmpuge, this);
+  if (rt.is_reg()) {
+    cmpule(rt.rm(), rs, rd);
+  } else {
+    // li handles the relocation.
+      UseScratchRegisterScope temps(this);
+      Register scratch = temps.hasAvailable() ? temps.Acquire() : t11;
+      DCHECK(rs != scratch);
+      li(scratch, rt);
+      cmpule(scratch, rs, rd);
+    }
+  }
+
+void TurboAssembler::Cmpgt(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Cmpgt, this);
+  if (rt.is_reg()) {
+    cmplt(rt.rm(), rs, rd);
+  } else {
+    // li handles the relocation.
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.hasAvailable() ? temps.Acquire() : t11;
+    BlockTrampolinePoolScope block_trampoline_pool(this);
+    DCHECK(rs != scratch);
+    li(scratch, rt);
+    cmplt(scratch, rs, rd);
+  }
+}
+
+void TurboAssembler::Cmpugt(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Cmpugt, this);
+  if (rt.is_reg()) {
+    cmpult(rt.rm(), rs, rd);
+  } else {
+    // li handles the relocation.
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.hasAvailable() ? temps.Acquire() : t11;
+    BlockTrampolinePoolScope block_trampoline_pool(this);
+    DCHECK(rs != scratch);
+    li(scratch, rt);
+    cmpult(scratch, rs, rd);
+  }
+}
+
+void TurboAssembler::Ror(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Ror, this);
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.hasAvailable() ? temps.Acquire() : t11;
+  DCHECK(rs != scratch);
+  DCHECK(rd != scratch);
+  if (rt.is_reg()) {
+    Register scratch1 = (scratch == t11) ? a5 : t11;
+    Register scratch2 = t12;
+    Register scratch3 = gp;  // avoid to get at in srlv and sllv
+    and_ins(rt.rm(), 0x1f, scratch1);
+    ldi(scratch2, 32, zero_reg);
+    subw(scratch2, scratch1, scratch2);
+    {
+      // srlv(scratch1, rs, scratch1);  // srlw(rs, scratch1, scratch1);
+      and_ins(scratch1, 0x1f, scratch3);
+      zapnot(rs, 0xf, scratch1);
+      srll(scratch1, scratch3, scratch1);
+      addw(scratch1, 0, scratch1);
+    }
+    {
+      // sllv(rd, rs, scratch2);  // sllw(rs, scratch2, rd);
+      and_ins(scratch2, 0x1f, scratch3);
+      slll(rs, scratch3, rd);
+      addw(rd, 0, rd);
+    }
+    bis(scratch1, rd, rd);
+    addw(rd, 0, rd);
+  } else {
+    int64_t ror_value = rt.immediate() % 32;
+    if (ror_value < 0) {
+      ror_value += 32;
+    }
+    Srlw(scratch, rs, ror_value);  // srlw(rs, ror_value, scratch);
+    Sllw(rd, rs, 32-ror_value);  // sllw(rs, 32-ror_value, rd);
+    bis(scratch, rd, rd);
+    addw(rd, 0, rd);
+  }
+}
+
+void TurboAssembler::Dror(Register rd, Register rs, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::Dror, this);
+#ifdef SW64
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.hasAvailable() ? temps.Acquire() : t11;
+  DCHECK(rs != scratch);
+  DCHECK(rd != scratch);
+  if (rt.is_reg()) {
+    Register scratch1 = (scratch == t11) ? a5 : t11;
+    Register scratch2 = t12;
+    srll(rs, rt.rm(), scratch1);
+    ldi(scratch2, 64, zero_reg);
+    subl(scratch2, rt.rm(), scratch2);
+    slll(rs, scratch2, rd);
+    bis(scratch1, rd, rd);
+  } else {
+    int64_t dror_value = rt.immediate() % 64;
+    if (dror_value < 0) dror_value += 64;
+    srll(rs, (int)dror_value, scratch);
+    slll(rs, (int)(64-dror_value), rd);
+    bis(scratch, rd, rd);
+  }
+#else
+  if (rt.is_reg()) {
+    roll(rs, (64-(rt.rm()).code()),rd);
+  } else {
+    int64_t dror_value = rt.immediate() % 64;
+    if (dror_value < 0) dror_value += 64;
+    if (dror_value <= 31) {
+      roll(rs, (64-dror_value),rd);
+    } else {
+      roll(rs, dror_value, rd);
+    }
+  }
+#endif
+}
+
+
+void TurboAssembler::Lsa(Register rd, Register rt, Register rs, uint8_t sa,
+                         Register scratch) {SCOPEMARK_NAME(TurboAssembler::Lsa, this);
+  DCHECK(sa >= 1 && sa <= 31);
+#ifdef SW64
+  if (sa == 3) {
+    s8addw(rs, rt, rd);  //rd = rs * 8 + rt
+  } else if (sa == 2) {
+    s4addw(rs, rt, rd);  //rd = rs * 4 + rt
+  } else {
+    Register tmp = rd == rt ? scratch : rd;
+    DCHECK(tmp != rt);
+    Sllw(tmp, rs, sa);  //not sw's sllw
+    addw(rt, tmp, rd);
+  }
+#else
+  //if (kArchVariant == kSw64r3 && sa <= 4) {
+  //  lsa(rd, rt, rs, sa - 1);
+  //} else {
+  Register tmp = rd == rt ? scratch : rd;
+  DCHECK(tmp != rt);
+  Sllw(tmp, rs, sa);
+  Addw(rd, rt, tmp);
+  //}
+#endif
+}
+
+void TurboAssembler::Dlsa(Register rd, Register rt, Register rs, uint8_t sa,
+                          Register scratch) {SCOPEMARK_NAME(TurboAssembler::Dlsa, this);
+  DCHECK(sa >= 1 && sa <= 63);
+#ifdef SW64
+  if (sa == 3) {
+    s8addl(rs, rt, rd);  //rd = rs * 8 + rt
+  } else if (sa == 2) {
+    s4addl(rs, rt, rd);  //rd = rs * 4 + rt
+  } else {
+    Register tmp = rd == rt ? scratch : rd;
+    DCHECK(tmp != rt);
+    slll(rs, sa, tmp);
+    Addl(rd, rt, tmp);
+  }
+#else
+  Register tmp = rd == rt ? scratch : rd;
+  DCHECK(tmp != rt);
+  slll(rs, sa, tmp);
+  Addl(rd, rt, tmp);
+#endif
+//  }
+}
+
+// ------------Pseudo-instructions-------------
+
+// Change endianness
+void TurboAssembler::ByteSwapSigned(Register dest, Register src,
+                                    int operand_size) {SCOPEMARK_NAME(TurboAssembler::ByteSwapSigned, this);
+  DCHECK(operand_size == 2 || operand_size == 4 || operand_size == 8);
+  DCHECK(kArchVariant == kSw64r3 || kArchVariant == kSw64r2);
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  DCHECK(src != scratch);
+  if (operand_size == 2) {
+    //HGFEDCBA ==> 000000AB
+    zapnot(src, 0x3, dest);
+    srll(dest, 8, scratch);
+    slll(dest, 24, dest);
+    addw(dest, 0, dest);
+    sral(dest, 16, dest);
+    or_ins(dest, scratch, dest);
+  } else if (operand_size == 4) {
+    //HGFEDCBA ==> 0000ABCD
+    zapnot(src, 0xf, dest);
+    srll(dest, 8, scratch); //0DCB
+    slll(dest, 24, dest); //A000
+    bis(dest, scratch, dest); //ADCB
+    srll(scratch, 16, scratch); //000D
+    xor_ins(dest, scratch, scratch);
+    and_ins(scratch, 0xff, scratch); //000B^D 
+    xor_ins(dest, scratch, dest); //ADCD
+    slll(scratch, 16, scratch); //0B^D00
+    xor_ins(dest, scratch, dest); //0000ABCD    
+    addw(dest, 0, dest);
+  } else {
+    // 87654321 ==> 12345678
+    srll(src, 8, scratch); //08765432 
+    slll(src, 56, dest); //10000000
+    bis(dest, scratch, dest); //18765432
+    // 8 <==> 2
+    srll(scratch, 48, scratch); //00000008
+    xor_ins(dest, scratch, scratch); 
+    and_ins(scratch, 0xff, scratch); //00000002^8
+    xor_ins(dest, scratch, dest); //18765438
+    slll(scratch, 48, scratch); //02^8000000
+    xor_ins(dest, scratch, dest); //12765438
+    // 7 <==> 3
+    srll(dest, 32, scratch); //00001276
+    xor_ins(dest, scratch, scratch); 
+    zapnot (scratch, 0x2, scratch); //0000003^70
+    xor_ins(dest, scratch, dest); //12765478
+    slll(scratch, 32, scratch); //03^7000000
+    xor_ins(dest, scratch, dest); //12365478    
+    // 6 <==> 4
+    srll(dest, 16, scratch); //00123654
+    xor_ins(dest, scratch, scratch); 
+    zapnot (scratch, 0x4, scratch); //000004^600
+    xor_ins(dest, scratch, dest); //12365678
+    slll(scratch, 16, scratch); //0004^60000
+    xor_ins(dest, scratch, dest); //12345678 
+  }
+}
+
+void TurboAssembler::ByteSwapUnsigned(Register dest, Register src,
+                                      int operand_size) {SCOPEMARK_NAME(TurboAssembler::ByteSwapUnsigned, this);
+  DCHECK(operand_size == 2 || operand_size == 4);
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  DCHECK(src != scratch);
+  if (operand_size == 2) {
+    zapnot(src, 0x3, dest);
+    srll(dest, 8, scratch);
+    slll(dest, 8, dest);
+    bis(dest, scratch, dest);
+//    slll(dest, 48, dest);
+    zapnot(dest, 0x3, dest);  
+  } else {
+    zapnot(src, 0xf, dest);
+    srll(dest, 8, scratch); //0DCB
+    slll(dest, 24, dest); //A000
+    bis(dest, scratch, dest); //ADCB
+    srll(scratch, 16, scratch); //000D
+    xor_ins(dest, scratch, scratch);
+    and_ins(scratch, 0xff, scratch); //000B^D 
+    xor_ins(dest, scratch, dest); //ADCD
+    slll(scratch, 16, scratch); //0B^D000
+    xor_ins(dest, scratch, dest); //0000ABCD
+//    slll(dest, 32, dest); //ABCD0000
+
+  }
+}
+
+void TurboAssembler::Uldw(Register rd, const MemOperand& rs) {SCOPEMARK_NAME(TurboAssembler::Uldw, this);
+  DCHECK(rd != at);
+  DCHECK(rs.rm() != at);
+  Ldw(rd, rs);
+}
+
+void TurboAssembler::Uldwu(Register rd, const MemOperand& rs) {SCOPEMARK_NAME(TurboAssembler::Uldwu, this);
+  if (kArchVariant == kSw64r3) {
+    Ldwu(rd, rs);
+  } else {
+    DCHECK_EQ(kArchVariant, kSw64r2);
+    Uldw(rd, rs);
+    zapnot(rd, 0xf, rd); //ZHJ Dext(rd, rd, 0, 32);
+  }
+}
+
+void TurboAssembler::Ustw(Register rd, const MemOperand& rs) {SCOPEMARK_NAME(TurboAssembler::Ustw, this);
+  DCHECK(rd != at);
+  DCHECK(rs.rm() != at);
+  DCHECK(rd != rs.rm());
+  Stw(rd, rs);
+}
+
+void TurboAssembler::Uldh(Register rd, const MemOperand& rs) {SCOPEMARK_NAME(TurboAssembler::Uldh, this);
+  DCHECK(rd != at);
+  DCHECK(rs.rm() != at);
+  DCHECK_EQ(kArchVariant, kSw64r2);
+  MemOperand source = rs;
+  // Adjust offset for two accesses and check if offset + 1 fits into int16_t.
+    AdjustBaseAndOffset(&source, OffsetAccessType::TWO_ACCESSES, 1);
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  if (source.rm() == scratch) {
+#if defined(V8_TARGET_LITTLE_ENDIAN)
+    Ldb(rd, MemOperand(source.rm(), source.offset() + 1));
+    Ldbu(scratch, source);
+#endif
+  } else {
+#if defined(V8_TARGET_LITTLE_ENDIAN)
+    Ldbu(scratch, source);
+    Ldb(rd, MemOperand(source.rm(), source.offset() + 1));
+#endif
+  }
+  slll(rd, 8, rd);
+  or_ins(rd, scratch, rd);
+}
+
+void TurboAssembler::Uldhu(Register rd, const MemOperand& rs) {SCOPEMARK_NAME(TurboAssembler::Uldhu, this);
+  DCHECK(rd != at);
+  DCHECK(rs.rm() != at);
+  DCHECK_EQ(kArchVariant, kSw64r2);
+  MemOperand source = rs;
+  // Adjust offset for two accesses and check if offset + 1 fits into int16_t.
+    AdjustBaseAndOffset(&source, OffsetAccessType::TWO_ACCESSES, 1);
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  if (source.rm() == scratch) {
+#if defined(V8_TARGET_LITTLE_ENDIAN)
+    Ldbu(rd, MemOperand(source.rm(), source.offset() + 1));
+    Ldbu(scratch, source);
+#endif
+  } else {
+#if defined(V8_TARGET_LITTLE_ENDIAN)
+    Ldbu(scratch, source);
+    Ldbu(rd, MemOperand(source.rm(), source.offset() + 1));
+#endif
+  }
+  slll(rd, 8, rd);
+  or_ins(rd, scratch, rd);
+}
+
+void TurboAssembler::Usth(Register rd, const MemOperand& rs, Register scratch) {SCOPEMARK_NAME(TurboAssembler::Usth, this);
+  DCHECK(rd != at);
+  DCHECK(rs.rm() != at);
+  DCHECK(rs.rm() != scratch);
+  DCHECK(scratch != at);
+  DCHECK_EQ(kArchVariant, kSw64r2);
+  MemOperand source = rs;
+  // Adjust offset for two accesses and check if offset + 1 fits into int16_t.
+    AdjustBaseAndOffset(&source, OffsetAccessType::TWO_ACCESSES, 1);
+
+  if (scratch != rd) {
+    mov(scratch, rd);
+  }
+
+#if defined(V8_TARGET_LITTLE_ENDIAN)
+  Stb(scratch, source);
+  Srlw(scratch, scratch, 8);
+  Stb(scratch, MemOperand(source.rm(), source.offset() + 1));
+#endif
+}
+
+void TurboAssembler::Uldl(Register rd, const MemOperand& rs) {SCOPEMARK_NAME(TurboAssembler::Uldl, this);
+  DCHECK(rd != at);
+  DCHECK(rs.rm() != at);
+  Ldl(rd, rs);
+}
+
+
+void TurboAssembler::Ustl(Register rd, const MemOperand& rs) {SCOPEMARK_NAME(TurboAssembler::Ustl, this);
+  DCHECK(rd != at);
+  DCHECK(rs.rm() != at);
+  Stl(rd, rs);
+}
+
+
+void TurboAssembler::Uflds(FPURegister fd, const MemOperand& rs,
+                           Register scratch) {SCOPEMARK_NAME(TurboAssembler::Uflds, this);
+  Flds(fd, rs);
+}
+
+void TurboAssembler::Ufsts(FPURegister fd, const MemOperand& rs,
+                           Register scratch) {SCOPEMARK_NAME(TurboAssembler::Ufsts, this);
+  Fsts(fd, rs);
+}
+
+void TurboAssembler::Ufldd(FPURegister fd, const MemOperand& rs,
+                           Register scratch) {SCOPEMARK_NAME(TurboAssembler::Ufldd, this);
+  DCHECK(scratch != at);
+  Fldd(fd, rs);
+}
+
+void TurboAssembler::Ufstd(FPURegister fd, const MemOperand& rs,
+                           Register scratch) {SCOPEMARK_NAME(TurboAssembler::Ufstd, this);
+  DCHECK(scratch != at);
+  Fstd(fd, rs);
+}
+
+void TurboAssembler::Ldb(Register rd, const MemOperand& rs) {SCOPEMARK_NAME(TurboAssembler::Ldb, this);
+  ldbu(rd, rs);
+  sextb(rd, rd);
+}
+
+void TurboAssembler::Ldbu(Register rd, const MemOperand& rs) {SCOPEMARK_NAME(TurboAssembler::Ldbu, this);
+  ldbu(rd, rs);
+}
+
+void TurboAssembler::Stb(Register rd, const MemOperand& rs) {SCOPEMARK_NAME(TurboAssembler::Stb, this);
+  stb(rd, rs);
+}
+
+void TurboAssembler::Ldh(Register rd, const MemOperand& rs) {SCOPEMARK_NAME(TurboAssembler::Ldh, this);
+  ldhu(rd, rs);
+  sexth(rd, rd);
+}
+
+void TurboAssembler::Ldhu(Register rd, const MemOperand& rs) {SCOPEMARK_NAME(TurboAssembler::Ldhu, this);
+  ldhu(rd, rs);
+}
+
+void TurboAssembler::Sth(Register rd, const MemOperand& rs) {SCOPEMARK_NAME(TurboAssembler::Sth, this);
+  sth(rd, rs);
+}
+
+void TurboAssembler::Ldw(Register rd, const MemOperand& rs) {SCOPEMARK_NAME(TurboAssembler::Ldw, this);
+  ldw(rd, rs);
+}
+
+void TurboAssembler::Ldwu(Register rd, const MemOperand& rs) {SCOPEMARK_NAME(TurboAssembler::Ldwu, this);
+  ldw(rd, rs);
+  zapnot(rd, 0xf, rd);
+}
+
+void TurboAssembler::Stw(Register rd, const MemOperand& rs) {SCOPEMARK_NAME(TurboAssembler::Stw, this);
+  stw(rd, rs);
+}
+
+void TurboAssembler::Ldl(Register rd, const MemOperand& rs) {SCOPEMARK_NAME(TurboAssembler::Ldl, this);
+  ldl(rd, rs);
+}
+
+void TurboAssembler::Stl(Register rd, const MemOperand& rs) {SCOPEMARK_NAME(TurboAssembler::Stl, this);
+  stl(rd, rs);
+}
+
+void TurboAssembler::Flds(FPURegister fd, const MemOperand& src) {SCOPEMARK_NAME(TurboAssembler::Flds, this);
+  flds(fd, src);
+}
+
+void TurboAssembler::Fsts(FPURegister fs, const MemOperand& src) {SCOPEMARK_NAME(TurboAssembler::Fsts, this);
+  fsts(fs, src);
+}
+
+void TurboAssembler::Fldd(FPURegister fd, const MemOperand& src) {SCOPEMARK_NAME(TurboAssembler::Fldd, this);
+  fldd(fd, src);
+}
+
+void TurboAssembler::Fstd(FPURegister fs, const MemOperand& src) {SCOPEMARK_NAME(TurboAssembler::Fstd, this);
+  fstd(fs, src);
+}
+
+void TurboAssembler::li(Register dst, Handle<HeapObject> value, LiFlags mode) {SCOPEMARK_NAME(TurboAssembler::li, this);
+    if (root_array_available_ && options().isolate_independent_code) {
+      IndirectLoadConstant(dst, value);
+      return;
+    }
+  li(dst, Operand(value), mode);
+}
+
+void TurboAssembler::li(Register dst, ExternalReference value, LiFlags mode) {SCOPEMARK_NAME(TurboAssembler::li, this);
+    if (root_array_available_ && options().isolate_independent_code) {
+      IndirectLoadExternalReference(dst, value);
+      return;
+    }
+  li(dst, Operand(value), mode);
+}
+
+void TurboAssembler::li(Register dst, const StringConstantBase* string,
+                        LiFlags mode) {SCOPEMARK_NAME(TurboAssembler::li, this);
+  li(dst, Operand::EmbeddedStringConstant(string), mode);
+}
+
+static inline int InstrCountForLiLower32Bit(int64_t value) {
+  int32_t lsb32 = static_cast<int32_t>(value);
+  int16_t lsb_h = (lsb32-static_cast<int16_t>(lsb32)) >> 16;
+  int16_t lsb_l = static_cast<int16_t>(lsb32);
+
+  if (is_int16(lsb32)) {
+    return 1;
+  } else {
+    if ( (int32_t)(lsb_h) == -32768 && (int32_t)(lsb_l) < 0 ) {
+      //  range from 0x7FFF8000 to 0x7FFFFFFF
+      return lsb_l ? 3 : 2;
+    } else {
+      return lsb_l ? 2 : 1;
+    }
+  }
+}
+
+void TurboAssembler::LiLower32BitHelper(Register rd, Operand j) {SCOPEMARK_NAME(TurboAssembler::LiLower32BitHelper, this);
+  int32_t lsb32 = static_cast<int32_t>(j.immediate());
+  
+  if (is_int16(lsb32)) {
+    ldi(rd, lsb32, zero_reg);
+  } else {
+    int16_t lsb_h = (lsb32-static_cast<int16_t>(lsb32)) >> 16;
+    int16_t lsb_l = static_cast<int16_t>(lsb32);  
+    if ( (int32_t)(lsb_h) == -32768 && (int32_t)(lsb_l) < 0 ) {
+      //  range from 0x7FFF8000 to 0x7FFFFFFF
+      ldih(rd, 0x4000, zero_reg);
+      ldih(rd, 0x4000, rd);
+      if (lsb_l)
+        ldi(rd, lsb_l, rd);
+    } else {
+      ldih(rd, lsb_h, zero_reg);
+      if (lsb_l) {
+        ldi(rd, lsb_l, rd);
+      }
+    }
+  }
+}
+
+
+int TurboAssembler::InstrCountForLi64Bit(int64_t value) {
+  int32_t lo = static_cast<int32_t>(value);
+  int32_t hi = static_cast<int32_t>((value - lo) >> 32);
+  int16_t lo_h16 = (lo - static_cast<int16_t>(lo))>>16;
+  int16_t lo_l16 = static_cast<int16_t>(lo);
+  int16_t hi_l16 = static_cast<int16_t>(hi);
+
+  if (is_int32(value)) {
+    return InstrCountForLiLower32Bit(value);
+  } else {
+    int count = 1;  // slll 32
+    if ( is_int16(hi) ) {
+      count += 1;
+    } else {
+      count += hi_l16 ? 2 : 1;
+    }
+    if ( lo != 0 ) {
+      if ( ((int32_t)lo_h16 == -32768) && ((int32_t)lo_l16 < 0)) {
+        // range from 0x7FFF8000 to 0x7FFFFFFF
+        count += lo_l16 ? 3 : 2;
+      } else {
+        count += lo_l16 ? 2 : 1;
+      }
+    }
+    return count;
+  }
+
+  UNREACHABLE();
+  return INT_MAX;
+}
+
+// All changes to if...else conditions here must be added to
+// InstrCountForLi64Bit as well.
+void TurboAssembler::li_optimized(Register rd, Operand j, LiFlags mode) {SCOPEMARK_NAME(TurboAssembler::li_optimized, this);
+  DCHECK(!j.is_reg());
+  DCHECK(!MustUseReg(j.rmode()));
+  DCHECK(mode == OPTIMIZE_SIZE);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  int64_t value = j.immediate();
+    int32_t lo = static_cast<int32_t>(value);
+    int32_t hi = static_cast<int32_t>((value - lo) >> 32);
+    int16_t lo_h16 = (lo - static_cast<int16_t>(lo))>>16;
+    int16_t lo_l16 = static_cast<int16_t>(lo);
+    int16_t hi_h16 = (hi - static_cast<int16_t>(hi))>>16;
+    int16_t hi_l16 = static_cast<int16_t>(hi);
+  // Normal load of an immediate value which does not need Relocation Info.
+  if (is_int32(value)) {
+    LiLower32BitHelper(rd, j);
+  } else {
+    if ( is_int16(hi) ) {
+      ldi(rd, hi, zero_reg);
+    } else {
+      ldih(rd, hi_h16, zero_reg);
+      if (hi_l16 != 0)
+        ldi(rd, hi_l16, rd);
+    }
+    slll(rd, 32, rd);
+    if ( lo != 0 ) {
+      if ( ((int32_t)lo_h16 == -32768) && ((int32_t)lo_l16 < 0)) {
+        // range from 0x7FFF8000 to 0x7FFFFFFF
+        ldih(rd, 0x4000, rd);
+        ldih(rd, 0x4000, rd);
+        if (lo_l16 != 0)
+          ldi(rd, lo_l16, rd);
+      } else {
+        ldih(rd, lo_h16, rd);
+        if (lo_l16 != 0)
+          ldi(rd, lo_l16, rd);
+      }
+    }
+  }
+}
+
+void TurboAssembler::li(Register rd, Operand j, LiFlags mode) {SCOPEMARK_NAME(TurboAssembler::li, this);
+  DCHECK(!j.is_reg());
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  if (!MustUseReg(j.rmode()) && mode == OPTIMIZE_SIZE) {
+    int li_count = InstrCountForLi64Bit(j.immediate());
+    int li_neg_count = InstrCountForLi64Bit(-j.immediate());
+    int li_not_count = InstrCountForLi64Bit(~j.immediate());
+    // Loading -MIN_INT64 could cause problems, but loading MIN_INT64 takes only
+    // two instructions so no need to check for this.
+    if (li_neg_count <= li_not_count && li_neg_count < li_count - 1) {
+      DCHECK(j.immediate() != std::numeric_limits<int64_t>::min());
+      li_optimized(rd, Operand(-j.immediate()), mode);
+      Subl(rd, zero_reg, rd);
+    } else if (li_neg_count > li_not_count && li_not_count < li_count - 1) {
+      DCHECK(j.immediate() != std::numeric_limits<int64_t>::min());
+      li_optimized(rd, Operand(~j.immediate()), mode);
+      ornot(zero_reg, rd, rd); // nor(rd, rd, rd);
+    } else {
+      li_optimized(rd, j, mode);
+    }
+  } else if (MustUseReg(j.rmode())) {
+    int64_t immediate;
+    if (j.IsHeapObjectRequest()) {
+      RequestHeapObject(j.heap_object_request());
+      immediate = 0;
+    } else {
+      immediate = j.immediate();
+    }
+
+    RecordRelocInfo(j.rmode(), immediate);
+
+    int32_t lsb32 = static_cast<int32_t> (immediate);
+    int32_t msb32 = static_cast<int32_t> ((immediate - lsb32) >> 32);
+    int16_t msb_l = static_cast<int16_t>(msb32);
+    int16_t lsb_h = (lsb32-static_cast<int16_t>(lsb32)) >> 16;
+    int16_t lsb_l = static_cast<int16_t>(lsb32);
+
+    //  lsb32's range should not be from 0x7FFF8000 to 0x7FFFFFFF.
+    DCHECK( !( (lsb32>0x7FFF8000)&&(lsb32<0x7FFFFFFF) ) );
+    ldi(rd, msb_l, zero_reg);
+    slll(rd, 32, rd);
+    ldih(rd, lsb_h, rd);
+    ldi(rd, lsb_l, rd);
+
+  } else if (mode == ADDRESS_LOAD)  {
+    // We always need the same number of instructions as we may need to patch
+    // this code to load another value which may need all 4 instructions.
+    int32_t lsb32 = static_cast<int32_t> (j.immediate());
+    int32_t msb32 = static_cast<int32_t> ((j.immediate() - lsb32) >> 32);
+
+    if (lsb32 < 0 && (j.immediate()>>32) > 0) {
+        return li_optimized(rd, j, OPTIMIZE_SIZE);
+    }
+
+    int16_t msb_l = static_cast<int16_t>(msb32);
+    int16_t lsb_h = (lsb32-static_cast<int16_t>(lsb32)) >> 16;
+    int16_t lsb_l = static_cast<int16_t>(lsb32);
+
+    //  lsb32's range should not be from 0x7FFF8000 to 0x7FFFFFFF.
+    DCHECK( !( (lsb32>0x7FFF8000)&&(lsb32<0x7FFFFFFF) ) );
+    ldi(rd, msb_l, zero_reg);
+    slll(rd, 32, rd);
+    ldih(rd, lsb_h, rd);
+    ldi(rd, lsb_l, rd);
+
+  } else {  // mode == CONSTANT_SIZE - always emit the same instruction
+            // sequence.
+    // CONSTANT_SIZE, must 5 instructions.
+    int64_t imm = j.immediate();
+    int32_t lsb32 = static_cast<int32_t> (imm);
+    int32_t msb32 = static_cast<int32_t> ((imm - lsb32) >> 32);
+    int16_t msb_h = (msb32-static_cast<int16_t>(msb32)) >> 16;
+    int16_t msb_l = static_cast<int16_t>(msb32);
+    int16_t lsb_h = (lsb32-static_cast<int16_t>(lsb32)) >> 16;
+    int16_t lsb_l = static_cast<int16_t>(lsb32);
+
+    ldih(rd, msb_h, zero_reg);
+    ldi(rd, msb_l, rd);
+    slll(rd, 32, rd);
+    ldih(rd, lsb_h, rd);
+    ldi(rd, lsb_l, rd);
+
+  }
+}
+
+void TurboAssembler::MultiPush(RegList regs) {SCOPEMARK_NAME(TurboAssembler::MultiPush, this);
+  int16_t num_to_push = base::bits::CountPopulation(regs);
+  int16_t stack_offset = num_to_push * kPointerSize;
+
+  Subl(sp, sp, Operand(stack_offset));
+  if ((regs & (1 << ra.code())) != 0) {
+    stack_offset -= kPointerSize;
+    stl(ToRegister(ra.code()), MemOperand(sp, stack_offset));
+  }
+  if ((regs & (1 << fp.code())) != 0) {
+    stack_offset -= kPointerSize;
+    stl(ToRegister(fp.code()), MemOperand(sp, stack_offset));
+  }
+  for (int16_t i = kNumRegisters - 1; i >= 0; i--) {
+    if ((regs & (1 << i)) != 0 && (i != ra.code()) && (i != fp.code())) {
+      stack_offset -= kPointerSize;
+      stl(ToRegister(i), MemOperand(sp, stack_offset));
+    }
+  }
+}
+
+
+void TurboAssembler::MultiPop(RegList regs) {SCOPEMARK_NAME(TurboAssembler::MultiPop, this);
+  int16_t stack_offset = 0;
+
+  for (int16_t i = 0; i < kNumRegisters; i++) {
+    if ((regs & (1 << i)) != 0 && (i != ra.code()) && (i != fp.code())) {
+      ldl(ToRegister(i), MemOperand(sp, stack_offset));
+      stack_offset += kPointerSize;
+    }
+  }
+  if ((regs & (1 << fp.code())) != 0) {
+    ldl(ToRegister(fp.code()), MemOperand(sp, stack_offset));
+    stack_offset += kPointerSize;
+  }
+  if ((regs & (1 << ra.code())) != 0) {
+    ldl(ToRegister(ra.code()), MemOperand(sp, stack_offset));
+    stack_offset += kPointerSize;
+  }
+  addl(sp, stack_offset, sp);
+}
+
+
+void TurboAssembler::MultiPushFPU(RegList regs) {SCOPEMARK_NAME(TurboAssembler::MultiPushFPU, this);
+  int16_t num_to_push = base::bits::CountPopulation(regs);
+  int16_t stack_offset = num_to_push * kDoubleSize;
+
+  Subl(sp, sp, Operand(stack_offset));
+  for (int16_t i = kNumRegisters - 1; i >= 0; i--) {
+    if ((regs & (1 << i)) != 0) {
+      stack_offset -= kDoubleSize;
+      fstd(FPURegister::from_code(i), MemOperand(sp, stack_offset));
+    }
+  }
+}
+
+
+void TurboAssembler::MultiPopFPU(RegList regs) {SCOPEMARK_NAME(TurboAssembler::MultiPopFPU, this);
+  int16_t stack_offset = 0;
+
+  for (int16_t i = 0; i < kNumRegisters; i++) {
+    if ((regs & (1 << i)) != 0) {
+      fldd(FPURegister::from_code(i), MemOperand(sp, stack_offset));
+      stack_offset += kDoubleSize;
+    }
+  }
+  addl(sp, stack_offset, sp);
+}
+
+
+void TurboAssembler::Ext(Register rt, Register rs, uint16_t pos,
+                         uint16_t size) {SCOPEMARK_NAME(TurboAssembler::Ext, this);
+  DCHECK_LT(pos, 32);
+  DCHECK_LT(pos + size, 33);
+#if SW64
+  // Ext is word-sign-extend.
+  if (pos == 0) {
+    if (size == 8 ) {
+      zapnot(rs, 0x1, rt);
+    } else if (size == 16 ) {
+      zapnot(rs, 0x3, rt);
+    } else if (size == 32 ) {
+      addw(rs, 0, rt);
+    } else {
+      long bitmask = (0x1L << size) - 1;
+      And(rt, rs, (int)bitmask);
+      addw(rt, 0, rt);
+    }
+  } else {
+    long bitmask = (0x1L << size) - 1;
+    srll(rs, pos, rt);
+    And(rt, rt, (int)bitmask);
+    addw(rt, 0, rt);
+  }
+#else
+  long bitsize = (0x1L << size) - 1;
+  srll(rs, pos, rt);
+  And(rt, rt, bitsize);
+#endif
+}
+
+void TurboAssembler::Dext(Register rt, Register rs, uint16_t pos,
+                          uint16_t size) {SCOPEMARK_NAME(TurboAssembler::Dext, this);
+DCHECK(pos < 64 && 0 < size && size <= 64 && 0 < pos + size &&
+         pos + size <= 64);
+#ifdef SW64
+// dext is zero-extend
+  if (pos != 0) {
+    srll(rs, pos, rt);
+
+    switch (size) {
+      case 8:
+        zapnot(rt, 0x1, rt);
+        break;
+      case 16:
+        zapnot(rt, 0x3, rt);
+        break;
+      case 32:
+        zapnot(rt, 0xf, rt);
+        break;
+      default: {
+        DCHECK(size < 64);
+        long bitmask = (0x1L << size) - 1;
+        And(rt, rt, (int)bitmask);
+      }
+    }
+  } else {
+    switch (size) {
+      case 8:
+        zapnot(rs, 0x1, rt);
+        break;
+      case 16:
+        zapnot(rs, 0x3, rt);
+        break;
+      case 32:
+        zapnot(rs, 0xf, rt);
+        break;
+      case 64:  // the result of 0x1L<<64 is 1.
+        mov(rt, rs);
+        break;
+      default: {
+        long bitmask = (0x1L << size) - 1;
+        And(rt, rs, (int)bitmask);
+      }
+    }
+  }
+#else
+  if ( (pos == 0) && (size == 32)) {
+    zapnot(rs, 0xf, rt);
+  } else {
+    long bitsize = (0x1L << size) - 1;
+    srll(rs, pos, rt);
+    And(rt, rt, bitsize);
+  }
+#endif
+}
+
+void TurboAssembler::Ins(Register rt, Register rs, uint16_t pos,
+                         uint16_t size) {SCOPEMARK_NAME(TurboAssembler::Ins, this);
+  DCHECK_LT(pos, 32);
+  DCHECK_LE(pos + size, 32);
+  DCHECK_NE(size, 0);
+  DCHECK(rs != t11 && rt != t11);
+  DCHECK(rt != at);
+
+  long bitsize = (0x1L << size) - 1;
+  li(t11, bitsize);
+  and_ins(rs, t11, at);
+  slll(at, pos, at);
+  slll(t11, pos, t11);
+  bic(rt, t11, rt);
+  bis(rt, at, rt);
+  addw(rt, 0, rt);
+}
+
+void TurboAssembler::Dins(Register rt, Register rs, uint16_t pos,
+                          uint16_t size) {SCOPEMARK_NAME(TurboAssembler::Dins, this);
+  DCHECK_LT(pos, 64);
+  DCHECK_LE(pos + size, 64);
+  DCHECK_NE(size, 0);
+  DCHECK(rs != t11 && rt != t11);
+  DCHECK(rt != at);
+
+  long bitsize = (size == 64) ? -1 : (0x1L << size) - 1;
+  li(t11, bitsize);
+  and_ins(rs, t11, at);
+  slll(at, pos, at);
+  slll(t11, pos, t11);
+  bic(rt, t11, rt);
+  bis(rt, at, rt);
+}
+
+void TurboAssembler::ExtractBits(Register dest, Register source, Register pos,
+                                 int size, bool sign_extend) {SCOPEMARK_NAME(TurboAssembler::ExtractBits, this);
+#ifdef SW64
+  sral(source, pos, dest);
+  if (sign_extend) {
+    switch (size) {
+      case 8:
+        sextb(dest, dest);
+        break;
+      case 16:
+        sexth(dest, dest);
+        break;
+      case 32:
+        // sign-extend word
+        addw(dest, 0, dest);  // Sllw(dest, dest, 0);
+        break;
+      default:
+        UNREACHABLE();
+    }
+  } else {
+    switch (size) {
+      case 8:
+        zapnot(dest, 0x1, dest);
+        break;
+      case 16:
+        zapnot(dest, 0x3, dest);
+        break;
+      case 32:
+        zapnot(dest, 0xf, dest);
+        break;
+      default:
+        UNREACHABLE();
+    }
+  }
+#endif
+}
+
+void TurboAssembler::InsertBits(Register dest, Register source, Register pos,
+                                int size) {SCOPEMARK_NAME(TurboAssembler::InsertBits, this);
+#ifdef SW64
+  DCHECK(source != t12 && dest != t12); 
+  DCHECK(source != t11 && dest != t11); 
+
+  long sizemask = (0x1L << size) -1;
+  li(t11, sizemask);
+  and_ins(source, t11, t12);
+  slll(t12, pos, t12);  // (source 0..size-1) << pos
+  slll(t11, pos, t11);
+  bic(dest, t11, dest);
+  bis(dest, t12, dest);
+#endif
+}
+
+void TurboAssembler::Fnegs(FPURegister fd, FPURegister fs) {SCOPEMARK_NAME(TurboAssembler::Fnegs, this);
+#ifdef SW64
+  fnegs(fs, fd);
+#endif
+}
+
+void TurboAssembler::Fnegd(FPURegister fd, FPURegister fs) {SCOPEMARK_NAME(TurboAssembler::Fnegd, this);
+#ifdef SW64
+  fnegd(fs, fd);
+#endif
+}
+
+void TurboAssembler::Cvt_d_uw(FPURegister fd, FPURegister fs) {SCOPEMARK_NAME(TurboAssembler::Cvt_d_uw, this);
+  // Move the data from fs to t11.
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  fimovs(fs,t11);
+  Cvt_d_uw(fd, t11);
+}
+
+void TurboAssembler::Cvt_d_uw(FPURegister fd, Register rs) {SCOPEMARK_NAME(TurboAssembler::Cvt_d_uw, this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+
+  // Convert rs to a FP value in fd.
+  DCHECK(rs != t12);
+  DCHECK(rs != at);
+
+  // Zero extend int32 in rs.
+  zapnot(rs, 0xf, t12); //ZHJ Dext(t12, rs, 0, 32);
+  ifmovd(t12, fd);
+  fcvtld_(fd, fd);
+}
+
+void TurboAssembler::Cvt_d_ul(FPURegister fd, FPURegister fs) {SCOPEMARK_NAME(TurboAssembler::Cvt_d_ul, this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  // Move the data from fs to t11.
+  fimovd(fs,t11);
+  Cvt_d_ul(fd, t11);
+}
+
+void TurboAssembler::Cvt_d_ul(FPURegister fd, Register rs) {SCOPEMARK_NAME(TurboAssembler::Cvt_d_ul, this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  // Convert rs to a FP value in fd.
+
+  DCHECK(rs != t12); 
+  DCHECK(rs != at);
+
+  Label msb_clear, conversion_done;
+
+  Branch(&msb_clear, ge, rs, Operand(zero_reg));
+
+  // Rs >= 2^63
+  and_ins(rs, 1,t12);
+  srll(rs, 1 ,rs);
+  or_ins(t12, rs, t12);
+  ifmovd(t12, fd);
+  fcvtld_(fd, fd);
+  faddd(fd, fd, fd);  // In delay slot.
+  Branch(&conversion_done);
+
+  bind(&msb_clear);
+  // Rs < 2^63, we can do simple conversion.
+  ifmovd(rs, fd);
+  fcvtld_(fd, fd);
+
+  bind(&conversion_done);
+}
+
+void TurboAssembler::Cvt_s_uw(FPURegister fd, FPURegister fs) {SCOPEMARK_NAME(TurboAssembler::Cvt_s_uw, this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  // Move the data from fs to t11.
+  fimovs(fs,t11);
+  Cvt_s_uw(fd, t11);
+}
+
+void TurboAssembler::Cvt_s_uw(FPURegister fd, Register rs) {SCOPEMARK_NAME(TurboAssembler::Cvt_s_uw, this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  // Convert rs to a FP value in fd.
+  DCHECK(rs != t12);
+  DCHECK(rs != at);
+
+  // Zero extend int32 in rs.
+  zapnot(rs, 0xf, t12);  //ZHJ Dext(t12, rs, 0, 32);
+  ifmovd(t12, fd);
+  fcvtls_(fd, fd);
+}
+
+void TurboAssembler::Cvt_s_ul(FPURegister fd, FPURegister fs) {SCOPEMARK_NAME(TurboAssembler::Cvt_s_ul, this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  // Move the data from fs to t11.
+  fimovd(fs,t11);
+  Cvt_s_ul(fd, t11);
+}
+
+void TurboAssembler::Cvt_s_ul(FPURegister fd, Register rs) {SCOPEMARK_NAME(TurboAssembler::Cvt_s_ul, this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  // Convert rs to a FP value in fd.
+
+  DCHECK(rs != t12);
+  DCHECK(rs != at);
+
+  Label positive, conversion_done;
+
+  Branch(&positive, ge, rs, Operand(zero_reg));
+
+  // Rs >= 2^31.
+  and_ins(rs, 1,t12);
+  srll(rs, 1, rs);
+  or_ins(t12, rs, t12);
+  ifmovd(t12, fd);
+  fcvtls_(fd, fd);
+  fadds(fd, fd, fd);  // In delay slot.
+  Branch(&conversion_done);
+
+  bind(&positive);
+  // Rs < 2^31, we can do simple conversion.
+  ifmovd(rs, fd);
+  fcvtls_(fd, fd);
+
+  bind(&conversion_done);
+}
+
+
+void MacroAssembler::Round_l_d(FPURegister fd, FPURegister fs) {SCOPEMARK_NAME(MacroAssembler::Round_l_d, this);
+  fcvtdl_g(fs, fd);  // rounding to nearest
+}
+
+
+void MacroAssembler::Floor_l_d(FPURegister fd, FPURegister fs) {SCOPEMARK_NAME(MacroAssembler::Floor_l_d, this);
+  fcvtdl_n(fs, fd);  // rounding down
+}
+
+
+void MacroAssembler::Ceil_l_d(FPURegister fd, FPURegister fs) {SCOPEMARK_NAME(MacroAssembler::Ceil_l_d, this);
+  fcvtdl_p(fs, fd);  // rounding up
+}
+
+
+void MacroAssembler::Trunc_l_d(FPURegister fd, FPURegister fs) {SCOPEMARK_NAME(MacroAssembler::Trunc_l_d, this);
+  fcvtdl_z(fs, fd);  // rounding toward zero
+}
+
+
+void MacroAssembler::Trunc_l_ud(FPURegister fd,
+                                FPURegister fs,
+                                FPURegister scratch) {SCOPEMARK_NAME(MacroAssembler::Trunc_l_ud, this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  // Load to GPR.
+  fimovd(fs,t11);
+  // Reset sign bit.
+  {
+    UseScratchRegisterScope temps(this);
+    Register scratch1 = temps.Acquire();
+    li(scratch1, 0x7FFFFFFFFFFFFFFF);
+    and_ins(t11, scratch1, t11);
+  }
+  ifmovd(t11, fs);
+  ftruncdl(fs, fd);
+}
+
+void TurboAssembler::Trunc_uw_d(FPURegister fd, FPURegister fs,
+                                FPURegister scratch) {SCOPEMARK_NAME(TurboAssembler::Trunc_uw_d, this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  Trunc_uw_d(t11, fs, scratch);
+  ifmovs(t11, fd);
+}
+
+void TurboAssembler::Trunc_uw_s(FPURegister fd, FPURegister fs,
+                                FPURegister scratch) {SCOPEMARK_NAME(TurboAssembler::Trunc_uw_s, this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  Trunc_uw_s(t11, fs, scratch);
+  ifmovs(t11, fd);
+}
+
+void TurboAssembler::Trunc_ul_d(FPURegister fd, FPURegister fs,
+                                FPURegister scratch, Register result) {SCOPEMARK_NAME(TurboAssembler::Trunc_ul_d, this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  Trunc_ul_d(t11, fs, scratch, result);
+  ifmovd(t11, fd);
+}
+
+void TurboAssembler::Trunc_ul_s(FPURegister fd, FPURegister fs,
+                                FPURegister scratch, Register result) {SCOPEMARK_NAME(TurboAssembler::Trunc_ul_s, this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  Trunc_ul_s(t11, fs, scratch, result);
+  ifmovd(t11, fd);
+}
+
+
+void MacroAssembler::Trunc_w_d(FPURegister fd, FPURegister fs) {SCOPEMARK_NAME(MacroAssembler::Trunc_w_d, this);
+  ftruncdw(fs, fd);
+}
+
+
+void MacroAssembler::Round_w_d(FPURegister fd, FPURegister fs) {SCOPEMARK_NAME(MacroAssembler::Round_w_d, this);
+  frounddw(fs, fd);
+}
+
+
+void MacroAssembler::Floor_w_d(FPURegister fd, FPURegister fs) {SCOPEMARK_NAME(MacroAssembler::Floor_w_d, this);
+  ffloordw(fs, fd);
+}
+
+
+void MacroAssembler::Ceil_w_d(FPURegister fd, FPURegister fs) {SCOPEMARK_NAME(MacroAssembler::Ceil_w_d, this);
+  fceildw(fs, fd);
+}
+
+void TurboAssembler::Trunc_uw_d(Register rd, FPURegister fs,
+                                FPURegister scratch) {SCOPEMARK_NAME(TurboAssembler::Trunc_uw_d, this);
+  DCHECK(fs != scratch);
+  DCHECK(rd != at);
+
+  {
+    // Load 2^31 into scratch as its float representation.
+    UseScratchRegisterScope temps(this);
+    Register scratch1 = temps.Acquire();
+    li(scratch1, 0x41E00000);
+    slll(scratch1, 32, scratch1);
+    ifmovd(scratch1, scratch);
+  }
+  // Test if scratch > fd.
+  // If fd < 2^31 we can convert it normally.
+  Label simple_convert;
+  CompareF64(OLT, fs, scratch);
+  BranchTrueShortF(&simple_convert);
+
+  // First we subtract 2^31 from fd, then trunc it to rs
+  // and add 2^31 to rs.
+  fsubd(fs, scratch, kDoubleCompareReg);
+  ftruncdw(kDoubleCompareReg, scratch);
+  fimovs(scratch,rd);
+  Or(rd, rd, 1 << 31);
+
+  Label done;
+  Branch(&done);
+  // Simple conversion.
+  bind(&simple_convert);
+  fcvtdl_z(fs, kScratchDoubleReg1);
+  fcvtlw(kScratchDoubleReg1, scratch);
+  fimovs(scratch, rd);
+
+  bind(&done);
+}
+
+void TurboAssembler::Trunc_uw_s(Register rd, FPURegister fs,
+                                FPURegister scratch) {SCOPEMARK_NAME(TurboAssembler::Trunc_uw_s, this);
+  DCHECK(fs != scratch);
+  DCHECK(rd != at);
+
+  {
+    // Load 2^31 into scratch as its float representation.
+    UseScratchRegisterScope temps(this);
+    Register scratch1 = temps.Acquire();
+    li(scratch1, 0x4F000000);
+    ifmovs(scratch1, scratch);
+  }
+  // Test if scratch > fs.
+  // If fs < 2^31 we can convert it normally.
+  Label simple_convert;
+  CompareF32(OLT, fs, scratch);
+  BranchTrueShortF(&simple_convert);
+
+  // First we subtract 2^31 from fs, then trunc it to rd
+  // and add 2^31 to rd.
+  fsubs(fs, scratch, kDoubleCompareReg);  // sub_s(scratch, fs, scratch);
+  ftruncsw(kDoubleCompareReg, scratch);  // trunc_w_s(scratch, scratch);
+  fimovs(scratch,rd);
+  Or(rd, rd, 1 << 31);
+
+  Label done;
+  Branch(&done);
+  // Simple conversion.
+  bind(&simple_convert);
+  ftruncsw(fs, scratch);
+  fimovs(scratch,rd);
+
+  bind(&done);
+}
+
+void TurboAssembler::Trunc_ul_d(Register rd, FPURegister fs,
+                                FPURegister scratch, Register result) {SCOPEMARK_NAME(TurboAssembler::Trunc_ul_d, this);
+  DCHECK(fs != scratch);
+  DCHECK(result.is_valid() ? !AreAliased(rd, result, at) : !AreAliased(rd, at));
+
+  Label simple_convert, done, fail;
+  if (result.is_valid()) {
+    mov(result, zero_reg);
+    Move(scratch, -1.0);
+    // If fd =< -1 or unordered, then the conversion fails.
+    CompareF64(OLE, fs, scratch);
+    BranchTrueShortF(&fail);
+    CompareIsNanF64(fs, scratch);
+    BranchTrueShortF(&fail);
+    // if fd >= (double)UINT64_MAX, then the conversion fails.
+    ldih(rd, 0x43f0, zero_reg);
+    slll(rd, 32, rd);
+    ifmovd(rd, scratch);
+    CompareF64(OLT, fs, scratch);
+    BranchFalseShortF(&fail);
+  }
+
+  // Load 2^63 into scratch as its double representation.
+  li(at, 0x43E0000000000000);
+  ifmovd(at, scratch);
+
+  // Test if scratch > fs.
+  // If fs < 2^63 we can convert it normally.
+  CompareF64(OLT, fs, scratch);
+  BranchTrueShortF(&simple_convert);
+
+  // First we subtract 2^63 from fs, then trunc it to rd
+  // and add 2^63 to rd.
+  fsubd(fs, scratch, kDoubleCompareReg);  // sub_d(scratch, fs, scratch);
+  fcvtdl_z(kDoubleCompareReg, scratch);  // trunc_l_d(scratch, scratch);
+  fimovd(scratch,rd);
+  Or(rd, rd, Operand(1UL << 63));
+  Branch(&done);
+
+  // Simple conversion.
+  bind(&simple_convert);
+  ftruncdl(fs, scratch);
+  fimovd(scratch,rd);
+
+  bind(&done);
+  if (result.is_valid()) {
+    // Conversion is failed if the result is negative.
+    {
+      UseScratchRegisterScope temps(this);
+      Register scratch1 = temps.Acquire();
+      Addw(scratch1, zero_reg, Operand(-1));
+      srll(scratch1, 1, scratch1);  // Load 2^62.
+      fimovd(scratch,result);
+      xor_ins(result, scratch1, result);
+    }
+    Cmplt(result, zero_reg, result);
+  }
+
+  bind(&fail);
+}
+
+void TurboAssembler::Trunc_ul_s(Register rd, FPURegister fs,
+                                FPURegister scratch, Register result) {SCOPEMARK_NAME(TurboAssembler::Trunc_ul_s, this);
+  DCHECK(fs != scratch);
+  DCHECK(result.is_valid() ? !AreAliased(rd, result, at) : !AreAliased(rd, at));
+
+  Label simple_convert, done, fail;
+  FPURegister fscratch2 = kScratchDoubleReg2;
+  if (result.is_valid()) {
+    mov(result, zero_reg);
+    Move(scratch, -1.0f);
+    // If fd =< -1 or unordered, then the conversion fails.
+    CompareF32(OLE, fs, scratch);
+    BranchTrueShortF(&fail);
+    CompareIsNanF32(fs, scratch);
+    BranchTrueShortF(&fail);
+    // if fd >= (float)UINT64_MAX, then the conversion fails.
+    ldih(rd, 0x5F80, zero_reg);
+    ifmovs(rd, scratch);
+    CompareF32(OLT, fs, scratch);
+    BranchFalseShortF(&fail);
+  }
+
+  {
+    // Load 2^63 into scratch as its float representation.
+    UseScratchRegisterScope temps(this);
+    Register scratch1 = temps.Acquire();
+    li(scratch1, 0x5F000000);
+    ifmovs(scratch1, scratch);
+  }
+
+  // Test if scratch > fs.
+  // If fs < 2^63 we can convert it normally.
+  CompareF32(OLT, fs, scratch);
+  BranchTrueShortF(&simple_convert);
+
+  // First we subtract 2^63 from fs, then trunc it to rd
+  // and add 2^63 to rd.
+  fsubs(fs, scratch, fscratch2);
+  ftruncsl(fscratch2, scratch);
+  fimovd(scratch,rd);
+  Or(rd, rd, Operand(1UL << 63));
+  Branch(&done);
+
+  // Simple conversion.
+  bind(&simple_convert);
+  ftruncsl(fs, scratch);
+  fimovd(scratch,rd);
+
+  bind(&done);
+  if (result.is_valid()) {
+    // Conversion is failed if the result is negative or unordered.
+    {
+      UseScratchRegisterScope temps(this);
+      Register scratch1 = temps.Acquire();
+      Addw(scratch1, zero_reg, Operand(-1));
+      srll(scratch1, 1 ,scratch1);  // Load 2^62.
+      fimovd(scratch,result);
+      xor_ins(result, scratch1, result);
+    }
+    Cmplt(result, zero_reg, result);
+  }
+
+  bind(&fail);
+}
+
+template <typename RoundFunc>
+void TurboAssembler::RoundDouble(FPURegister dst, FPURegister src,
+                                 FPURoundingMode mode, RoundFunc round) {SCOPEMARK_NAME(TurboAssembler::RoundDouble, this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  Register scratch = t11;
+    Label done;
+    fimovd(src, scratch);
+    srll(scratch, 32, scratch);
+    srll(scratch, HeapNumber::kExponentShift, at);
+    li(gp, (0x1L<<HeapNumber::kExponentBits)-1);
+    and_ins(at, gp, at);
+    addw(at, 0, at);
+    fmovd(src, dst);
+    li(gp, HeapNumber::kExponentBias + HeapNumber::kMantissaBits);
+    cmpult(at, gp, at);
+    beq(at, &done);
+    round(this, dst, src);
+    fimovd(dst,at);
+    fcvtld_(dst, dst);
+    bne(at, &done);
+    srll(scratch, 31, at);
+    slll(at, 31 + 32, at);
+    ifmovd(at, dst);
+    bind(&done);
+}
+
+void TurboAssembler::Floor_d_d(FPURegister dst, FPURegister src) {SCOPEMARK_NAME(TurboAssembler::Floor_d_d, this);
+  RoundDouble(dst, src, mode_floor,
+              [](TurboAssembler* tasm, FPURegister dst, FPURegister src) {
+                tasm->ffloordl(src,dst);
+              });
+}
+
+void TurboAssembler::Ceil_d_d(FPURegister dst, FPURegister src) {SCOPEMARK_NAME(TurboAssembler::Ceil_d_d, this);
+  RoundDouble(dst, src, mode_ceil,
+              [](TurboAssembler* tasm, FPURegister dst, FPURegister src) {
+                tasm->fceildl(src, dst);
+              });
+}
+
+void TurboAssembler::Trunc_d_d(FPURegister dst, FPURegister src) {SCOPEMARK_NAME(TurboAssembler::Trunc_d_d, this);
+  RoundDouble(dst, src, mode_trunc,
+              [](TurboAssembler* tasm, FPURegister dst, FPURegister src) {
+                tasm->ftruncdl(src,dst);
+              });
+}
+
+void TurboAssembler::Round_d_d(FPURegister dst, FPURegister src) {SCOPEMARK_NAME(TurboAssembler::Round_d_d, this);
+  RoundDouble(dst, src, mode_round,
+              [](TurboAssembler* tasm, FPURegister dst, FPURegister src) {
+                tasm->frounddl(src,dst);
+              });
+}
+
+template <typename RoundFunc>
+void TurboAssembler::RoundFloat(FPURegister dst, FPURegister src,
+                                FPURoundingMode mode, RoundFunc round) {SCOPEMARK_NAME(TurboAssembler::RoundFloat, this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  Register scratch = t11;
+  Register scratch2 = t8;
+    int32_t kFloat32ExponentBias = 127;
+    int32_t kFloat32MantissaBits = 23;
+//    int32_t kFloat32ExponentBits = 8;
+    Label done;
+    fimovd(src, scratch);
+    srll(scratch, 32, scratch);
+    srll(scratch, 20, scratch2);//sign + exponent, 12 bits
+    and_ins(scratch2, 0x7F, at);//low 7 exponent bits
+    addw(at, 0, at);
+    srll(scratch2, 3, gp);
+    and_ins(gp, 0x80, gp);
+    addw(gp, 0, gp);
+    or_ins(at, gp, at);
+    addw(at, 0, at);
+    fmovs(src, dst);
+    li(gp, kFloat32ExponentBias + kFloat32MantissaBits);
+    cmpult(at, gp, at);
+    beq(at, &done);
+    round(this, dst, src);
+    fimovs(dst, at);
+    fcvtws(dst, dst);
+    bne(at, &done);
+    srll(scratch, 31, at);
+    slll(at, 31 + 32, at);
+    ifmovd(at, dst);
+    bind(&done);
+}
+
+void TurboAssembler::Floor_s_s(FPURegister dst, FPURegister src) {SCOPEMARK_NAME(TurboAssembler::Floor_s_s, this);
+  RoundFloat(dst, src, mode_floor,
+             [](Assembler* tasm, FPURegister dst, FPURegister src) {
+               tasm->ffloorsw(src, dst);
+             });
+}
+
+void TurboAssembler::Ceil_s_s(FPURegister dst, FPURegister src) {SCOPEMARK_NAME(TurboAssembler::Ceil_s_s, this);
+  RoundFloat(dst, src, mode_ceil,
+             [](TurboAssembler* tasm, FPURegister dst, FPURegister src) {
+               tasm->fceilsw(src, dst);
+             });
+}
+
+void TurboAssembler::Trunc_s_s(FPURegister dst, FPURegister src) {SCOPEMARK_NAME(TurboAssembler::Trunc_s_s, this);
+  RoundFloat(dst, src, mode_trunc,
+             [](TurboAssembler* tasm, FPURegister dst, FPURegister src) {
+               tasm->ftruncsw(src, dst);
+             });
+}
+
+void TurboAssembler::Round_s_s(FPURegister dst, FPURegister src) {SCOPEMARK_NAME(TurboAssembler::Round_s_s, this);
+  RoundFloat(dst, src, mode_round,
+             [](TurboAssembler* tasm, FPURegister dst, FPURegister src) {
+               tasm->froundsw(src, dst);
+             });
+}
+//SKTODO
+#if 0
+void TurboAssembler::MSARoundW(MSARegister dst, MSARegister src,
+                               FPURoundingMode mode) {
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  Register scratch = t8;
+  Register scratch2 = at;
+  cfcmsa(scratch, MSACSR);
+  if (mode == kRoundToNearest) {
+    scratch2 = zero_reg;
+  } else {
+    li(scratch2, Operand(mode));
+  }
+  ctcmsa(MSACSR, scratch2);
+  frint_w(dst, src);
+  ctcmsa(MSACSR, scratch);
+}
+
+void TurboAssembler::MSARoundD(MSARegister dst, MSARegister src,
+                               FPURoundingMode mode) {
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  Register scratch = t8;
+  Register scratch2 = at;
+  cfcmsa(scratch, MSACSR);
+  if (mode == kRoundToNearest) {
+    scratch2 = zero_reg;
+  } else {
+    li(scratch2, Operand(mode));
+  }
+  ctcmsa(MSACSR, scratch2);
+  frint_d(dst, src);
+  ctcmsa(MSACSR, scratch);
+}
+#endif
+
+void MacroAssembler::Madd_s(FPURegister fd, FPURegister fr, FPURegister fs,
+                            FPURegister ft, FPURegister scratch) {SCOPEMARK_NAME(MacroAssembler::Madd_s, this);
+  DCHECK(fr != scratch && fs != scratch && ft != scratch);
+  fmuls(fs, ft, scratch);
+  fadds(fr, scratch, fd);
+}
+
+void MacroAssembler::Madd_d(FPURegister fd, FPURegister fr, FPURegister fs,
+    FPURegister ft, FPURegister scratch) {SCOPEMARK_NAME(MacroAssembler::Madd_d, this);
+  DCHECK(fr != scratch && fs != scratch && ft != scratch);
+  fmuld(fs, ft, scratch);
+  faddd(fr, scratch, fd);
+}
+
+void MacroAssembler::Msub_s(FPURegister fd, FPURegister fr, FPURegister fs,
+                            FPURegister ft, FPURegister scratch) {SCOPEMARK_NAME(MacroAssembler::Msub_s, this);
+  DCHECK(fr != scratch && fs != scratch && ft != scratch);
+  fmuls(fs, ft, scratch);
+  fsubs(scratch, fr, fd);
+}
+
+void MacroAssembler::Msub_d(FPURegister fd, FPURegister fr, FPURegister fs,
+                            FPURegister ft, FPURegister scratch) {SCOPEMARK_NAME(MacroAssembler::Msub_d, this);
+  DCHECK(fr != scratch && fs != scratch && ft != scratch);
+  fmuld(fs, ft, scratch);
+  fsubd(scratch, fr, fd);
+}
+
+void TurboAssembler::CompareF(SecondaryField sizeField, FPUCondition cc,
+                              FPURegister cmp1, FPURegister cmp2) {SCOPEMARK_NAME(TurboAssembler::CompareF, this);
+    sizeField = sizeField == D ? L : W;
+    DCHECK(cmp1 != kDoubleCompareReg && cmp2 != kDoubleCompareReg);
+    cmp(cc, sizeField, kDoubleCompareReg, cmp1, cmp2);
+}
+
+void TurboAssembler::CompareIsNanF(SecondaryField sizeField, FPURegister cmp1,
+                                   FPURegister cmp2) {SCOPEMARK_NAME(TurboAssembler::CompareIsNanF, this);
+  CompareF(sizeField, UN, cmp1, cmp2);
+}
+
+void TurboAssembler::BranchTrueShortF(Label* target, BranchDelaySlot bd) {SCOPEMARK_NAME(TurboAssembler::BranchTrueShortF, this);
+  fbne(kDoubleCompareReg, target);
+}
+
+void TurboAssembler::BranchFalseShortF(Label* target, BranchDelaySlot bd) {SCOPEMARK_NAME(TurboAssembler::BranchFalseShortF, this);
+  fbeq(kDoubleCompareReg, target);
+}
+
+void TurboAssembler::BranchTrueF(Label* target, BranchDelaySlot bd) {SCOPEMARK_NAME(TurboAssembler::BranchTrueF, this);
+  bool long_branch =
+      target->is_bound() ? !is_near(target) : is_trampoline_emitted();
+  if (long_branch) {
+    Label skip;
+    BranchFalseShortF(&skip);
+    BranchLong(target, bd);
+    bind(&skip);
+  } else {
+    BranchTrueShortF(target, bd);
+  }
+}
+
+void TurboAssembler::BranchFalseF(Label* target, BranchDelaySlot bd) {SCOPEMARK_NAME(TurboAssembler::BranchFalseF, this);
+  bool long_branch =
+      target->is_bound() ? !is_near(target) : is_trampoline_emitted();
+  if (long_branch) {
+    Label skip;
+    BranchTrueShortF(&skip);
+    BranchLong(target, bd);
+    bind(&skip);
+  } else {
+    BranchFalseShortF(target, bd);
+  }
+}
+
+void TurboAssembler::BranchMSA(Label* target, MSABranchDF df,
+                               MSABranchCondition cond, MSARegister wt,
+                               BranchDelaySlot bd) {SCOPEMARK_NAME(TurboAssembler::BranchMSA, this);
+  UNREACHABLE();
+}
+
+void TurboAssembler::BranchShortMSA(MSABranchDF df, Label* target,
+                                    MSABranchCondition cond, MSARegister wt,
+                                    BranchDelaySlot bd) {SCOPEMARK_NAME(TurboAssembler::BranchShortMSA, this);
+  UNREACHABLE();
+}
+
+void TurboAssembler::FmoveLow(FPURegister dst, Register src_low) {SCOPEMARK_NAME(TurboAssembler::FmoveLow, this);
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  Register scratch1 = t8;
+  DCHECK(src_low != scratch);
+#ifdef SW64
+  fimovd(dst, scratch);
+  srll(scratch, 32, scratch);
+  slll(scratch, 32, scratch);
+  zapnot(src_low, 0xf, scratch1);
+  or_ins(scratch, scratch1, scratch);
+  ifmovd(scratch, dst);
+#endif
+}
+
+void TurboAssembler::Move(FPURegister dst, uint32_t src) {SCOPEMARK_NAME(TurboAssembler::Move, this);
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  li(scratch, Operand(static_cast<int32_t>(src)));
+  ifmovs(scratch, dst);
+}
+
+void TurboAssembler::Move(FPURegister dst, uint64_t src) {SCOPEMARK_NAME(TurboAssembler::Move, this);
+  // Handle special values first.
+  if (src == bit_cast<uint64_t>(0.0) /*&& has_double_zero_reg_set_*/) {
+    fmovd(kDoubleRegZero,dst);
+  } else if (src == bit_cast<uint64_t>(-0.0) /*&& has_double_zero_reg_set_*/) {
+    Fnegd(dst, kDoubleRegZero);
+  } else {
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    li(scratch, Operand(bit_cast<int64_t>(src)));
+    ifmovd(scratch, dst);
+  }
+}
+
+void TurboAssembler::Seleq(Register rd, Register rs, Register rt) {SCOPEMARK_NAME(TurboAssembler::Seleq, this);
+  seleq(rt, rs, rd, rd);
+}
+
+void TurboAssembler::Selne(Register rd, Register rs, Register rt) {SCOPEMARK_NAME(TurboAssembler::Selne, this);
+  selne(rt, rs, rd, rd);
+}
+
+void TurboAssembler::LoadZeroOnCondition(Register rd, Register rs,
+                                         const Operand& rt, Condition cond) {SCOPEMARK_NAME(TurboAssembler::LoadZeroOnCondition, this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  switch (cond) {
+    case cc_always:
+      mov(rd, zero_reg);
+      break;
+    case eq:
+      if (rs == zero_reg) {
+        if (rt.is_reg()) {
+          LoadZeroIfConditionZero(rd, rt.rm());
+        } else {
+          if (rt.immediate() == 0) {
+            mov(rd, zero_reg);
+          } else {
+            nop();
+          }
+        }
+      } else if (IsZero(rt)) {
+        LoadZeroIfConditionZero(rd, rs);
+      } else {
+        Subl(t12, rs, rt);
+        LoadZeroIfConditionZero(rd, t12);
+      }
+      break;
+    case ne:
+      if (rs == zero_reg) {
+        if (rt.is_reg()) {
+          LoadZeroIfConditionNotZero(rd, rt.rm());
+        } else {
+          if (rt.immediate() != 0) {
+            mov(rd, zero_reg);
+          } else {
+            nop();
+          }
+        }
+      } else if (IsZero(rt)) {
+        LoadZeroIfConditionNotZero(rd, rs);
+      } else {
+        Subl(t12, rs, rt);
+        LoadZeroIfConditionNotZero(rd, t12);
+      }
+      break;
+
+    // Signed comparison.
+    case greater:
+      Cmpgt(t12, rs, rt);
+      LoadZeroIfConditionNotZero(rd, t12);
+      break;
+    case greater_equal:
+      Cmpge(t12, rs, rt);
+      LoadZeroIfConditionNotZero(rd, t12);
+      // rs >= rt
+      break;
+    case less:
+      Cmplt(t12, rs, rt);
+      LoadZeroIfConditionNotZero(rd, t12);
+      // rs < rt
+      break;
+    case less_equal:
+      Cmple(t12, rs, rt);
+      LoadZeroIfConditionNotZero(rd, t12);
+      // rs <= rt
+      break;
+
+    // Unsigned comparison.
+    case Ugreater:
+      Cmpugt(t12, rs, rt);
+      LoadZeroIfConditionNotZero(rd, t12);
+      // rs > rt
+      break;
+
+    case Ugreater_equal:
+      Cmpuge(t12, rs, rt);
+      LoadZeroIfConditionNotZero(rd, t12);
+      // rs >= rt
+      break;
+    case Uless:
+      Cmpult(t12, rs, rt);
+      LoadZeroIfConditionNotZero(rd, t12);
+      // rs < rt
+      break;
+    case Uless_equal:
+      Cmpule(t12, rs, rt);
+      LoadZeroIfConditionNotZero(rd, t12);
+      // rs <= rt
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+void TurboAssembler::LoadZeroIfConditionNotZero(Register dest,
+                                                Register condition) {SCOPEMARK_NAME(TurboAssembler::LoadZeroIfConditionNotZero, this);
+  Selne(dest, zero_reg, condition);
+}
+
+void TurboAssembler::LoadZeroIfConditionZero(Register dest,
+                                             Register condition) {SCOPEMARK_NAME(TurboAssembler::LoadZeroIfConditionZero, this);
+  Seleq(dest, zero_reg, condition);
+}
+
+void TurboAssembler::LoadZeroIfFPUCondition(Register dest) {SCOPEMARK_NAME(TurboAssembler::LoadZeroIfFPUCondition, this);
+#ifdef SW64
+  fimovd(kDoubleCompareReg, kScratchReg);
+  LoadZeroIfConditionNotZero(dest, kScratchReg);
+#endif
+}
+
+void TurboAssembler::LoadZeroIfNotFPUCondition(Register dest) {SCOPEMARK_NAME(TurboAssembler::LoadZeroIfNotFPUCondition, this);
+#ifdef SW64
+  fimovd(kDoubleCompareReg, kScratchReg);
+  LoadZeroIfConditionZero(dest, kScratchReg);
+#endif
+}
+
+
+void TurboAssembler::Clz(Register rd, Register rs) {SCOPEMARK_NAME(TurboAssembler::Clz, this);
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  addw(rs, 0, scratch);  // sign extend
+  sellt(scratch, zero_reg, rd, rd);  // (int)rs < 0 => rd = 0;
+  blt(scratch, 3);
+  ctlz(scratch, rd);  // rs>0 => rd=rd-32;
+  ldi(scratch, 32, zero_reg);
+  subl(rd, scratch, rd);
+}
+
+void TurboAssembler::Dclz(Register rd, Register rs) { ctlz(rs, rd); }
+
+void TurboAssembler::Ctz(Register rd, Register rs) {SCOPEMARK_NAME(TurboAssembler::Ctz, this);
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  cttz(rs, rd);
+  ldi(scratch, 32, zero_reg);
+  subl(rd, scratch, scratch);
+  selge(scratch, 32, rd, rd);
+}
+
+void TurboAssembler::Dctz(Register rd, Register rs) {SCOPEMARK_NAME(TurboAssembler::Dctz, this);
+  cttz(rs, rd);
+}
+
+void TurboAssembler::Popcnt(Register rd, Register rs) {SCOPEMARK_NAME(TurboAssembler::Popcnt, this);
+  zapnot(rs, 0xf, rd);
+  ctpop(rd, rd);
+}
+
+void TurboAssembler::Dpopcnt(Register rd, Register rs) {SCOPEMARK_NAME(TurboAssembler::Dpopcnt, this);
+  ctpop(rs, rd);
+}
+
+void MacroAssembler::EmitFPUTruncate(FPURoundingMode rounding_mode,
+                                     Register result,
+                                     DoubleRegister double_input,
+                                     Register scratch,
+                                     DoubleRegister double_scratch,
+                                     Register except_flag,
+                                     CheckForInexactConversion check_inexact) {SCOPEMARK_NAME(MacroAssembler::EmitFPUTruncate, this);
+  DCHECK(result != scratch);
+  DCHECK(double_input != double_scratch);
+  DCHECK(except_flag != scratch);
+
+  Label done;
+
+  // Clear the except flag (0 = no exception)
+  mov(except_flag, zero_reg);
+
+  // Test for values that can be exactly represented as a signed 32-bit integer.
+  fcvtdl(double_input, double_scratch);
+  fcvtlw(double_scratch, kScratchDoubleReg1);
+  fimovs(kScratchDoubleReg1, result);
+  
+  fcvtwd(double_scratch, double_scratch);
+  CompareF64(EQ, double_input, double_scratch);
+  BranchTrueShortF(&done);
+
+  int64_t except_mask = sFCSRFlagMask;  // int32_t except_mask = kFCSRFlagMask;  // Assume interested in all exceptions.
+
+  if (check_inexact == kDontCheckForInexactConversion) {
+    // Ignore inexact exceptions.
+    except_mask &= ~sFCSRInexactFlagMask;  // except_mask &= ~kFCSRInexactFlagMask;
+  }
+
+  // Save FCSR.
+  rfpcr(kScratchDoubleReg);
+  // Disable FPU exceptions.
+  // SW64 neednot clear FPCR 20150513.
+  //in order to have same effection , we should do four steps in sw:
+  //1) set fpcr = 0
+  //2) Rounding: sw(10), round-to-even
+  //3) set trap bit: sw(62~61,51~49), exception controlled by fpcr but not trap
+  //4) set exception mode: sw(00) setfpec0
+  li(scratch, sFCSRControlMask | sFCSRRound1Mask); //1), 2), 3)
+  ifmovd(scratch, double_scratch);
+  wfpcr(double_scratch);
+  setfpec1();//4)
+
+  // Do operation based on rounding mode.
+  switch (rounding_mode) {
+    case kRoundToNearest:
+      fcvtdl_g(double_input, double_scratch);
+      break;
+    case kRoundToZero:
+      fcvtdl_z(double_input, double_scratch);
+      break;
+    case kRoundToPlusInf:
+      fcvtdl_p(double_input, double_scratch);
+      break;
+    case kRoundToMinusInf:
+      fcvtdl_n(double_input, double_scratch);
+      break;
+  }  // End of switch-statement.
+
+  // Move the converted value into the result register.
+  fcvtlw(double_scratch, kScratchDoubleReg1);
+  fimovs(kScratchDoubleReg1, result);
+
+  // Retrieve FCSR.
+  rfpcr(double_scratch);
+  fimovd(double_scratch, except_flag);
+  // Restore FCSR.
+  wfpcr(kScratchDoubleReg); 
+  setfpec1();
+
+  // Check for fpu exceptions.
+  And(except_flag, except_flag, Operand(except_mask));
+
+  bind(&done);
+}
+
+//SKTODO
+void TurboAssembler::TryInlineTruncateDoubleToI(Register result,
+                                                DoubleRegister double_input,
+                                                Label* done) {SCOPEMARK_NAME(TurboAssembler::TryInlineTruncateDoubleToI, this);
+  DoubleRegister single_scratch = kScratchDoubleReg;
+  UseScratchRegisterScope temps(this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  Register scratch = temps.Acquire();
+  Register scratch2 = t12;
+
+  DoubleRegister fp_scratch =  f20;
+  DoubleRegister fp_scratch2 = f21;
+  DCHECK(fp_scratch != double_input && fp_scratch2 != double_input && single_scratch != double_input);
+  DCHECK(scratch != result && scratch2 != result);
+
+  MultiPushFPU(fp_scratch.bit() | fp_scratch2.bit());
+
+  // Clear cumulative exception flags and save the FCSR.
+  // SW64 FPCR, equal to DoubleToIStub::Generate
+  //in order to have same effection, we should do four steps in sw:
+  //1) set fpcr = 0
+  //2) Rounding: sw(10), round-to-even
+  //3) set trap bit: sw(62~61,51~49), exception controlled by fpcr but not trap
+  //4) set exception mode: sw(00) setfpec0
+  rfpcr(fp_scratch2);
+  li(scratch, sFCSRControlMask | sFCSRRound1Mask); //1), 2), 3)
+  ifmovd(scratch, fp_scratch);
+  wfpcr(fp_scratch);
+  setfpec1();//4)
+
+  // Try a conversion to a signed integer.
+  fcvtdl_z(double_input, single_scratch);
+  fcvtlw(single_scratch, fp_scratch);
+  fimovs(fp_scratch, result);
+
+  // Retrieve and restore the FCSR.
+  rfpcr(fp_scratch);
+  wfpcr(fp_scratch2);
+  setfpec1();
+  fimovd(fp_scratch, scratch);
+
+  MultiPopFPU(fp_scratch.bit() | fp_scratch2.bit());
+
+  // Check for overflow and NaNs.
+  li(scratch2, sFCSROverflowFlagMask | sFCSRUnderflowFlagMask |
+               sFCSRInvalidOpFlagMask);
+  And(scratch, scratch, Operand(scratch2));
+
+  // If we had no exceptions we are done.
+  Branch(done, eq, scratch, Operand(zero_reg));
+}
+
+void TurboAssembler::TruncateDoubleToI(Isolate* isolate, Zone* zone,
+                                       Register result,
+                                       DoubleRegister double_input,
+                                       StubCallMode stub_mode) {SCOPEMARK_NAME(TurboAssembler::TruncateDoubleToI, this);
+  Label done;
+
+  TryInlineTruncateDoubleToI(result, double_input, &done);
+
+  // If we fell through then inline version didn't succeed - call stub instead.
+  push(ra);
+  Subl(sp, sp, Operand(kDoubleSize));  // Put input on stack.
+  Fstd(double_input, MemOperand(sp, 0));
+
+  if (stub_mode == StubCallMode::kCallWasmRuntimeStub) {
+    Call(wasm::WasmCode::kDoubleToI, RelocInfo::WASM_STUB_CALL);
+  } else {
+    Call(BUILTIN_CODE(isolate, DoubleToI), RelocInfo::CODE_TARGET);
+  }
+  Ldl(result, MemOperand(sp, 0));
+
+  Addl(sp, sp, Operand(kDoubleSize));
+  pop(ra);
+
+  bind(&done);
+}
+
+// Emulated condtional branches do not emit a nop in the branch delay slot.
+//
+// BRANCH_ARGS_CHECK checks that conditional jump arguments are correct.
+#define BRANCH_ARGS_CHECK(cond, rs, rt)                                  \
+  DCHECK((cond == cc_always && rs == zero_reg && rt.rm() == zero_reg) || \
+         (cond != cc_always && (rs != zero_reg || rt.rm() != zero_reg)))
+
+void TurboAssembler::Branch(int32_t offset, BranchDelaySlot bdslot) {SCOPEMARK_NAME(TurboAssembler::Branch, this);
+  DCHECK_EQ(kArchVariant, kSw64r3 ? is_int26(offset) : is_int21(offset));
+  BranchShort(offset, bdslot);
+}
+
+void TurboAssembler::Branch(int32_t offset, Condition cond, Register rs,
+                            const Operand& rt, BranchDelaySlot bdslot) {SCOPEMARK_NAME(TurboAssembler::Branch, this);
+  bool is_near = BranchShortCheck(offset, nullptr, cond, rs, rt, bdslot);
+  DCHECK(is_near);
+  USE(is_near);
+}
+
+void TurboAssembler::Branch(Label* L, BranchDelaySlot bdslot) {SCOPEMARK_NAME(TurboAssembler::Branch, this);
+  if (L->is_bound()) {
+    if (is_near_branch(L)) {
+      BranchShort(L, bdslot);
+    } else {
+      BranchLong(L, bdslot);
+    }
+  } else {
+    if (is_trampoline_emitted()) {
+      BranchLong(L, bdslot);
+    } else {
+      BranchShort(L, bdslot);
+    }
+  }
+}
+
+void TurboAssembler::Branch(Label* L, Condition cond, Register rs,
+                            const Operand& rt, BranchDelaySlot bdslot) {SCOPEMARK_NAME(TurboAssembler::Branch, this);
+  if (L->is_bound()) {
+    if (!BranchShortCheck(0, L, cond, rs, rt, bdslot)) {
+      if (cond != cc_always) {
+        Label skip;
+        Condition neg_cond = NegateCondition(cond);
+        BranchShort(&skip, neg_cond, rs, rt);
+        BranchLong(L, bdslot);
+        bind(&skip);
+      } else {
+        BranchLong(L, bdslot);
+      }
+    }
+  } else {
+    if (is_trampoline_emitted()) {
+      if (cond != cc_always) {
+        Label skip;
+        Condition neg_cond = NegateCondition(cond);
+        BranchShort(&skip, neg_cond, rs, rt);
+        BranchLong(L, bdslot);
+        bind(&skip);
+      } else {
+        BranchLong(L, bdslot);
+      }
+    } else {
+      BranchShort(L, cond, rs, rt, bdslot);
+    }
+  }
+}
+
+void TurboAssembler::Branch(Label* L, Condition cond, Register rs,
+                            RootIndex index, BranchDelaySlot bdslot) {SCOPEMARK_NAME(TurboAssembler::Branch, this);
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  LoadRoot(scratch, index);
+  Branch(L, cond, rs, Operand(scratch), bdslot);
+}
+
+void TurboAssembler::BranchShortHelper(int32_t offset, Label* L,
+                                       BranchDelaySlot bdslot) {SCOPEMARK_NAME(TurboAssembler::BranchShortHelper, this);
+  DCHECK(L == nullptr || offset == 0);
+  offset = GetOffset(offset, L, OffsetSize::kOffset21);
+  br(offset);
+}
+
+void TurboAssembler::BranchShort(int32_t offset, BranchDelaySlot bdslot) {SCOPEMARK_NAME(TurboAssembler::BranchShort, this);
+  DCHECK(is_int21(offset));
+  BranchShortHelper(offset, nullptr, bdslot);
+}
+
+void TurboAssembler::BranchShort(Label* L, BranchDelaySlot bdslot) {SCOPEMARK_NAME(TurboAssembler::BranchShort, this);
+  BranchShortHelper(0, L, bdslot);
+}
+
+
+int32_t TurboAssembler::GetOffset(int32_t offset, Label* L, OffsetSize bits) {SCOPEMARK_NAME(TurboAssembler::GetOffset, this);
+  if (L) {
+    offset = branch_offset_helper(L, bits) >> 2;
+  } else {
+    DCHECK(is_intn(offset, bits));
+  }
+  return offset;
+}
+
+Register TurboAssembler::GetRtAsRegisterHelper(const Operand& rt,
+                                               Register scratch) {SCOPEMARK_NAME(TurboAssembler::GetRtAsRegisterHelper, this);
+  Register r2 = no_reg;
+  if (rt.is_reg()) {
+    r2 = rt.rm();
+  } else {
+    r2 = scratch;
+    li(r2, rt);
+  }
+
+  return r2;
+}
+
+bool TurboAssembler::CalculateOffset(Label* L, int32_t* offset,
+                                     OffsetSize bits) {SCOPEMARK_NAME(TurboAssembler::CalculateOffset, this);
+  if (!is_near(L, bits)) return false;
+  *offset = GetOffset(*offset, L, bits);
+  return true;
+}
+
+bool TurboAssembler::CalculateOffset(Label* L, int32_t* offset, OffsetSize bits,
+                                     Register* scratch, const Operand& rt) {SCOPEMARK_NAME(TurboAssembler::CalculateOffset, this);
+  if (!is_near(L, bits)) return false;
+  *scratch = GetRtAsRegisterHelper(rt, *scratch);
+  *offset = GetOffset(*offset, L, bits);
+  return true;
+}
+
+bool TurboAssembler::BranchShortHelper(int32_t offset, Label* L, Condition cond,
+                                       Register rs, const Operand& rt,
+                                       BranchDelaySlot bdslot) {SCOPEMARK_NAME(TurboAssembler::BranchShortHelper, this);
+  DCHECK(L == nullptr || offset == 0);
+  if (!is_near(L, OffsetSize::kOffset21)) return false;
+
+  UseScratchRegisterScope temps(this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  Register scratch = temps.hasAvailable() ? temps.Acquire() : t11;
+  Register scratch2 = gp;
+  int32_t offset32;
+
+  // Be careful to always use shifted_branch_offset only just before the
+  // branch instruction, as the location will be remember for patching the
+  // target.
+  {
+    BlockTrampolinePoolScope block_trampoline_pool(this);
+    switch (cond) {
+      case cc_always:
+        offset32 = GetOffset(offset, L, OffsetSize::kOffset21);
+        br(offset32);  // b(offset32);
+        break;
+      case eq:
+        if (IsZero(rt)) {
+          offset32 = GetOffset(offset, L, OffsetSize::kOffset21);
+          beq(rs, offset32);  // beq(rs, zero_reg, offset32);
+        } else {
+          // We don't want any other register but scratch clobbered.
+          scratch = GetRtAsRegisterHelper(rt, scratch);
+          DCHECK(rs!=scratch2 && scratch != scratch2);
+          cmpeq(rs, scratch, scratch2);
+          offset32 = GetOffset(offset, L, OffsetSize::kOffset21);
+          bne(scratch2, offset32);
+        }
+        break;
+      case ne:
+        if (IsZero(rt)) {
+          offset32 = GetOffset(offset, L, OffsetSize::kOffset21);
+          bne(rs, offset32);  // bne(rs, zero_reg, offset32);
+        } else {
+          // We don't want any other register but scratch clobbered.
+          scratch = GetRtAsRegisterHelper(rt, scratch);
+          DCHECK(rs!=scratch2 && scratch != scratch2);
+          cmpeq(rs, scratch, scratch2);
+          offset32 = GetOffset(offset, L, OffsetSize::kOffset21);
+          beq(scratch2, offset32);
+        }
+        break;
+
+      // Signed comparison.
+      case greater:
+        if (IsZero(rt)) {
+          offset32 = GetOffset(offset, L, OffsetSize::kOffset21);
+          bgt(rs, offset32); // bgtz(rs, offset32);
+        } else {
+          Cmplt(scratch, GetRtAsRegisterHelper(rt, scratch), rs);
+          offset32 = GetOffset(offset, L, OffsetSize::kOffset21);
+          bne(scratch, offset32);  // bne(scratch, zero_reg, offset32);
+        }
+        break;
+      case greater_equal:
+        if (IsZero(rt)) {
+          offset32 = GetOffset(offset, L, OffsetSize::kOffset21);
+          bge(rs, offset32);  // bgez(rs, offset32);
+        } else {
+          Cmplt(scratch, rs, rt);
+          offset32 = GetOffset(offset, L, OffsetSize::kOffset21);
+          beq(scratch, offset32);  // beq(scratch, zero_reg, offset32);
+        }
+        break;
+      case less:
+        if (IsZero(rt)) {
+          offset32 = GetOffset(offset, L, OffsetSize::kOffset21);
+          blt(rs, offset32);  // bltz(rs, offset32);
+        } else {
+          Cmplt(scratch, rs, rt);
+          offset32 = GetOffset(offset, L, OffsetSize::kOffset21);
+          bne(scratch, offset32);  // bne(scratch, zero_reg, offset32);
+        }
+        break;
+      case less_equal:
+        if (IsZero(rt)) {
+          offset32 = GetOffset(offset, L, OffsetSize::kOffset21);
+          ble(rs, offset32);  // blez(rs, offset32);
+        } else {
+          Cmplt(scratch, GetRtAsRegisterHelper(rt, scratch), rs);
+          offset32 = GetOffset(offset, L, OffsetSize::kOffset21);
+          beq(scratch, offset32);  // beq(scratch, zero_reg, offset32);
+        }
+        break;
+
+      // Unsigned comparison.
+      case Ugreater:
+        if (IsZero(rt)) {
+          offset32 = GetOffset(offset, L, OffsetSize::kOffset21);
+          bne(rs, offset32);  // bne(rs, zero_reg, offset32);
+        } else {
+          Cmpult(scratch, GetRtAsRegisterHelper(rt, scratch), rs);
+          offset32 = GetOffset(offset, L, OffsetSize::kOffset21);
+          bne(scratch, offset32);  // bne(scratch, zero_reg, offset32);
+        }
+        break;
+      case Ugreater_equal:
+        if (IsZero(rt)) {
+          offset32 = GetOffset(offset, L, OffsetSize::kOffset21);
+          br(offset32);  // b(offset32);
+        } else {
+          Cmpult(scratch, rs, rt);
+          offset32 = GetOffset(offset, L, OffsetSize::kOffset21);
+          beq(scratch, offset32);  // beq(scratch, zero_reg, offset32);
+        }
+        break;
+      case Uless:
+        if (IsZero(rt)) {
+          return true;  // No code needs to be emitted.
+        } else {
+          Cmpult(scratch, rs, rt);
+          offset32 = GetOffset(offset, L, OffsetSize::kOffset21);
+          bne(scratch, offset32);  // bne(scratch, zero_reg, offset32);
+        }
+        break;
+      case Uless_equal:
+        if (IsZero(rt)) {
+          offset32 = GetOffset(offset, L, OffsetSize::kOffset21);
+          beq(rs, offset32);  // beq(rs, zero_reg, offset32);
+        } else {
+          Cmpult(scratch, GetRtAsRegisterHelper(rt, scratch), rs);
+          offset32 = GetOffset(offset, L, OffsetSize::kOffset21);
+          beq(scratch, offset32);  // beq(scratch, zero_reg, offset32);
+        }
+        break;
+      default:
+        UNREACHABLE();
+    }
+  }
+
+  return true;
+}
+
+bool TurboAssembler::BranchShortCheck(int32_t offset, Label* L, Condition cond,
+                                      Register rs, const Operand& rt,
+                                      BranchDelaySlot bdslot) {SCOPEMARK_NAME(TurboAssembler::BranchShortCheck, this);
+  BRANCH_ARGS_CHECK(cond, rs, rt);
+
+  if (!L) {
+    DCHECK(is_int21(offset));
+    return BranchShortHelper(offset, nullptr, cond, rs, rt, bdslot);
+  } else {
+    DCHECK_EQ(offset, 0);
+    return BranchShortHelper(0, L, cond, rs, rt, bdslot);
+  }
+  return false;
+}
+
+void TurboAssembler::BranchShort(int32_t offset, Condition cond, Register rs,
+                                 const Operand& rt, BranchDelaySlot bdslot) {SCOPEMARK_NAME(TurboAssembler::BranchShort, this);
+  BranchShortCheck(offset, nullptr, cond, rs, rt, bdslot);
+}
+
+void TurboAssembler::BranchShort(Label* L, Condition cond, Register rs,
+                                 const Operand& rt, BranchDelaySlot bdslot) {SCOPEMARK_NAME(TurboAssembler::BranchShort, this);
+  BranchShortCheck(0, L, cond, rs, rt, bdslot);
+}
+
+void TurboAssembler::BranchAndLink(int32_t offset, BranchDelaySlot bdslot) {SCOPEMARK_NAME(TurboAssembler::BranchAndLink, this);
+  BranchAndLinkShort(offset, bdslot);
+}
+
+void TurboAssembler::BranchAndLink(int32_t offset, Condition cond, Register rs,
+                                   const Operand& rt, BranchDelaySlot bdslot) {SCOPEMARK_NAME(TurboAssembler::BranchAndLink, this);
+  bool is_near = BranchAndLinkShortCheck(offset, nullptr, cond, rs, rt, bdslot);
+  DCHECK(is_near);
+  USE(is_near);
+}
+
+void TurboAssembler::BranchAndLink(Label* L, BranchDelaySlot bdslot) {SCOPEMARK_NAME(TurboAssembler::BranchAndLink, this);
+  if (L->is_bound()) {
+    if (is_near_branch(L)) {
+      BranchAndLinkShort(L, bdslot);
+    } else {
+      BranchAndLinkLong(L, bdslot);
+    }
+  } else {
+    if (is_trampoline_emitted()) {
+      BranchAndLinkLong(L, bdslot);
+    } else {
+      BranchAndLinkShort(L, bdslot);
+    }
+  }
+}
+
+void TurboAssembler::BranchAndLink(Label* L, Condition cond, Register rs,
+                                   const Operand& rt, BranchDelaySlot bdslot) {SCOPEMARK_NAME(TurboAssembler::BranchAndLink, this);
+  if (L->is_bound()) {
+    if (!BranchAndLinkShortCheck(0, L, cond, rs, rt, bdslot)) {
+      Label skip;
+      Condition neg_cond = NegateCondition(cond);
+      BranchShort(&skip, neg_cond, rs, rt);
+      BranchAndLinkLong(L, bdslot);
+      bind(&skip);
+    }
+  } else {
+    if (is_trampoline_emitted()) {
+      Label skip;
+      Condition neg_cond = NegateCondition(cond);
+      BranchShort(&skip, neg_cond, rs, rt);
+      BranchAndLinkLong(L, bdslot);
+      bind(&skip);
+    } else {
+      BranchAndLinkShortCheck(0, L, cond, rs, rt, bdslot);
+    }
+  }
+}
+
+void TurboAssembler::BranchAndLinkShortHelper(int32_t offset, Label* L,
+                                              BranchDelaySlot bdslot) {SCOPEMARK_NAME(TurboAssembler::BranchAndLinkShortHelper, this);
+  DCHECK(L == nullptr || offset == 0);
+  offset = GetOffset(offset, L, OffsetSize::kOffset21);
+  br(ra, offset);  // bal(offset);
+}
+
+void TurboAssembler::BranchAndLinkShort(int32_t offset,
+                                        BranchDelaySlot bdslot) {SCOPEMARK_NAME(TurboAssembler::BranchAndLinkShort, this);
+  DCHECK(is_int21(offset));
+  BranchAndLinkShortHelper(offset, nullptr, bdslot);
+}
+
+void TurboAssembler::BranchAndLinkShort(Label* L, BranchDelaySlot bdslot) {SCOPEMARK_NAME(TurboAssembler::BranchAndLinkShort, this);
+  BranchAndLinkShortHelper(0, L, bdslot);
+}
+
+// Pre r6 we need to use a bgezal or bltzal, but they can't be used directly
+// with the cmplt instructions. We could use sub or add instead but we would miss
+// overflow cases, so we keep cmplt and add an intermediate third instruction.
+bool TurboAssembler::BranchAndLinkShortHelper(int32_t offset, Label* L,
+                                              Condition cond, Register rs,
+                                              const Operand& rt,
+                                              BranchDelaySlot bdslot) {SCOPEMARK_NAME(TurboAssembler::BranchAndLinkShortHelper, this);
+  DCHECK(L == nullptr || offset == 0);
+  if (!is_near(L, OffsetSize::kOffset21)) return false;
+
+  Register scratch = t11;
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+
+  switch (cond) {
+    case cc_always:
+      offset = GetOffset(offset, L, OffsetSize::kOffset21);
+      br(ra, offset);  // bal(offset);
+      break;
+    case eq:
+      cmpeq(rs, GetRtAsRegisterHelper(rt, scratch), scratch);  beq(scratch, 1); // bne(rs, GetRtAsRegisterHelper(rt, scratch), 2);
+      offset = GetOffset(offset, L, OffsetSize::kOffset21);
+      br(ra, offset);  // bal(offset);
+      break;
+    case ne:
+      cmpeq(rs, GetRtAsRegisterHelper(rt, scratch), scratch);  bne(scratch, 1); // beq(rs, GetRtAsRegisterHelper(rt, scratch), 2);
+      offset = GetOffset(offset, L, OffsetSize::kOffset21);
+      br(ra, offset);  // bal(offset);
+      break;
+
+    // Signed comparison.
+    case greater:
+      Cmplt(scratch, GetRtAsRegisterHelper(rt, scratch), rs);
+      Addw(scratch, scratch, Operand(-1));
+      blt(scratch, 1);  bsr(L);  // bgezal(scratch, offset);
+      break;
+    case greater_equal:
+      Cmplt(scratch, rs, rt);
+      Addw(scratch, scratch, Operand(-1));
+      bge(scratch, 1);  bsr(L);  // bltzal(scratch, offset);
+      break;
+    case less:
+      Cmplt(scratch, rs, rt);
+      Addw(scratch, scratch, Operand(-1));
+      offset = GetOffset(offset, L, OffsetSize::kOffset21);
+      blt(scratch, 1);  br(ra, offset);  // bgezal(scratch, offset);
+      break;
+    case less_equal:
+      Cmplt(scratch, GetRtAsRegisterHelper(rt, scratch), rs);
+      Addw(scratch, scratch, Operand(-1));
+      bge(scratch, 1);  bsr(L);  // bltzal(scratch, offset);
+      break;
+
+    // Unsigned comparison.
+    case Ugreater:
+      Cmpult(scratch, GetRtAsRegisterHelper(rt, scratch), rs);
+      Addw(scratch, scratch, Operand(-1));
+      blt(scratch, 1);  bsr(L);  // bgezal(scratch, offset);
+      break;
+    case Ugreater_equal:
+      Cmpult(scratch, rs, rt);
+      Addw(scratch, scratch, Operand(-1));
+      bge(scratch, 1);  bsr(L);  // bltzal(scratch, offset);
+      break;
+    case Uless:
+      Cmpult(scratch, rs, rt);
+      Addw(scratch, scratch, Operand(-1));
+      blt(scratch, 1);  bsr(L);  // bgezal(scratch, offset);
+      break;
+    case Uless_equal:
+      Cmpult(scratch, GetRtAsRegisterHelper(rt, scratch), rs);
+      Addw(scratch, scratch, Operand(-1));
+      bge(scratch, 1);  bsr(L);  // bltzal(scratch, offset);
+      break;
+
+    default:
+      UNREACHABLE();
+  }
+
+  return true;
+}
+
+bool TurboAssembler::BranchAndLinkShortCheck(int32_t offset, Label* L,
+                                             Condition cond, Register rs,
+                                             const Operand& rt,
+                                             BranchDelaySlot bdslot) {SCOPEMARK_NAME(TurboAssembler::BranchAndLinkShortCheck, this);
+  BRANCH_ARGS_CHECK(cond, rs, rt);
+
+  if (!L) {
+    DCHECK(is_int21(offset));
+    return BranchAndLinkShortHelper(offset, nullptr, cond, rs, rt, bdslot);
+  } else {
+    DCHECK_EQ(offset, 0);
+    return BranchAndLinkShortHelper(0, L, cond, rs, rt, bdslot);
+  }
+  return false;
+}
+
+void TurboAssembler::LoadFromConstantsTable(Register destination,
+                                            int constant_index) {SCOPEMARK_NAME(TurboAssembler::LoadFromConstantsTable, this);
+  DCHECK(RootsTable::IsImmortalImmovable(RootIndex::kBuiltinsConstantsTable));
+  LoadRoot(destination, RootIndex::kBuiltinsConstantsTable);
+  Ldl(destination,
+     FieldMemOperand(destination,
+                     FixedArray::kHeaderSize + constant_index * kPointerSize));
+}
+
+void TurboAssembler::LoadRootRelative(Register destination, int32_t offset) {SCOPEMARK_NAME(TurboAssembler::LoadRootRelative, this);
+  Ldl(destination, MemOperand(kRootRegister, offset));
+}
+
+void TurboAssembler::LoadRootRegisterOffset(Register destination,
+                                            intptr_t offset) {SCOPEMARK_NAME(TurboAssembler::LoadRootRegisterOffset, this);
+  if (offset == 0) {
+    Move(destination, kRootRegister);
+  } else {
+    Addl(destination, kRootRegister, Operand(offset));
+  }
+}
+
+void TurboAssembler::Jump(Register target, Condition cond, Register rs,
+                          const Operand& rt, BranchDelaySlot bd) {SCOPEMARK_NAME(TurboAssembler::Jump, this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  DCHECK_EQ(bd, PROTECT);
+  if (cond == cc_always) {
+    Assembler::jmp(zero_reg, target, 0);
+  } else {
+    BRANCH_ARGS_CHECK(cond, rs, rt);
+    Branch(1, NegateCondition(cond), rs, rt);
+    Assembler::jmp(zero_reg, target, 0);
+  }
+}
+
+void TurboAssembler::Jump(intptr_t target, RelocInfo::Mode rmode,
+                          Condition cond, Register rs, const Operand& rt,
+                          BranchDelaySlot bd) {SCOPEMARK_NAME(TurboAssembler::Jump, this);
+  Label skip;
+  DCHECK_EQ(bd, PROTECT);
+  {
+    BlockTrampolinePoolScope block_trampoline_pool(this);
+    li(t12, Operand(target, rmode));
+    if (cond != cc_always) {
+        Branch(&skip, NegateCondition(cond), rs, rt);
+    }
+    Jump(t12, al, zero_reg, Operand(zero_reg), bd);
+    bind(&skip);
+  }
+}
+
+void TurboAssembler::Jump(Address target, RelocInfo::Mode rmode, Condition cond,
+                          Register rs, const Operand& rt, BranchDelaySlot bd) {SCOPEMARK_NAME(TurboAssembler::Jump, this);
+  DCHECK(!RelocInfo::IsCodeTarget(rmode));
+  Jump(static_cast<intptr_t>(target), rmode, cond, rs, rt, bd);
+}
+
+void TurboAssembler::Jump(Handle<Code> code, RelocInfo::Mode rmode,
+                          Condition cond, Register rs, const Operand& rt,
+                          BranchDelaySlot bd) {SCOPEMARK_NAME(TurboAssembler::Jump, this);
+  DCHECK(RelocInfo::IsCodeTarget(rmode));
+    BlockTrampolinePoolScope block_trampoline_pool(this);
+    if (root_array_available_ && options().isolate_independent_code) {
+      IndirectLoadConstant(t12, code);
+      Addl(t12, t12, Operand(Code::kHeaderSize - kHeapObjectTag));
+      Jump(t12, cond, rs, rt, bd);
+      return;
+    } else if (options().inline_offheap_trampolines) {
+      int builtin_index = Builtins::kNoBuiltinId;
+      if (isolate()->builtins()->IsBuiltinHandle(code, &builtin_index) &&
+          Builtins::IsIsolateIndependent(builtin_index)) {
+        // Inline the trampoline.
+        RecordCommentForOffHeapTrampoline(builtin_index);
+        CHECK_NE(builtin_index, Builtins::kNoBuiltinId);
+        EmbeddedData d = EmbeddedData::FromBlob();
+        Address entry = d.InstructionStartOfBuiltin(builtin_index);
+        li(t12, Operand(entry, RelocInfo::OFF_HEAP_TARGET));
+        Jump(t12, cond, rs, rt, bd);
+        return;
+      }
+    }
+  Jump(static_cast<intptr_t>(code.address()), rmode, cond, rs, rt, bd);
+}
+
+void TurboAssembler::Jump(const ExternalReference& reference) {
+  li(t12, reference);
+  Jump(t12);
+}
+
+
+// Note: To call gcc-compiled C code on sw64, you must call through t12.
+void TurboAssembler::Call(Register target, Condition cond, Register rs,
+                          const Operand& rt, BranchDelaySlot bd) {SCOPEMARK_NAME(TurboAssembler::Call, this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  DCHECK(bd == PROTECT);
+  if (cond == cc_always) {
+    call(ra, target, 0);
+  } else {
+    BRANCH_ARGS_CHECK(cond, rs, rt);
+    Branch(1, NegateCondition(cond), rs, rt);
+    call(ra, target, 0);
+  }
+  set_last_call_pc_(pc_);
+}
+
+void MacroAssembler::JumpIfIsInRange(Register value, unsigned lower_limit,
+                                     unsigned higher_limit,
+                                     Label* on_in_range) {
+  if (lower_limit != 0) {
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    Subl(scratch, value, Operand(lower_limit));
+    Branch(on_in_range, ls, scratch, Operand(higher_limit - lower_limit));
+  } else {
+    Branch(on_in_range, ls, value, Operand(higher_limit - lower_limit));
+  }
+}
+
+void TurboAssembler::Call(Address target, RelocInfo::Mode rmode, Condition cond,
+                          Register rs, const Operand& rt, BranchDelaySlot bd) {SCOPEMARK_NAME(TurboAssembler::Call, this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  li(t12, Operand(static_cast<int64_t>(target), rmode), ADDRESS_LOAD);
+  Call(t12, cond, rs, rt, bd);
+}
+
+void TurboAssembler::Call(Handle<Code> code, RelocInfo::Mode rmode,
+                          Condition cond, Register rs, const Operand& rt,
+                          BranchDelaySlot bd) {SCOPEMARK_NAME(TurboAssembler::Call, this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+    if (root_array_available_ && options().isolate_independent_code) {
+      IndirectLoadConstant(t12, code);
+      Addl(t12, t12, Operand(Code::kHeaderSize - kHeapObjectTag));
+      Call(t12, cond, rs, rt, bd);
+      return;
+    } else if (options().inline_offheap_trampolines) {
+      int builtin_index = Builtins::kNoBuiltinId;
+      if (isolate()->builtins()->IsBuiltinHandle(code, &builtin_index) &&
+          Builtins::IsIsolateIndependent(builtin_index)) {
+        // Inline the trampoline.
+        RecordCommentForOffHeapTrampoline(builtin_index);
+        CHECK_NE(builtin_index, Builtins::kNoBuiltinId);
+        EmbeddedData d = EmbeddedData::FromBlob();
+        Address entry = d.InstructionStartOfBuiltin(builtin_index);
+        li(t12, Operand(entry, RelocInfo::OFF_HEAP_TARGET));
+        Call(t12, cond, rs, rt, bd);
+        return;
+      }
+    }
+  DCHECK(RelocInfo::IsCodeTarget(rmode));
+  DCHECK(code->IsExecutable());
+  Call(code.address(), rmode, cond, rs, rt, bd);
+}
+
+void TurboAssembler::LoadEntryFromBuiltinIndex(Register builtin_index) {
+  STATIC_ASSERT(kSystemPointerSize == 8);
+  STATIC_ASSERT(kSmiTagSize == 1);
+  STATIC_ASSERT(kSmiTag == 0);
+
+  // The builtin_index register contains the builtin index as a Smi.
+  SmiUntag(builtin_index, builtin_index);
+  Dlsa(builtin_index, kRootRegister, builtin_index, kSystemPointerSizeLog2);
+  Ldl(builtin_index,
+     MemOperand(builtin_index, IsolateData::builtin_entry_table_offset()));
+}
+
+void TurboAssembler::CallBuiltinByIndex(Register builtin_index) {
+  LoadEntryFromBuiltinIndex(builtin_index);
+  Call(builtin_index);
+}
+
+void TurboAssembler::PatchAndJump(Address target) {
+  if (kArchVariant != kSw64r3) {
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    mov(scratch, ra);
+    Align(8);
+    br(ra, 0);                                 // jump to ld
+    ldl(t12, MemOperand(ra, kInstrSize * 3));  // ra == pc_
+    mov(ra, scratch);  
+    Assembler::jmp(zero_reg, t12, 0);
+    DCHECK_EQ(reinterpret_cast<uint64_t>(pc_) % 8, 0);
+    *reinterpret_cast<uint64_t*>(pc_) = target;  // pc_ should be align.
+    pc_ += sizeof(uint64_t);
+  } else {
+    // TODO(sw64 r3): Implement.
+    UNIMPLEMENTED();
+  }
+}
+
+void TurboAssembler::StoreReturnAddressAndCall(Register target) {SCOPEMARK_NAME(TurboAssembler::StoreReturnAddressAndCall, this);
+  // This generates the final instruction sequence for calls to C functions
+  // once an exit frame has been constructed.
+  //
+  // Note that this assumes the caller code (i.e. the Code object currently
+  // being generated) is immovable or that the callee function cannot trigger
+  // GC, since the callee function will return to it.
+
+  // Compute the return address in lr to return to after the jump below. The pc
+  // is already at '+ 8' from the current instruction; but return is after three
+  // instructions, so add another 4 to pc to get the return address.
+
+Assembler::BlockTrampolinePoolScope block_trampoline_pool(this);
+  static constexpr int kNumInstructionsToJump = 2;
+  Label find_ra;
+
+  Move(t12, target);
+  br(kScratchReg, 0);
+  addl(kScratchReg, (kNumInstructionsToJump + 1) * kInstrSize, ra);
+  bind(&find_ra);
+  stl(ra, MemOperand(sp, 0));
+  call(ra, t12, 0);
+  DCHECK_EQ(kNumInstructionsToJump, InstructionsGeneratedSince(&find_ra));
+  setfpec1();
+}
+
+void TurboAssembler::Ret(Condition cond, Register rs, const Operand& rt,
+                         BranchDelaySlot bd) {SCOPEMARK_NAME(TurboAssembler::Ret, this);
+  Jump(ra, cond, rs, rt, bd);
+}
+
+void TurboAssembler::BranchLong(Label* L, BranchDelaySlot bdslot) {SCOPEMARK_NAME(TurboAssembler::BranchLong, this);
+    // use jmp only when Label is bound and target is beyond br's limit
+  if (L->is_bound() && !is_near_pre_r3(L)) {
+        BlockTrampolinePoolScope block_trampoline_pool(this);
+        int64_t imm64;
+        imm64 = branch_long_offset(L);
+        DCHECK(is_int32(imm64));
+        br(t8, 0);
+        li(t12, Operand(imm64));
+        addl(t8, t12, t12);
+        Assembler::jmp(zero_reg, t12, 0);
+    } else {
+        BlockTrampolinePoolScope block_trampoline_pool(this);
+        {
+            BlockGrowBufferScope block_buf_growth(this);
+            // Buffer growth (and relocation) must be blocked for internal references
+            // until associated instructions are emitted and available to be patched.
+            RecordRelocInfo(RelocInfo::INTERNAL_REFERENCE_ENCODED);
+            br(L);  //ZHJ j(L);
+        }
+    }
+}
+
+void TurboAssembler::BranchAndLinkLong(Label* L, BranchDelaySlot bdslot) {SCOPEMARK_NAME(TurboAssembler::BranchAndLinkLong, this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  {
+    BlockGrowBufferScope block_buf_growth(this);
+    // Buffer growth (and relocation) must be blocked for internal references
+    // until associated instructions are emitted and available to be patched.
+    RecordRelocInfo(RelocInfo::INTERNAL_REFERENCE_ENCODED);
+    bsr(L);  // jal(L);
+  }
+}
+
+void TurboAssembler::DropAndRet(int drop) {SCOPEMARK_NAME(TurboAssembler::DropAndRet, this);
+  if(is_uint8(drop * kPointerSize)){
+    addl(sp, drop * kPointerSize, sp);
+  }else{
+    DCHECK(is_int16(drop * kPointerSize));
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    ldi(scratch, drop * kPointerSize, zero_reg);
+    addl(sp, scratch, sp);
+  }
+  Ret();
+}
+
+void TurboAssembler::DropAndRet(int drop, Condition cond, Register r1,
+                                const Operand& r2) {SCOPEMARK_NAME(TurboAssembler::DropAndRet, this);
+  // Both Drop and Ret need to be conditional.
+  Label skip;
+  if (cond != cc_always) {
+    Branch(&skip, NegateCondition(cond), r1, r2);
+  }
+
+  Drop(drop);
+  Ret();
+
+  if (cond != cc_always) {
+    bind(&skip);
+  }
+}
+
+void TurboAssembler::Drop(int count, Condition cond, Register reg,
+                          const Operand& op) {SCOPEMARK_NAME(TurboAssembler::Drop, this);
+  if (count <= 0) {
+    return;
+  }
+
+  Label skip;
+
+  if (cond != al) {
+     Branch(&skip, NegateCondition(cond), reg, op);
+  }
+
+  Addl(sp, sp, Operand(count * kPointerSize));
+
+  if (cond != al) {
+    bind(&skip);
+  }
+}
+
+
+
+void MacroAssembler::Swap(Register reg1,
+                          Register reg2,
+                          Register scratch) {SCOPEMARK_NAME(MacroAssembler::Swap, this);
+  if (scratch == no_reg) {
+    Xor(reg1, reg1, Operand(reg2));
+    Xor(reg2, reg2, Operand(reg1));
+    Xor(reg1, reg1, Operand(reg2));
+  } else {
+    mov(scratch, reg1);
+    mov(reg1, reg2);
+    mov(reg2, scratch);
+  }
+}
+
+void TurboAssembler::Call(Label* target) { SCOPEMARK_NAME(TurboAssembler::Call, this);
+  BranchAndLink(target); }
+
+void TurboAssembler::LoadAddress(Register dst, Label* target) {
+  uint64_t address = jump_address(target);
+  li(dst, address);
+}
+
+void TurboAssembler::Push(Smi smi) {SCOPEMARK_NAME(TurboAssembler::Push, this);
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  li(scratch, Operand(smi));
+  push(scratch);
+}
+
+void TurboAssembler::Push(Handle<HeapObject> handle) {SCOPEMARK_NAME(TurboAssembler::Push, this);
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  li(scratch, Operand(handle));
+  push(scratch);
+}
+
+//SKTODO
+void TurboAssembler::PushArray(Register array, Register size, Register scratch,
+                               Register scratch2, PushArrayOrder order) {
+  DCHECK(!AreAliased(array, size, scratch, scratch2));
+  Label loop, entry;
+  if (order == PushArrayOrder::kReverse) {
+    mov(scratch, zero_reg);
+    jmp(&entry);
+    bind(&loop);
+    Dlsa(scratch2, array, scratch, kPointerSizeLog2);
+    Ldl(scratch2, MemOperand(scratch2));
+    push(scratch2);
+    Addl(scratch, scratch, Operand(1));
+    bind(&entry);
+    Branch(&loop, less, scratch, Operand(size));
+  } else {
+    mov(scratch, size);
+    jmp(&entry);
+    bind(&loop);
+    Dlsa(scratch2, array, scratch, kPointerSizeLog2);
+    Ldl(scratch2, MemOperand(scratch2));
+    push(scratch2);
+    bind(&entry);
+    Addl(scratch, scratch, Operand(-1));
+    Branch(&loop, greater_equal, scratch, Operand(zero_reg));
+  }
+}
+
+void MacroAssembler::MaybeDropFrames() {SCOPEMARK_NAME(MacroAssembler::MaybeDropFrames, this);
+  // Check whether we need to drop frames to restart a function on the stack.
+  li(a1, ExternalReference::debug_restart_fp_address(isolate()));
+  Ldl(a1, MemOperand(a1));
+  Jump(BUILTIN_CODE(isolate(), FrameDropperTrampoline), RelocInfo::CODE_TARGET,
+       ne, a1, Operand(zero_reg));
+}
+
+// ---------------------------------------------------------------------------
+// Exception handling.
+
+void MacroAssembler::PushStackHandler() {SCOPEMARK_NAME(MacroAssembler::PushStackHandler, this);
+  // Adjust this code if not the case.
+  STATIC_ASSERT(StackHandlerConstants::kSize == 2 * kPointerSize);
+  STATIC_ASSERT(StackHandlerConstants::kNextOffset == 0 * kPointerSize);
+
+  Push(Smi::zero());  // Padding.
+
+  // Link the current handler as the next handler.
+  li(t9,
+     ExternalReference::Create(IsolateAddressId::kHandlerAddress, isolate()));
+  Ldl(t1, MemOperand(t9));
+  push(t1);
+
+  // Set this new handler as the current one.
+  Stl(sp, MemOperand(t9));
+}
+
+
+void MacroAssembler::PopStackHandler() {SCOPEMARK_NAME(MacroAssembler::PopStackHandler, this);
+  STATIC_ASSERT(StackHandlerConstants::kNextOffset == 0);
+  pop(a1);
+  Addl(sp, sp, Operand(static_cast<int64_t>(StackHandlerConstants::kSize -
+                                             kPointerSize)));
+//  UseScratchRegisterScope temps(this);
+//  Register scratch = temps.Acquire();
+//  li(scratch,
+  li(t9,
+     ExternalReference::Create(IsolateAddressId::kHandlerAddress, isolate()));
+  Stl(a1, MemOperand(t9));
+}
+
+void TurboAssembler::FPUCanonicalizeNaN(const DoubleRegister dst,
+                                        const DoubleRegister src) {SCOPEMARK_NAME(TurboAssembler::FPUCanonicalizeNaN, this);
+  fsubd(src, kDoubleRegZero, dst);
+}
+
+void TurboAssembler::MovFromFloatResult(const DoubleRegister dst) {SCOPEMARK_NAME(TurboAssembler::MovFromFloatResult, this);
+  Move(dst, f0);  // Reg f0 is o32 ABI FP return value.
+}
+
+void TurboAssembler::MovFromFloatParameter(const DoubleRegister dst) {SCOPEMARK_NAME(TurboAssembler::MovFromFloatParameter, this);
+  Move(dst, f16);  // Reg f16 is sw64 ABI FP first argument value.
+}
+
+void TurboAssembler::MovToFloatParameter(DoubleRegister src) {SCOPEMARK_NAME(TurboAssembler::MovToFloatParameter, this);
+  Move(f16, src);
+}
+
+void TurboAssembler::MovToFloatResult(DoubleRegister src) {SCOPEMARK_NAME(TurboAssembler::MovToFloatResult, this);
+  Move(f0, src);
+}
+
+void TurboAssembler::MovToFloatParameters(DoubleRegister fsrc0,
+                                          DoubleRegister fsrc1) {SCOPEMARK_NAME(TurboAssembler::MovToFloatParameters, this);
+    const DoubleRegister fparg2 = f17;
+    if (fsrc1 == f16) {
+      DCHECK(fsrc0 != fparg2);
+      Move(fparg2, fsrc1);
+      Move(f16, fsrc0);
+    } else {
+      Move(f16, fsrc0);
+      Move(fparg2, fsrc1);
+    }
+}
+
+#ifdef SW64
+void TurboAssembler::MovFromGeneralResult(const Register dst) {SCOPEMARK_NAME(TurboAssembler::MovFromGeneralResult, this);
+  Move(dst, v0);
+}
+
+void TurboAssembler::MovFromGeneralParameter(const Register dst) {SCOPEMARK_NAME(TurboAssembler::MovFromGeneralParameter, this);
+  Move(dst, a0);
+}
+
+void TurboAssembler::MovToGeneralParameter(Register src) {SCOPEMARK_NAME(TurboAssembler::MovToGeneralParameter, this);
+  Move(a0, src);
+}
+
+void TurboAssembler::MovToGeneralResult(Register src) {SCOPEMARK_NAME(TurboAssembler::MovToGeneralResult, this);
+  Move(a0, src);
+}
+
+void TurboAssembler::MovToGeneralParameters(Register src0,
+                                            Register src1) {SCOPEMARK_NAME(TurboAssembler::MovToGeneralParameters, this);
+    if (src1 == a0) {
+      if (src0 != a1) {
+        Move(a1, src1);    // src1 = a0
+        Move(a0, src0);
+      } else {
+        UseScratchRegisterScope temps(this);
+        Register scratch = temps.Acquire();
+        Move(scratch, src1);  // at = src1(a0)
+        Move(a0, src0);       // a0 = src0(a1)
+        Move(a1, scratch);    // a1 = at
+      }
+    } else {
+      Move(a0, src0);
+      Move(a1, src1);
+    }
+}
+#endif
+
+// -----------------------------------------------------------------------------
+// JavaScript invokes.
+
+void TurboAssembler::PrepareForTailCall(Register callee_args_count,
+                                        Register caller_args_count,
+                                        Register scratch0, Register scratch1) {
+  // Calculate the end of destination area where we will put the arguments
+  // after we drop current frame. We add kPointerSize to count the receiver
+  // argument which is not included into formal parameters count.
+  Register dst_reg = scratch0;
+  s8addl(caller_args_count, fp, dst_reg);  DCHECK_EQ(kPointerSizeLog2, 3);
+  Addl(dst_reg, dst_reg,
+        Operand(StandardFrameConstants::kCallerSPOffset + kPointerSize));
+
+  Register src_reg = caller_args_count;
+  // Calculate the end of source area. +kPointerSize is for the receiver.
+  Dlsa(src_reg, sp, callee_args_count, kPointerSizeLog2);
+  Addl(src_reg, src_reg, Operand(kPointerSize));
+
+  if (FLAG_debug_code) {
+    Check(lo, AbortReason::kStackAccessBelowStackPointer, src_reg,
+          Operand(dst_reg));
+  }
+
+  // Restore caller's frame pointer and return address now as they will be
+  // overwritten by the copying loop.
+  Ldl(ra, MemOperand(fp, StandardFrameConstants::kCallerPCOffset));
+  Ldl(fp, MemOperand(fp, StandardFrameConstants::kCallerFPOffset));
+
+  // Now copy callee arguments to the caller frame going backwards to avoid
+  // callee arguments corruption (source and destination areas could overlap).
+
+  // Both src_reg and dst_reg are pointing to the word after the one to copy,
+  // so they must be pre-decremented in the loop.
+  Register tmp_reg = scratch1;
+  Label loop, entry;
+  Branch(&entry);
+  bind(&loop);
+  Subl(src_reg, src_reg, Operand(kPointerSize));
+  Subl(dst_reg, dst_reg, Operand(kPointerSize));
+  Ldl(tmp_reg, MemOperand(src_reg));
+  Stl(tmp_reg, MemOperand(dst_reg));
+  bind(&entry);
+  Branch(&loop, ne, sp, Operand(src_reg));
+
+  // Leave current frame.
+  mov(sp, dst_reg);
+}
+
+void MacroAssembler::InvokePrologue(Register expected_parameter_count,
+                                    Register actual_parameter_count,
+                                    Label* done, InvokeFlag flag) {
+  Label regular_invoke;
+
+  //  a0: actual arguments count
+  //  a1: function (passed through to callee)
+  //  a2: expected arguments count
+
+  DCHECK_EQ(actual_parameter_count, a0);
+  DCHECK_EQ(expected_parameter_count, a2);
+
+  Branch(&regular_invoke, eq, expected_parameter_count,
+         Operand(actual_parameter_count));
+
+    Handle<Code> adaptor = BUILTIN_CODE(isolate(), ArgumentsAdaptorTrampoline);
+    if (flag == CALL_FUNCTION) {
+      Call(adaptor);
+      Branch(done);
+    } else {
+      Jump(adaptor, RelocInfo::CODE_TARGET);
+    }
+
+    bind(&regular_invoke);
+  }
+
+void MacroAssembler::CheckDebugHook(Register fun, Register new_target,
+                                    Register expected_parameter_count,
+                                    Register actual_parameter_count) {
+  Label skip_hook;
+
+  li(t0, ExternalReference::debug_hook_on_function_call_address(isolate()));
+  Ldb(t0, MemOperand(t0));
+  Branch(&skip_hook, eq, t0, Operand(zero_reg));
+
+  {
+    // Load receiver to pass it later to DebugOnFunctionCall hook.
+    LoadReceiver(t0, actual_parameter_count);
+
+    FrameScope frame(this,
+                     has_frame() ? StackFrame::NONE : StackFrame::INTERNAL);
+    SmiTag(expected_parameter_count);
+    Push(expected_parameter_count);
+
+    SmiTag(actual_parameter_count);
+    Push(actual_parameter_count);
+
+    if (new_target.is_valid()) {
+      Push(new_target);
+    }
+    Push(fun);
+    Push(fun);
+    Push(t0);
+    CallRuntime(Runtime::kDebugOnFunctionCall);
+    Pop(fun);
+    if (new_target.is_valid()) {
+      Pop(new_target);
+    }
+
+    Pop(actual_parameter_count);
+    SmiUntag(actual_parameter_count);
+
+    Pop(expected_parameter_count);
+    SmiUntag(expected_parameter_count);
+  }
+  bind(&skip_hook);
+}
+
+void MacroAssembler::InvokeFunctionCode(Register function, Register new_target,
+                                        Register expected_parameter_count,
+                                        Register actual_parameter_count,
+                                        InvokeFlag flag) {
+  // You can't call a function without a valid frame.
+  DCHECK_IMPLIES(flag == CALL_FUNCTION, has_frame());
+  DCHECK_EQ(function, a1);
+  DCHECK_IMPLIES(new_target.is_valid(), new_target == a3);
+
+  // On function call, call into the debugger if necessary.
+  CheckDebugHook(function, new_target, expected_parameter_count,
+                 actual_parameter_count);
+
+  // Clear the new.target register if not given.
+  if (!new_target.is_valid()) {
+    LoadRoot(a3, RootIndex::kUndefinedValue);
+  }
+
+  Label done;
+  InvokePrologue(expected_parameter_count, actual_parameter_count, &done, flag);
+    // We call indirectly through the code field in the function to
+    // allow recompilation to take effect without changing any of the
+    // call sites.
+    Register code = kJavaScriptCallCodeStartRegister;
+    Ldl(code, FieldMemOperand(function, JSFunction::kCodeOffset));
+    if (flag == CALL_FUNCTION) {
+      Addl(code, code, Operand(Code::kHeaderSize - kHeapObjectTag));
+      Call(code);
+    } else {
+      DCHECK(flag == JUMP_FUNCTION);
+      Addl(code, code, Operand(Code::kHeaderSize - kHeapObjectTag));
+      Jump(code);
+    }
+    // Continue here if InvokePrologue does handle the invocation due to
+    // mismatched parameter counts.
+    bind(&done);
+  }
+
+void MacroAssembler::InvokeFunctionWithNewTarget(
+    Register function, Register new_target, Register actual_parameter_count,
+    InvokeFlag flag) {
+  // You can't call a function without a valid frame.
+  DCHECK_IMPLIES(flag == CALL_FUNCTION, has_frame());
+
+  // Contract with called JS functions requires that function is passed in a1.
+  DCHECK_EQ(function, a1);
+  Register expected_parameter_count = a2;
+  Register temp_reg = t0;
+  Ldl(temp_reg, FieldMemOperand(a1, JSFunction::kSharedFunctionInfoOffset));
+  Ldl(cp, FieldMemOperand(a1, JSFunction::kContextOffset));
+  // The argument count is stored as int32_t on 64-bit platforms.
+  // TODO(plind): Smi on 32-bit platforms.
+  Ldhu(expected_parameter_count,
+      FieldMemOperand(temp_reg,
+                      SharedFunctionInfo::kFormalParameterCountOffset));
+
+  InvokeFunctionCode(a1, new_target, expected_parameter_count,
+                     actual_parameter_count, flag);
+}
+
+void MacroAssembler::InvokeFunction(Register function,
+                                    Register expected_parameter_count,
+                                    Register actual_parameter_count,
+                                    InvokeFlag flag) {
+  // You can't call a function without a valid frame.
+  DCHECK_IMPLIES(flag == CALL_FUNCTION, has_frame());
+
+  // Contract with called JS functions requires that function is passed in a1.
+  DCHECK_EQ(function, a1);
+
+  // Get the function and setup the context.
+  Ldl(cp, FieldMemOperand(a1, JSFunction::kContextOffset));
+
+  InvokeFunctionCode(a1, no_reg, expected_parameter_count,
+                     actual_parameter_count, flag);
+}
+
+
+// ---------------------------------------------------------------------------
+// Support functions.
+
+void MacroAssembler::GetObjectType(Register object,
+                                   Register map,
+                                   Register type_reg) {SCOPEMARK_NAME(MacroAssembler::GetObjectType, this);
+  LoadMap(map, object);
+  Ldhu(type_reg, FieldMemOperand(map, Map::kInstanceTypeOffset));
+}
+
+
+// -----------------------------------------------------------------------------
+// Runtime calls.
+
+void TurboAssembler::DaddOverflow(Register dst, Register left,
+                                  const Operand& right, Register overflow) {SCOPEMARK_NAME(TurboAssembler::DaddOverflow, this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  Register right_reg = no_reg;
+  Register scratch = t11;
+  if (!right.is_reg()) {
+    li(at, Operand(right));
+    right_reg = at;
+  } else {
+    right_reg = right.rm();
+  }
+
+  DCHECK(left != scratch && right_reg != scratch && dst != scratch &&
+         overflow != scratch);
+  DCHECK(overflow != left && overflow != right_reg);
+
+  if (dst == left || dst == right_reg) {
+    addl(left, right_reg,scratch);
+    xor_ins(scratch, left, overflow);
+    xor_ins(scratch, right_reg, at);
+    and_ins(overflow, at, overflow);
+    mov(dst, scratch);
+  } else {
+    addl(left, right_reg,dst);
+    xor_ins(dst, left, overflow);
+    xor_ins(dst, right_reg, at);
+    and_ins(overflow, at, overflow);
+
+  }
+}
+
+void TurboAssembler::DsubOverflow(Register dst, Register left,
+                                  const Operand& right, Register overflow) {SCOPEMARK_NAME(TurboAssembler::DsubOverflow, this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  Register right_reg = no_reg;
+  Register scratch = t11;
+  if (!right.is_reg()) {
+    li(at, Operand(right));
+    right_reg = at;
+  } else {
+    right_reg = right.rm();
+  }
+
+  DCHECK(left != scratch && right_reg != scratch && dst != scratch &&
+         overflow != scratch);
+  DCHECK(overflow != left && overflow != right_reg);
+
+  if (dst == left || dst == right_reg) {
+    subl(left, right_reg,scratch);
+    xor_ins(left, scratch, overflow);
+    xor_ins(left, right_reg, at);
+    and_ins(overflow, at, overflow);
+    mov(dst, scratch);
+  } else {
+    subl(left, right_reg,dst);
+    xor_ins(left, dst, overflow);
+    xor_ins(left, right_reg, at);
+    and_ins(overflow, at, overflow);
+  }
+}
+
+void TurboAssembler::MulOverflow(Register dst, Register left,
+                                 const Operand& right, Register overflow) {SCOPEMARK_NAME(TurboAssembler::MulOverflow, this);
+  Register right_reg = no_reg;
+  Register scratch = t11;
+  if (!right.is_reg()) {
+    li(at, Operand(right));
+    right_reg = at;
+  } else {
+    right_reg = right.rm();
+  }
+
+  DCHECK(left != scratch && right_reg != scratch && dst != scratch &&
+         overflow != scratch);
+  DCHECK(overflow != left && overflow != right_reg);
+
+  if (dst == left || dst == right_reg) {
+    Mulw(scratch, left, right_reg);
+    Mulwh(overflow, left, right_reg);
+    mov(dst, scratch);
+  } else {
+    Mulw(dst, left, right_reg);
+    Mulwh(overflow, left, right_reg);
+  }
+
+  sral(dst, 32 ,scratch);
+  xor_ins(overflow, scratch, overflow);
+}
+
+void MacroAssembler::CallRuntime(const Runtime::Function* f, int num_arguments,
+                                 SaveFPRegsMode save_doubles) {SCOPEMARK_NAME(MacroAssembler::CallRuntime, this);
+  // All parameters are on the stack. v0 has the return value after call.
+
+  // If the expected number of arguments of the runtime function is
+  // constant, we check that the actual number of arguments match the
+  // expectation.
+  CHECK(f->nargs < 0 || f->nargs == num_arguments);
+
+  // TODO(1236192): Most runtime routines don't need the number of
+  // arguments passed in because it is constant. At some point we
+  // should remove this need and make the runtime routine entry code
+  // smarter.
+  PrepareCEntryArgs(num_arguments);
+  PrepareCEntryFunction(ExternalReference::Create(f));
+  Handle<Code> code =
+      CodeFactory::CEntry(isolate(), f->result_size, save_doubles);
+  Call(code, RelocInfo::CODE_TARGET);
+}
+
+void MacroAssembler::TailCallRuntime(Runtime::FunctionId fid) {SCOPEMARK_NAME(MacroAssembler::TailCallRuntime, this);
+  const Runtime::Function* function = Runtime::FunctionForId(fid);
+  DCHECK_EQ(1, function->result_size);
+  if (function->nargs >= 0) {
+    PrepareCEntryArgs(function->nargs);
+  }
+  JumpToExternalReference(ExternalReference::Create(fid));
+}
+
+void MacroAssembler::JumpToExternalReference(const ExternalReference& builtin,
+                                             BranchDelaySlot bd,
+                                             bool builtin_exit_frame) {SCOPEMARK_NAME(MacroAssembler::JumpToExternalReference, this);
+  PrepareCEntryFunction(builtin);
+  Handle<Code> code = CodeFactory::CEntry(isolate(), 1, kDontSaveFPRegs,
+                                          kArgvOnStack, builtin_exit_frame);
+  Jump(code, RelocInfo::CODE_TARGET, al, zero_reg, Operand(zero_reg), bd);
+}
+
+void MacroAssembler::JumpToInstructionStream(Address entry) {SCOPEMARK_NAME(MacroAssembler::JumpToExternalReference, this);
+  li(kOffHeapTrampolineRegister, Operand(entry, RelocInfo::OFF_HEAP_TARGET));
+  Jump(kOffHeapTrampolineRegister);
+}
+
+void MacroAssembler::LoadWeakValue(Register out, Register in,
+                                   Label* target_if_cleared) {SCOPEMARK_NAME(MacroAssembler::LoadWeakValue, this);
+  Branch(target_if_cleared, eq, in, Operand(kClearedWeakHeapObjectLower32));
+
+  And(out, in, Operand(~kWeakHeapObjectMask));
+}
+
+void MacroAssembler::IncrementCounter(StatsCounter* counter, int value,
+                                      Register scratch1, Register scratch2) {SCOPEMARK_NAME(MacroAssembler::IncrementCounter, this);
+  DCHECK_GT(value, 0);
+  if (FLAG_native_code_counters && counter->Enabled()) {
+    li(scratch2, ExternalReference::Create(counter));
+    Ldw(scratch1, MemOperand(scratch2));
+    Addw(scratch1, scratch1, Operand(value));
+    Stw(scratch1, MemOperand(scratch2));
+  }
+}
+
+
+void MacroAssembler::DecrementCounter(StatsCounter* counter, int value,
+                                      Register scratch1, Register scratch2) {SCOPEMARK_NAME(MacroAssembler::DecrementCounter, this);
+  DCHECK_GT(value, 0);
+  if (FLAG_native_code_counters && counter->Enabled()) {
+    li(scratch2, ExternalReference::Create(counter));
+    Ldw(scratch1, MemOperand(scratch2));
+    Subw(scratch1, scratch1, Operand(value));
+    Stw(scratch1, MemOperand(scratch2));
+  }
+}
+
+
+// -----------------------------------------------------------------------------
+// Debugging.
+
+void TurboAssembler::Trap() { halt(); }
+void TurboAssembler::DebugBreak() { halt(); }
+
+void TurboAssembler::Assert(Condition cc, AbortReason reason, Register rs,
+                            Operand rt) {SCOPEMARK_NAME(TurboAssembler::Assert, this);
+  if (emit_debug_code())
+    Check(cc, reason, rs, rt);
+}
+
+void TurboAssembler::Check(Condition cc, AbortReason reason, Register rs,
+                           Operand rt) {SCOPEMARK_NAME(TurboAssembler::Check, this);
+  Label L;
+  Branch(&L, cc, rs, rt);
+  Abort(reason);
+  // Will not return here.
+  bind(&L);
+}
+
+void TurboAssembler::Abort(AbortReason reason) {SCOPEMARK_NAME(TurboAssembler::Abort, this);
+  Label abort_start;
+  bind(&abort_start);
+#ifdef DEBUG
+  const char* msg = GetAbortReason(reason);
+  RecordComment("Abort message: ");
+  RecordComment(msg);
+#endif
+
+  // Avoid emitting call to builtin if requested.
+  if (trap_on_abort()) {
+    halt();//stop(msg);
+    return;
+  }
+
+  if (should_abort_hard()) {
+    // We don't care if we constructed a frame. Just pretend we did.
+    FrameScope assume_frame(this, StackFrame::NONE);
+    PrepareCallCFunction(0, a0);
+    li(a0, Operand(static_cast<int>(reason)));
+    CallCFunction(ExternalReference::abort_with_reason(), 1);
+    return;
+  }
+
+  Move(a0, Smi::FromInt(static_cast<int>(reason)));
+
+  // Disable stub call restrictions to always allow calls to abort.
+  if (!has_frame()) {
+    // We don't actually want to generate a pile of code for this, so just
+    // claim there is a stack frame, without generating one.
+    FrameScope scope(this, StackFrame::NONE);
+    Call(BUILTIN_CODE(isolate(), Abort), RelocInfo::CODE_TARGET);
+  } else {
+    Call(BUILTIN_CODE(isolate(), Abort), RelocInfo::CODE_TARGET);
+  }
+  // Will not return here.
+  if (is_trampoline_pool_blocked()) {
+    // If the calling code cares about the exact number of
+    // instructions generated, we insert padding here to keep the size
+    // of the Abort macro constant.
+    // Currently in debug mode with debug_code enabled the number of
+    // generated instructions is 10, so we use this as a maximum value.
+    static const int kExpectedAbortInstructions = 10;
+    int abort_instructions = InstructionsGeneratedSince(&abort_start);
+    DCHECK_LE(abort_instructions, kExpectedAbortInstructions);
+    while (abort_instructions++ < kExpectedAbortInstructions) {
+      nop();
+    }
+  }
+}
+
+void MacroAssembler::LoadMap(Register destination, Register object) {
+  Ldl(destination, FieldMemOperand(object, HeapObject::kMapOffset));
+}
+
+void MacroAssembler::LoadNativeContextSlot(int index, Register dst) {
+  LoadMap(dst, cp);
+  Ldl(dst,
+     FieldMemOperand(dst, Map::kConstructorOrBackPointerOrNativeContextOffset));
+  Ldl(dst, MemOperand(dst, Context::SlotOffset(index)));
+}
+
+void TurboAssembler::StubPrologue(StackFrame::Type type) {SCOPEMARK_NAME(TurboAssembler::StubPrologue, this);
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  li(scratch, Operand(StackFrame::TypeToMarker(type)));
+  PushCommonFrame(scratch);
+}
+
+void TurboAssembler::Prologue() {SCOPEMARK_NAME(TurboAssembler::Prologue, this);
+  PushStandardFrame(a1); }
+
+void TurboAssembler::EnterFrame(StackFrame::Type type) {SCOPEMARK_NAME(TurboAssembler::EnterFrame, this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  int stack_offset = -3 * kPointerSize;
+  const int fp_offset = 1 * kPointerSize;
+  subl(sp, (-stack_offset), sp);  //stack_offset is negtive
+  stack_offset = -stack_offset - kPointerSize;
+  Stl(ra, MemOperand(sp, stack_offset));
+  stack_offset -= kPointerSize;
+  Stl(fp, MemOperand(sp, stack_offset));
+  stack_offset -= kPointerSize;
+  li(t12, Operand(StackFrame::TypeToMarker(type)));
+  Stl(t12, MemOperand(sp, stack_offset));
+  // Adjust FP to point to saved FP.
+  DCHECK_EQ(stack_offset, 0);
+  Addl(fp, sp, Operand(fp_offset));
+}
+
+void TurboAssembler::LeaveFrame(StackFrame::Type type) {SCOPEMARK_NAME(TurboAssembler::LeaveFrame, this);
+  addl(fp, 2 * kPointerSize, sp);
+  Ldl(ra, MemOperand(fp, 1 * kPointerSize));
+  Ldl(fp, MemOperand(fp, 0 * kPointerSize));
+}
+
+void MacroAssembler::EnterExitFrame(bool save_doubles, int stack_space,
+                                    StackFrame::Type frame_type) {SCOPEMARK_NAME(MacroAssembler::EnterExitFrame, this);
+  DCHECK(frame_type == StackFrame::EXIT ||
+         frame_type == StackFrame::BUILTIN_EXIT);
+
+  // Set up the frame structure on the stack.
+  STATIC_ASSERT(2 * kPointerSize == ExitFrameConstants::kCallerSPDisplacement);
+  STATIC_ASSERT(1 * kPointerSize == ExitFrameConstants::kCallerPCOffset);
+  STATIC_ASSERT(0 * kPointerSize == ExitFrameConstants::kCallerFPOffset);
+
+  // This is how the stack will look:
+  // fp + 2 (==kCallerSPDisplacement) - old stack's end
+  // [fp + 1 (==kCallerPCOffset)] - saved old ra
+  // [fp + 0 (==kCallerFPOffset)] - saved old fp
+  // [fp - 1 StackFrame::EXIT Smi
+  // [fp - 2 (==kSPOffset)] - sp of the called function
+  // fp - (2 + stack_space + alignment) == sp == [fp - kSPOffset] - top of the
+  //   new stack (will contain saved ra)
+
+  // Save registers and reserve room for saved entry sp and code object.
+  subl(sp, 2 * kPointerSize + ExitFrameConstants::kFixedFrameSizeFromFp, sp);
+  Stl(ra, MemOperand(sp, 3 * kPointerSize));
+  Stl(fp, MemOperand(sp, 2 * kPointerSize));
+  {
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    li(scratch, Operand(StackFrame::TypeToMarker(frame_type)));
+    Stl(scratch, MemOperand(sp, 1 * kPointerSize));
+  }
+  // Set up new frame pointer.
+  addl(sp, ExitFrameConstants::kFixedFrameSizeFromFp, fp);
+
+  if (emit_debug_code()) {
+    Stl(zero_reg, MemOperand(fp, ExitFrameConstants::kSPOffset));
+  }
+
+  {
+    BlockTrampolinePoolScope block_trampoline_pool(this);
+    // Save the frame pointer and the context in top.
+    li(t11, ExternalReference::Create(IsolateAddressId::kCEntryFPAddress,
+                                     isolate()));
+    Stl(fp, MemOperand(t11));
+    li(t11,
+       ExternalReference::Create(IsolateAddressId::kContextAddress, isolate()));
+    Stl(cp, MemOperand(t11));
+  }
+
+  const int frame_alignment = MacroAssembler::ActivationFrameAlignment();
+  if (save_doubles) {
+    // The stack is already aligned to 0 modulo 8 for stores with sdc1.
+    int kNumOfSavedRegisters = FPURegister::kNumRegisters;
+    int space = kNumOfSavedRegisters * kDoubleSize;
+    Subl(sp, sp, Operand(space));
+    // Remember: we only need to save every 2nd double FPU value.
+    for (int i = 0; i < kNumOfSavedRegisters; i++) {
+      FPURegister reg = FPURegister::from_code(i);
+      Fstd(reg, MemOperand(sp, i * kDoubleSize));
+    }
+  }
+
+  // Reserve place for the return address, stack space and an optional slot
+  // (used by DirectCEntry to hold the return value if a struct is
+  // returned) and align the frame preparing for calling the runtime function.
+  DCHECK_GE(stack_space, 0);
+  Subl(sp, sp, Operand((stack_space + 2) * kPointerSize));
+  if (frame_alignment > 0) {
+    DCHECK(base::bits::IsPowerOfTwo(frame_alignment));
+    And(sp, sp, Operand(-frame_alignment));  // Align stack.
+  }
+
+  // Set the exit frame sp value to point just before the return address
+  // location.
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  addl(sp, kPointerSize, scratch);
+  Stl(scratch, MemOperand(fp, ExitFrameConstants::kSPOffset));
+}
+
+void MacroAssembler::LeaveExitFrame(bool save_doubles, Register argument_count,
+                                    bool do_return,
+                                    bool argument_count_is_length) {SCOPEMARK_NAME(MacroAssembler::LeaveExitFrame, this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  // Optionally restore all double registers.
+  if (save_doubles) {
+    // Remember: we only need to restore every 2nd double FPU value.
+    int kNumOfSavedRegisters = FPURegister::kNumRegisters;
+    Subl(t11, fp, Operand(ExitFrameConstants::kFixedFrameSizeFromFp +
+                          kNumOfSavedRegisters * kDoubleSize));
+    for (int i = 0; i < kNumOfSavedRegisters; i++) {
+      FPURegister reg = FPURegister::from_code(i);
+      Fldd(reg, MemOperand(t11, i * kDoubleSize));
+    }
+  }
+
+  // Clear top frame.
+  li(t11,
+     ExternalReference::Create(IsolateAddressId::kCEntryFPAddress, isolate()));
+  Stl(zero_reg, MemOperand(t11));
+
+  // Restore current context from top and clear it in debug mode.
+  li(t11,
+     ExternalReference::Create(IsolateAddressId::kContextAddress, isolate()));
+  Ldl(cp, MemOperand(t11));
+
+#ifdef DEBUG
+  li(t11,
+     ExternalReference::Create(IsolateAddressId::kContextAddress, isolate()));
+  Stl(a3, MemOperand(t11));
+#endif
+
+  // Pop the arguments, restore registers, and return.
+  mov(sp, fp);  // Respect ABI stack constraint.
+  Ldl(fp, MemOperand(sp, ExitFrameConstants::kCallerFPOffset));
+  Ldl(ra, MemOperand(sp, ExitFrameConstants::kCallerPCOffset));
+
+  if (argument_count.is_valid()) {
+    if (argument_count_is_length) {
+      addl(sp, argument_count,sp);
+    } else {
+      s8addl(argument_count, sp, sp);  DCHECK_EQ(kPointerSizeLog2, 3);
+    }
+  }
+
+  addl(sp, 2 * kPointerSize, sp);
+  if (do_return) {
+    // If returning, the instruction in the delay slot will be the addw below.
+    Ret();
+  }
+}
+
+int TurboAssembler::ActivationFrameAlignment() {
+#if V8_HOST_ARCH_SW64
+  // Running on the real platform. Use the alignment as mandated by the local
+  // environment.
+  // Note: This will break if we ever start generating snapshots on one Sw64
+  // platform for another Sw64 platform with a different alignment.
+  return base::OS::ActivationFrameAlignment();
+#endif  // V8_HOST_ARCH_SW64
+}
+
+
+void MacroAssembler::AssertStackIsAligned() {SCOPEMARK_NAME(MacroAssembler::AssertStackIsAligned, this);
+  if (emit_debug_code()) {
+    const int frame_alignment = ActivationFrameAlignment();
+    const int frame_alignment_mask = frame_alignment - 1;
+
+    if (frame_alignment > kPointerSize) {
+      Label alignment_as_expected;
+      DCHECK(base::bits::IsPowerOfTwo(frame_alignment));
+      {
+        UseScratchRegisterScope temps(this);
+        Register scratch = temps.Acquire();
+        and_ins(sp, frame_alignment_mask,scratch);
+        Branch(&alignment_as_expected, eq, scratch, Operand(zero_reg));
+      }
+      // Don't use Check here, as it will call Runtime_Abort re-entering here.
+      halt();//stop("Unexpected stack alignment");
+      bind(&alignment_as_expected);
+    }
+  }
+}
+
+void TurboAssembler::SmiUntag(Register dst, const MemOperand& src) {SCOPEMARK_NAME(TurboAssembler::SmiUntag, this);
+  if (SmiValuesAre32Bits()) {
+    Ldw(dst, MemOperand(src.rm(), SmiWordOffset(src.offset())));
+  } else {
+    DCHECK(SmiValuesAre31Bits());
+    Ldw(dst, src);
+    SmiUntag(dst);
+  }
+}
+
+void TurboAssembler::JumpIfSmi(Register value, Label* smi_label,
+                               Register scratch, BranchDelaySlot bd) {SCOPEMARK_NAME(TurboAssembler::JumpIfSmi, this);
+  DCHECK_EQ(0, kSmiTag);
+  and_ins(value, kSmiTagMask,scratch);
+  Branch(bd, smi_label, eq, scratch, Operand(zero_reg));
+}
+
+void MacroAssembler::JumpIfNotSmi(Register value,
+                                  Label* not_smi_label,
+                                  Register scratch,
+                                  BranchDelaySlot bd) {SCOPEMARK_NAME(MacroAssembler::JumpIfNotSmi, this);
+  DCHECK_EQ(0, kSmiTag);
+  and_ins(value, kSmiTagMask,scratch);
+  Branch(bd, not_smi_label, ne, scratch, Operand(zero_reg));
+}
+
+void MacroAssembler::AssertNotSmi(Register object) {SCOPEMARK_NAME(MacroAssembler::AssertNotSmi, this);
+  if (emit_debug_code()) {
+    STATIC_ASSERT(kSmiTag == 0);
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    and_ins(object, kSmiTagMask,scratch);
+    Check(ne, AbortReason::kOperandIsASmi, scratch, Operand(zero_reg));
+  }
+}
+
+
+void MacroAssembler::AssertSmi(Register object) {SCOPEMARK_NAME(MacroAssembler::AssertSmi, this);
+  if (emit_debug_code()) {
+    STATIC_ASSERT(kSmiTag == 0);
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    and_ins(object, kSmiTagMask,scratch);
+    Check(eq, AbortReason::kOperandIsASmi, scratch, Operand(zero_reg));
+  }
+}
+
+void MacroAssembler::AssertConstructor(Register object) {SCOPEMARK_NAME(MacroAssembler::AssertConstructor, this);
+  if (emit_debug_code()) {
+    BlockTrampolinePoolScope block_trampoline_pool(this);
+    STATIC_ASSERT(kSmiTag == 0);
+    SmiTst(object, t11);
+    Check(ne, AbortReason::kOperandIsASmiAndNotAConstructor, t11,
+          Operand(zero_reg));
+    LoadMap(t11, object);
+    Ldbu(t11, FieldMemOperand(t11, Map::kBitFieldOffset));
+    And(t11, t11, Operand(Map::Bits1::IsConstructorBit::kMask));
+    Check(ne, AbortReason::kOperandIsNotAConstructor, t11, Operand(zero_reg));
+  }
+}
+
+void MacroAssembler::AssertFunction(Register object) {SCOPEMARK_NAME(MacroAssembler::AssertFunction, this);
+  if (emit_debug_code()) {
+    BlockTrampolinePoolScope block_trampoline_pool(this);
+    STATIC_ASSERT(kSmiTag == 0);
+    SmiTst(object, t11);
+    Check(ne, AbortReason::kOperandIsASmiAndNotAFunction, t11,
+          Operand(zero_reg));
+    GetObjectType(object, t11, t11);
+    Check(eq, AbortReason::kOperandIsNotAFunction, t11,
+          Operand(JS_FUNCTION_TYPE));
+  }
+}
+
+
+void MacroAssembler::AssertBoundFunction(Register object) {SCOPEMARK_NAME(MacroAssembler::AssertBoundFunction, this);
+  if (emit_debug_code()) {
+    BlockTrampolinePoolScope block_trampoline_pool(this);
+    STATIC_ASSERT(kSmiTag == 0);
+    SmiTst(object, t11);
+    Check(ne, AbortReason::kOperandIsASmiAndNotABoundFunction, t11,
+          Operand(zero_reg));
+    GetObjectType(object, t11, t11);
+    Check(eq, AbortReason::kOperandIsNotABoundFunction, t11,
+          Operand(JS_BOUND_FUNCTION_TYPE));
+  }
+}
+
+void MacroAssembler::AssertGeneratorObject(Register object) {SCOPEMARK_NAME(MacroAssembler::AssertGeneratorObject, this);
+  if (!emit_debug_code()) return;
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  STATIC_ASSERT(kSmiTag == 0);
+  SmiTst(object, t11);
+  Check(ne, AbortReason::kOperandIsASmiAndNotAGeneratorObject, t11,
+        Operand(zero_reg));
+
+  GetObjectType(object, t11, t11);
+
+  Label done;
+
+  // Check if JSGeneratorObject
+  Branch(&done, eq, t11, Operand(JS_GENERATOR_OBJECT_TYPE));
+
+  // Check if JSAsyncFunctionObject (See MacroAssembler::CompareInstanceType)
+  Branch(&done, eq, t11, Operand(JS_ASYNC_FUNCTION_OBJECT_TYPE));
+
+  // Check if JSAsyncGeneratorObject
+  Branch(&done, eq, t11, Operand(JS_ASYNC_GENERATOR_OBJECT_TYPE));
+
+  Abort(AbortReason::kOperandIsNotAGeneratorObject);
+
+  bind(&done);
+}
+
+void MacroAssembler::AssertUndefinedOrAllocationSite(Register object,
+                                                     Register scratch) {SCOPEMARK_NAME(MacroAssembler::AssertUndefinedOrAllocationSite, this);
+  if (emit_debug_code()) {
+    Label done_checking;
+    AssertNotSmi(object);
+    LoadRoot(scratch, RootIndex::kUndefinedValue);
+    Branch(&done_checking, eq, object, Operand(scratch));
+    GetObjectType(object, scratch, scratch);
+    Assert(eq, AbortReason::kExpectedUndefinedOrCell, scratch,
+           Operand(ALLOCATION_SITE_TYPE));
+    bind(&done_checking);
+  }
+}
+
+
+void TurboAssembler::Float32Max(FPURegister dst, FPURegister src1,
+                                FPURegister src2, Label* out_of_line) {SCOPEMARK_NAME(TurboAssembler::Float32Max, this);
+  if (src1 == src2) {
+    Move_s(dst, src1);
+    return;
+  }
+
+  // Check if one of operands is NaN.
+  CompareIsNanF32(src1, src2);
+  BranchTrueF(out_of_line);
+
+  if (kArchVariant >= kSw64r3) {
+    UNREACHABLE();
+  } else {
+    Label return_left, return_right, done;
+
+    CompareF32(OLT, src1, src2);
+    BranchTrueShortF(&return_right);
+    CompareF32(OLT, src2, src1);
+    BranchTrueShortF(&return_left);
+
+    // Operands are equal, but check for +/-0.
+    {
+      BlockTrampolinePoolScope block_trampoline_pool(this);
+      fimovs(src1,t11);
+      slll(t11, 32, t11);
+      Branch(&return_left, eq, t11, Operand(zero_reg));
+      Branch(&return_right);
+    }
+
+    bind(&return_right);
+    if (src2 != dst) {
+      Move_s(dst, src2);
+    }
+    Branch(&done);
+
+    bind(&return_left);
+    if (src1 != dst) {
+      Move_s(dst, src1);
+    }
+
+    bind(&done);
+  }
+}
+
+void TurboAssembler::Float32MaxOutOfLine(FPURegister dst, FPURegister src1,
+                                         FPURegister src2) {SCOPEMARK_NAME(TurboAssembler::Float32MaxOutOfLine, this);
+  fadds(src1, src2, dst);
+}
+
+void TurboAssembler::Float32Min(FPURegister dst, FPURegister src1,
+                                FPURegister src2, Label* out_of_line) {SCOPEMARK_NAME(TurboAssembler::Float32Min, this);
+  if (src1 == src2) {
+    Move_s(dst, src1);
+    return;
+  }
+
+  // Check if one of operands is NaN.
+  CompareIsNanF32(src1, src2);
+  BranchTrueF(out_of_line);
+
+  if (kArchVariant >= kSw64r3) {
+    UNREACHABLE();
+  } else {
+    Label return_left, return_right, done;
+
+    CompareF32(OLT, src1, src2);
+    BranchTrueShortF(&return_left);
+    CompareF32(OLT, src2, src1);
+    BranchTrueShortF(&return_right);
+
+    // Left equals right => check for -0.
+    {
+      BlockTrampolinePoolScope block_trampoline_pool(this);
+      fimovs(src1,t11);
+      slll(t11, 32, t11);
+      Branch(&return_right, eq, t11, Operand(zero_reg));
+      Branch(&return_left);
+    }
+
+    bind(&return_right);
+    if (src2 != dst) {
+      Move_s(dst, src2);
+    }
+    Branch(&done);
+
+    bind(&return_left);
+    if (src1 != dst) {
+      Move_s(dst, src1);
+    }
+
+    bind(&done);
+  }
+}
+
+void TurboAssembler::Float32MinOutOfLine(FPURegister dst, FPURegister src1,
+                                         FPURegister src2) {SCOPEMARK_NAME(TurboAssembler::Float32MinOutOfLine, this);
+  fadds(src1, src2, dst);
+}
+
+void TurboAssembler::Float64Max(FPURegister dst, FPURegister src1,
+                                FPURegister src2, Label* out_of_line) {SCOPEMARK_NAME(TurboAssembler::Float64Max, this);
+  if (src1 == src2) {
+    Move_d(dst, src1);
+    return;
+  }
+
+  // Check if one of operands is NaN.
+  CompareIsNanF64(src1, src2);
+  BranchTrueF(out_of_line);
+
+  if (kArchVariant >= kSw64r3) {
+    UNREACHABLE();
+  } else {
+    Label return_left, return_right, done;
+
+    CompareF64(OLT, src1, src2);
+    BranchTrueShortF(&return_right);
+    CompareF64(OLT, src2, src1);
+    BranchTrueShortF(&return_left);
+
+    // Left equals right => check for -0.
+    {
+      BlockTrampolinePoolScope block_trampoline_pool(this);
+      fimovd(src1,t11);
+      Branch(&return_left, eq, t11, Operand(zero_reg));
+      Branch(&return_right);
+    }
+
+    bind(&return_right);
+    if (src2 != dst) {
+      Move_d(dst, src2);
+    }
+    Branch(&done);
+
+    bind(&return_left);
+    if (src1 != dst) {
+      Move_d(dst, src1);
+    }
+
+    bind(&done);
+  }
+}
+
+void TurboAssembler::Float64MaxOutOfLine(FPURegister dst, FPURegister src1,
+                                         FPURegister src2) {SCOPEMARK_NAME(TurboAssembler::Float64MaxOutOfLine, this);
+  faddd(src1, src2, dst);
+}
+
+void TurboAssembler::Float64Min(FPURegister dst, FPURegister src1,
+                                FPURegister src2, Label* out_of_line) {SCOPEMARK_NAME(TurboAssembler::Float64Min, this);
+  if (src1 == src2) {
+    Move_d(dst, src1);
+    return;
+  }
+
+  // Check if one of operands is NaN.
+  CompareIsNanF64(src1, src2);
+  BranchTrueF(out_of_line);
+
+  if (kArchVariant >= kSw64r3) {
+    UNREACHABLE();
+  } else {
+    Label return_left, return_right, done;
+
+    CompareF64(OLT, src1, src2);
+    BranchTrueShortF(&return_left);
+    CompareF64(OLT, src2, src1);
+    BranchTrueShortF(&return_right);
+
+    // Left equals right => check for -0.
+    {
+      BlockTrampolinePoolScope block_trampoline_pool(this);
+      fimovd(src1,t11);
+      Branch(&return_right, eq, t11, Operand(zero_reg));
+      Branch(&return_left);
+    }
+
+    bind(&return_right);
+    if (src2 != dst) {
+      Move_d(dst, src2);
+    }
+    Branch(&done);
+
+    bind(&return_left);
+    if (src1 != dst) {
+      Move_d(dst, src1);
+    }
+
+    bind(&done);
+  }
+}
+
+void TurboAssembler::Float64MinOutOfLine(FPURegister dst, FPURegister src1,
+                                         FPURegister src2) {SCOPEMARK_NAME(TurboAssembler::Float64MinOutOfLine, this);
+  faddd(src1, src2, dst);
+}
+
+static const int kRegisterPassedArguments = 6;
+
+int TurboAssembler::CalculateStackPassedWords(int num_reg_arguments,
+                                              int num_double_arguments) {
+  int stack_passed_words = 0;
+  num_reg_arguments += 2 * num_double_arguments;
+
+  // O32: Up to four simple arguments are passed in registers a0..a3.
+  // N64: Up to eight simple arguments are passed in registers a0..a5.
+  if (num_reg_arguments > kRegisterPassedArguments) {
+    stack_passed_words += num_reg_arguments - kRegisterPassedArguments;
+  }
+  stack_passed_words += kCArgSlotCount;
+  return stack_passed_words;
+}
+
+void TurboAssembler::PrepareCallCFunction(int num_reg_arguments,
+                                          int num_double_arguments,
+                                          Register scratch) {SCOPEMARK_NAME(TurboAssembler::PrepareCallCFunction, this);
+  int frame_alignment = ActivationFrameAlignment();
+
+  // n64: Up to eight simple arguments in a0..a3, a4..a5, No argument slots.
+  // O32: Up to four simple arguments are passed in registers a0..a3.
+  // Those four arguments must have reserved argument slots on the stack for
+  // sw64, even though those argument slots are not normally used.
+  // Both ABIs: Remaining arguments are pushed on the stack, above (higher
+  // address than) the (O32) argument slots. (arg slot calculation handled by
+  // CalculateStackPassedWords()).
+  int stack_passed_arguments = CalculateStackPassedWords(
+      num_reg_arguments, num_double_arguments);
+  if (frame_alignment > kPointerSize) {
+    // Make stack end at alignment and make room for num_arguments - 4 words
+    // and the original value of sp.
+    mov(scratch, sp);
+    Subl(sp, sp, Operand((stack_passed_arguments + 1) * kPointerSize));
+    DCHECK(base::bits::IsPowerOfTwo(frame_alignment));
+    And(sp, sp, Operand(-frame_alignment));
+    Stl(scratch, MemOperand(sp, stack_passed_arguments * kPointerSize));
+  } else {
+    Subl(sp, sp, Operand(stack_passed_arguments * kPointerSize));
+  }
+}
+
+void TurboAssembler::PrepareCallCFunction(int num_reg_arguments,
+                                          Register scratch) {SCOPEMARK_NAME(TurboAssembler::PrepareCallCFunction, this);
+  PrepareCallCFunction(num_reg_arguments, 0, scratch);
+}
+
+void TurboAssembler::CallCFunction(ExternalReference function,
+                                   int num_reg_arguments,
+                                   int num_double_arguments) {SCOPEMARK_NAME(TurboAssembler::CallCFunction, this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  li(t12, function);
+  CallCFunctionHelper(t12, num_reg_arguments, num_double_arguments);
+}
+
+void TurboAssembler::CallCFunction(Register function, int num_reg_arguments,
+                                   int num_double_arguments) {SCOPEMARK_NAME(TurboAssembler::CallCFunction, this);
+  CallCFunctionHelper(function, num_reg_arguments, num_double_arguments);
+}
+
+void TurboAssembler::CallCFunction(ExternalReference function,
+                                   int num_arguments) {SCOPEMARK_NAME(TurboAssembler::CallCFunction, this);
+  CallCFunction(function, num_arguments, 0);
+}
+
+void TurboAssembler::CallCFunction(Register function, int num_arguments) {SCOPEMARK_NAME(TurboAssembler::CallCFunction, this);
+  CallCFunction(function, num_arguments, 0);
+}
+
+void TurboAssembler::CallCFunctionHelper(Register function,
+                                         int num_reg_arguments,
+                                         int num_double_arguments) {SCOPEMARK_NAME(TurboAssembler::CallCFunctionHelper, this);
+  DCHECK_LE(num_reg_arguments + num_double_arguments, kMaxCParameters);
+  DCHECK(has_frame());
+  // Make sure that the stack is aligned before calling a C function unless
+  // running in the simulator. The simulator has its own alignment check which
+  // provides more information.
+  // The argument stots are presumed to have been set up by
+  // PrepareCallCFunction. The C function must be called via t12, for sw64 ABI.
+
+#if V8_HOST_ARCH_SW64
+  if (emit_debug_code()) {
+    int frame_alignment = ActivationFrameAlignment();
+    int frame_alignment_mask = frame_alignment - 1;
+    if (frame_alignment > kPointerSize) {
+      DCHECK(base::bits::IsPowerOfTwo(frame_alignment));
+      Label alignment_as_expected;
+      {
+        UseScratchRegisterScope temps(this);
+        Register scratch = temps.Acquire();
+        And(scratch, sp, Operand(frame_alignment_mask));
+        Branch(&alignment_as_expected, eq, scratch, Operand(zero_reg));
+      }
+      // Don't use Check here, as it will call Runtime_Abort possibly
+      // re-entering here.
+      halt();//stop("Unexpected alignment in CallCFunction");
+      bind(&alignment_as_expected);
+    }
+  }
+#endif  // V8_HOST_ARCH_SW64
+
+  // Just call directly. The function called cannot cause a GC, or
+  // allow preemption, so the return address in the link register
+  // stays correct.
+  {
+    BlockTrampolinePoolScope block_trampoline_pool(this);
+    if (function != t12) {
+      mov(t12, function);
+      function = t12;
+    }
+
+    // Save the frame pointer and PC so that the stack layout remains iterable,
+    // even without an ExitFrame which normally exists between JS and C frames.
+    // 't' registers are caller-saved so this is safe as a scratch register.
+    Register pc_scratch = t1;
+    Register scratch = t2;
+    DCHECK(!AreAliased(pc_scratch, scratch, function));
+
+#ifdef SW64  //ZHJ20210817
+    br(pc_scratch, 0);
+#else
+    mov(scratch, ra);
+    mov(pc_scratch, ra);
+    mov(ra, scratch);
+#endif
+
+    // See x64 code for reasoning about how to address the isolate data fields.
+    if (root_array_available()) {
+      Stl(pc_scratch, MemOperand(kRootRegister,
+                                IsolateData::fast_c_call_caller_pc_offset()));
+      Stl(fp, MemOperand(kRootRegister,
+                        IsolateData::fast_c_call_caller_fp_offset()));
+    } else {
+      DCHECK_NOT_NULL(isolate());
+      li(scratch, ExternalReference::fast_c_call_caller_pc_address(isolate()));
+      Stl(pc_scratch, MemOperand(scratch));
+      li(scratch, ExternalReference::fast_c_call_caller_fp_address(isolate()));
+      Stl(fp, MemOperand(scratch));
+    }
+
+    Call(function);
+
+      // We don't unset the PC; the FP is the source of truth.
+    if (root_array_available()) {
+      Stl(zero_reg, MemOperand(kRootRegister,
+                              IsolateData::fast_c_call_caller_fp_offset()));
+    } else {
+      DCHECK_NOT_NULL(isolate());
+      li(scratch, ExternalReference::fast_c_call_caller_fp_address(isolate()));
+      Stl(zero_reg, MemOperand(scratch));
+    }
+  }
+
+  int stack_passed_arguments = CalculateStackPassedWords(
+      num_reg_arguments, num_double_arguments);
+
+  if (ActivationFrameAlignment() > kPointerSize) {
+    Ldl(sp, MemOperand(sp, stack_passed_arguments * kPointerSize));
+  } else {
+    Addl(sp, sp, Operand(stack_passed_arguments * kPointerSize));
+  }
+}
+
+
+#undef BRANCH_ARGS_CHECK
+
+void TurboAssembler::CheckPageFlag(Register object, Register scratch, int mask,
+                                   Condition cc, Label* condition_met) {SCOPEMARK_NAME(TurboAssembler::CheckPageFlag, this);
+And(scratch, object, Operand(~kPageAlignmentMask));
+  Ldl(scratch, MemOperand(scratch, BasicMemoryChunk::kFlagsOffset));
+  And(scratch, scratch, Operand(mask));
+  Branch(condition_met, cc, scratch, Operand(zero_reg));
+}
+
+
+Register GetRegisterThatIsNotOneOf(Register reg1,
+                                   Register reg2,
+                                   Register reg3,
+                                   Register reg4,
+                                   Register reg5,
+                                   Register reg6) {
+  RegList regs = 0;
+  if (reg1.is_valid()) regs |= reg1.bit();
+  if (reg2.is_valid()) regs |= reg2.bit();
+  if (reg3.is_valid()) regs |= reg3.bit();
+  if (reg4.is_valid()) regs |= reg4.bit();
+  if (reg5.is_valid()) regs |= reg5.bit();
+  if (reg6.is_valid()) regs |= reg6.bit();
+
+  const RegisterConfiguration* config = RegisterConfiguration::Default();
+  for (int i = 0; i < config->num_allocatable_general_registers(); ++i) {
+    int code = config->GetAllocatableGeneralCode(i);
+    Register candidate = Register::from_code(code);
+    if (regs & candidate.bit()) continue;
+    return candidate;
+  }
+  UNREACHABLE();
+}
+
+void TurboAssembler::ComputeCodeStartAddress(Register dst) {SCOPEMARK_NAME(TurboAssembler::ComputeCodeStartAddress, this);
+  // This push on ra and the pop below together ensure that we restore the
+  // register ra, which is needed while computing the code start address.
+  push(ra);
+
+  // The bal instruction puts the address of the current instruction into
+  // the return address (ra) register, which we can use later on.
+  br(ra, 0);
+  int pc = pc_offset();
+  li(dst, Operand(pc));
+  Subl(dst, ra, dst);
+
+  pop(ra);  // Restore ra
+}
+
+void TurboAssembler::ResetSpeculationPoisonRegister() {SCOPEMARK_NAME(TurboAssembler::ResetSpeculationPoisonRegister, this);
+  li(kSpeculationPoisonRegister, -1);
+}
+
+void TurboAssembler::CallForDeoptimization(Address target, int deopt_id,
+                                           Label* exit, DeoptimizeKind kind) {
+  USE(exit, kind);
+  NoRootArrayScope no_root_array(this);
+
+  // Save the deopt id in kRootRegister (we don't need the roots array from now
+  // on).
+  DCHECK_LE(deopt_id, 0xFFFF);
+  li(kRootRegister, deopt_id);
+  Call(target, RelocInfo::RUNTIME_ENTRY);
+}
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_TARGET_ARCH_SW64
diff --git a/src/3rdparty/chromium/v8/src/codegen/sw64/macro-assembler-sw64.h b/src/3rdparty/chromium/v8/src/codegen/sw64/macro-assembler-sw64.h
new file mode 100755
index 0000000000..1abfdf29bf
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/codegen/sw64/macro-assembler-sw64.h
@@ -0,0 +1,1321 @@
+// Copyright 2012 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+//
+#ifndef INCLUDED_FROM_MACRO_ASSEMBLER_H
+#error This header must be included via macro-assembler.h
+#endif
+
+#ifndef V8_CODEGEN_SW64_MACRO_ASSEMBLER_SW64_H_
+#define V8_CODEGEN_SW64_MACRO_ASSEMBLER_SW64_H_
+
+#ifdef PRODUCT
+#define SCOPEMARK /* nothing */
+#define SCOPEMARK_NAME(name, tasm) /* nothing */
+#else
+#define SCOPEMARK \
+char line[200]; sprintf(line,"%s:%d",__FILE__, __LINE__);\
+ScopeMark scopeMark(_tasm, line);
+
+#define SCOPEMARK_NAME(name, tasm) \
+char line[200]; sprintf(line,"%s:%d",__FILE__, __LINE__);\
+ScopeMark scopeMark(tasm, line, #name); 
+
+#endif
+
+#include "src/codegen/assembler.h"
+#include "src/codegen/sw64/assembler-sw64.h"
+#include "src/common/globals.h"
+
+namespace v8 {
+namespace internal {
+
+// Forward declarations.
+enum class AbortReason : uint8_t;
+
+// Reserved Register Usage Summary.
+//
+// Registers t11, t12, and at are reserved for use by the MacroAssembler.
+//
+// The programmer should know that the MacroAssembler may clobber these three,
+// but won't touch other registers except in special cases.
+//
+// Per the SW64 ABI, register t12 must be used for indirect function call
+// via 'jalr t12' or 'jr t12' instructions. This is relied upon by gcc when
+// trying to update gp register for position-independent-code. Whenever
+// SW64 generated code calls C code, it must be via t12 register.
+
+
+// Flags used for LeaveExitFrame function.
+enum LeaveExitFrameMode {
+  EMIT_RETURN = true,
+  NO_EMIT_RETURN = false
+};
+
+// Allow programmer to use Branch Delay Slot of Branches, Jumps, Calls.
+enum BranchDelaySlot {
+  USE_DELAY_SLOT,
+  PROTECT
+};
+
+// Flags used for the li macro-assembler function.
+enum LiFlags {
+  // If the constant value can be represented in just 16 bits, then
+  // optimize the li to use a single instruction, rather than lui/ori/dsll
+  // sequence. A number of other optimizations that emits less than
+  // maximum number of instructions exists.
+  OPTIMIZE_SIZE = 0,
+  // Always use 6 instructions (lui/ori/dsll sequence) for release 2 or 4
+  // instructions for release 6 (lui/ori/dahi/dati), even if the constant
+  // could be loaded with just one, so that this value is patchable later.
+  CONSTANT_SIZE = 1,
+  // For address loads only 4 instruction are required. Used to mark
+  // constant load that will be used as address without relocation
+  // information. It ensures predictable code size, so specific sites
+  // in code are patchable.
+  ADDRESS_LOAD = 2
+};
+
+enum RememberedSetAction { EMIT_REMEMBERED_SET, OMIT_REMEMBERED_SET };
+enum SmiCheck { INLINE_SMI_CHECK, OMIT_SMI_CHECK };
+enum RAStatus { kRAHasNotBeenSaved, kRAHasBeenSaved };
+
+Register GetRegisterThatIsNotOneOf(Register reg1,
+                                   Register reg2 = no_reg,
+                                   Register reg3 = no_reg,
+                                   Register reg4 = no_reg,
+                                   Register reg5 = no_reg,
+                                   Register reg6 = no_reg);
+
+// -----------------------------------------------------------------------------
+// Static helper functions.
+
+#if defined(V8_TARGET_LITTLE_ENDIAN)
+#define SmiWordOffset(offset) (offset + kPointerSize / 2)
+#else
+#define SmiWordOffset(offset) offset
+#endif
+
+// Generate a MemOperand for loading a field from an object.
+inline MemOperand FieldMemOperand(Register object, int offset) {
+  return MemOperand(object, offset - kHeapObjectTag);
+}
+
+// Generate a MemOperand for storing arguments 7..N on the stack
+// when calling CallCFunction().
+// TODO(plind): Currently ONLY used for O32. Should be fixed for
+//              n64, and used in RegExp code, and other places
+//              with more than 8 arguments.
+inline MemOperand CFunctionArgumentOperand(int index) {
+  DCHECK_GT(index, kCArgSlotCount);
+  // Argument 5 takes the slot just past the four Arg-slots.
+  int offset = (index - 5) * kPointerSize + kCArgsSlotsSize;
+  return MemOperand(sp, offset);
+}
+
+class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
+ public:
+  using TurboAssemblerBase::TurboAssemblerBase;
+
+  // Activation support.
+  void EnterFrame(StackFrame::Type type);
+  void EnterFrame(StackFrame::Type type, bool load_constant_pool_pointer_reg) {
+    // Out-of-line constant pool not implemented on sw64.
+    UNREACHABLE();
+  }
+  void LeaveFrame(StackFrame::Type type);
+
+  // Generates function and stub prologue code.
+  void StubPrologue(StackFrame::Type type);
+  void Prologue();
+
+  void InitializeRootRegister() {
+    ExternalReference isolate_root = ExternalReference::isolate_root(isolate());
+    li(kRootRegister, Operand(isolate_root));
+  }
+
+  // Jump unconditionally to given label.
+  // We NEED a nop in the branch delay slot, as it used by v8, for example in
+  // CodeGenerator::ProcessDeferred().
+  // Currently the branch delay slot is filled by the MacroAssembler.
+  // Use rather b(Label) for code generation.
+  void jmp(Label* L) { Branch(L); }
+
+  // -------------------------------------------------------------------------
+  // Debugging.
+
+  void Trap() override;
+  void DebugBreak() override;
+
+  // Calls Abort(msg) if the condition cc is not satisfied.
+  // Use --debug_code to enable.
+  void Assert(Condition cc, AbortReason reason, Register rs, Operand rt);
+
+  // Like Assert(), but always enabled.
+  void Check(Condition cc, AbortReason reason, Register rs, Operand rt);
+
+  // Print a message to stdout and abort execution.
+  void Abort(AbortReason msg);
+
+  // Arguments macros.
+#define COND_TYPED_ARGS Condition cond, Register r1, const Operand& r2
+#define COND_ARGS cond, r1, r2
+
+  // Cases when relocation is not needed.
+#define DECLARE_NORELOC_PROTOTYPE(Name, target_type)            \
+  void Name(target_type target, BranchDelaySlot bd = PROTECT);  \
+  inline void Name(BranchDelaySlot bd, target_type target) {    \
+    Name(target, bd);                                           \
+  }                                                             \
+  void Name(target_type target,                                 \
+            COND_TYPED_ARGS,                                    \
+            BranchDelaySlot bd = PROTECT);                      \
+  inline void Name(BranchDelaySlot bd,                          \
+                   target_type target,                          \
+                   COND_TYPED_ARGS) {                           \
+    Name(target, COND_ARGS, bd);                                \
+  }
+
+#define DECLARE_BRANCH_PROTOTYPES(Name)   \
+  DECLARE_NORELOC_PROTOTYPE(Name, Label*) \
+  DECLARE_NORELOC_PROTOTYPE(Name, int32_t)
+
+  DECLARE_BRANCH_PROTOTYPES(Branch)
+  DECLARE_BRANCH_PROTOTYPES(BranchAndLink)
+  DECLARE_BRANCH_PROTOTYPES(BranchShort)
+
+#undef DECLARE_BRANCH_PROTOTYPES
+#undef COND_TYPED_ARGS
+#undef COND_ARGS
+
+  // Floating point branches
+  void CompareF32(FPUCondition cc, FPURegister cmp1, FPURegister cmp2) {
+    CompareF(S, cc, cmp1, cmp2);
+  }
+
+  void CompareIsNanF32(FPURegister cmp1, FPURegister cmp2) {
+    CompareIsNanF(S, cmp1, cmp2);
+  }
+
+  void CompareF64(FPUCondition cc, FPURegister cmp1, FPURegister cmp2) {
+    CompareF(D, cc, cmp1, cmp2);
+  }
+
+  void CompareIsNanF64(FPURegister cmp1, FPURegister cmp2) {
+    CompareIsNanF(D, cmp1, cmp2);
+  }
+
+  void BranchTrueShortF(Label* target, BranchDelaySlot bd = PROTECT);
+  void BranchFalseShortF(Label* target, BranchDelaySlot bd = PROTECT);
+
+  void BranchTrueF(Label* target, BranchDelaySlot bd = PROTECT);
+  void BranchFalseF(Label* target, BranchDelaySlot bd = PROTECT);
+
+  // MSA branches
+  void BranchMSA(Label* target, MSABranchDF df, MSABranchCondition cond,
+                 MSARegister wt, BranchDelaySlot bd = PROTECT);
+
+  void Branch(Label* L, Condition cond, Register rs, RootIndex index,
+              BranchDelaySlot bdslot = PROTECT);
+
+  static int InstrCountForLi64Bit(int64_t value);
+  inline void LiLower32BitHelper(Register rd, Operand j);
+  void li_optimized(Register rd, Operand j, LiFlags mode = OPTIMIZE_SIZE);
+  // Load int32 in the rd register.
+  void li(Register rd, Operand j, LiFlags mode = OPTIMIZE_SIZE);
+  inline void li(Register rd, int64_t j, LiFlags mode = OPTIMIZE_SIZE) {
+    li(rd, Operand(j), mode);
+  }
+  void li(Register dst, Handle<HeapObject> value, LiFlags mode = OPTIMIZE_SIZE);
+  void li(Register dst, ExternalReference value, LiFlags mode = OPTIMIZE_SIZE);
+  void li(Register dst, const StringConstantBase* string,
+          LiFlags mode = OPTIMIZE_SIZE);
+
+  void LoadFromConstantsTable(Register destination,
+                              int constant_index) override;
+  void LoadRootRegisterOffset(Register destination, intptr_t offset) override;
+  void LoadRootRelative(Register destination, int32_t offset) override;
+
+// Jump, Call, and Ret pseudo instructions implementing inter-working.
+#define COND_ARGS Condition cond = al, Register rs = zero_reg, \
+  const Operand& rt = Operand(zero_reg), BranchDelaySlot bd = PROTECT
+
+  void Jump(Register target, COND_ARGS);
+  void Jump(intptr_t target, RelocInfo::Mode rmode, COND_ARGS);
+  void Jump(Address target, RelocInfo::Mode rmode, COND_ARGS);
+  // Deffer from li, this method save target to the memory, and then load
+  // it to register use ld, it can be used in wasm jump table for concurrent
+  // patching.
+  void PatchAndJump(Address target);
+  void Jump(Handle<Code> code, RelocInfo::Mode rmode, COND_ARGS);
+  void Jump(const ExternalReference& reference) override;
+  void Call(Register target, COND_ARGS);
+  void Call(Address target, RelocInfo::Mode rmode, COND_ARGS);
+  void Call(Handle<Code> code,
+            RelocInfo::Mode rmode = RelocInfo::CODE_TARGET,
+            COND_ARGS);
+  void Call(Label* target);
+  void LoadAddress(Register dst, Label* target);
+
+  // Load the builtin given by the Smi in |builtin_index| into the same
+  // register.
+  void LoadEntryFromBuiltinIndex(Register builtin_index);
+  void CallBuiltinByIndex(Register builtin_index) override;
+
+  void LoadCodeObjectEntry(Register destination,
+                           Register code_object) override {
+    // TODO(sw64): Implement.
+    UNIMPLEMENTED();
+  }
+  void CallCodeObject(Register code_object) override {
+    // TODO(sw64): Implement.
+    UNIMPLEMENTED();
+  }
+  void JumpCodeObject(Register code_object) override {
+    // TODO(sw64): Implement.
+    UNIMPLEMENTED();
+  }
+
+  // Generates an instruction sequence s.t. the return address points to the
+  // instruction following the call.
+  // The return address on the stack is used by frame iteration.
+  void StoreReturnAddressAndCall(Register target);
+
+  void CallForDeoptimization(Address target, int deopt_id, Label* exit,
+                             DeoptimizeKind kind);
+
+  void Ret(COND_ARGS);
+  inline void Ret(BranchDelaySlot bd, Condition cond = al,
+    Register rs = zero_reg, const Operand& rt = Operand(zero_reg)) {
+    Ret(cond, rs, rt, bd);
+  }
+
+  // Emit code to discard a non-negative number of pointer-sized elements
+  // from the stack, clobbering only the sp register.
+  void Drop(int count,
+            Condition cond = cc_always,
+            Register reg = no_reg,
+            const Operand& op = Operand(no_reg));
+
+  // Trivial case of DropAndRet that utilizes the delay slot and only emits
+  // 2 instructions.
+  void DropAndRet(int drop);
+
+  void DropAndRet(int drop,
+                  Condition cond,
+                  Register reg,
+                  const Operand& op);
+
+  void Ldl(Register rd, const MemOperand& rs);
+  void Stl(Register rd, const MemOperand& rs);
+
+  void push(Register src) {
+    Addl(sp, sp, Operand(-kPointerSize));
+    Stl(src, MemOperand(sp, 0));
+  }
+  void Push(Register src) { push(src); }
+  void Push(Handle<HeapObject> handle);
+  void Push(Smi smi);
+
+  // Push two registers. Pushes leftmost register first (to highest address).
+  void Push(Register src1, Register src2) {
+    Subl(sp, sp, Operand(2 * kPointerSize));
+    Stl(src1, MemOperand(sp, 1 * kPointerSize));
+    Stl(src2, MemOperand(sp, 0 * kPointerSize));
+  }
+
+  // Push three registers. Pushes leftmost register first (to highest address).
+  void Push(Register src1, Register src2, Register src3) {
+    Subl(sp, sp, Operand(3 * kPointerSize));
+    Stl(src1, MemOperand(sp, 2 * kPointerSize));
+    Stl(src2, MemOperand(sp, 1 * kPointerSize));
+    Stl(src3, MemOperand(sp, 0 * kPointerSize));
+  }
+
+  // Push four registers. Pushes leftmost register first (to highest address).
+  void Push(Register src1, Register src2, Register src3, Register src4) {
+    Subl(sp, sp, Operand(4 * kPointerSize));
+    Stl(src1, MemOperand(sp, 3 * kPointerSize));
+    Stl(src2, MemOperand(sp, 2 * kPointerSize));
+    Stl(src3, MemOperand(sp, 1 * kPointerSize));
+    Stl(src4, MemOperand(sp, 0 * kPointerSize));
+  }
+
+  // Push five registers. Pushes leftmost register first (to highest address).
+  void Push(Register src1, Register src2, Register src3, Register src4,
+            Register src5) {
+    Subl(sp, sp, Operand(5 * kPointerSize));
+    Stl(src1, MemOperand(sp, 4 * kPointerSize));
+    Stl(src2, MemOperand(sp, 3 * kPointerSize));
+    Stl(src3, MemOperand(sp, 2 * kPointerSize));
+    Stl(src4, MemOperand(sp, 1 * kPointerSize));
+    Stl(src5, MemOperand(sp, 0 * kPointerSize));
+  }
+
+  void Push(Register src, Condition cond, Register tst1, Register tst2) {
+    // Since we don't have conditional execution we use a Branch.
+    Branch(3, cond, tst1, Operand(tst2));
+    Subl(sp, sp, Operand(kPointerSize));
+    Stl(src, MemOperand(sp, 0));
+  }
+
+  enum PushArrayOrder { kNormal, kReverse };
+  void PushArray(Register array, Register size, Register scratch,
+                 Register scratch2, PushArrayOrder order = kNormal);
+
+  void SaveRegisters(RegList registers);
+  void RestoreRegisters(RegList registers);
+
+  void CallRecordWriteStub(Register object, Register address,
+                           RememberedSetAction remembered_set_action,
+                           SaveFPRegsMode fp_mode);
+  void CallRecordWriteStub(Register object, Register address,
+                           RememberedSetAction remembered_set_action,
+                           SaveFPRegsMode fp_mode, Address wasm_target);
+  void CallEphemeronKeyBarrier(Register object, Register address,
+                               SaveFPRegsMode fp_mode);
+
+  // Push multiple registers on the stack.
+  // Registers are saved in numerical order, with higher numbered registers
+  // saved in higher memory addresses.
+  void MultiPush(RegList regs);
+  void MultiPushFPU(RegList regs);
+
+  // Calculate how much stack space (in bytes) are required to store caller
+  // registers excluding those specified in the arguments.
+  int RequiredStackSizeForCallerSaved(SaveFPRegsMode fp_mode,
+                                      Register exclusion1 = no_reg,
+                                      Register exclusion2 = no_reg,
+                                      Register exclusion3 = no_reg) const;
+
+  // Push caller saved registers on the stack, and return the number of bytes
+  // stack pointer is adjusted.
+  int PushCallerSaved(SaveFPRegsMode fp_mode, Register exclusion1 = no_reg,
+                      Register exclusion2 = no_reg,
+                      Register exclusion3 = no_reg);
+  // Restore caller saved registers from the stack, and return the number of
+  // bytes stack pointer is adjusted.
+  int PopCallerSaved(SaveFPRegsMode fp_mode, Register exclusion1 = no_reg,
+                     Register exclusion2 = no_reg,
+                     Register exclusion3 = no_reg);
+
+  void pop(Register dst) {
+    Ldl(dst, MemOperand(sp, 0));
+    Addl(sp, sp, Operand(kPointerSize));
+  }
+  void Pop(Register dst) { pop(dst); }
+
+  // Pop two registers. Pops rightmost register first (from lower address).
+  void Pop(Register src1, Register src2) {
+    DCHECK(src1 != src2);
+    Ldl(src2, MemOperand(sp, 0 * kPointerSize));
+    Ldl(src1, MemOperand(sp, 1 * kPointerSize));
+    Addl(sp, sp, 2 * kPointerSize);
+  }
+
+  // Pop three registers. Pops rightmost register first (from lower address).
+  void Pop(Register src1, Register src2, Register src3) {
+    Ldl(src3, MemOperand(sp, 0 * kPointerSize));
+    Ldl(src2, MemOperand(sp, 1 * kPointerSize));
+    Ldl(src1, MemOperand(sp, 2 * kPointerSize));
+    Addl(sp, sp, 3 * kPointerSize);
+  }
+
+  // Pop four registers. Pops rightmost register first (from lower address).
+  void Pop(Register src1, Register src2, Register src3, Register src4) {
+    Ldl(src4, MemOperand(sp, 0 * kPointerSize));
+    Ldl(src3, MemOperand(sp, 1 * kPointerSize));
+    Ldl(src2, MemOperand(sp, 2 * kPointerSize));
+    Ldl(src1, MemOperand(sp, 3 * kPointerSize));
+    Addl(sp, sp, 4 * kPointerSize);
+  }
+
+  void Pop(uint32_t count = 1) { Addl(sp, sp, Operand(count * kPointerSize)); }
+
+  // Pops multiple values from the stack and load them in the
+  // registers specified in regs. Pop order is the opposite as in MultiPush.
+  void MultiPop(RegList regs);
+  void MultiPopFPU(RegList regs);
+
+#define DEFINE_INSTRUCTION(instr)                          \
+  void instr(Register rd, Register rs, const Operand& rt); \
+  void instr(Register rd, Register rs, Register rt) {      \
+    instr(rd, rs, Operand(rt));                            \
+  }                                                        \
+  void instr(Register rs, Register rt, int32_t j) { instr(rs, rt, Operand(j)); }
+
+#define DEFINE_INSTRUCTION2(instr)                                 \
+  void instr(Register rs, const Operand& rt);                      \
+  void instr(Register rs, Register rt) { instr(rs, Operand(rt)); } \
+  void instr(Register rs, int32_t j) { instr(rs, Operand(j)); }
+
+  DEFINE_INSTRUCTION(Addw)
+  DEFINE_INSTRUCTION(Addl)
+  DEFINE_INSTRUCTION(Divw)
+  DEFINE_INSTRUCTION(Divwu)
+  DEFINE_INSTRUCTION(Divl)
+  DEFINE_INSTRUCTION(Divlu)
+  DEFINE_INSTRUCTION(Modw)
+  DEFINE_INSTRUCTION(Modwu)
+  DEFINE_INSTRUCTION(Modl)
+  DEFINE_INSTRUCTION(Modlu)
+  DEFINE_INSTRUCTION(Subw)
+  DEFINE_INSTRUCTION(Subl)
+  DEFINE_INSTRUCTION(Dmodu)
+  DEFINE_INSTRUCTION(Mulw)
+  DEFINE_INSTRUCTION(Mulwh)
+  DEFINE_INSTRUCTION(Mulhu)
+  DEFINE_INSTRUCTION(Mull)
+  DEFINE_INSTRUCTION(Dmulh)
+
+  DEFINE_INSTRUCTION(Sllw)
+  DEFINE_INSTRUCTION(Srlw)
+  DEFINE_INSTRUCTION(Sraw)
+
+  DEFINE_INSTRUCTION(And)
+  DEFINE_INSTRUCTION(Or)
+  DEFINE_INSTRUCTION(Xor)
+  DEFINE_INSTRUCTION(Nor)
+  DEFINE_INSTRUCTION2(Neg)
+
+  DEFINE_INSTRUCTION(Cmplt)
+  DEFINE_INSTRUCTION(Cmpult)
+  DEFINE_INSTRUCTION(Cmple)
+  DEFINE_INSTRUCTION(Cmpule)
+  DEFINE_INSTRUCTION(Cmpgt)
+  DEFINE_INSTRUCTION(Cmpugt)
+  DEFINE_INSTRUCTION(Cmpge)
+  DEFINE_INSTRUCTION(Cmpuge)
+
+  DEFINE_INSTRUCTION(Ror)
+  DEFINE_INSTRUCTION(Dror)
+
+#undef DEFINE_INSTRUCTION
+#undef DEFINE_INSTRUCTION2
+#undef DEFINE_INSTRUCTION3
+
+  void SmiUntag(Register dst, const MemOperand& src);
+  void SmiUntag(Register dst, Register src) {
+    if (SmiValuesAre32Bits()) {
+      sral(src, kSmiShift ,dst);
+    } else {
+      DCHECK(SmiValuesAre31Bits());
+      Sraw(dst, src, kSmiShift);
+    }
+  }
+
+  void SmiUntag(Register reg) { SmiUntag(reg, reg); }
+
+  // Removes current frame and its arguments from the stack preserving
+  // the arguments and a return address pushed to the stack for the next call.
+  // Both |callee_args_count| and |caller_args_count_reg| do not include
+  // receiver. |callee_args_count| is not modified, |caller_args_count_reg|
+  // is trashed.
+  void PrepareForTailCall(Register callee_args_count,
+                          Register caller_args_count, Register scratch0,
+                          Register scratch1);
+
+  int CalculateStackPassedWords(int num_reg_arguments,
+                                int num_double_arguments);
+
+  // Before calling a C-function from generated code, align arguments on stack
+  // and add space for the four sw64 argument slots.
+  // After aligning the frame, non-register arguments must be stored on the
+  // stack, after the argument-slots using helper: CFunctionArgumentOperand().
+  // The argument count assumes all arguments are word sized.
+  // Some compilers/platforms require the stack to be aligned when calling
+  // C++ code.
+  // Needs a scratch register to do some arithmetic. This register will be
+  // trashed.
+  void PrepareCallCFunction(int num_reg_arguments, int num_double_registers,
+                            Register scratch);
+  void PrepareCallCFunction(int num_reg_arguments, Register scratch);
+
+  // Calls a C function and cleans up the space for arguments allocated
+  // by PrepareCallCFunction. The called function is not allowed to trigger a
+  // garbage collection, since that might move the code and invalidate the
+  // return address (unless this is somehow accounted for by the called
+  // function).
+  void CallCFunction(ExternalReference function, int num_arguments);
+  void CallCFunction(Register function, int num_arguments);
+  void CallCFunction(ExternalReference function, int num_reg_arguments,
+                     int num_double_arguments);
+  void CallCFunction(Register function, int num_reg_arguments,
+                     int num_double_arguments);
+  void MovFromFloatResult(DoubleRegister fdst);
+  void MovFromFloatParameter(DoubleRegister fdst);
+
+  // There are two ways of passing double arguments on SW64, depending on
+  // whether soft or hard floating point ABI is used. These functions
+  // abstract parameter passing for the three different ways we call
+  // C functions from generated code.
+  void MovToFloatParameter(DoubleRegister fsrc);
+  void MovToFloatParameters(DoubleRegister fsrc0, DoubleRegister fsrc1);
+  void MovToFloatResult(DoubleRegister fsrc);
+
+#ifdef SW64
+  void MovFromGeneralResult(Register dst);
+  void MovFromGeneralParameter(Register dst);
+  void MovToGeneralParameter(Register src);
+  void MovToGeneralParameters(Register src0, Register src1);
+  void MovToGeneralResult(Register src);
+#endif
+
+  // See comments at the beginning of Builtins::Generate_CEntry.
+  inline void PrepareCEntryArgs(int num_args) { li(a0, num_args); }
+  inline void PrepareCEntryFunction(const ExternalReference& ref) {
+    li(a1, ref);
+  }
+
+  void CheckPageFlag(Register object, Register scratch, int mask, Condition cc,
+                     Label* condition_met);
+#undef COND_ARGS
+
+  // Performs a truncating conversion of a floating point number as used by
+  // the JS bitwise operations. See ECMA-262 9.5: ToInt32.
+  // Exits with 'result' holding the answer.
+  void TruncateDoubleToI(Isolate* isolate, Zone* zone, Register result,
+                         DoubleRegister double_input, StubCallMode stub_mode);
+
+  // Conditional move.
+  void Seleq(Register rd, Register rs, Register rt);
+  void Selne(Register rd, Register rs, Register rt);
+
+  void LoadZeroIfFPUCondition(Register dest);
+  void LoadZeroIfNotFPUCondition(Register dest);
+
+  void LoadZeroIfConditionNotZero(Register dest, Register condition);
+  void LoadZeroIfConditionZero(Register dest, Register condition);
+  void LoadZeroOnCondition(Register rd, Register rs, const Operand& rt,
+                           Condition cond);
+
+  void Clz(Register rd, Register rs);
+  void Dclz(Register rd, Register rs);
+  void Ctz(Register rd, Register rs);
+  void Dctz(Register rd, Register rs);
+  void Popcnt(Register rd, Register rs);
+  void Dpopcnt(Register rd, Register rs);
+
+  // SW64 R2 instruction macro.
+  void Ext(Register rt, Register rs, uint16_t pos, uint16_t size);
+  void Dext(Register rt, Register rs, uint16_t pos, uint16_t size);
+  void Ins(Register rt, Register rs, uint16_t pos, uint16_t size);
+  void Dins(Register rt, Register rs, uint16_t pos, uint16_t size);
+  void ExtractBits(Register dest, Register source, Register pos, int size,
+                   bool sign_extend = false);
+  void InsertBits(Register dest, Register source, Register pos, int size);
+  void Fnegs(FPURegister fd, FPURegister fs);
+  void Fnegd(FPURegister fd, FPURegister fs);
+
+  // SW64 R6 instruction macros.
+  void Bovc(Register rt, Register rs, Label* L);
+  void Bnvc(Register rt, Register rs, Label* L);
+
+  // Convert single to unsigned word.
+  void Trunc_uw_s(FPURegister fd, FPURegister fs, FPURegister scratch);
+  void Trunc_uw_s(Register rd, FPURegister fs, FPURegister scratch);
+
+  // Change endianness
+  void ByteSwapSigned(Register dest, Register src, int operand_size);
+  void ByteSwapUnsigned(Register dest, Register src, int operand_size);
+
+  void Uldh(Register rd, const MemOperand& rs);
+  void Uldhu(Register rd, const MemOperand& rs);
+  void Usth(Register rd, const MemOperand& rs, Register scratch);
+
+  void Uldw(Register rd, const MemOperand& rs);
+  void Uldwu(Register rd, const MemOperand& rs);
+  void Ustw(Register rd, const MemOperand& rs);
+
+  void Uldl(Register rd, const MemOperand& rs);
+  void Ustl(Register rd, const MemOperand& rs);
+
+  void Uflds(FPURegister fd, const MemOperand& rs, Register scratch);
+  void Ufsts(FPURegister fd, const MemOperand& rs, Register scratch);
+
+  void Ufldd(FPURegister fd, const MemOperand& rs, Register scratch);
+  void Ufstd(FPURegister fd, const MemOperand& rs, Register scratch);
+
+  void Ldb(Register rd, const MemOperand& rs);
+  void Ldbu(Register rd, const MemOperand& rs);
+  void Stb(Register rd, const MemOperand& rs);
+
+  void Ldh(Register rd, const MemOperand& rs);
+  void Ldhu(Register rd, const MemOperand& rs);
+  void Sth(Register rd, const MemOperand& rs);
+
+  void Ldw(Register rd, const MemOperand& rs);
+  void Ldwu(Register rd, const MemOperand& rs);
+  void Stw(Register rd, const MemOperand& rs);
+
+  void Flds(FPURegister fd, const MemOperand& src);
+  void Fsts(FPURegister fs, const MemOperand& dst);
+
+  void Fldd(FPURegister fd, const MemOperand& src);
+  void Fstd(FPURegister fs, const MemOperand& dst);
+
+  void Ll(Register rd, const MemOperand& rs);
+  void Sc(Register rd, const MemOperand& rs);
+
+  void Lld(Register rd, const MemOperand& rs);
+  void Scd(Register rd, const MemOperand& rs);
+  void Abs_sw(FPURegister fd, FPURegister fs);
+
+  // Perform a floating-point min or max operation with the
+  // (IEEE-754-compatible) semantics of SW32's Release 6 MIN.fmt/MAX.fmt.
+  // Some cases, typically NaNs or +/-0.0, are expected to be rare and are
+  // handled in out-of-line code. The specific behaviour depends on supported
+  // instructions.
+  //
+  // These functions assume (and assert) that src1!=src2. It is permitted
+  // for the result to alias either input register.
+  void Float32Max(FPURegister dst, FPURegister src1, FPURegister src2,
+                  Label* out_of_line);
+  void Float32Min(FPURegister dst, FPURegister src1, FPURegister src2,
+                  Label* out_of_line);
+  void Float64Max(FPURegister dst, FPURegister src1, FPURegister src2,
+                  Label* out_of_line);
+  void Float64Min(FPURegister dst, FPURegister src1, FPURegister src2,
+                  Label* out_of_line);
+
+  // Generate out-of-line cases for the macros above.
+  void Float32MaxOutOfLine(FPURegister dst, FPURegister src1, FPURegister src2);
+  void Float32MinOutOfLine(FPURegister dst, FPURegister src1, FPURegister src2);
+  void Float64MaxOutOfLine(FPURegister dst, FPURegister src1, FPURegister src2);
+  void Float64MinOutOfLine(FPURegister dst, FPURegister src1, FPURegister src2);
+
+  bool IsDoubleZeroRegSet() { return has_double_zero_reg_set_; }
+
+  void mov(Register rd, Register rt) { if(rt != rd) bis(zero_reg, rt, rd); }
+
+  inline void Move(Register dst, Handle<HeapObject> handle) { li(dst, handle); }
+  inline void Move(Register dst, Smi smi) { li(dst, Operand(smi)); }
+
+  inline void Move(Register dst, Register src) {
+    if (dst != src) {
+      mov(dst, src);
+    }
+  }
+
+  inline void Move(FPURegister dst, FPURegister src) { Move_d(dst, src); }
+
+  inline void Move(Register dst_low, Register dst_high, FPURegister src) {
+    fimovd(src, dst_low);
+    sral(dst_low, 32, dst_high);
+    addw(dst_low, 0, dst_low);
+  }
+
+  inline void Move(Register dst, FPURegister src) { fimovd(src,dst); }
+
+  inline void Move(FPURegister dst, Register src) { ifmovd(src, dst); }
+
+  inline void FmoveHigh(Register dst_high, FPURegister src) {
+    fimovd(src, dst_high);
+    sral(dst_high, 32, dst_high);
+  }
+
+  inline void FmoveHigh(FPURegister dst, Register src_high) {
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    Register scratch1 = t8;
+    DCHECK(src_high != scratch && src_high != scratch1);
+    fimovd(dst, scratch); 
+    zapnot(scratch, 0xf, scratch);
+    slll(src_high, 32, scratch1);
+    or_ins(scratch, scratch1, scratch);
+    ifmovd(scratch, dst);
+  }
+
+  inline void FmoveLow(Register dst_low, FPURegister src) {
+    fimovd(src, dst_low);
+    addw(dst_low, 0, dst_low);
+  }
+
+  void FmoveLow(FPURegister dst, Register src_low);
+
+  inline void Move(FPURegister dst, Register src_low, Register src_high) {
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    DCHECK(src_high != scratch && src_low != scratch);
+    slll(src_high, 32, scratch);
+    or_ins(scratch, src_low, scratch);
+    ifmovd(scratch, dst);
+  }
+
+  inline void Move_d(FPURegister dst, FPURegister src) {
+    if (dst != src) {
+      fmovd(src, dst);
+    }
+  }
+
+  inline void Move_s(FPURegister dst, FPURegister src) {
+    if (dst != src) {
+      fmovs(src, dst);
+    }
+  }
+
+  void Move(FPURegister dst, float imm) { Move(dst, bit_cast<uint32_t>(imm)); }
+  void Move(FPURegister dst, double imm) { Move(dst, bit_cast<uint64_t>(imm)); }
+  void Move(FPURegister dst, uint32_t src);
+  void Move(FPURegister dst, uint64_t src);
+
+  // DaddOverflow sets overflow register to a negative value if
+  // overflow occured, otherwise it is zero or positive
+  void DaddOverflow(Register dst, Register left, const Operand& right,
+                    Register overflow);
+  // DsubOverflow sets overflow register to a negative value if
+  // overflow occured, otherwise it is zero or positive
+  void DsubOverflow(Register dst, Register left, const Operand& right,
+                    Register overflow);
+  // MulOverflow sets overflow register to zero if no overflow occured
+  void MulOverflow(Register dst, Register left, const Operand& right,
+                   Register overflow);
+
+// Number of instructions needed for calculation of switch table entry address
+  static const int kSwitchTablePrologueSize = 5;  // 4 + 1 (ALIGN may 1 nop)
+
+  // GetLabelFunction must be lambda '[](size_t index) -> Label*' or a
+  // functor/function with 'Label *func(size_t index)' declaration.
+  template <typename Func>
+  void GenerateSwitchTable(Register index, size_t case_count,
+                           Func GetLabelFunction);
+
+  // Load an object from the root table.
+  void LoadRoot(Register destination, RootIndex index) override;
+  void LoadRoot(Register destination, RootIndex index, Condition cond,
+                Register src1, const Operand& src2);
+
+  // If the value is a NaN, canonicalize the value else, do nothing.
+  void FPUCanonicalizeNaN(const DoubleRegister dst, const DoubleRegister src);
+
+  // ---------------------------------------------------------------------------
+  // FPU macros. These do not handle special cases like NaN or +- inf.
+
+  // Convert unsigned word to double.
+  void Cvt_d_uw(FPURegister fd, FPURegister fs);
+  void Cvt_d_uw(FPURegister fd, Register rs);
+
+  // Convert unsigned long to double.
+  void Cvt_d_ul(FPURegister fd, FPURegister fs);
+  void Cvt_d_ul(FPURegister fd, Register rs);
+
+  // Convert unsigned word to float.
+  void Cvt_s_uw(FPURegister fd, FPURegister fs);
+  void Cvt_s_uw(FPURegister fd, Register rs);
+
+  // Convert unsigned long to float.
+  void Cvt_s_ul(FPURegister fd, FPURegister fs);
+  void Cvt_s_ul(FPURegister fd, Register rs);
+
+  // Convert double to unsigned word.
+  void Trunc_uw_d(FPURegister fd, FPURegister fs, FPURegister scratch);
+  void Trunc_uw_d(Register rd, FPURegister fs, FPURegister scratch);
+
+  // Convert double to unsigned long.
+  void Trunc_ul_d(FPURegister fd, FPURegister fs, FPURegister scratch,
+                  Register result = no_reg);
+  void Trunc_ul_d(Register rd, FPURegister fs, FPURegister scratch,
+                  Register result = no_reg);
+
+  // Convert single to unsigned long.
+  void Trunc_ul_s(FPURegister fd, FPURegister fs, FPURegister scratch,
+                  Register result = no_reg);
+  void Trunc_ul_s(Register rd, FPURegister fs, FPURegister scratch,
+                  Register result = no_reg);
+
+  // Round double functions
+  void Trunc_d_d(FPURegister fd, FPURegister fs);
+  void Round_d_d(FPURegister fd, FPURegister fs);
+  void Floor_d_d(FPURegister fd, FPURegister fs);
+  void Ceil_d_d(FPURegister fd, FPURegister fs);
+
+  // Round float functions
+  void Trunc_s_s(FPURegister fd, FPURegister fs);
+  void Round_s_s(FPURegister fd, FPURegister fs);
+  void Floor_s_s(FPURegister fd, FPURegister fs);
+  void Ceil_s_s(FPURegister fd, FPURegister fs);
+
+  void MSARoundW(MSARegister dst, MSARegister src, FPURoundingMode mode);
+  void MSARoundD(MSARegister dst, MSARegister src, FPURoundingMode mode);
+
+  // Jump the register contains a smi.
+  void JumpIfSmi(Register value, Label* smi_label, Register scratch = at,
+                 BranchDelaySlot bd = PROTECT);
+
+  void JumpIfEqual(Register a, int32_t b, Label* dest) {
+    li(kScratchReg, Operand(b));
+    Branch(dest, eq, a, Operand(kScratchReg));
+  }
+
+  void JumpIfLessThan(Register a, int32_t b, Label* dest) {
+    li(kScratchReg, Operand(b));
+    Branch(dest, lt, a, Operand(kScratchReg));
+  }
+
+  // Push a standard frame, consisting of ra, fp, context and JS function.
+  void PushStandardFrame(Register function_reg);
+
+  // Get the actual activation frame alignment for target environment.
+  static int ActivationFrameAlignment();
+
+  // Load Scaled Address instructions. Parameter sa (shift argument) must be
+  // between [1, 31] (inclusive). On pre-r6 architectures the scratch register
+  // may be clobbered.
+  void Lsa(Register rd, Register rs, Register rt, uint8_t sa,
+           Register scratch = at);
+  void Dlsa(Register rd, Register rs, Register rt, uint8_t sa,
+            Register scratch = at);
+
+  // Compute the start of the generated instruction stream from the current PC.
+  // This is an alternative to embedding the {CodeObject} handle as a reference.
+  void ComputeCodeStartAddress(Register dst);
+
+  void ResetSpeculationPoisonRegister();
+
+  // Control-flow integrity:
+
+  // Define a function entrypoint. This doesn't emit any code for this
+  // architecture, as control-flow integrity is not supported for it.
+  void CodeEntry() {}
+  // Define an exception handler.
+  void ExceptionHandler() {}
+  // Define an exception handler and bind a label.
+  void BindExceptionHandler(Label* label) { bind(label); }
+
+ protected:
+  inline Register GetRtAsRegisterHelper(const Operand& rt, Register scratch);
+  inline int32_t GetOffset(int32_t offset, Label* L, OffsetSize bits);
+
+  //20181127 Macro assembler to emit code.
+  MacroAssembler* masm() const { return masm_; }
+
+ private:
+  bool has_double_zero_reg_set_ = false;
+
+  // Performs a truncating conversion of a floating point number as used by
+  // the JS bitwise operations. See ECMA-262 9.5: ToInt32. Goes to 'done' if it
+  // succeeds, otherwise falls through if result is saturated. On return
+  // 'result' either holds answer, or is clobbered on fall through.
+  void TryInlineTruncateDoubleToI(Register result, DoubleRegister input,
+                                  Label* done);
+
+  void CompareF(SecondaryField sizeField, FPUCondition cc, FPURegister cmp1,
+                FPURegister cmp2);
+
+  void CompareIsNanF(SecondaryField sizeField, FPURegister cmp1,
+                     FPURegister cmp2);
+
+  void BranchShortMSA(MSABranchDF df, Label* target, MSABranchCondition cond,
+                      MSARegister wt, BranchDelaySlot bd = PROTECT);
+
+  void CallCFunctionHelper(Register function, int num_reg_arguments,
+                           int num_double_arguments);
+
+  // TODO(sw64) Reorder parameters so out parameters come last.
+  bool CalculateOffset(Label* L, int32_t* offset, OffsetSize bits);
+  bool CalculateOffset(Label* L, int32_t* offset, OffsetSize bits,
+                       Register* scratch, const Operand& rt);
+
+  void BranchShortHelperR6(int32_t offset, Label* L);
+  void BranchShortHelper(int32_t offset, Label* L, BranchDelaySlot bdslot);
+  bool BranchShortHelperR6(int32_t offset, Label* L, Condition cond,
+                           Register rs, const Operand& rt);
+  bool BranchShortHelper(int32_t offset, Label* L, Condition cond, Register rs,
+                         const Operand& rt, BranchDelaySlot bdslot);
+  bool BranchShortCheck(int32_t offset, Label* L, Condition cond, Register rs,
+                        const Operand& rt, BranchDelaySlot bdslot);
+
+  void BranchAndLinkShortHelperR6(int32_t offset, Label* L);
+  void BranchAndLinkShortHelper(int32_t offset, Label* L,
+                                BranchDelaySlot bdslot);
+  void BranchAndLinkShort(int32_t offset, BranchDelaySlot bdslot = PROTECT);
+  void BranchAndLinkShort(Label* L, BranchDelaySlot bdslot = PROTECT);
+  bool BranchAndLinkShortHelperR6(int32_t offset, Label* L, Condition cond,
+                                  Register rs, const Operand& rt);
+  bool BranchAndLinkShortHelper(int32_t offset, Label* L, Condition cond,
+                                Register rs, const Operand& rt,
+                                BranchDelaySlot bdslot);
+  bool BranchAndLinkShortCheck(int32_t offset, Label* L, Condition cond,
+                               Register rs, const Operand& rt,
+                               BranchDelaySlot bdslot);
+  void BranchLong(Label* L, BranchDelaySlot bdslot);
+  void BranchAndLinkLong(Label* L, BranchDelaySlot bdslot);
+
+  template <typename RoundFunc>
+  void RoundDouble(FPURegister dst, FPURegister src, FPURoundingMode mode,
+                   RoundFunc round);
+
+  template <typename RoundFunc>
+  void RoundFloat(FPURegister dst, FPURegister src, FPURoundingMode mode,
+                  RoundFunc round);
+
+  // Push a fixed frame, consisting of ra, fp.
+  void PushCommonFrame(Register marker_reg = no_reg);
+
+  void CallRecordWriteStub(Register object, Register address,
+                           RememberedSetAction remembered_set_action,
+                           SaveFPRegsMode fp_mode, Handle<Code> code_target,
+                           Address wasm_target);
+
+  //20181127
+  MacroAssembler* masm_;
+};
+
+class ScopeMark {
+ private:
+  TurboAssembler* _tasm;
+  char _begin[50];
+  char _end[300];
+
+ public:
+
+  ScopeMark(TurboAssembler* tasm, const char* position, const char* comment) : _tasm(tasm) {
+    ::sprintf(_begin, "{ %s", comment);
+    ::sprintf(_end, "} %s", position);
+
+    _tasm->RecordComment(_begin);
+  }
+
+  ~ScopeMark() {
+    _tasm->RecordComment(_end);
+  }
+};
+
+// MacroAssembler implements a collection of frequently used macros.
+class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
+ public:
+  using TurboAssembler::TurboAssembler;
+
+  // It assumes that the arguments are located below the stack pointer.
+  // argc is the number of arguments not including the receiver.
+  // TODO(victorgomes): Remove this function once we stick with the reversed
+  // arguments order.
+  void LoadReceiver(Register dest, Register argc) {
+    Ldl(dest, MemOperand(sp, 0));
+  }
+
+  void StoreReceiver(Register rec, Register argc, Register scratch) {
+    Stl(rec, MemOperand(sp, 0));
+  }
+
+  bool IsNear(Label* L, Condition cond, int rs_reg);
+
+  // Swap two registers.  If the scratch register is omitted then a slightly
+  // less efficient form using xor instead of mov is emitted.
+  void Swap(Register reg1, Register reg2, Register scratch = no_reg);
+
+  void PushRoot(RootIndex index) {
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    LoadRoot(scratch, index);
+    Push(scratch);
+  }
+
+  // Compare the object in a register to a value and jump if they are equal.
+  void JumpIfRoot(Register with, RootIndex index, Label* if_equal) {
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    LoadRoot(scratch, index);
+    Branch(if_equal, eq, with, Operand(scratch));
+  }
+
+  // Compare the object in a register to a value and jump if they are not equal.
+  void JumpIfNotRoot(Register with, RootIndex index, Label* if_not_equal) {
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    LoadRoot(scratch, index);
+    Branch(if_not_equal, ne, with, Operand(scratch));
+  }
+
+  // Checks if value is in range [lower_limit, higher_limit] using a single
+  // comparison.
+  void JumpIfIsInRange(Register value, unsigned lower_limit,
+                       unsigned higher_limit, Label* on_in_range);
+
+  // ---------------------------------------------------------------------------
+  // GC Support
+
+  // Notify the garbage collector that we wrote a pointer into an object.
+  // |object| is the object being stored into, |value| is the object being
+  // stored.  value and scratch registers are clobbered by the operation.
+  // The offset is the offset from the start of the object, not the offset from
+  // the tagged HeapObject pointer.  For use with FieldOperand(reg, off).
+  void RecordWriteField(
+      Register object, int offset, Register value, Register scratch,
+      RAStatus ra_status, SaveFPRegsMode save_fp,
+      RememberedSetAction remembered_set_action = EMIT_REMEMBERED_SET,
+      SmiCheck smi_check = INLINE_SMI_CHECK);
+
+  // For a given |object| notify the garbage collector that the slot |address|
+  // has been written.  |value| is the object being stored. The value and
+  // address registers are clobbered by the operation.
+  void RecordWrite(
+      Register object, Register address, Register value, RAStatus ra_status,
+      SaveFPRegsMode save_fp,
+      RememberedSetAction remembered_set_action = EMIT_REMEMBERED_SET,
+      SmiCheck smi_check = INLINE_SMI_CHECK);
+
+
+  // Convert double to unsigned long.
+  void Trunc_l_ud(FPURegister fd, FPURegister fs, FPURegister scratch);
+
+  void Trunc_l_d(FPURegister fd, FPURegister fs);
+  void Round_l_d(FPURegister fd, FPURegister fs);
+  void Floor_l_d(FPURegister fd, FPURegister fs);
+  void Ceil_l_d(FPURegister fd, FPURegister fs);
+
+  void Trunc_w_d(FPURegister fd, FPURegister fs);
+  void Round_w_d(FPURegister fd, FPURegister fs);
+  void Floor_w_d(FPURegister fd, FPURegister fs);
+  void Ceil_w_d(FPURegister fd, FPURegister fs);
+
+  void Madd_s(FPURegister fd, FPURegister fr, FPURegister fs, FPURegister ft,
+              FPURegister scratch);
+  void Madd_d(FPURegister fd, FPURegister fr, FPURegister fs, FPURegister ft,
+              FPURegister scratch);
+  void Msub_s(FPURegister fd, FPURegister fr, FPURegister fs, FPURegister ft,
+              FPURegister scratch);
+  void Msub_d(FPURegister fd, FPURegister fr, FPURegister fs, FPURegister ft,
+              FPURegister scratch);
+
+  void BranchShortMSA(MSABranchDF df, Label* target, MSABranchCondition cond,
+                      MSARegister wt, BranchDelaySlot bd = PROTECT);
+
+  // Truncates a double using a specific rounding mode, and writes the value
+  // to the result register.
+  // The except_flag will contain any exceptions caused by the instruction.
+  // If check_inexact is kDontCheckForInexactConversion, then the inexact
+  // exception is masked.
+  void EmitFPUTruncate(
+      FPURoundingMode rounding_mode, Register result,
+      DoubleRegister double_input, Register scratch,
+      DoubleRegister double_scratch, Register except_flag,
+      CheckForInexactConversion check_inexact = kDontCheckForInexactConversion);
+
+  // Enter exit frame.
+  // argc - argument count to be dropped by LeaveExitFrame.
+  // save_doubles - saves FPU registers on stack, currently disabled.
+  // stack_space - extra stack space.
+  void EnterExitFrame(bool save_doubles, int stack_space = 0,
+                      StackFrame::Type frame_type = StackFrame::EXIT);
+
+  // Leave the current exit frame.
+  void LeaveExitFrame(bool save_doubles, Register arg_count,
+                      bool do_return = NO_EMIT_RETURN,
+                      bool argument_count_is_length = false);
+
+  void LoadMap(Register destination, Register object);
+
+  // Make sure the stack is aligned. Only emits code in debug mode.
+  void AssertStackIsAligned();
+
+  // Load the global proxy from the current context.
+  void LoadGlobalProxy(Register dst) {
+    LoadNativeContextSlot(Context::GLOBAL_PROXY_INDEX, dst);
+  }
+
+  void LoadNativeContextSlot(int index, Register dst);
+
+  // Load the initial map from the global function. The registers
+  // function and map can be the same, function is then overwritten.
+  void LoadGlobalFunctionInitialMap(Register function,
+                                    Register map,
+                                    Register scratch);
+
+  // -------------------------------------------------------------------------
+  // JavaScript invokes.
+
+  // Invoke the JavaScript function code by either calling or jumping.
+  void InvokeFunctionCode(Register function, Register new_target,
+                          Register expected_parameter_count,
+                          Register actual_parameter_count, InvokeFlag flag);
+
+  // On function call, call into the debugger if necessary.
+  void CheckDebugHook(Register fun, Register new_target,
+                      Register expected_parameter_count,
+                      Register actual_parameter_count);
+
+  // Invoke the JavaScript function in the given register. Changes the
+  // current context to the context in the function before invoking.
+  void InvokeFunctionWithNewTarget(Register function, Register new_target,
+                                   Register actual_parameter_count,
+                                   InvokeFlag flag);
+  void InvokeFunction(Register function, Register expected_parameter_count,
+                      Register actual_parameter_count, InvokeFlag flag);
+
+  // Frame restart support.
+  void MaybeDropFrames();
+
+  // Exception handling.
+
+  // Push a new stack handler and link into stack handler chain.
+  void PushStackHandler();
+
+  // Unlink the stack handler on top of the stack from the stack handler chain.
+  // Must preserve the result register.
+  void PopStackHandler();
+
+  // -------------------------------------------------------------------------
+  // Support functions.
+
+  void GetObjectType(Register function,
+                     Register map,
+                     Register type_reg);
+
+  // -------------------------------------------------------------------------
+  // Runtime calls.
+
+  // Call a runtime routine.
+  void CallRuntime(const Runtime::Function* f, int num_arguments,
+                   SaveFPRegsMode save_doubles = kDontSaveFPRegs);
+
+  // Convenience function: Same as above, but takes the fid instead.
+  void CallRuntime(Runtime::FunctionId fid,
+                   SaveFPRegsMode save_doubles = kDontSaveFPRegs) {
+    const Runtime::Function* function = Runtime::FunctionForId(fid);
+    CallRuntime(function, function->nargs, save_doubles);
+  }
+
+  // Convenience function: Same as above, but takes the fid instead.
+  void CallRuntime(Runtime::FunctionId fid, int num_arguments,
+                   SaveFPRegsMode save_doubles = kDontSaveFPRegs) {
+    CallRuntime(Runtime::FunctionForId(fid), num_arguments, save_doubles);
+  }
+
+  // Convenience function: tail call a runtime routine (jump).
+  void TailCallRuntime(Runtime::FunctionId fid);
+
+  // Jump to the builtin routine.
+  void JumpToExternalReference(const ExternalReference& builtin,
+                               BranchDelaySlot bd = PROTECT,
+                               bool builtin_exit_frame = false);
+
+  // Generates a trampoline to jump to the off-heap instruction stream.
+  void JumpToInstructionStream(Address entry);
+
+  // ---------------------------------------------------------------------------
+  // In-place weak references.
+  void LoadWeakValue(Register out, Register in, Label* target_if_cleared);
+
+  // -------------------------------------------------------------------------
+  // StatsCounter support.
+
+  void IncrementCounter(StatsCounter* counter, int value,
+                        Register scratch1, Register scratch2);
+  void DecrementCounter(StatsCounter* counter, int value,
+                        Register scratch1, Register scratch2);
+
+  // -------------------------------------------------------------------------
+  // Smi utilities.
+
+  void SmiTag(Register dst, Register src) {
+    STATIC_ASSERT(kSmiTag == 0);
+    if (SmiValuesAre32Bits()) {
+      slll(src, 32 ,dst);
+    } else {
+      DCHECK(SmiValuesAre31Bits());
+      Addw(dst, src, src);
+    }
+  }
+
+  void SmiTag(Register reg) {
+    SmiTag(reg, reg);
+  }
+
+  // Left-shifted from int32 equivalent of Smi.
+  void SmiScale(Register dst, Register src, int scale) {
+    if (SmiValuesAre32Bits()) {
+      // The int portion is upper 32-bits of 64-bit word.
+      sral(src, kSmiShift - scale, dst);
+    } else {
+      DCHECK(SmiValuesAre31Bits());
+      DCHECK_GE(scale, kSmiTagSize);
+      Sllw(dst, src, scale - kSmiTagSize);
+    }
+  }
+
+  // Test if the register contains a smi.
+  inline void SmiTst(Register value, Register scratch) {
+    And(scratch, value, Operand(kSmiTagMask));
+  }
+
+  // Jump if the register contains a non-smi.
+  void JumpIfNotSmi(Register value,
+                    Label* not_smi_label,
+                    Register scratch = at,
+                    BranchDelaySlot bd = PROTECT);
+
+  // Abort execution if argument is a smi, enabled via --debug-code.
+  void AssertNotSmi(Register object);
+  void AssertSmi(Register object);
+
+  // Abort execution if argument is not a Constructor, enabled via --debug-code.
+  void AssertConstructor(Register object);
+
+  // Abort execution if argument is not a JSFunction, enabled via --debug-code.
+  void AssertFunction(Register object);
+
+  // Abort execution if argument is not a JSBoundFunction,
+  // enabled via --debug-code.
+  void AssertBoundFunction(Register object);
+
+  // Abort execution if argument is not a JSGeneratorObject (or subclass),
+  // enabled via --debug-code.
+  void AssertGeneratorObject(Register object);
+
+  // Abort execution if argument is not undefined or an AllocationSite, enabled
+  // via --debug-code.
+  void AssertUndefinedOrAllocationSite(Register object, Register scratch);
+
+  template<typename Field>
+  void DecodeField(Register dst, Register src) {
+    Ext(dst, src, Field::kShift, Field::kSize);
+  }
+
+  template<typename Field>
+  void DecodeField(Register reg) {
+    DecodeField<Field>(reg, reg);
+  }
+
+ private:
+  // Helper functions for generating invokes.
+  void InvokePrologue(Register expected_parameter_count,
+                      Register actual_parameter_count, Label* done,
+                      InvokeFlag flag);
+
+  // Compute memory operands for safepoint stack slots.
+  static int SafepointRegisterStackIndex(int reg_code);
+
+  // Needs access to SafepointRegisterStackIndex for compiled frame
+  // traversal.
+  friend class StandardFrame;
+
+  DISALLOW_IMPLICIT_CONSTRUCTORS(MacroAssembler);
+};
+
+template <typename Func>
+void TurboAssembler::GenerateSwitchTable(Register index, size_t case_count,
+                                         Func GetLabelFunction) {
+  // Ensure that dd-ed labels following this instruction use 8 bytes aligned
+  // addresses.
+  BlockTrampolinePoolFor(static_cast<int>(case_count) * 2 +
+                         kSwitchTablePrologueSize);
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  Align(8);
+  int instr_num = 3;  //Added 20190117
+  br(scratch, 0);
+  s8addl(index, scratch, scratch);  // get_mem = cur_pc + index * 8 (kPointerSizeLog2);
+  Ldl(scratch, MemOperand(scratch, instr_num * v8::internal::kInstrSize));
+  Assembler::jmp(zero_reg, scratch, 0);
+  for (size_t index = 0; index < case_count; ++index) {
+    dd(GetLabelFunction(index));
+  }
+}
+
+#define ACCESS_MASM(masm) masm->
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_CODEGEN_SW64_MACRO_ASSEMBLER_SW64_H_
diff --git a/src/3rdparty/chromium/v8/src/codegen/sw64/register-sw64.h b/src/3rdparty/chromium/v8/src/codegen/sw64/register-sw64.h
new file mode 100755
index 0000000000..ebcdf711ce
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/codegen/sw64/register-sw64.h
@@ -0,0 +1,406 @@
+// Copyright 2018 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef V8_CODEGEN_SW64_REGISTER_SW64_H_
+#define V8_CODEGEN_SW64_REGISTER_SW64_H_
+
+#include "src/codegen/sw64/constants-sw64.h"
+#include "src/codegen/register.h"
+#include "src/codegen/reglist.h"
+
+namespace v8 {
+namespace internal {
+
+// clang-format off
+#define GENERAL_REGISTERS(V)                              \
+  V(v0)  \
+  V(t0)  V(t1)  V(t2)  V(t3) V(t4) V(t5) V(t6) V(t7)  \
+  V(s0)  V(s1)  V(s2)  V(s3) V(s4) V(s5) V(fp)  \
+  V(a0)  V(a1)  V(a2)  V(a3) V(a4) V(a5) \
+  V(t8)  V(t9)  V(t10) V(t11)   \
+  V(ra)       \
+  V(t12)      \
+  V(at)       \
+  V(gp)       \
+  V(sp)       \
+  V(zero_reg)
+
+// t7, t8 used as two scratch regs instead of s3, s4; so they 
+// should not be added to allocatable lists.
+#define ALLOCATABLE_GENERAL_REGISTERS(V) \
+  V(a0)  V(a1)  V(a2)  V(a3)  V(a4)  V(a5)  \
+  V(t0)  V(t1)  V(t2)  V(t3)  V(t5)  V(t6)  V(t9)  V(t10) V(s5) \
+  V(v0)  V(t4)
+
+#define DOUBLE_REGISTERS(V)                               \
+  V(f0)  V(f1)  V(f2)  V(f3)  V(f4)  V(f5)  V(f6)  V(f7)  \
+  V(f8)  V(f9)  V(f10) V(f11) V(f12) V(f13) V(f14) V(f15) \
+  V(f16) V(f17) V(f18) V(f19) V(f20) V(f21) V(f22) V(f23) \
+  V(f24) V(f25) V(f26) V(f27) V(f28) V(f29) V(f30) V(f31)
+
+#define FLOAT_REGISTERS DOUBLE_REGISTERS
+#define SIMD128_REGISTERS(V)                              \
+  V(w0)  V(w1)  V(w2)  V(w3)  V(w4)  V(w5)  V(w6)  V(w7)  \
+  V(w8)  V(w9)  V(w10) V(w11) V(w12) V(w13) V(w14) V(w15) \
+  V(w16) V(w17) V(w18) V(w19) V(w20) V(w21) V(w22) V(w23) \
+  V(w24) V(w25) V(w26) V(w27) V(w28) V(w29) V(w30) V(w31)
+
+/* f27-f30 scratch fregisters */
+#define ALLOCATABLE_DOUBLE_REGISTERS(V)                   \
+  V(f0)  V(f1)  V(f2)  V(f3)  V(f4)  V(f5)  V(f6)  V(f7)  \
+  V(f8)  V(f9)  V(f10) V(f11) V(f12) V(f13) V(f14) V(f15) \
+  V(f16) V(f17) V(f18) V(f19) V(f20) V(f21) V(f22) V(f23) \
+  V(f24) V(f25) V(f26) 
+// clang-format on
+
+// Note that the bit values must match those used in actual instruction
+// encoding.
+const int kNumRegs = 32;
+
+const RegList kJSCallerSaved = 1 << 0 |   // v0
+                               1 << 1  |  // t0
+                               1 << 2  |  // t1
+                               1 << 3  |  // t2
+                               1 << 4  |  // t3
+                               1 << 5  |  // t4
+                               1 << 6  |  // t5
+                               1 << 7  |  // t6
+                               1 << 8  |  // t7
+                               1 << 16 |  // a0
+                               1 << 17 |  // a1
+                               1 << 18 |  // a2
+                               1 << 19 |  // a3
+                               1 << 20 |  // a4
+                               1 << 21 |  // a5
+                               1 << 22 |  // t8
+                               1 << 23 |  // t9
+                               1 << 24;   // t10
+
+const int kNumJSCallerSaved = 18;
+
+// Callee-saved registers preserved when switching from C to JavaScript.
+const RegList kCalleeSaved = 1 << 9  |  // s0
+                             1 << 10 |  // s1
+                             1 << 11 |  // s2
+                             1 << 12 |  // s3
+                             1 << 13 |  // s4 (roots in Javascript code)
+                             1 << 14 |  // s5 (cp in Javascript code)
+                             1 << 15;   // fp/s6
+
+const int kNumCalleeSaved = 7;
+
+const RegList kCalleeSavedFPU = 1 << 2 |  // f2
+                                1 << 3 |  // f3
+                                1 << 4 |  // f4
+                                1 << 5 |  // f5
+                                1 << 6 |  // f6
+                                1 << 7 |  // f7
+                                1 << 8 |  // f8
+                                1 << 9;   // f9
+
+const int kNumCalleeSavedFPU = 8;
+
+const RegList kCallerSavedFPU = 1 << 0 |   // f0
+                                1 << 1 |   // f1
+                                1 << 10 |  // f10
+                                1 << 11 |  // f11
+                                1 << 12 |  // f12
+                                1 << 13 |  // f13
+                                1 << 14 |  // f14
+                                1 << 15 |  // f15
+                                1 << 16 |  // f16
+                                1 << 17 |  // f17
+                                1 << 18 |  // f18
+                                1 << 19 |  // f19
+                                1 << 20 |  // f20
+                                1 << 21 |  // f21
+                                1 << 22 |  // f22
+                                1 << 23 |  // f23
+                                1 << 24 |  // f24
+                                1 << 25 |  // f25
+                                1 << 26 |  // f26
+                                1 << 27 |  // f27
+                                1 << 28 |  // f28
+                                1 << 29 |  // f29
+                                1 << 30;   // f30
+
+// Number of registers for which space is reserved in safepoints. Must be a
+// multiple of 8.
+const int kNumSafepointRegisters = 32;
+
+// Define the list of registers actually saved at safepoints.
+// Note that the number of saved registers may be smaller than the reserved
+// space, i.e. kNumSafepointSavedRegisters <= kNumSafepointRegisters.
+const RegList kSafepointSavedRegisters = kJSCallerSaved | kCalleeSaved;
+const int kNumSafepointSavedRegisters = kNumJSCallerSaved + kNumCalleeSaved;
+
+const int kUndefIndex = -1;
+// Map with indexes on stack that corresponds to codes of saved registers.
+const int kSafepointRegisterStackIndexMap[kNumRegs] = {0,            // v0
+                                                       1,            // t0
+                                                       2,            // t1
+                                                       3,            // t2
+                                                       4,            // t3
+                                                       5,            // t4
+                                                       6,            // t5
+                                                       7,            // t6
+                                                       8,            // t7
+                                                       9,            // s0
+                                                       10,           // s1
+                                                       11,           // s2
+                                                       12,           // s3
+                                                       13,           // s4
+                                                       14,           // s5
+                                                       24,           // fp      <---- a trick?
+                                                       15,           // a0
+                                                       16,           // a1
+                                                       17,           // a2
+                                                       18,           // a3
+                                                       19,           // a4
+                                                       20,           // a5
+                                                       21,           // t8
+                                                       22,           // t9
+                                                       23,           // t10
+                                                       kUndefIndex,  // t11
+                                                       kUndefIndex,  // ra
+                                                       kUndefIndex,  // t12
+                                                       kUndefIndex,  // at
+                                                       kUndefIndex,  // gp
+                                                       kUndefIndex,  // sp
+                                                       kUndefIndex}; // zero_reg
+
+// CPU Registers.
+//
+// 1) We would prefer to use an enum, but enum values are assignment-
+// compatible with int, which has caused code-generation bugs.
+//
+// 2) We would prefer to use a class instead of a struct but we don't like
+// the register initialization to depend on the particular initialization
+// order (which appears to be different on OS X, Linux, and Windows for the
+// installed versions of C++ we tried). Using a struct permits C-style
+// "initialization". Also, the Register objects cannot be const as this
+// forces initialization stubs in MSVC, making us dependent on initialization
+// order.
+//
+// 3) By not using an enum, we are possibly preventing the compiler from
+// doing certain constant folds, which may significantly reduce the
+// code generated for some assembly instructions (because they boil down
+// to a few constants). If this is a problem, we could change the code
+// such that we use an enum in optimized mode, and the struct in debug
+// mode. This way we get the compile-time error checking in debug mode
+// and best performance in optimized code.
+
+// -----------------------------------------------------------------------------
+// Implementation of Register and FPURegister.
+
+enum RegisterCode {
+#define REGISTER_CODE(R) kRegCode_##R,
+  GENERAL_REGISTERS(REGISTER_CODE)
+#undef REGISTER_CODE
+      kRegAfterLast
+};
+
+class Register : public RegisterBase<Register, kRegAfterLast> {
+ public:
+#if defined(V8_TARGET_LITTLE_ENDIAN)
+  static constexpr int kMantissaOffset = 0;
+  static constexpr int kExponentOffset = 4;
+#else
+#error Unknown endianness
+#endif
+
+ private:
+  friend class RegisterBase;
+  explicit constexpr Register(int code) : RegisterBase(code) {}
+};
+
+// s5: context register
+// t7: lithium scratch
+// t8: lithium scratch2
+#define DECLARE_REGISTER(R) \
+  constexpr Register R = Register::from_code(kRegCode_##R);
+GENERAL_REGISTERS(DECLARE_REGISTER)
+#undef DECLARE_REGISTER
+
+constexpr Register no_reg = Register::no_reg();
+
+int ToNumber(Register reg);
+
+Register ToRegister(int num);
+
+constexpr bool kPadArguments = false;
+constexpr bool kSimpleFPAliasing = true;
+constexpr bool kSimdMaskRegisters = false;
+
+enum DoubleRegisterCode {
+#define REGISTER_CODE(R) kDoubleCode_##R,
+  DOUBLE_REGISTERS(REGISTER_CODE)
+#undef REGISTER_CODE
+      kDoubleAfterLast
+};
+
+// Coprocessor register.
+class FPURegister : public RegisterBase<FPURegister, kDoubleAfterLast> {
+ public:
+  // TODO(plind): Warning, inconsistent numbering here. kNumFPURegisters refers
+  // to number of 32-bit FPU regs, but kNumAllocatableRegisters refers to
+  // number of Double regs (64-bit regs, or FPU-reg-pairs).
+
+ private:
+  friend class RegisterBase;
+  explicit constexpr FPURegister(int code) : RegisterBase(code) {}
+};
+
+enum MSARegisterCode {
+#define REGISTER_CODE(R) kMsaCode_##R,
+  SIMD128_REGISTERS(REGISTER_CODE)
+#undef REGISTER_CODE
+      kMsaAfterLast
+};
+
+// SW64 SIMD (MSA) register
+class MSARegister : public RegisterBase<MSARegister, kMsaAfterLast> {
+  friend class RegisterBase;
+  explicit constexpr MSARegister(int code) : RegisterBase(code) {}
+};
+
+// A few double registers are reserved: one as a scratch register and one to
+// hold 0.0.
+//  f31: 0.0
+//  f30: scratch register.
+
+// V8 now supports the O32 ABI, and the FPU Registers are organized as 32
+// 32-bit registers, f0 through f31. When used as 'double' they are used
+// in pairs, starting with the even numbered register. So a double operation
+// on f0 really uses f0 and f1.
+// (Modern sw64 hardware also supports 32 64-bit registers, via setting
+// (privileged) Status Register FR bit to 1. This is used by the N32 ABI,
+// but it is not in common use. Someday we will want to support this in v8.)
+
+// For O32 ABI, Floats and Doubles refer to same set of 32 32-bit registers.
+using FloatRegister = FPURegister;
+
+using DoubleRegister = FPURegister;
+
+#define DECLARE_DOUBLE_REGISTER(R) \
+  constexpr DoubleRegister R = DoubleRegister::from_code(kDoubleCode_##R);
+DOUBLE_REGISTERS(DECLARE_DOUBLE_REGISTER)
+#undef DECLARE_DOUBLE_REGISTER
+
+constexpr DoubleRegister no_dreg = DoubleRegister::no_reg();
+
+// SIMD registers.
+using Simd128Register = MSARegister;
+
+#define DECLARE_SIMD128_REGISTER(R) \
+  constexpr Simd128Register R = Simd128Register::from_code(kMsaCode_##R);
+SIMD128_REGISTERS(DECLARE_SIMD128_REGISTER)
+#undef DECLARE_SIMD128_REGISTER
+
+const Simd128Register no_msareg = Simd128Register::no_reg();
+
+// Register aliases.
+// cp is assumed to be a callee saved register.
+constexpr Register kRootRegister = s4;
+constexpr Register cp = s5;
+constexpr Register kScratchReg = t7;
+constexpr Register kScratchReg2 = t8;
+constexpr DoubleRegister kDoubleRegZero = f31;
+constexpr DoubleRegister kScratchDoubleReg = f29;  // change f30 to f29, use f28, f29 as scratch
+constexpr DoubleRegister kScratchDoubleReg1 = f28;
+constexpr DoubleRegister kScratchDoubleReg2 = f27;
+// Used on sw64r3 for compare operations.
+// We use the last non-callee saved odd register for N64 ABI
+constexpr DoubleRegister kDoubleCompareReg = f30;  // change f28 to f30
+// MSA zero and scratch regs must have the same numbers as FPU zero and scratch
+constexpr Simd128Register kSimd128RegZero = w31;  // may be used as scratch
+constexpr Simd128Register kSimd128ScratchReg = w30;
+
+// FPU (coprocessor 1) control registers.
+// Currently only FCSR (#31) is implemented.
+struct FPUControlRegister {
+  bool is_valid() const { return reg_code == kFCSRRegister; }
+  bool is(FPUControlRegister creg) const { return reg_code == creg.reg_code; }
+  int code() const {
+    DCHECK(is_valid());
+    return reg_code;
+  }
+  int bit() const {
+    DCHECK(is_valid());
+    return 1 << reg_code;
+  }
+  void setcode(int f) {
+    reg_code = f;
+    DCHECK(is_valid());
+  }
+  // Unfortunately we can't make this private in a struct.
+  int reg_code;
+};
+
+constexpr FPUControlRegister no_fpucreg = {kInvalidFPUControlRegister};
+constexpr FPUControlRegister FCSR = {kFCSRRegister};
+
+// MSA control registers
+struct MSAControlRegister {
+  bool is_valid() const {
+    return (reg_code == kMSAIRRegister) || (reg_code == kMSACSRRegister);
+  }
+  bool is(MSAControlRegister creg) const { return reg_code == creg.reg_code; }
+  int code() const {
+    DCHECK(is_valid());
+    return reg_code;
+  }
+  int bit() const {
+    DCHECK(is_valid());
+    return 1 << reg_code;
+  }
+  void setcode(int f) {
+    reg_code = f;
+    DCHECK(is_valid());
+  }
+  // Unfortunately we can't make this private in a struct.
+  int reg_code;
+};
+
+constexpr MSAControlRegister no_msacreg = {kInvalidMSAControlRegister};
+constexpr MSAControlRegister MSAIR = {kMSAIRRegister};
+constexpr MSAControlRegister MSACSR = {kMSACSRRegister};
+
+// Define {RegisterName} methods for the register types.
+DEFINE_REGISTER_NAMES(Register, GENERAL_REGISTERS)
+DEFINE_REGISTER_NAMES(FPURegister, DOUBLE_REGISTERS)
+DEFINE_REGISTER_NAMES(MSARegister, SIMD128_REGISTERS)
+
+// Give alias names to registers for calling conventions.
+constexpr Register kReturnRegister0 = v0;
+constexpr Register kReturnRegister1 = a5;  // v1;
+constexpr Register kReturnRegister2 = a0;
+constexpr Register kJSFunctionRegister = a1;
+constexpr Register kContextRegister = s5;  // s7;
+constexpr Register kAllocateSizeRegister = a0;
+constexpr Register kSpeculationPoisonRegister = t10; // a7;
+constexpr Register kInterpreterAccumulatorRegister = v0;
+constexpr Register kInterpreterBytecodeOffsetRegister = t0;
+constexpr Register kInterpreterBytecodeArrayRegister = t1;
+constexpr Register kInterpreterDispatchTableRegister = t2;
+
+constexpr Register kJavaScriptCallArgCountRegister = a0;
+constexpr Register kJavaScriptCallCodeStartRegister = a2;
+constexpr Register kJavaScriptCallTargetRegister = kJSFunctionRegister;
+constexpr Register kJavaScriptCallNewTargetRegister = a3;
+constexpr Register kJavaScriptCallExtraArg1Register = a2;
+
+constexpr Register kOffHeapTrampolineRegister = at;
+constexpr Register kRuntimeCallFunctionRegister = a1;
+constexpr Register kRuntimeCallArgCountRegister = a0;
+constexpr Register kRuntimeCallArgvRegister = a2;
+constexpr Register kWasmInstanceRegister = a0;
+constexpr Register kWasmCompileLazyFuncIndexRegister = t0;
+
+constexpr DoubleRegister kFPReturnRegister0 = f0;
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_SW64_REGISTER_SW64_H_
diff --git a/src/3rdparty/chromium/v8/src/common/globals.h b/src/3rdparty/chromium/v8/src/common/globals.h
index 9ed449dd60..7b41a7c40a 100644
--- a/src/3rdparty/chromium/v8/src/common/globals.h
+++ b/src/3rdparty/chromium/v8/src/common/globals.h
@@ -58,6 +58,9 @@ constexpr int GB = MB * 1024;
 #if (V8_TARGET_ARCH_S390 && !V8_HOST_ARCH_S390)
 #define USE_SIMULATOR 1
 #endif
+#if (V8_TARGET_ARCH_SW64 && !V8_HOST_ARCH_SW64)
+#define USE_SIMULATOR 1
+#endif
 #endif
 
 // Determine whether the architecture uses an embedded constant pool
diff --git a/src/3rdparty/chromium/v8/src/compiler/backend/instruction-codes.h b/src/3rdparty/chromium/v8/src/compiler/backend/instruction-codes.h
index 8772a78df0..6b9ef52c2d 100644
--- a/src/3rdparty/chromium/v8/src/compiler/backend/instruction-codes.h
+++ b/src/3rdparty/chromium/v8/src/compiler/backend/instruction-codes.h
@@ -23,6 +23,8 @@
 #include "src/compiler/backend/ppc/instruction-codes-ppc.h"
 #elif V8_TARGET_ARCH_S390
 #include "src/compiler/backend/s390/instruction-codes-s390.h"
+#elif V8_TARGET_ARCH_SW64
+#include "src/compiler/backend/sw64/instruction-codes-sw64.h"
 #else
 #define TARGET_ARCH_OPCODE_LIST(V)
 #define TARGET_ADDRESSING_MODE_LIST(V)
diff --git a/src/3rdparty/chromium/v8/src/compiler/backend/instruction-selector.cc b/src/3rdparty/chromium/v8/src/compiler/backend/instruction-selector.cc
index 1c14832bbf..ee86d52c57 100644
--- a/src/3rdparty/chromium/v8/src/compiler/backend/instruction-selector.cc
+++ b/src/3rdparty/chromium/v8/src/compiler/backend/instruction-selector.cc
@@ -2623,7 +2623,7 @@ void InstructionSelector::VisitWord32AtomicPairCompareExchange(Node* node) {
 #endif  // !V8_TARGET_ARCH_IA32 && !V8_TARGET_ARCH_ARM && !V8_TARGET_ARCH_MIPS
 
 #if !V8_TARGET_ARCH_X64 && !V8_TARGET_ARCH_ARM64 && !V8_TARGET_ARCH_MIPS64 && \
-    !V8_TARGET_ARCH_S390 && !V8_TARGET_ARCH_PPC64
+    !V8_TARGET_ARCH_S390 && !V8_TARGET_ARCH_PPC64 && !V8_TARGET_ARCH_SW64
 void InstructionSelector::VisitWord64AtomicLoad(Node* node) { UNIMPLEMENTED(); }
 
 void InstructionSelector::VisitWord64AtomicStore(Node* node) {
@@ -2662,7 +2662,7 @@ void InstructionSelector::VisitI64x2ReplaceLaneI32Pair(Node* node) {
 
 #if !V8_TARGET_ARCH_X64 && !V8_TARGET_ARCH_S390X
 #if !V8_TARGET_ARCH_ARM64
-#if !V8_TARGET_ARCH_MIPS64
+#if !V8_TARGET_ARCH_MIPS64 && !V8_TARGET_ARCH_SW64
 void InstructionSelector::VisitI64x2Splat(Node* node) { UNIMPLEMENTED(); }
 void InstructionSelector::VisitI64x2ExtractLane(Node* node) { UNIMPLEMENTED(); }
 void InstructionSelector::VisitI64x2ReplaceLane(Node* node) { UNIMPLEMENTED(); }
diff --git a/src/3rdparty/chromium/v8/src/compiler/backend/sw64/OWNERS b/src/3rdparty/chromium/v8/src/compiler/backend/sw64/OWNERS
new file mode 100755
index 0000000000..42582e993a
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/compiler/backend/sw64/OWNERS
@@ -0,0 +1,3 @@
+ivica.bogosavljevic@sw64.com
+Miran.Karic@sw64.com
+sreten.kovacevic@sw64.com
diff --git a/src/3rdparty/chromium/v8/src/compiler/backend/sw64/code-generator-sw64.cc b/src/3rdparty/chromium/v8/src/compiler/backend/sw64/code-generator-sw64.cc
new file mode 100755
index 0000000000..02840f457a
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/compiler/backend/sw64/code-generator-sw64.cc
@@ -0,0 +1,4209 @@
+// Copyright 2014 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#include "src/codegen/assembler-inl.h"
+#include "src/codegen/callable.h"
+#include "src/codegen/macro-assembler.h"
+#include "src/codegen/sw64/constants-sw64.h"
+#include "src/codegen/optimized-compilation-info.h"
+#include "src/compiler/backend/code-generator-impl.h"
+#include "src/compiler/backend/code-generator.h"
+#include "src/compiler/backend/gap-resolver.h"
+#include "src/compiler/node-matchers.h"
+#include "src/compiler/osr.h"
+#include "src/heap/memory-chunk.h"
+#include "src/wasm/wasm-code-manager.h"
+
+namespace v8 {
+namespace internal {
+namespace compiler {
+
+#define __ tasm()->
+
+// TODO(plind): consider renaming these macros.
+#define TRACE_MSG(msg)                                                      \
+  PrintF("code_gen: \'%s\' in function %s at line %d\n", msg, __FUNCTION__, \
+         __LINE__)
+
+#define TRACE_UNIMPL()                                                       \
+  PrintF("UNIMPLEMENTED code_generator_sw64: %s at line %d\n", __FUNCTION__, \
+         __LINE__)
+
+// Adds Sw64-specific methods to convert InstructionOperands.
+class Sw64OperandConverter final : public InstructionOperandConverter {
+ public:
+  Sw64OperandConverter(CodeGenerator* gen, Instruction* instr)
+      : InstructionOperandConverter(gen, instr) {}
+
+  FloatRegister OutputSingleRegister(size_t index = 0) {
+    return ToSingleRegister(instr_->OutputAt(index));
+  }
+
+  FloatRegister InputSingleRegister(size_t index) {
+    return ToSingleRegister(instr_->InputAt(index));
+  }
+
+  FloatRegister ToSingleRegister(InstructionOperand* op) {
+    // Single (Float) and Double register namespace is same on SW64,
+    // both are typedefs of FPURegister.
+    return ToDoubleRegister(op);
+  }
+
+  Register InputOrZeroRegister(size_t index) {
+    if (instr_->InputAt(index)->IsImmediate()) {
+      DCHECK_EQ(0, InputInt32(index));
+      return zero_reg;
+    }
+    return InputRegister(index);
+  }
+
+  DoubleRegister InputOrZeroDoubleRegister(size_t index) {
+    if (instr_->InputAt(index)->IsImmediate()) return kDoubleRegZero;
+
+    return InputDoubleRegister(index);
+  }
+
+  DoubleRegister InputOrZeroSingleRegister(size_t index) {
+    if (instr_->InputAt(index)->IsImmediate()) return kDoubleRegZero;
+
+    return InputSingleRegister(index);
+  }
+
+  Operand InputImmediate(size_t index) {
+    Constant constant = ToConstant(instr_->InputAt(index));
+    switch (constant.type()) {
+      case Constant::kInt32:
+        return Operand(constant.ToInt32());
+      case Constant::kInt64:
+        return Operand(constant.ToInt64());
+      case Constant::kFloat32:
+        return Operand::EmbeddedNumber(constant.ToFloat32());
+      case Constant::kFloat64:
+        return Operand::EmbeddedNumber(constant.ToFloat64().value());
+      case Constant::kExternalReference:
+      case Constant::kCompressedHeapObject:
+      case Constant::kHeapObject:
+        // TODO(plind): Maybe we should handle ExtRef & HeapObj here?
+        //    maybe not done on arm due to const pool ??
+        break;
+      case Constant::kDelayedStringConstant:
+        return Operand::EmbeddedStringConstant(
+            constant.ToDelayedStringConstant());
+      case Constant::kRpoNumber:
+        UNREACHABLE();  // TODO(titzer): RPO immediates on sw64?
+        break;
+    }
+    UNREACHABLE();
+  }
+
+  Operand InputOperand(size_t index) {
+    InstructionOperand* op = instr_->InputAt(index);
+    if (op->IsRegister()) {
+      return Operand(ToRegister(op));
+    }
+    return InputImmediate(index);
+  }
+
+  MemOperand MemoryOperand(size_t* first_index) {
+    const size_t index = *first_index;
+    switch (AddressingModeField::decode(instr_->opcode())) {
+      case kMode_None:
+        break;
+      case kMode_MRI:
+        *first_index += 2;
+        return MemOperand(InputRegister(index + 0), InputInt32(index + 1));
+      case kMode_MRR:
+        // TODO(plind): r6 address mode, to be implemented ...
+        UNREACHABLE();
+    }
+    UNREACHABLE();
+  }
+
+  MemOperand MemoryOperand(size_t index = 0) { return MemoryOperand(&index); }
+
+  MemOperand ToMemOperand(InstructionOperand* op) const {
+    DCHECK_NOT_NULL(op);
+    DCHECK(op->IsStackSlot() || op->IsFPStackSlot());
+    return SlotToMemOperand(AllocatedOperand::cast(op)->index());
+  }
+
+  MemOperand SlotToMemOperand(int slot) const {
+    FrameOffset offset = frame_access_state()->GetFrameOffset(slot);
+    return MemOperand(offset.from_stack_pointer() ? sp : fp, offset.offset());
+  }
+};
+
+static inline bool HasRegisterInput(Instruction* instr, size_t index) {
+  return instr->InputAt(index)->IsRegister();
+}
+
+namespace {
+
+class OutOfLineRecordWrite final : public OutOfLineCode {
+ public:
+  OutOfLineRecordWrite(CodeGenerator* gen, Register object, Register index,
+                       Register value, Register scratch0, Register scratch1,
+                       RecordWriteMode mode, StubCallMode stub_mode)
+      : OutOfLineCode(gen),
+        object_(object),
+        index_(index),
+        value_(value),
+        scratch0_(scratch0),
+        scratch1_(scratch1),
+        mode_(mode),
+        stub_mode_(stub_mode),
+        must_save_lr_(!gen->frame_access_state()->has_frame()),
+        zone_(gen->zone()) {}
+
+  void Generate() final {
+    if (mode_ > RecordWriteMode::kValueIsPointer) {
+      __ JumpIfSmi(value_, exit());
+    }
+    __ CheckPageFlag(value_, scratch0_,
+                     MemoryChunk::kPointersToHereAreInterestingMask, eq,
+                     exit());
+    __ Addl(scratch1_, object_, index_);
+    RememberedSetAction const remembered_set_action =
+        mode_ > RecordWriteMode::kValueIsMap ? EMIT_REMEMBERED_SET
+                                             : OMIT_REMEMBERED_SET;
+    SaveFPRegsMode const save_fp_mode =
+        frame()->DidAllocateDoubleRegisters() ? kSaveFPRegs : kDontSaveFPRegs;
+    if (must_save_lr_) {
+      // We need to save and restore ra if the frame was elided.
+      __ Push(ra);
+    }
+    if (mode_ == RecordWriteMode::kValueIsEphemeronKey) {
+      __ CallEphemeronKeyBarrier(object_, scratch1_, save_fp_mode);
+    } else if (stub_mode_ == StubCallMode::kCallWasmRuntimeStub) {
+      // A direct call to a wasm runtime stub defined in this module.
+      // Just encode the stub index. This will be patched when the code
+      // is added to the native module and copied into wasm code space.
+      __ CallRecordWriteStub(object_, scratch1_, remembered_set_action,
+                             save_fp_mode, wasm::WasmCode::kRecordWrite);
+    } else {
+      __ CallRecordWriteStub(object_, scratch1_, remembered_set_action,
+                             save_fp_mode);
+    }
+    if (must_save_lr_) {
+      __ Pop(ra);
+    }
+  }
+
+ private:
+  Register const object_;
+  Register const index_;
+  Register const value_;
+  Register const scratch0_;
+  Register const scratch1_;
+  RecordWriteMode const mode_;
+  StubCallMode const stub_mode_;
+  bool must_save_lr_;
+  Zone* zone_;
+};
+
+#define CREATE_OOL_CLASS(ool_name, tasm_ool_name, T)                 \
+  class ool_name final : public OutOfLineCode {                      \
+   public:                                                           \
+    ool_name(CodeGenerator* gen, T dst, T src1, T src2)              \
+        : OutOfLineCode(gen), dst_(dst), src1_(src1), src2_(src2) {} \
+                                                                     \
+    void Generate() final { __ tasm_ool_name(dst_, src1_, src2_); }  \
+                                                                     \
+   private:                                                          \
+    T const dst_;                                                    \
+    T const src1_;                                                   \
+    T const src2_;                                                   \
+  }
+
+CREATE_OOL_CLASS(OutOfLineFloat32Max, Float32MaxOutOfLine, FPURegister);
+CREATE_OOL_CLASS(OutOfLineFloat32Min, Float32MinOutOfLine, FPURegister);
+CREATE_OOL_CLASS(OutOfLineFloat64Max, Float64MaxOutOfLine, FPURegister);
+CREATE_OOL_CLASS(OutOfLineFloat64Min, Float64MinOutOfLine, FPURegister);
+
+#undef CREATE_OOL_CLASS
+
+Condition FlagsConditionToConditionCmp(FlagsCondition condition) {
+  switch (condition) {
+    case kEqual:
+      return eq;
+    case kNotEqual:
+      return ne;
+    case kSignedLessThan:
+      return lt;
+    case kSignedGreaterThanOrEqual:
+      return ge;
+    case kSignedLessThanOrEqual:
+      return le;
+    case kSignedGreaterThan:
+      return gt;
+    case kUnsignedLessThan:
+      return lo;
+    case kUnsignedGreaterThanOrEqual:
+      return hs;
+    case kUnsignedLessThanOrEqual:
+      return ls;
+    case kUnsignedGreaterThan:
+      return hi;
+    case kUnorderedEqual:
+    case kUnorderedNotEqual:
+      break;
+    default:
+      break;
+  }
+  UNREACHABLE();
+}
+
+Condition FlagsConditionToConditionTst(FlagsCondition condition) {
+  switch (condition) {
+    case kNotEqual:
+      return ne;
+    case kEqual:
+      return eq;
+    default:
+      break;
+  }
+  UNREACHABLE();
+}
+
+Condition FlagsConditionToConditionOvf(FlagsCondition condition) {
+  switch (condition) {
+    case kOverflow:
+      return ne;
+    case kNotOverflow:
+      return eq;
+    default:
+      break;
+  }
+  UNREACHABLE();
+}
+
+FPUCondition FlagsConditionToConditionCmpFPU(bool* predicate,
+                                             FlagsCondition condition) {
+  switch (condition) {
+    case kEqual:
+      *predicate = true;
+      return EQ;
+    case kNotEqual:
+      *predicate = false;
+      return EQ;
+    case kUnsignedLessThan:
+      *predicate = true;
+      return OLT;
+    case kUnsignedGreaterThanOrEqual:
+      *predicate = false;
+      return OLT;
+    case kUnsignedLessThanOrEqual:
+      *predicate = true;
+      return OLE;
+    case kUnsignedGreaterThan:
+      *predicate = false;
+      return OLE;
+    case kUnorderedEqual:
+    case kUnorderedNotEqual:
+      *predicate = true;
+      break;
+    default:
+      *predicate = true;
+      break;
+  }
+  UNREACHABLE();
+}
+
+void EmitWordLoadPoisoningIfNeeded(CodeGenerator* codegen,
+                                   InstructionCode opcode, Instruction* instr,
+                                   Sw64OperandConverter const& i) {
+  const MemoryAccessMode access_mode =
+      static_cast<MemoryAccessMode>(MiscField::decode(opcode));
+  if (access_mode == kMemoryAccessPoisoned) {
+    Register value = i.OutputRegister();
+    codegen->tasm()->And(value, value, kSpeculationPoisonRegister);
+  }
+}
+
+}  // namespace
+
+#define ASSEMBLE_ATOMIC_LOAD_INTEGER(asm_instr)          \
+  do {                                                   \
+    __ asm_instr(i.OutputRegister(), i.MemoryOperand()); \
+    /*__ memb();*/                                       \
+  } while (0)
+
+#define ASSEMBLE_ATOMIC_STORE_INTEGER(asm_instr)               \
+  do {                                                         \
+    /*__ memb();*/                                             \
+    __ asm_instr(i.InputOrZeroRegister(2), i.MemoryOperand()); \
+    /*__ memb();*/                                             \
+  } while (0)
+
+#define ASSEMBLE_ATOMIC_BINOP(load_linked, store_conditional, bin_instr)       \
+  do {                                                                         \
+    Label binop;                                                               \
+    __ Addl(i.TempRegister(0), i.InputRegister(0), i.InputRegister(1));        \
+    /*__ memb();*/                                                             \
+    __ bind(&binop);                                                           \
+    __ load_linked(i.OutputRegister(0), 0, i.TempRegister(0));                 \
+    __ ldi(gp, 1, zero_reg);                                                   \
+    __ wr_f(gp);                                                               \
+    __ bin_instr(i.TempRegister(1), i.OutputRegister(0),                       \
+                 Operand(i.InputRegister(2)));                                 \
+    __ Align(8);                                                               \
+    __ store_conditional(i.TempRegister(1), 0, i.TempRegister(0));             \
+    __ rd_f(i.TempRegister(1));                                                \
+    __ BranchShort(&binop, eq, i.TempRegister(1), Operand(zero_reg));          \
+    /*__ memb();*/                                                             \
+  } while (0)
+
+#define ASSEMBLE_ATOMIC_BINOP_EXT(load_linked, store_conditional, sign_extend, \
+                                  size, bin_instr, representation)             \
+  do {                                                                         \
+    Label binop;                                                               \
+    __ addl(i.InputRegister(0), i.InputRegister(1),i.TempRegister(0));         \
+    if (representation == 32) {                                                \
+      __ and_ins(i.TempRegister(0), 0x3,i.TempRegister(3));                    \
+    } else {                                                                   \
+      DCHECK_EQ(representation, 64);                                           \
+      __ and_ins(i.TempRegister(0), 0x7,i.TempRegister(3));                    \
+    }                                                                          \
+    __ Subl(i.TempRegister(0), i.TempRegister(0),                              \
+             Operand(i.TempRegister(3)));                                      \
+    __ slll(i.TempRegister(3), 3, i.TempRegister(3));                          \
+    /*__ memb();*/                                                             \
+    __ bind(&binop);                                                           \
+    __ load_linked(i.TempRegister(1), 0, i.TempRegister(0));                   \
+    __ ldi(gp, 1, zero_reg);                                                   \
+    __ wr_f(gp);                                                               \
+    __ ExtractBits(i.OutputRegister(0), i.TempRegister(1), i.TempRegister(3),  \
+                   size, sign_extend);                                         \
+    __ bin_instr(i.TempRegister(2), i.OutputRegister(0),                       \
+                 Operand(i.InputRegister(2)));                                 \
+    __ InsertBits(i.TempRegister(1), i.TempRegister(2), i.TempRegister(3),     \
+                  size);                                                       \
+    __ Align(8);                                                               \
+    __ store_conditional(i.TempRegister(1), 0, i.TempRegister(0));             \
+    __ rd_f(i.TempRegister(1));                                                \
+    __ BranchShort(&binop, eq, i.TempRegister(1), Operand(zero_reg));          \
+    /*__ memb();*/                                                             \
+  } while (0)
+
+#define ASSEMBLE_ATOMIC_EXCHANGE_INTEGER(load_linked, store_conditional)       \
+  do {                                                                         \
+    Label exchange;                                                            \
+    /*__ memb();*/                                                             \
+    __ bind(&exchange);                                                        \
+    __ addl(i.InputRegister(0), i.InputRegister(1),i.TempRegister(0));         \
+    __ load_linked(i.OutputRegister(0), 0, i.TempRegister(0));                 \
+    __ ldi(gp, 1, zero_reg);                                                   \
+    __ wr_f(gp);                                                               \
+    __ mov(i.TempRegister(1), i.InputRegister(2));                             \
+    __ Align(8);                                                               \
+    __ store_conditional(i.TempRegister(1), 0, i.TempRegister(0));             \
+    __ rd_f(i.TempRegister(1));                                                \
+    __ BranchShort(&exchange, eq, i.TempRegister(1), Operand(zero_reg));       \
+    /*__ memb();*/                                                             \
+  } while (0)
+
+#define ASSEMBLE_ATOMIC_EXCHANGE_INTEGER_EXT(                                  \
+    load_linked, store_conditional, sign_extend, size, representation)         \
+  do {                                                                         \
+    Label exchange;                                                            \
+    __ addl(i.InputRegister(0), i.InputRegister(1),i.TempRegister(0));         \
+    if (representation == 32) {                                                \
+      __ and_ins(i.TempRegister(0), 0x3,i.TempRegister(1));                    \
+    } else {                                                                   \
+      DCHECK_EQ(representation, 64);                                           \
+      __ and_ins(i.TempRegister(0), 0x7,i.TempRegister(1));                    \
+    }                                                                          \
+    __ Subl(i.TempRegister(0), i.TempRegister(0),                              \
+             Operand(i.TempRegister(1)));                                      \
+    __ Sllw(i.TempRegister(1), i.TempRegister(1), 3);                          \
+    /*__ memb();*/                                                             \
+    __ bind(&exchange);                                                        \
+    __ load_linked(i.TempRegister(2), 0, i.TempRegister(0));                   \
+    __ ldi(gp, 1, zero_reg);                                                   \
+    __ wr_f(gp);                                                               \
+    __ ExtractBits(i.OutputRegister(0), i.TempRegister(2), i.TempRegister(1),  \
+                   size, sign_extend);                                         \
+    __ InsertBits(i.TempRegister(2), i.InputRegister(2), i.TempRegister(1),    \
+                  size);                                                       \
+    __ Align(8);                                                               \
+    __ store_conditional(i.TempRegister(2), 0, i.TempRegister(0));             \
+    __ rd_f(i.TempRegister(2));                                                \
+    __ BranchShort(&exchange, eq, i.TempRegister(2), Operand(zero_reg));       \
+    /*__ memb();*/                                                             \
+  } while (0)
+
+
+#define ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER(load_linked,                  \
+                                                 store_conditional)            \
+  do {                                                                         \
+    Label compareExchange;                                                     \
+    Label exit;                                                                \
+    __ addl(i.InputRegister(0), i.InputRegister(1),i.TempRegister(0));         \
+    /*__ memb();*/                                                             \
+    __ bind(&compareExchange);                                                 \
+    __ load_linked(i.OutputRegister(0), 0, i.TempRegister(0));                 \
+    __ cmpeq(i.OutputRegister(0), i.InputRegister(2), i.TempRegister(1));      \
+    __ wr_f(i.TempRegister(1));                                                \
+    __ mov(i.TempRegister(2), i.InputRegister(3));                             \
+    __ Align(8);                                                               \
+    __ store_conditional(i.TempRegister(2), 0, i.TempRegister(0));             \
+    __ rd_f(i.TempRegister(2));                                                \
+    __ beq(i.TempRegister(1), &exit);                                          \
+    __ beq(i.TempRegister(2), &compareExchange);                               \
+    __ bind(&exit);                                                            \
+    /*__ memb();*/                                                             \
+  } while (0)
+
+
+#define ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(                          \
+    load_linked, store_conditional, sign_extend, size, representation)         \
+  do {                                                                         \
+    Label compareExchange;                                                     \
+    Label exit;                                                                \
+    __ addl(i.InputRegister(0), i.InputRegister(1),i.TempRegister(0));         \
+    if (representation == 32) {                                                \
+      __ and_ins(i.TempRegister(0), 0x3,i.TempRegister(1));                    \
+    } else {                                                                   \
+      DCHECK_EQ(representation, 64);                                           \
+      __ and_ins(i.TempRegister(0), 0x7,i.TempRegister(1));                    \
+    }                                                                          \
+    __ Subl(i.TempRegister(0), i.TempRegister(0),                              \
+             Operand(i.TempRegister(1)));                                      \
+    __ slll(i.TempRegister(1), 3, i.TempRegister(1));                          \
+    /*__ memb();*/                                                             \
+    __ bind(&compareExchange);                                                 \
+    __ load_linked(i.TempRegister(2), 0, i.TempRegister(0));                   \
+    __ mov(i.OutputRegister(0),i.TempRegister(2));                             \
+    __ cmpeq(i.OutputRegister(0), i.InputRegister(2), gp);                     \
+    __ ExtractBits(i.OutputRegister(0), i.TempRegister(2), i.TempRegister(1),  \
+                   size, sign_extend);                                         \
+    __ ExtractBits(i.InputRegister(2), i.InputRegister(2), i.TempRegister(1),  \
+                   size, sign_extend);                                         \
+    __ wr_f(gp);                                                               \
+    __ InsertBits(i.TempRegister(2), i.InputRegister(3), i.TempRegister(1),    \
+                  size);                                                       \
+    __ Align(8);                                                               \
+    __ store_conditional(i.TempRegister(2), 0, i.TempRegister(0));             \
+    __ rd_f(i.TempRegister(2));                                                \
+    __ beq(gp, &exit);                                                         \
+    __ beq(i.TempRegister(2), &compareExchange);                               \
+    __ bind(&exit);                                                            \
+    /*__ memb();*/                                                             \
+  } while (0)      
+
+
+#define ASSEMBLE_IEEE754_BINOP(name)                                        \
+  do {                                                                      \
+    FrameScope scope(tasm(), StackFrame::MANUAL);                           \
+    __ PrepareCallCFunction(0, 2, kScratchReg);                             \
+    __ MovToFloatParameters(i.InputDoubleRegister(0),                       \
+                            i.InputDoubleRegister(1));                      \
+    __ CallCFunction(ExternalReference::ieee754_##name##_function(), 0, 2); \
+    /* Move the result in the double result register. */                    \
+    __ MovFromFloatResult(i.OutputDoubleRegister());                        \
+  } while (0)
+
+#define ASSEMBLE_IEEE754_UNOP(name)                                         \
+  do {                                                                      \
+    FrameScope scope(tasm(), StackFrame::MANUAL);                           \
+    __ PrepareCallCFunction(0, 1, kScratchReg);                             \
+    __ MovToFloatParameter(i.InputDoubleRegister(0));                       \
+    __ CallCFunction(ExternalReference::ieee754_##name##_function(), 0, 1); \
+    /* Move the result in the double result register. */                    \
+    __ MovFromFloatResult(i.OutputDoubleRegister());                        \
+  } while (0)
+
+#define ASSEMBLE_F64X2_ARITHMETIC_BINOP(op)                     \
+  do {                                                          \
+    __ op(i.OutputSimd128Register(), i.InputSimd128Register(0), \
+          i.InputSimd128Register(1));                           \
+  } while (0)
+
+void CodeGenerator::AssembleDeconstructFrame() {
+  __ mov(sp, fp);
+  __ Pop(ra, fp);
+}
+
+void CodeGenerator::AssemblePrepareTailCall() {
+  if (frame_access_state()->has_frame()) {
+    __ Ldl(ra, MemOperand(fp, StandardFrameConstants::kCallerPCOffset));
+    __ Ldl(fp, MemOperand(fp, StandardFrameConstants::kCallerFPOffset));
+  }
+  frame_access_state()->SetFrameAccessToSP();
+}
+
+void CodeGenerator::AssemblePopArgumentsAdaptorFrame(Register args_reg,
+                                                     Register scratch1,
+                                                     Register scratch2,
+                                                     Register scratch3) {
+  DCHECK(!AreAliased(args_reg, scratch1, scratch2, scratch3));
+  Label done;
+
+  // Check if current frame is an arguments adaptor frame.
+  __ Ldl(scratch3, MemOperand(fp, StandardFrameConstants::kContextOffset));
+  __ Branch(&done, ne, scratch3,
+            Operand(StackFrame::TypeToMarker(StackFrame::ARGUMENTS_ADAPTOR)));
+
+  // Load arguments count from current arguments adaptor frame (note, it
+  // does not include receiver).
+  Register caller_args_count_reg = scratch1;
+  __ Ldl(caller_args_count_reg,
+        MemOperand(fp, ArgumentsAdaptorFrameConstants::kLengthOffset));
+  __ SmiUntag(caller_args_count_reg);
+
+  __ PrepareForTailCall(args_reg, caller_args_count_reg, scratch2, scratch3);
+  __ bind(&done);
+}
+
+namespace {
+
+void AdjustStackPointerForTailCall(TurboAssembler* tasm,
+                                   FrameAccessState* state,
+                                   int new_slot_above_sp,
+                                   bool allow_shrinkage = true) {
+  int current_sp_offset = state->GetSPToFPSlotCount() +
+                          StandardFrameConstants::kFixedSlotCountAboveFp;
+  int stack_slot_delta = new_slot_above_sp - current_sp_offset;
+  if (stack_slot_delta > 0) {
+    tasm->Subl(sp, sp, stack_slot_delta * kSystemPointerSize);
+    state->IncreaseSPDelta(stack_slot_delta);
+  } else if (allow_shrinkage && stack_slot_delta < 0) {
+    tasm->Addl(sp, sp, -stack_slot_delta * kSystemPointerSize);
+    state->IncreaseSPDelta(stack_slot_delta);
+  }
+}
+
+}  // namespace
+
+void CodeGenerator::AssembleTailCallBeforeGap(Instruction* instr,
+                                              int first_unused_stack_slot) {
+  AdjustStackPointerForTailCall(tasm(), frame_access_state(),
+                                first_unused_stack_slot, false);
+}
+
+void CodeGenerator::AssembleTailCallAfterGap(Instruction* instr,
+                                             int first_unused_stack_slot) {
+  AdjustStackPointerForTailCall(tasm(), frame_access_state(),
+                                first_unused_stack_slot);
+}
+
+// Check that {kJavaScriptCallCodeStartRegister} is correct.
+void CodeGenerator::AssembleCodeStartRegisterCheck() {
+  __ ComputeCodeStartAddress(kScratchReg);
+  __ Assert(eq, AbortReason::kWrongFunctionCodeStart,
+            kJavaScriptCallCodeStartRegister, Operand(kScratchReg));
+}
+
+// Check if the code object is marked for deoptimization. If it is, then it
+// jumps to the CompileLazyDeoptimizedCode builtin. In order to do this we need
+// to:
+//    1. read from memory the word that contains that bit, which can be found in
+//       the flags in the referenced {CodeDataContainer} object;
+//    2. test kMarkedForDeoptimizationBit in those flags; and
+//    3. if it is not zero then it jumps to the builtin.
+void CodeGenerator::BailoutIfDeoptimized() {
+  int offset = Code::kCodeDataContainerOffset - Code::kHeaderSize;
+  __ Ldl(kScratchReg, MemOperand(kJavaScriptCallCodeStartRegister, offset));
+  __ Ldw(kScratchReg,
+        FieldMemOperand(kScratchReg,
+                        CodeDataContainer::kKindSpecificFlagsOffset));
+  __ And(kScratchReg, kScratchReg,
+         Operand(1 << Code::kMarkedForDeoptimizationBit));
+  __ Jump(BUILTIN_CODE(isolate(), CompileLazyDeoptimizedCode),
+          RelocInfo::CODE_TARGET, ne, kScratchReg, Operand(zero_reg));
+}
+
+void CodeGenerator::GenerateSpeculationPoisonFromCodeStartRegister() {
+  // Calculate a mask which has all bits set in the normal case, but has all
+  // bits cleared if we are speculatively executing the wrong PC.
+  //    difference = (current - expected) | (expected - current)
+  //    poison = ~(difference >> (kBitsPerPointer - 1))
+  __ ComputeCodeStartAddress(kScratchReg);
+  __ Move(kSpeculationPoisonRegister, kScratchReg);
+  __ Subw(kSpeculationPoisonRegister, kSpeculationPoisonRegister,
+          Operand(kJavaScriptCallCodeStartRegister));
+  __ Subw(kJavaScriptCallCodeStartRegister, kJavaScriptCallCodeStartRegister,
+          Operand(kScratchReg));
+  __ or_ins(kSpeculationPoisonRegister, kJavaScriptCallCodeStartRegister,              kSpeculationPoisonRegister);
+  __ Sraw(kSpeculationPoisonRegister, kSpeculationPoisonRegister,
+         kBitsPerSystemPointer - 1);
+  __ ornot(zero_reg, kSpeculationPoisonRegister,
+         kSpeculationPoisonRegister);
+}
+
+void CodeGenerator::AssembleRegisterArgumentPoisoning() {
+  __ And(kJSFunctionRegister, kJSFunctionRegister, kSpeculationPoisonRegister);
+  __ And(kContextRegister, kContextRegister, kSpeculationPoisonRegister);
+  __ And(sp, sp, kSpeculationPoisonRegister);
+}
+
+// Assembles an instruction after register allocation, producing machine code.
+CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
+    Instruction* instr) {
+  Sw64OperandConverter i(this, instr);
+  InstructionCode opcode = instr->opcode();
+  ArchOpcode arch_opcode = ArchOpcodeField::decode(opcode);
+  switch (arch_opcode) {
+    case kArchCallCodeObject: {
+      if (instr->InputAt(0)->IsImmediate()) {
+        __ Call(i.InputCode(0), RelocInfo::CODE_TARGET);
+      } else {
+        Register reg = i.InputRegister(0);
+        DCHECK_IMPLIES(
+            HasCallDescriptorFlag(instr, CallDescriptor::kFixedTargetRegister),
+            reg == kJavaScriptCallCodeStartRegister);
+        __ addl(reg, Code::kHeaderSize - kHeapObjectTag, reg);
+        __ Call(reg);
+      }
+      RecordCallPosition(instr);
+      frame_access_state()->ClearSPDelta();
+      break;
+    }
+    case kArchCallBuiltinPointer: {
+      DCHECK(!instr->InputAt(0)->IsImmediate());
+      Register builtin_index = i.InputRegister(0);
+      __ CallBuiltinByIndex(builtin_index);
+      RecordCallPosition(instr);
+      frame_access_state()->ClearSPDelta();
+      break;
+    }
+    case kArchCallWasmFunction: {
+      if (arch_opcode == kArchTailCallCodeObjectFromJSFunction) {
+        AssemblePopArgumentsAdaptorFrame(kJavaScriptCallArgCountRegister,
+                                         i.TempRegister(0), i.TempRegister(1),
+                                         i.TempRegister(2));
+      }
+      if (instr->InputAt(0)->IsImmediate()) {
+        Constant constant = i.ToConstant(instr->InputAt(0));
+        Address wasm_code = static_cast<Address>(constant.ToInt64());
+        __ Call(wasm_code, constant.rmode());
+      } else {
+        __ addl(i.InputRegister(0), 0, kScratchReg);
+        __ Call(kScratchReg);
+      }
+      RecordCallPosition(instr);
+      frame_access_state()->ClearSPDelta();
+      break;
+    }
+    case kArchTailCallCodeObjectFromJSFunction:
+    case kArchTailCallCodeObject: {
+      if (arch_opcode == kArchTailCallCodeObjectFromJSFunction) {
+        AssemblePopArgumentsAdaptorFrame(kJavaScriptCallArgCountRegister,
+                                         i.TempRegister(0), i.TempRegister(1),
+                                         i.TempRegister(2));
+      }
+      if (instr->InputAt(0)->IsImmediate()) {
+        __ Jump(i.InputCode(0), RelocInfo::CODE_TARGET);
+      } else {
+        Register reg = i.InputRegister(0);
+        DCHECK_IMPLIES(
+            HasCallDescriptorFlag(instr, CallDescriptor::kFixedTargetRegister),
+            reg == kJavaScriptCallCodeStartRegister);
+        __ addl(reg, Code::kHeaderSize - kHeapObjectTag, reg);
+        __ Jump(reg);
+      }
+      frame_access_state()->ClearSPDelta();
+      frame_access_state()->SetFrameAccessToDefault();
+      break;
+    }
+    case kArchTailCallWasm: {
+      if (instr->InputAt(0)->IsImmediate()) {
+        Constant constant = i.ToConstant(instr->InputAt(0));
+        Address wasm_code = static_cast<Address>(constant.ToInt64());
+        __ Jump(wasm_code, constant.rmode());
+      } else {
+        __ addl(i.InputRegister(0), 0, kScratchReg);
+        __ Jump(kScratchReg);
+      }
+      frame_access_state()->ClearSPDelta();
+      frame_access_state()->SetFrameAccessToDefault();
+      break;
+    }
+    case kArchTailCallAddress: {
+      CHECK(!instr->InputAt(0)->IsImmediate());
+      Register reg = i.InputRegister(0);
+      DCHECK_IMPLIES(
+          HasCallDescriptorFlag(instr, CallDescriptor::kFixedTargetRegister),
+          reg == kJavaScriptCallCodeStartRegister);
+      __ Jump(reg);
+      frame_access_state()->ClearSPDelta();
+      frame_access_state()->SetFrameAccessToDefault();
+      break;
+    }
+    case kArchCallJSFunction: {
+      Register func = i.InputRegister(0);
+      if (FLAG_debug_code) {
+        // Check the function's context matches the context argument.
+        __ Ldl(kScratchReg, FieldMemOperand(func, JSFunction::kContextOffset));
+        __ Assert(eq, AbortReason::kWrongFunctionContext, cp,
+                  Operand(kScratchReg));
+      }
+      static_assert(kJavaScriptCallCodeStartRegister == a2, "ABI mismatch");
+      __ Ldl(a2, FieldMemOperand(func, JSFunction::kCodeOffset));
+      __ Addl(a2, a2, Operand(Code::kHeaderSize - kHeapObjectTag));
+      __ Call(a2);
+      RecordCallPosition(instr);
+      frame_access_state()->ClearSPDelta();
+      break;
+    }
+    case kArchPrepareCallCFunction: {
+      int const num_parameters = MiscField::decode(instr->opcode());
+      __ PrepareCallCFunction(num_parameters, kScratchReg);
+      // Frame alignment requires using FP-relative frame addressing.
+      frame_access_state()->SetFrameAccessToFP();
+      break;
+    }
+    case kArchSaveCallerRegisters: {
+      fp_mode_ =
+          static_cast<SaveFPRegsMode>(MiscField::decode(instr->opcode()));
+      DCHECK(fp_mode_ == kDontSaveFPRegs || fp_mode_ == kSaveFPRegs);
+      // kReturnRegister0 should have been saved before entering the stub.
+      int bytes = __ PushCallerSaved(fp_mode_, kReturnRegister0);
+      DCHECK(IsAligned(bytes, kSystemPointerSize));
+      DCHECK_EQ(0, frame_access_state()->sp_delta());
+      frame_access_state()->IncreaseSPDelta(bytes / kSystemPointerSize);
+      DCHECK(!caller_registers_saved_);
+      caller_registers_saved_ = true;
+      break;
+    }
+    case kArchRestoreCallerRegisters: {
+      DCHECK(fp_mode_ ==
+             static_cast<SaveFPRegsMode>(MiscField::decode(instr->opcode())));
+      DCHECK(fp_mode_ == kDontSaveFPRegs || fp_mode_ == kSaveFPRegs);
+      // Don't overwrite the returned value.
+      int bytes = __ PopCallerSaved(fp_mode_, kReturnRegister0);
+      frame_access_state()->IncreaseSPDelta(-(bytes / kSystemPointerSize));
+      DCHECK_EQ(0, frame_access_state()->sp_delta());
+      DCHECK(caller_registers_saved_);
+      caller_registers_saved_ = false;
+      break;
+    }
+    case kArchPrepareTailCall:
+      AssemblePrepareTailCall();
+      break;
+    case kArchCallCFunction: {
+      int const num_parameters = MiscField::decode(instr->opcode());
+      Label start_call;
+      bool isWasmCapiFunction =
+          linkage()->GetIncomingDescriptor()->IsWasmCapiFunction();
+      // from start_call to return address.
+      int offset = __ root_array_available() ? 76 : 88;
+#if V8_HOST_ARCH_SW64
+      if (__ emit_debug_code()) {
+        offset += 16;
+      }
+#endif
+      if (isWasmCapiFunction) {
+        // Put the return address in a stack slot.
+        __ mov(kScratchReg, ra);
+        __ bind(&start_call);
+        __ br(ra, 0);  // __ nal(); // __ nop();
+        __ Addl(ra, ra, offset - 4); // 4 = br(ra, 0);
+        __ stl(ra, MemOperand(fp, WasmExitFrameConstants::kCallingPCOffset));
+        __ mov(ra, kScratchReg);
+      }
+      if (instr->InputAt(0)->IsImmediate()) {
+        ExternalReference ref = i.InputExternalReference(0);
+        __ CallCFunction(ref, num_parameters);
+      } else {
+        Register func = i.InputRegister(0);
+        __ CallCFunction(func, num_parameters);
+      }
+      if (isWasmCapiFunction) {
+        CHECK_EQ(offset, __ SizeOfCodeGeneratedSince(&start_call));
+        RecordSafepoint(instr->reference_map(), Safepoint::kNoLazyDeopt);
+      }
+
+      frame_access_state()->SetFrameAccessToDefault();
+      // Ideally, we should decrement SP delta to match the change of stack
+      // pointer in CallCFunction. However, for certain architectures (e.g.
+      // ARM), there may be more strict alignment requirement, causing old SP
+      // to be saved on the stack. In those cases, we can not calculate the SP
+      // delta statically.
+      frame_access_state()->ClearSPDelta();
+      if (caller_registers_saved_) {
+        // Need to re-memb SP delta introduced in kArchSaveCallerRegisters.
+        // Here, we assume the sequence to be:
+        //   kArchSaveCallerRegisters;
+        //   kArchCallCFunction;
+        //   kArchRestoreCallerRegisters;
+        int bytes =
+            __ RequiredStackSizeForCallerSaved(fp_mode_, kReturnRegister0);
+        frame_access_state()->IncreaseSPDelta(bytes / kSystemPointerSize);
+      }
+      break;
+    }
+    case kArchJmp:
+      AssembleArchJump(i.InputRpo(0));
+      break;
+    case kArchBinarySearchSwitch:
+      AssembleArchBinarySearchSwitch(instr);
+      break;
+    case kArchTableSwitch:
+      AssembleArchTableSwitch(instr);
+      break;
+    case kArchAbortCSAAssert:
+      DCHECK(i.InputRegister(0) == a0);
+      {
+        // We don't actually want to generate a pile of code for this, so just
+        // claim there is a stack frame, without generating one.
+        FrameScope scope(tasm(), StackFrame::NONE);
+        __ Call(
+            isolate()->builtins()->builtin_handle(Builtins::kAbortCSAAssert),
+                RelocInfo::CODE_TARGET);
+      }
+      __ halt();//stop("kArchDebugAbort");
+      break;
+    case kArchDebugBreak:
+      __ halt();//stop("kArchDebugBreak");
+      break;
+    case kArchComment:
+      __ RecordComment(reinterpret_cast<const char*>(i.InputInt64(0)));
+      break;
+    case kArchNop:
+    case kArchThrowTerminator:
+      // don't emit code for nops.
+      break;
+    case kArchDeoptimize: {
+      DeoptimizationExit* exit =
+          BuildTranslation(instr, -1, 0, OutputFrameStateCombine::Ignore());
+      CodeGenResult result = AssembleDeoptimizerCall(exit);
+      if (result != kSuccess) return result;
+      break;
+    }
+    case kArchRet:
+      AssembleReturn(instr->InputAt(0));
+      break;
+    case kArchStackPointerGreaterThan:
+      // Pseudo-instruction used for cmp/branch. No opcode emitted here.
+      break;
+    case kArchStackCheckOffset:
+      __ Move(i.OutputRegister(), Smi::FromInt(GetStackCheckOffset()));
+      break;
+    case kArchFramePointer:
+      __ mov(i.OutputRegister(), fp);
+      break;
+    case kArchParentFramePointer:
+      if (frame_access_state()->has_frame()) {
+        __ Ldl(i.OutputRegister(), MemOperand(fp, 0));
+      } else {
+        __ mov(i.OutputRegister(), fp);
+      }
+      break;
+    case kArchTruncateDoubleToI:
+      __ TruncateDoubleToI(isolate(), zone(), i.OutputRegister(),
+                           i.InputDoubleRegister(0), DetermineStubCallMode());
+      break;
+    case kArchStoreWithWriteBarrier: {
+      RecordWriteMode mode =
+          static_cast<RecordWriteMode>(MiscField::decode(instr->opcode()));
+      Register object = i.InputRegister(0);
+      Register index = i.InputRegister(1);
+      Register value = i.InputRegister(2);
+      Register scratch0 = i.TempRegister(0);
+      Register scratch1 = i.TempRegister(1);
+      auto ool = zone()->New<OutOfLineRecordWrite>(this, object, index, value,
+                                                   scratch0, scratch1, mode,
+                                                   DetermineStubCallMode());
+      __ Addl(kScratchReg, object, index);
+      __ Stl(value, MemOperand(kScratchReg));
+      __ CheckPageFlag(object, scratch0,
+                       MemoryChunk::kPointersFromHereAreInterestingMask, ne,
+                       ool->entry());
+      __ bind(ool->exit());
+      break;
+    }
+    case kArchStackSlot: {
+      FrameOffset offset =
+          frame_access_state()->GetFrameOffset(i.InputInt32(0));
+      Register base_reg = offset.from_stack_pointer() ? sp : fp;
+      __ Addl(i.OutputRegister(), base_reg, Operand(offset.offset()));
+      int alignment = i.InputInt32(1);
+      DCHECK(alignment == 0 || alignment == 4 || alignment == 8 ||
+             alignment == 16);
+      if (FLAG_debug_code && alignment > 0) {
+        // Verify that the output_register is properly aligned
+        __ And(kScratchReg, i.OutputRegister(),
+               Operand(kSystemPointerSize - 1));
+        __ Assert(eq, AbortReason::kAllocationIsNotDoubleAligned, kScratchReg,
+                  Operand(zero_reg));
+      }
+      if (alignment == 2 * kSystemPointerSize) {
+        Label done;
+        __ Addl(kScratchReg, base_reg, Operand(offset.offset()));
+        __ And(kScratchReg, kScratchReg, Operand(alignment - 1));
+        __ BranchShort(&done, eq, kScratchReg, Operand(zero_reg));
+        __ Addl(i.OutputRegister(), i.OutputRegister(), kSystemPointerSize);
+        __ bind(&done);
+      } else if (alignment > 2 * kSystemPointerSize) {
+        Label done;
+        __ Addl(kScratchReg, base_reg, Operand(offset.offset()));
+        __ And(kScratchReg, kScratchReg, Operand(alignment - 1));
+        __ BranchShort(&done, eq, kScratchReg, Operand(zero_reg));
+        __ li(kScratchReg2, alignment);
+        __ Subl(kScratchReg2, kScratchReg2, Operand(kScratchReg));
+        __ Addl(i.OutputRegister(), i.OutputRegister(), kScratchReg2);
+        __ bind(&done);
+      }
+
+      break;
+    }
+    case kArchWordPoisonOnSpeculation:
+      __ And(i.OutputRegister(), i.InputRegister(0),
+             kSpeculationPoisonRegister);
+      break;
+    case kIeee754Float64Acos:
+      ASSEMBLE_IEEE754_UNOP(acos);
+      break;
+    case kIeee754Float64Acosh:
+      ASSEMBLE_IEEE754_UNOP(acosh);
+      break;
+    case kIeee754Float64Asin:
+      ASSEMBLE_IEEE754_UNOP(asin);
+      break;
+    case kIeee754Float64Asinh:
+      ASSEMBLE_IEEE754_UNOP(asinh);
+      break;
+    case kIeee754Float64Atan:
+      ASSEMBLE_IEEE754_UNOP(atan);
+      break;
+    case kIeee754Float64Atanh:
+      ASSEMBLE_IEEE754_UNOP(atanh);
+      break;
+    case kIeee754Float64Atan2:
+      ASSEMBLE_IEEE754_BINOP(atan2);
+      break;
+    case kIeee754Float64Cos:
+      ASSEMBLE_IEEE754_UNOP(cos);
+      break;
+    case kIeee754Float64Cosh:
+      ASSEMBLE_IEEE754_UNOP(cosh);
+      break;
+    case kIeee754Float64Cbrt:
+      ASSEMBLE_IEEE754_UNOP(cbrt);
+      break;
+    case kIeee754Float64Exp:
+      ASSEMBLE_IEEE754_UNOP(exp);
+      break;
+    case kIeee754Float64Expm1:
+      ASSEMBLE_IEEE754_UNOP(expm1);
+      break;
+    case kIeee754Float64Log:
+      ASSEMBLE_IEEE754_UNOP(log);
+      break;
+    case kIeee754Float64Log1p:
+      ASSEMBLE_IEEE754_UNOP(log1p);
+      break;
+    case kIeee754Float64Log2:
+      ASSEMBLE_IEEE754_UNOP(log2);
+      break;
+    case kIeee754Float64Log10:
+      ASSEMBLE_IEEE754_UNOP(log10);
+      break;
+    case kIeee754Float64Pow: 
+      ASSEMBLE_IEEE754_BINOP(pow);
+      break;
+    case kIeee754Float64Sin:
+      ASSEMBLE_IEEE754_UNOP(sin);
+      break;
+    case kIeee754Float64Sinh:
+      ASSEMBLE_IEEE754_UNOP(sinh);
+      break;
+    case kIeee754Float64Tan:
+      ASSEMBLE_IEEE754_UNOP(tan);
+      break;
+    case kIeee754Float64Tanh:
+      ASSEMBLE_IEEE754_UNOP(tanh);
+      break;
+    case kSw64Add:
+      __ Addw(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      break;
+    case kSw64Dadd:
+      __ Addl(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      break;
+    case kSw64DaddOvf:
+      __ DaddOverflow(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1),
+                      kScratchReg);
+      break;
+    case kSw64Sub:
+      __ Subw(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      break;
+    case kSw64Dsub:
+      __ Subl(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      break;
+    case kSw64DsubOvf:
+      __ DsubOverflow(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1),
+                      kScratchReg);
+      break;
+    case kSw64Mul:
+      __ Mulw(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      break;
+    case kSw64MulOvf:
+      __ MulOverflow(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1),
+                     kScratchReg);
+      break;
+    case kSw64MulHigh:
+      __ Mulwh(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      break;
+    case kSw64MulHighU:
+      __ Mulhu(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      break;
+    case kSw64DMulHigh:
+      __ Dmulh(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      break;
+    case kSw64Div: {
+      __ Divw(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      break;
+    }
+    case kSw64DivU: {
+      __ Divwu(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      break;
+    }
+    case kSw64Mod: {
+      __ Modw(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      break;
+    }
+    case kSw64ModU: {
+      __ Modwu(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      break;
+    }
+    case kSw64Dmul:
+      __ Mull(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      break;
+    case kSw64Ddiv: {
+      Label ldiv, exit;
+      __ slll(i.InputRegister(0), 0xb, at);
+      __ sral(at, 0xb, at);
+      __ cmpeq(i.InputRegister(0), at, at);
+      __ beq(at, &ldiv);
+      __ Divl(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      __ beq(zero_reg, &exit);
+
+      __ bind(&ldiv);
+      RegList saved_regs = (kJSCallerSaved | ra.bit()) & (~(i.OutputRegister().bit())) ;
+      __ MultiPush(saved_regs);
+      FrameScope scope(tasm(), StackFrame::MANUAL);
+      __ PrepareCallCFunction(2, 0, kScratchReg);
+      __ MovToGeneralParameters(i.InputRegister(0), i.InputRegister(1));
+      __ CallCFunction(ExternalReference::math_sw_ddiv_function(), 2, 0);
+      __ MovFromGeneralResult(i.OutputRegister());
+      __ MultiPop(saved_regs);
+
+      __ bind(&exit);
+      break;
+    }
+    case kSw64DdivU: {
+      Label ldivu, exit;
+      __ blt(i.InputRegister(0), &ldivu);
+      __ blt(i.InputRegister(1), &ldivu);
+      __ srll(i.InputRegister(0), 0x35, at);
+      __ bne(at, &ldivu);
+      __ Divlu(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      __ beq(zero_reg, &exit);
+
+      __ bind(&ldivu);
+      RegList saved_regs = (kJSCallerSaved | ra.bit()) & (~(i.OutputRegister().bit())) ;
+      __ MultiPush(saved_regs);
+      FrameScope scope(tasm(), StackFrame::MANUAL);
+      __ PrepareCallCFunction(2, 0, kScratchReg);
+      __ MovToGeneralParameters(i.InputRegister(0), i.InputRegister(1));
+      __ CallCFunction(ExternalReference::math_sw_ddivu_function(), 2, 0);
+      __ MovFromGeneralResult(i.OutputRegister());
+      __ MultiPop(saved_regs);
+
+      __ bind(&exit);
+      break;
+    }
+    case kSw64Dmod: {
+      Label modl, exit;
+      __ slll(i.InputRegister(0), 0xb, at);
+      __ sral(at, 0xb, at);
+      __ cmpeq(i.InputRegister(0), at, at);
+      __ beq(at, &modl);
+      __ Modl(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      __ beq(zero_reg, &exit);
+
+      __ bind(&modl);
+      RegList saved_regs = (kJSCallerSaved | ra.bit()) & (~(i.OutputRegister().bit())) ;
+      __ MultiPush(saved_regs);
+      FrameScope scope(tasm(), StackFrame::MANUAL);
+      __ PrepareCallCFunction(2, 0, kScratchReg);
+      __ MovToGeneralParameters(i.InputRegister(0), i.InputRegister(1));
+      __ CallCFunction(ExternalReference::math_sw_dmod_function(), 2, 0);
+      __ MovFromGeneralResult(i.OutputRegister());
+      __ MultiPop(saved_regs);
+
+      __ bind(&exit);
+      break;
+    }
+    case kSw64DmodU: {
+      Label modlu, exit;
+      __ blt(i.InputRegister(0), &modlu);
+      __ blt(i.InputRegister(1), &modlu);
+      __ srll(i.InputRegister(0), 0x35, at);
+      __ bne(at, &modlu);
+      __ Modlu(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      __ beq(zero_reg, &exit);
+
+      __ bind(&modlu);
+      RegList saved_regs = (kJSCallerSaved | ra.bit()) & (~(i.OutputRegister().bit())) ;
+      __ MultiPush(saved_regs);
+      FrameScope scope(tasm(), StackFrame::MANUAL);
+      __ PrepareCallCFunction(2, 0, kScratchReg);
+      __ MovToGeneralParameters(i.InputRegister(0), i.InputRegister(1));
+      __ CallCFunction(ExternalReference::math_sw_dmodu_function(), 2, 0);
+      __ MovFromGeneralResult(i.OutputRegister());
+      __ MultiPop(saved_regs);
+      __ beq(zero_reg, &exit);
+
+      __ bind(&exit);
+      break;
+    }
+    case kSw64Dlsa:
+      DCHECK(instr->InputAt(2)->IsImmediate());
+      __ Dlsa(i.OutputRegister(), i.InputRegister(0), i.InputRegister(1),
+              i.InputInt8(2));
+      break;
+    case kSw64Lsa:
+      DCHECK(instr->InputAt(2)->IsImmediate());
+      __ Lsa(i.OutputRegister(), i.InputRegister(0), i.InputRegister(1),
+             i.InputInt8(2));
+      break;
+    case kSw64And:
+      __ And(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      break;
+    case kSw64And32:
+      __ And(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      __ addw(i.OutputRegister(), 0x0, i.OutputRegister());
+      break;
+    case kSw64Or:
+      __ Or(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      break;
+    case kSw64Or32:
+      __ Or(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      __ addw(i.OutputRegister(), 0x0, i.OutputRegister());
+      break;
+    case kSw64Nor:
+      if (instr->InputAt(1)->IsRegister()) {
+        __ Nor(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      } else {
+        DCHECK_EQ(0, i.InputOperand(1).immediate());
+        __ ornot(zero_reg, i.InputRegister(0), i.OutputRegister());
+      }
+      break;
+    case kSw64Nor32:
+      if (instr->InputAt(1)->IsRegister()) {
+        __ Nor(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+	__ addw(i.OutputRegister(), 0x0, i.OutputRegister());
+      } else {
+        DCHECK_EQ(0, i.InputOperand(1).immediate());
+        __ ornot(zero_reg, i.InputRegister(0), i.OutputRegister());
+	__ addw(i.OutputRegister(), 0x0, i.OutputRegister());
+      }
+      break;
+    case kSw64Xor:
+      __ Xor(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      break;
+    case kSw64Xor32:
+      __ Xor(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      __ addw(i.OutputRegister(), 0x0, i.OutputRegister());
+      break;
+    case kSw64Clz:
+      __ Clz(i.OutputRegister(), i.InputRegister(0));
+      break;
+    case kSw64Dclz:
+      __ ctlz(i.InputRegister(0),i.OutputRegister());
+      break;
+    case kSw64Ctz: {
+      Register src = i.InputRegister(0);
+      Register dst = i.OutputRegister();
+      __ Ctz(dst, src);
+    } break;
+    case kSw64Dctz: {
+      Register src = i.InputRegister(0);
+      Register dst = i.OutputRegister();
+      __ Dctz(dst, src);
+    } break;
+    case kSw64Popcnt: {
+      Register src = i.InputRegister(0);
+      Register dst = i.OutputRegister();
+      __ Popcnt(dst, src);
+    } break;
+    case kSw64Dpopcnt: {
+      Register src = i.InputRegister(0);
+      Register dst = i.OutputRegister();
+      __ Dpopcnt(dst, src);
+    } break;
+    case kSw64Shl:
+      if (instr->InputAt(1)->IsRegister()) {
+        __ Sllw(i.OutputRegister(), i.InputRegister(0), i.InputRegister(1));
+      } else {
+        int64_t imm = i.InputOperand(1).immediate();
+        if (imm == 0) {
+          __ addw(i.InputRegister(0), 0, i.OutputRegister());
+        } else {
+        __ Sllw(i.OutputRegister(), i.InputRegister(0),
+               static_cast<uint16_t>(imm));
+        }
+      }
+      break;
+    case kSw64Shr:
+      if (instr->InputAt(1)->IsRegister()) {
+        __ Srlw(i.OutputRegister(), i.InputRegister(0), i.InputRegister(1));
+      } else {
+        int64_t imm = i.InputOperand(1).immediate();
+        __ Srlw(i.OutputRegister(), i.InputRegister(0),
+               static_cast<uint16_t>(imm));
+      }
+      break;
+    case kSw64Sar:
+      if (instr->InputAt(1)->IsRegister()) {
+        __ Sraw(i.OutputRegister(), i.InputRegister(0), i.InputRegister(1));
+      } else {
+        int64_t imm = i.InputOperand(1).immediate();
+        __ Sraw(i.OutputRegister(), i.InputRegister(0),
+               static_cast<uint16_t>(imm));
+      }
+      break;
+    case kSw64Ext:
+      __ Ext(i.OutputRegister(), i.InputRegister(0), i.InputInt8(1),
+             i.InputInt8(2));
+      break;
+    case kSw64Ins:
+      if (instr->InputAt(1)->IsImmediate() && i.InputInt8(1) == 0) {
+//        __ Ins(i.OutputRegister(), zero_reg, i.InputInt8(1), i.InputInt8(2));
+        if(i.InputInt8(2) == 8){
+          __ zap(i.OutputRegister(), 0x1, i.OutputRegister());
+          __ addw(i.OutputRegister(), 0, i.OutputRegister());
+        }else if(i.InputInt8(2) == 16){
+          __ zap(i.OutputRegister(), 0x3, i.OutputRegister());
+          __ addw(i.OutputRegister(), 0, i.OutputRegister());
+        }else if(i.InputInt8(2) == 24){
+          __ zap(i.OutputRegister(), 0x7, i.OutputRegister());
+          __ addw(i.OutputRegister(), 0, i.OutputRegister());
+        }else if(i.InputInt8(2) == 32){
+          __ bis(i.OutputRegister(), zero_reg, i.OutputRegister());      
+        }else {
+          long bitsize = (0x1L << i.InputInt8(2)) - 1;
+          if (is_uint8(bitsize)) {
+            __ bic(i.OutputRegister(), bitsize, i.OutputRegister());
+          } else {
+          __ li(t11, bitsize);
+          __ bic(i.OutputRegister(), t11, i.OutputRegister());
+          }
+        __ addw(i.OutputRegister(), 0, i.OutputRegister());
+        }
+      } else {
+        __ Ins(i.OutputRegister(), i.InputRegister(0), i.InputInt8(1),
+               i.InputInt8(2));
+      }
+      break;
+    case kSw64Dext: {
+      __ Dext(i.OutputRegister(), i.InputRegister(0), i.InputInt8(1),
+              i.InputInt8(2));
+      break;
+    }
+    case kSw64Dins:
+      if (instr->InputAt(1)->IsImmediate() && i.InputInt8(1) == 0) {
+//        __ Dins(i.OutputRegister(), zero_reg, i.InputInt8(1), i.InputInt8(2));
+        if(i.InputInt8(2) == 8){
+          __ zap(i.OutputRegister(), 0x1, i.OutputRegister());
+        }else if(i.InputInt8(2) == 16){
+          __ zap(i.OutputRegister(), 0x3, i.OutputRegister());
+        }else if(i.InputInt8(2) == 24){
+          __ zap(i.OutputRegister(), 0x7, i.OutputRegister());
+        }else if(i.InputInt8(2) == 32){
+          __ zap(i.OutputRegister(), 0xf, i.OutputRegister());    
+        }else {
+          long bitsize = (0x1L << i.InputInt8(2)) - 1;
+          if (is_uint8(bitsize)) {
+            __ bic(i.OutputRegister(), bitsize, i.OutputRegister());
+          } else {
+          __ li(t11, bitsize);
+          __ bic(i.OutputRegister(), t11, i.OutputRegister());
+          }
+        }          
+      } else {
+        __ Dins(i.OutputRegister(), i.InputRegister(0), i.InputInt8(1),
+                i.InputInt8(2));
+      }
+      break;
+    case kSw64Dshl:
+      if (instr->InputAt(1)->IsRegister()) {
+        __ slll(i.InputRegister(0), i.InputRegister(1),i.OutputRegister());
+      } else {
+        int64_t imm = i.InputOperand(1).immediate();
+        __ slll(i.InputRegister(0),
+                static_cast<uint16_t>(imm), i.OutputRegister());
+      }
+      break;
+    case kSw64Dshr:
+      if (instr->InputAt(1)->IsRegister()) {
+        __ srll(i.InputRegister(0), i.InputRegister(1), i.OutputRegister());
+      } else {
+        int64_t imm = i.InputOperand(1).immediate();
+        __ srll(i.InputRegister(0),
+                static_cast<uint16_t>(imm), i.OutputRegister());
+      }
+      break;
+    case kSw64Dsar:
+      if (instr->InputAt(1)->IsRegister()) {
+        __ sral(i.InputRegister(0), i.InputRegister(1), i.OutputRegister());
+      } else {
+        int64_t imm = i.InputOperand(1).immediate();
+        __ sral(i.InputRegister(0), imm , i.OutputRegister());
+      }
+      break;
+    case kSw64Ror:
+      __ Ror(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      break;
+    case kSw64Dror:
+      __ Dror(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      break;
+    case kSw64Tst:
+      __ And(kScratchReg, i.InputRegister(0), i.InputOperand(1));
+      // Pseudo-instruction used for cmp/branch. No opcode emitted here.
+      break;
+    case kSw64Cmp:
+      // Pseudo-instruction used for cmp/branch. No opcode emitted here.
+      break;
+    case kSw64Mov:
+      // TODO(plind): Should we combine mov/li like this, or use separate instr?
+      //    - Also see x64 ASSEMBLE_BINOP & RegisterOrOperandType
+      if (HasRegisterInput(instr, 0)) {
+        __ mov(i.OutputRegister(), i.InputRegister(0));
+      } else {
+        __ li(i.OutputRegister(), i.InputOperand(0));
+      }
+      break;
+
+    case kSw64CmpS: {
+      FPURegister left = i.InputOrZeroSingleRegister(0);
+      FPURegister right = i.InputOrZeroSingleRegister(1);
+      bool predicate;
+      FPUCondition cc =
+          FlagsConditionToConditionCmpFPU(&predicate, instr->flags_condition());
+      __ CompareF32(cc, left, right);
+    } break;
+    case kSw64AddS:
+      // TODO(plind): add special case: combine mult & add.
+      __ fadds(i.InputDoubleRegister(0),
+               i.InputDoubleRegister(1), i.OutputDoubleRegister());
+      break;
+    case kSw64SubS:
+      __ fsubs(i.InputDoubleRegister(0),
+               i.InputDoubleRegister(1), i.OutputDoubleRegister());
+      break;
+    case kSw64MulS:
+      // TODO(plind): add special case: right op is -1.0, see arm port.
+      __ fmuls(i.InputDoubleRegister(0),
+               i.InputDoubleRegister(1), i.OutputDoubleRegister());
+      break;
+    case kSw64DivS:
+      __ fdivs(i.InputDoubleRegister(0),
+               i.InputDoubleRegister(1), i.OutputDoubleRegister());
+      break;
+    case kSw64ModS: {
+      // TODO(bmeurer): We should really get rid of this special instruction,
+      // and generate a CallAddress instruction instead.
+      FrameScope scope(tasm(), StackFrame::MANUAL);
+      __ PrepareCallCFunction(0, 2, kScratchReg);
+      __ MovToFloatParameters(i.InputDoubleRegister(0),
+                              i.InputDoubleRegister(1));
+      // TODO(balazs.kilvady): implement mod_two_floats_operation(isolate())
+      __ CallCFunction(ExternalReference::mod_two_doubles_operation(), 0, 2);
+      // Move the result in the double result register.
+      __ MovFromFloatResult(i.OutputSingleRegister());
+      break;
+    }
+    case kSw64AbsS:
+      __ Abs_sw(i.OutputSingleRegister(), i.InputSingleRegister(0));
+      break;
+    case kSw64NegS:
+      __ Fnegs(i.OutputSingleRegister(), i.InputSingleRegister(0));
+      break;
+    case kSw64SqrtS: {
+      __ fsqrts(i.InputDoubleRegister(0),i.OutputDoubleRegister());
+      break;
+    }
+    case kSw64MaxS:{
+    	UNREACHABLE();
+    }
+    case kSw64MinS:{
+	UNREACHABLE();		
+    }
+    case kSw64CmpD: {
+      FPURegister left = i.InputOrZeroDoubleRegister(0);
+      FPURegister right = i.InputOrZeroDoubleRegister(1);
+      bool predicate;
+      FPUCondition cc =
+          FlagsConditionToConditionCmpFPU(&predicate, instr->flags_condition());
+      __ CompareF64(cc, left, right);
+    } break;
+    case kSw64AddD:
+      // TODO(plind): add special case: combine mult & add.
+      __ faddd(i.InputDoubleRegister(0),
+               i.InputDoubleRegister(1), i.OutputDoubleRegister());
+      break;
+    case kSw64SubD:
+      __ fsubd(i.InputDoubleRegister(0),
+               i.InputDoubleRegister(1), i.OutputDoubleRegister());
+      break;
+    case kSw64MulD:
+      // TODO(plind): add special case: right op is -1.0, see arm port.
+      __ fmuld(i.InputDoubleRegister(0),
+               i.InputDoubleRegister(1), i.OutputDoubleRegister());
+      break;
+    case kSw64DivD:
+      __ fdivd(i.InputDoubleRegister(0),
+               i.InputDoubleRegister(1), i.OutputDoubleRegister());
+      break;
+    case kSw64ModD: {
+      // TODO(bmeurer): We should really get rid of this special instruction,
+      // and generate a CallAddress instruction instead.
+      FrameScope scope(tasm(), StackFrame::MANUAL);
+      __ PrepareCallCFunction(0, 2, kScratchReg);
+      __ MovToFloatParameters(i.InputDoubleRegister(0),
+                              i.InputDoubleRegister(1));
+      __ CallCFunction(ExternalReference::mod_two_doubles_operation(), 0, 2);
+      // Move the result in the double result register.
+      __ MovFromFloatResult(i.OutputDoubleRegister());
+      break;
+    }
+    case kSw64AbsD:
+      __ Abs_sw(i.OutputDoubleRegister(), i.InputDoubleRegister(0));
+      break;
+    case kSw64NegD:
+      __ Fnegd(i.OutputDoubleRegister(), i.InputDoubleRegister(0));
+      break;
+    case kSw64SqrtD: {
+      __ fsqrtd(i.InputDoubleRegister(0),i.OutputDoubleRegister());
+      break;
+    }
+    case kSw64MaxD:{
+    	UNREACHABLE();
+    }
+    case kSw64MinD: {
+        UNREACHABLE();
+    }
+    case kSw64Float64RoundDown: {
+      __ Floor_d_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0));
+      break;
+    }
+    case kSw64Float32RoundDown: {
+      __ Floor_s_s(i.OutputSingleRegister(), i.InputSingleRegister(0));
+      break;
+    }
+    case kSw64Float64RoundTruncate: {
+      __ Trunc_d_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0));
+      break;
+    }
+    case kSw64Float32RoundTruncate: {
+      __ Trunc_s_s(i.OutputSingleRegister(), i.InputSingleRegister(0));
+      break;
+    }
+    case kSw64Float64RoundUp: {
+      __ Ceil_d_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0));
+      break;
+    }
+    case kSw64Float32RoundUp: {
+      __ Ceil_s_s(i.OutputSingleRegister(), i.InputSingleRegister(0));
+      break;
+    }
+    case kSw64Float64RoundTiesEven: {
+      __ Round_d_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0));
+      break;
+    }
+    case kSw64Float32RoundTiesEven: {
+      __ Round_s_s(i.OutputSingleRegister(), i.InputSingleRegister(0));
+      break;
+    }
+    case kSw64Float32Max: {
+      FPURegister dst = i.OutputSingleRegister();
+      FPURegister src1 = i.InputSingleRegister(0);
+      FPURegister src2 = i.InputSingleRegister(1);
+      auto ool = zone()->New<OutOfLineFloat32Max>(this, dst, src1, src2);
+      __ Float32Max(dst, src1, src2, ool->entry());
+      __ bind(ool->exit());
+      break;
+    }
+    case kSw64Float64Max: {
+      FPURegister dst = i.OutputDoubleRegister();
+      FPURegister src1 = i.InputDoubleRegister(0);
+      FPURegister src2 = i.InputDoubleRegister(1);
+      auto ool = zone()->New<OutOfLineFloat64Max>(this, dst, src1, src2);
+      __ Float64Max(dst, src1, src2, ool->entry());
+      __ bind(ool->exit());
+      break;
+    }
+    case kSw64Float32Min: {
+      FPURegister dst = i.OutputSingleRegister();
+      FPURegister src1 = i.InputSingleRegister(0);
+      FPURegister src2 = i.InputSingleRegister(1);
+      auto ool = zone()->New<OutOfLineFloat32Min>(this, dst, src1, src2);
+      __ Float32Min(dst, src1, src2, ool->entry());
+      __ bind(ool->exit());
+      break;
+    }
+    case kSw64Float64Min: {
+      FPURegister dst = i.OutputDoubleRegister();
+      FPURegister src1 = i.InputDoubleRegister(0);
+      FPURegister src2 = i.InputDoubleRegister(1);
+      auto ool = zone()->New<OutOfLineFloat64Min>(this, dst, src1, src2);
+      __ Float64Min(dst, src1, src2, ool->entry());
+      __ bind(ool->exit());
+      break;
+    }
+    case kSw64Float64SilenceNaN:
+      __ FPUCanonicalizeNaN(i.OutputDoubleRegister(), i.InputDoubleRegister(0));
+      break;
+    case kSw64CvtSD:
+      __ fcvtds(i.InputDoubleRegister(0), i.OutputSingleRegister());
+      break;
+    case kSw64CvtDS:
+      __ fcvtsd(i.InputSingleRegister(0), i.OutputDoubleRegister());
+      break;
+    case kSw64CvtDW: {
+      FPURegister scratch = kScratchDoubleReg;
+      __ ifmovd(i.InputRegister(0), scratch);
+      __ fcvtld(scratch, i.OutputDoubleRegister());
+      break;
+    }
+    case kSw64CvtSW: {
+      FPURegister scratch = kScratchDoubleReg;
+      __ ifmovs(i.InputRegister(0), scratch);
+      __ fcvtws(scratch,i.OutputDoubleRegister());
+      break;
+    }
+    case kSw64CvtSUw: {
+      __ Cvt_s_uw(i.OutputDoubleRegister(), i.InputRegister(0));
+      break;
+    }
+    case kSw64CvtSL: {
+      FPURegister scratch = kScratchDoubleReg;
+      __ ifmovd(i.InputRegister(0), scratch);
+      __ fcvtls(scratch, i.OutputDoubleRegister());
+      break;
+    }
+    case kSw64CvtDL: {
+      FPURegister scratch = kScratchDoubleReg;
+      __ ifmovd(i.InputRegister(0), scratch);
+      __ fcvtld(scratch, i.OutputDoubleRegister());
+      break;
+    }
+    case kSw64CvtDUw: {
+      __ Cvt_d_uw(i.OutputDoubleRegister(), i.InputRegister(0));
+      break;
+    }
+    case kSw64CvtDUl: {
+      __ Cvt_d_ul(i.OutputDoubleRegister(), i.InputRegister(0));
+      break;
+    }
+    case kSw64CvtSUl: {
+      __ Cvt_s_ul(i.OutputDoubleRegister(), i.InputRegister(0));
+      break;
+    }
+    case kSw64FloorWD: {
+      FPURegister scratch = kScratchDoubleReg;
+      __ ffloordw(i.InputDoubleRegister(0),scratch);
+      __ fimovs(scratch, i.OutputRegister());
+      break;
+    }
+    case kSw64CeilWD: {
+      FPURegister scratch = kScratchDoubleReg;
+      __ fceildw(i.InputDoubleRegister(0),scratch);
+      __ fimovs(scratch, i.OutputRegister());
+      break;
+    }
+    case kSw64RoundWD: {
+      FPURegister scratch = kScratchDoubleReg;
+      __ frounddw(i.InputDoubleRegister(0), scratch);
+      __ fimovs(scratch, i.OutputRegister());
+      break;
+    }
+    case kSw64TruncWD: {
+      FPURegister scratch = kScratchDoubleReg;
+      // Other arches use round to zero here, so we follow.
+      __ ftruncdw(i.InputDoubleRegister(0), scratch);
+      __ fimovs(scratch, i.OutputRegister());
+      break;
+    }
+    case kSw64FloorWS: {
+      FPURegister scratch = kScratchDoubleReg;
+      __ ffloorsw(i.InputDoubleRegister(0),scratch);
+      __ fimovs(scratch,i.OutputRegister());
+      break;
+    }
+    case kSw64CeilWS: {
+      FPURegister scratch = kScratchDoubleReg;
+      __ fceilsw(i.InputDoubleRegister(0), scratch);
+      __ fimovs(scratch,i.OutputRegister());
+      break;
+    }
+    case kSw64RoundWS: {
+      FPURegister scratch = kScratchDoubleReg;
+      __ froundsw(i.InputDoubleRegister(0), scratch);
+      __ fimovs(scratch, i.OutputRegister());
+      break;
+    }
+    case kSw64TruncWS: {
+      FPURegister scratch = kScratchDoubleReg;
+      __ ftruncsw(i.InputDoubleRegister(0), scratch);
+      __ fimovs(scratch, i.OutputRegister());
+      // Avoid INT32_MAX as an overflow indicator and use INT32_MIN instead,
+      // because INT32_MIN allows easier out-of-bounds detection.
+      __ Addw(kScratchReg, i.OutputRegister(), Operand(1));
+      __ cmplt(kScratchReg, i.OutputRegister(), kScratchReg2);
+      __ Selne(i.OutputRegister(), kScratchReg, kScratchReg2);
+      break;
+    }
+    case kSw64TruncLS: {
+      FPURegister scratch = kScratchDoubleReg;
+      FPURegister scratch1 = kScratchDoubleReg1;
+      FPURegister scratch2 = kScratchDoubleReg2;
+      Register temp = kScratchReg;
+      Register result = kScratchReg2;
+
+      bool load_status = instr->OutputCount() > 1;
+      if (load_status) {
+#ifdef SW64
+        // Save FCSR.
+        __ rfpcr(scratch2);
+        // SW64 neednot clear FPCR in 20150513.
+        //in order to have same effection, we should do four steps in sw:
+        //1) set fpcr = 0
+        //2) Rounding: sw(10), round-to-even
+        //3) set trap bit: sw(62~61,51~49), exception controlled by fpcr but not trap
+        //4) set exception mode: sw(00) setfpec0
+        __ li(temp, sFCSRControlMask | sFCSRRound1Mask); //1), 2), 3)
+        __ ifmovd(temp, scratch1);
+        __ wfpcr(scratch1);
+        __ setfpec1();//4)
+#endif
+      }
+      // Other arches use round to zero here, so we follow.
+      __ fcvtsd(i.InputDoubleRegister(0), scratch);
+      __ fcvtdl_z(scratch, scratch1);
+      __ fimovd(scratch1, i.OutputRegister());
+      
+      if (load_status) {
+#ifdef SW64
+        __ rfpcr(scratch1);
+        __ fimovd(scratch1, result);
+
+        // Check for overflow and NaNs.
+        __ li(temp, sFCSROverflowFlagMask | sFCSRUnderflowFlagMask |
+                    sFCSRInvalidOpFlagMask);
+        __ and_ins(result, temp, result);
+        __ Cmplt(result, zero_reg, result);
+        __ xor_ins(result, 1, result);
+        __ mov(i.OutputRegister(1), result);
+        // Restore FCSR
+        __ wfpcr(scratch2);
+        __ setfpec1();
+#endif
+      }
+      break;
+    }
+    case kSw64TruncLD: {
+      FPURegister scratch = kScratchDoubleReg;
+      FPURegister scratch1 = kScratchDoubleReg1;
+      FPURegister scratch2 = kScratchDoubleReg2;
+      Register temp = kScratchReg;
+      Register result = kScratchReg2;
+
+      bool load_status = instr->OutputCount() > 1;
+      if (load_status) {
+#ifdef SW64
+        // Save FCSR.
+        __ rfpcr(scratch2);
+        // SW64 neednot clear FPCR in 20150513.
+        //in order to have same effection, we should do four steps in sw:
+        //1) set fpcr = 0
+        //2) Rounding: sw(10), round-to-even
+        //3) set trap bit: sw(62~61,51~49), exception controlled by fpcr but not trap
+        //4) set exception mode: sw(00) setfpec0
+        __ li(temp, sFCSRControlMask | sFCSRRound1Mask); //1), 2), 3)
+        __ ifmovd(temp, scratch1);
+        __ wfpcr(scratch1);
+        __ setfpec1();//4)
+#endif
+      }
+      // Other arches use round to zero here, so we follow.
+      __ ftruncdl(i.InputDoubleRegister(0), scratch);
+      __ fimovd(scratch,i.OutputRegister(0));
+      if (load_status) {
+#ifdef SW64
+        __ rfpcr(scratch1);
+        __ fimovd(scratch1, result);
+
+        // Check for overflow and NaNs.
+        __ li(temp, sFCSROverflowFlagMask | sFCSRUnderflowFlagMask |
+                    sFCSRInvalidOpFlagMask);
+        __ and_ins(result, temp, result);
+        __ Cmplt(result, zero_reg, result);
+        __ xor_ins(result, 1, result);
+        __ mov(i.OutputRegister(1), result);
+        // Restore FCSR
+        __ wfpcr(scratch2);
+        __ setfpec1();
+#endif
+      }
+      break;
+    }
+    case kSw64TruncUwD: {
+      FPURegister scratch = kScratchDoubleReg;
+      __ Trunc_uw_d(i.OutputRegister(), i.InputDoubleRegister(0), scratch);
+      break;
+    }
+    case kSw64TruncUwS: {
+      FPURegister scratch = kScratchDoubleReg;
+      __ Trunc_uw_s(i.OutputRegister(), i.InputDoubleRegister(0), scratch);
+      // Avoid UINT32_MAX as an overflow indicator and use 0 instead,
+      // because 0 allows easier out-of-bounds detection.
+      __ Addw(kScratchReg, i.OutputRegister(), Operand(1));
+      __ Seleq(i.OutputRegister(), zero_reg, kScratchReg);
+      break;
+    }
+    case kSw64TruncUlS: {
+      FPURegister scratch = kScratchDoubleReg;
+      Register result = instr->OutputCount() > 1 ? i.OutputRegister(1) : no_reg;
+      __ Trunc_ul_s(i.OutputRegister(), i.InputDoubleRegister(0), scratch,
+                    result);
+      break;
+    }
+    case kSw64TruncUlD: {
+      FPURegister scratch = kScratchDoubleReg;
+      Register result = instr->OutputCount() > 1 ? i.OutputRegister(1) : no_reg;
+      __ Trunc_ul_d(i.OutputRegister(0), i.InputDoubleRegister(0), scratch,
+                    result);
+      break;
+    }
+    case kSw64BitcastDL:    // D -> L
+      __ fimovd(i.InputDoubleRegister(0),i.OutputRegister());
+      break;
+    case kSw64BitcastLD:
+      __ ifmovd(i.InputRegister(0), i.OutputDoubleRegister());
+      break;
+#ifdef SW64
+    case kSw64BitcastSW:    // W -> S
+      __ ifmovs(i.InputRegister(0), i.OutputDoubleRegister()); 
+      break;
+    case kSw64BitcastWS:    // S -> W
+      __ fimovs(i.InputDoubleRegister(0), i.OutputRegister());
+      break;
+#endif
+    case kSw64Float64ExtractLowWord32:
+      __ FmoveLow(i.OutputRegister(), i.InputDoubleRegister(0));
+      break;
+    case kSw64Float64ExtractHighWord32:
+      __ FmoveHigh(i.OutputRegister(), i.InputDoubleRegister(0));
+      break;
+    case kSw64Float64InsertLowWord32:
+      __ FmoveLow(i.OutputDoubleRegister(), i.InputRegister(1));
+      break;
+    case kSw64Float64InsertHighWord32:
+      __ FmoveHigh(i.OutputDoubleRegister(), i.InputRegister(1));
+      break;
+    // ... more basic instructions ...
+
+    case kSw64Seb:
+      __ sextb(i.InputRegister(0), i.OutputRegister());
+      break;
+    case kSw64Seh:
+      __ sexth(i.InputRegister(0), i.OutputRegister());
+      break;
+    case kSw64Ldbu:
+      __ Ldbu(i.OutputRegister(), i.MemoryOperand());
+      EmitWordLoadPoisoningIfNeeded(this, opcode, instr, i);
+      break;
+    case kSw64Ldb:
+      __ Ldb(i.OutputRegister(), i.MemoryOperand());
+      EmitWordLoadPoisoningIfNeeded(this, opcode, instr, i);
+      break;
+    case kSw64Stb:
+      __ Stb(i.InputOrZeroRegister(2), i.MemoryOperand());
+      break;
+    case kSw64Ldhu:
+      __ Ldhu(i.OutputRegister(), i.MemoryOperand());
+      EmitWordLoadPoisoningIfNeeded(this, opcode, instr, i);
+      break;
+    case kSw64Uldhu:
+      __ Uldhu(i.OutputRegister(), i.MemoryOperand());
+      EmitWordLoadPoisoningIfNeeded(this, opcode, instr, i);
+      break;
+    case kSw64Ldh:
+      __ Ldh(i.OutputRegister(), i.MemoryOperand());
+      EmitWordLoadPoisoningIfNeeded(this, opcode, instr, i);
+      break;
+    case kSw64Uldh:
+      __ Uldh(i.OutputRegister(), i.MemoryOperand());
+      EmitWordLoadPoisoningIfNeeded(this, opcode, instr, i);
+      break;
+    case kSw64Sth:
+      __ Sth(i.InputOrZeroRegister(2), i.MemoryOperand());
+      break;
+    case kSw64Usth:
+      __ Usth(i.InputOrZeroRegister(2), i.MemoryOperand(), kScratchReg);
+      break;
+    case kSw64Ldw:
+      __ Ldw(i.OutputRegister(), i.MemoryOperand());
+      EmitWordLoadPoisoningIfNeeded(this, opcode, instr, i);
+      break;
+    case kSw64Uldw:
+      __ Uldw(i.OutputRegister(), i.MemoryOperand());
+      EmitWordLoadPoisoningIfNeeded(this, opcode, instr, i);
+      break;
+    case kSw64Ldwu:
+      __ Ldwu(i.OutputRegister(), i.MemoryOperand());
+      EmitWordLoadPoisoningIfNeeded(this, opcode, instr, i);
+      break;
+    case kSw64Uldwu:
+      __ Uldwu(i.OutputRegister(), i.MemoryOperand());
+      EmitWordLoadPoisoningIfNeeded(this, opcode, instr, i);
+      break;
+    case kSw64Ldl:
+      __ Ldl(i.OutputRegister(), i.MemoryOperand());
+      EmitWordLoadPoisoningIfNeeded(this, opcode, instr, i);
+      break;
+    case kSw64Uldl:
+      __ Uldl(i.OutputRegister(), i.MemoryOperand());
+      EmitWordLoadPoisoningIfNeeded(this, opcode, instr, i);
+      break;
+    case kSw64Stw:
+      __ Stw(i.InputOrZeroRegister(2), i.MemoryOperand());
+      break;
+    case kSw64Ustw:
+      __ Ustw(i.InputOrZeroRegister(2), i.MemoryOperand());
+      break;
+    case kSw64Stl:
+      __ Stl(i.InputOrZeroRegister(2), i.MemoryOperand());
+      break;
+    case kSw64Ustl:
+      __ Ustl(i.InputOrZeroRegister(2), i.MemoryOperand());
+      break;
+    case kSw64Flds: {
+      __ Flds(i.OutputSingleRegister(), i.MemoryOperand());
+      break;
+    }
+    case kSw64Uflds: {
+      __ Uflds(i.OutputSingleRegister(), i.MemoryOperand(), kScratchReg);
+      break;
+    }
+    case kSw64Fsts: {
+      size_t index = 0;
+      MemOperand operand = i.MemoryOperand(&index);
+      FPURegister ft = i.InputOrZeroSingleRegister(index);
+      __ Fsts(ft, operand);
+      break;
+    }
+    case kSw64Ufsts: {
+      size_t index = 0;
+      MemOperand operand = i.MemoryOperand(&index);
+      FPURegister ft = i.InputOrZeroSingleRegister(index);
+      __ Ufsts(ft, operand, kScratchReg);
+      break;
+    }
+    case kSw64Fldd:
+      __ Fldd(i.OutputDoubleRegister(), i.MemoryOperand());
+      break;
+    case kSw64Ufldd:
+      __ Ufldd(i.OutputDoubleRegister(), i.MemoryOperand(), kScratchReg);
+      break;
+    case kSw64Fstd: {
+      FPURegister ft = i.InputOrZeroDoubleRegister(2);
+      __ Fstd(ft, i.MemoryOperand());
+      break;
+    }
+    case kSw64Ufstd: {
+      FPURegister ft = i.InputOrZeroDoubleRegister(2);
+      __ Ufstd(ft, i.MemoryOperand(), kScratchReg);
+      break;
+    }
+    case kSw64Sync: {
+      __ memb();
+      break;
+    }
+    case kSw64Push:
+      if (instr->InputAt(0)->IsFPRegister()) {
+        __ Fstd(i.InputDoubleRegister(0), MemOperand(sp, -kDoubleSize));
+        __ Subw(sp, sp, Operand(kDoubleSize));
+        frame_access_state()->IncreaseSPDelta(kDoubleSize / kSystemPointerSize);
+      } else {
+        __ Push(i.InputRegister(0));
+        frame_access_state()->IncreaseSPDelta(1);
+      }
+      break;
+    case kSw64Peek: {
+      int reverse_slot = i.InputInt32(0);
+      int offset =
+          FrameSlotToFPOffset(frame()->GetTotalFrameSlotCount() - reverse_slot);
+      if (instr->OutputAt(0)->IsFPRegister()) {
+        LocationOperand* op = LocationOperand::cast(instr->OutputAt(0));
+        if (op->representation() == MachineRepresentation::kFloat64) {
+          __ Fldd(i.OutputDoubleRegister(), MemOperand(fp, offset));
+        } else if (op->representation() == MachineRepresentation::kFloat32) {
+          __ Flds(
+              i.OutputSingleRegister(0),
+              MemOperand(fp, offset + kLessSignificantWordInDoublewordOffset));
+        }
+      } else {
+        __ Ldl(i.OutputRegister(0), MemOperand(fp, offset));
+      }
+      break;
+    }
+    case kSw64StackClaim: {
+      __ Subl(sp, sp, Operand(i.InputInt32(0)));
+      frame_access_state()->IncreaseSPDelta(i.InputInt32(0) / kPointerSize);
+      break;
+    }
+    case kSw64StoreToStackSlot: {
+      if (instr->InputAt(0)->IsFPRegister()) {
+        if (instr->InputAt(0)->IsSimd128Register()) {
+          //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+          //__ st_b(i.InputSimd128Register(0), MemOperand(sp, i.InputInt32(1)));
+	  UNREACHABLE();
+        } else if (instr->InputAt(0)->IsDoubleRegister()) {
+          __ Fstd(i.InputDoubleRegister(0), MemOperand(sp, i.InputInt32(1)));
+        } else {
+          __ Fsts(i.InputFloatRegister(0), MemOperand(sp, i.InputInt32(1)));
+        }
+      } else {
+        __ Stl(i.InputRegister(0), MemOperand(sp, i.InputInt32(1)));
+      }
+      break;
+    }
+    case kSw64ByteSwap64: {
+      __ ByteSwapSigned(i.OutputRegister(0), i.InputRegister(0), 8);
+      break;
+    }
+    case kSw64ByteSwap32: {
+      __ ByteSwapSigned(i.OutputRegister(0), i.InputRegister(0), 4);
+//      __ srll(i.OutputRegister(0), 32 ,i.OutputRegister(0));
+      break;
+    }
+    case kWord32AtomicLoadInt8:
+      ASSEMBLE_ATOMIC_LOAD_INTEGER(Ldb);
+      break;
+    case kWord32AtomicLoadUint8:
+      ASSEMBLE_ATOMIC_LOAD_INTEGER(Ldbu);
+      break;
+    case kWord32AtomicLoadInt16:
+      ASSEMBLE_ATOMIC_LOAD_INTEGER(Ldh);
+      break;
+    case kWord32AtomicLoadUint16:
+      ASSEMBLE_ATOMIC_LOAD_INTEGER(Ldhu);
+      break;
+    case kWord32AtomicLoadWord32:
+      ASSEMBLE_ATOMIC_LOAD_INTEGER(Ldw);
+      break;
+    case kSw64Word64AtomicLoadUint8:
+      ASSEMBLE_ATOMIC_LOAD_INTEGER(Ldbu);
+      break;
+    case kSw64Word64AtomicLoadUint16:
+      ASSEMBLE_ATOMIC_LOAD_INTEGER(Ldhu);
+      break;
+    case kSw64Word64AtomicLoadUint32:
+      ASSEMBLE_ATOMIC_LOAD_INTEGER(Ldwu);
+      break;
+    case kSw64Word64AtomicLoadUint64:
+      ASSEMBLE_ATOMIC_LOAD_INTEGER(Ldl);
+      break;
+    case kWord32AtomicStoreWord8:
+      ASSEMBLE_ATOMIC_STORE_INTEGER(Stb);
+      break;
+    case kWord32AtomicStoreWord16:
+      ASSEMBLE_ATOMIC_STORE_INTEGER(Sth);
+      break;
+    case kWord32AtomicStoreWord32:
+      ASSEMBLE_ATOMIC_STORE_INTEGER(Stw);
+      break;
+    case kSw64Word64AtomicStoreWord8:
+      ASSEMBLE_ATOMIC_STORE_INTEGER(Stb);
+      break;
+    case kSw64Word64AtomicStoreWord16:
+      ASSEMBLE_ATOMIC_STORE_INTEGER(Sth);
+      break;
+    case kSw64Word64AtomicStoreWord32:
+      ASSEMBLE_ATOMIC_STORE_INTEGER(Stw);
+      break;
+    case kSw64Word64AtomicStoreWord64:
+      ASSEMBLE_ATOMIC_STORE_INTEGER(Stl);
+      break;
+    case kWord32AtomicExchangeInt8:
+      ASSEMBLE_ATOMIC_EXCHANGE_INTEGER_EXT(lldw, lstw, true, 8, 32);
+      break;
+    case kWord32AtomicExchangeUint8:
+      ASSEMBLE_ATOMIC_EXCHANGE_INTEGER_EXT(lldw, lstw, false, 8, 32);
+      break;
+    case kWord32AtomicExchangeInt16:
+      ASSEMBLE_ATOMIC_EXCHANGE_INTEGER_EXT(lldw, lstw, true, 16, 32);
+      break;
+    case kWord32AtomicExchangeUint16:
+      ASSEMBLE_ATOMIC_EXCHANGE_INTEGER_EXT(lldw, lstw, false, 16, 32);
+      break;
+    case kWord32AtomicExchangeWord32:
+      ASSEMBLE_ATOMIC_EXCHANGE_INTEGER(lldw, lstw);
+      break;
+    case kSw64Word64AtomicExchangeUint8:
+      ASSEMBLE_ATOMIC_EXCHANGE_INTEGER_EXT(lldl, lstl, false, 8, 64);
+      break;
+    case kSw64Word64AtomicExchangeUint16:
+      ASSEMBLE_ATOMIC_EXCHANGE_INTEGER_EXT(lldl, lstl, false, 16, 64);
+      break;
+    case kSw64Word64AtomicExchangeUint32:
+      ASSEMBLE_ATOMIC_EXCHANGE_INTEGER_EXT(lldl, lstl, false, 32, 64);
+      break;
+    case kSw64Word64AtomicExchangeUint64:
+      ASSEMBLE_ATOMIC_EXCHANGE_INTEGER(lldl, lstl);
+      break;
+    case kWord32AtomicCompareExchangeInt8:
+      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(lldw, lstw, true, 8, 32);
+      break;
+    case kWord32AtomicCompareExchangeUint8:
+      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(lldw, lstw, false, 8, 32);
+      break;
+    case kWord32AtomicCompareExchangeInt16:
+      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(lldw, lstw, true, 16, 32);
+      break;
+    case kWord32AtomicCompareExchangeUint16:
+      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(lldw, lstw, false, 16, 32);
+      break;
+    case kWord32AtomicCompareExchangeWord32:
+      __ addw(i.InputRegister(2), 0x0, i.InputRegister(2));
+      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER(lldw, lstw);
+      break;
+    case kSw64Word64AtomicCompareExchangeUint8:
+      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(lldl, lstl, false, 8, 64);
+      break;
+    case kSw64Word64AtomicCompareExchangeUint16:
+      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(lldl, lstl, false, 16, 64);
+      break;
+    case kSw64Word64AtomicCompareExchangeUint32:
+      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(lldl, lstl, false, 32, 64);
+      break;
+    case kSw64Word64AtomicCompareExchangeUint64:
+      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER(lldl, lstl);
+      break;
+#define ATOMIC_BINOP_CASE(op, inst)             \
+  case kWord32Atomic##op##Int8:                 \
+    ASSEMBLE_ATOMIC_BINOP_EXT(lldw, lstw, true, 8, inst, 32);   \
+    break;                                      \
+  case kWord32Atomic##op##Uint8:                \
+    ASSEMBLE_ATOMIC_BINOP_EXT(lldw, lstw, false, 8, inst, 32);  \
+    break;                                      \
+  case kWord32Atomic##op##Int16:                \
+    ASSEMBLE_ATOMIC_BINOP_EXT(lldw, lstw, true, 16, inst, 32);  \
+    break;                                      \
+  case kWord32Atomic##op##Uint16:               \
+    ASSEMBLE_ATOMIC_BINOP_EXT(lldw, lstw, false, 16, inst, 32); \
+    break;                                      \
+  case kWord32Atomic##op##Word32:               \
+    ASSEMBLE_ATOMIC_BINOP(lldw, lstw, inst);                \
+    break;
+      ATOMIC_BINOP_CASE(Add, Addw)
+      ATOMIC_BINOP_CASE(Sub, Subw)
+      ATOMIC_BINOP_CASE(And, And)
+      ATOMIC_BINOP_CASE(Or, Or)
+      ATOMIC_BINOP_CASE(Xor, Xor)
+#undef ATOMIC_BINOP_CASE
+#define ATOMIC_BINOP_CASE(op, inst)                           \
+  case kSw64Word64Atomic##op##Uint8:                        \
+    ASSEMBLE_ATOMIC_BINOP_EXT(lldl, lstl, false, 8, inst, 64);  \
+    break;                                                    \
+  case kSw64Word64Atomic##op##Uint16:                       \
+    ASSEMBLE_ATOMIC_BINOP_EXT(lldl, lstl, false, 16, inst, 64); \
+    break;                                                    \
+  case kSw64Word64Atomic##op##Uint32:                       \
+    ASSEMBLE_ATOMIC_BINOP_EXT(lldl, lstl, false, 32, inst, 64); \
+    break;                                                    \
+  case kSw64Word64Atomic##op##Uint64:                       \
+    ASSEMBLE_ATOMIC_BINOP(lldl, lstl, inst);                    \
+    break;
+      ATOMIC_BINOP_CASE(Add, Addl)
+      ATOMIC_BINOP_CASE(Sub, Subl)
+      ATOMIC_BINOP_CASE(And, And)
+      ATOMIC_BINOP_CASE(Or, Or)
+      ATOMIC_BINOP_CASE(Xor, Xor)
+#undef ATOMIC_BINOP_CASE
+    case kSw64AssertEqual:
+      __ Assert(eq, static_cast<AbortReason>(i.InputOperand(2).immediate()),
+                i.InputRegister(0), Operand(i.InputRegister(1)));
+      break;
+    case kSw64S128Const: {
+//      CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+//      Simd128Register dst = i.OutputSimd128Register();
+//      uint64_t imm1 = make_uint64(i.InputUint32(1), i.InputUint32(0));
+//      uint64_t imm2 = make_uint64(i.InputUint32(3), i.InputUint32(2));
+//      __ li(kScratchReg, imm1);
+//      __ insert_d(dst, 0, kScratchReg);
+//      __ li(kScratchReg, imm2);
+//      __ insert_d(dst, 1, kScratchReg);
+      UNREACHABLE();
+//      break;
+    }
+    case kSw64S128Zero: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ xor_v(i.OutputSimd128Register(), i.OutputSimd128Register(),
+      //         i.OutputSimd128Register());
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64S128AllOnes: {
+//      CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+//      Simd128Register dst = i.OutputSimd128Register();
+//      __ ceq_d(dst, dst, dst);
+      UNREACHABLE();
+//      break;
+    }
+    case kSw64I32x4Splat: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ fill_w(i.OutputSimd128Register(), i.InputRegister(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I32x4ExtractLane: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ copy_s_w(i.OutputRegister(), i.InputSimd128Register(0),
+      //            i.InputInt8(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I32x4ReplaceLane: {
+      CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      Simd128Register src = i.InputSimd128Register(0);
+      Simd128Register dst = i.OutputSimd128Register();
+      if (src != dst) {
+        //__ move_v(dst, src);
+	UNREACHABLE();
+      }
+      //__ insert_w(dst, i.InputInt8(1), i.InputRegister(2));
+      UNREACHABLE();
+    }
+    case kSw64I32x4Add: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ addv_w(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //          i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I32x4Sub: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ subv_w(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //          i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64F64x2Abs: {
+      UNREACHABLE();
+    }
+    case kSw64F64x2Neg: {
+      UNREACHABLE();
+    }
+    case kSw64F64x2Sqrt: {
+      UNREACHABLE();
+    }
+    case kSw64F64x2Add: {
+      UNREACHABLE();
+    }
+    case kSw64F64x2Sub: {
+      UNREACHABLE();
+    }
+    case kSw64F64x2Mul: {
+      UNREACHABLE();
+    }
+    case kSw64F64x2Div: {
+      UNREACHABLE();
+    }
+    case kSw64F64x2Min: {
+      UNREACHABLE();
+    }
+    case kSw64F64x2Max: {
+      UNREACHABLE();
+    }
+    case kSw64F64x2Eq: {
+      UNREACHABLE();
+    }
+    case kSw64F64x2Ne: {
+      UNREACHABLE();
+    }
+    case kSw64F64x2Lt: {
+      UNREACHABLE();
+    }
+    case kSw64F64x2Le: {
+      UNREACHABLE();
+    }
+    case kSw64F64x2Splat: {
+      UNREACHABLE();
+    }
+    case kSw64F64x2ExtractLane: {
+      UNREACHABLE();
+    }
+    case kSw64F64x2ReplaceLane: {
+      UNREACHABLE();
+    }
+    case kSw64I64x2Splat: {
+      UNREACHABLE();
+    }
+    case kSw64I64x2ExtractLane: {
+      UNREACHABLE();
+    }
+    case kSw64F64x2Pmin: {
+      UNREACHABLE();
+    }
+    case kSw64F64x2Pmax: {
+      UNREACHABLE();
+    }
+    case kSw64F64x2Ceil: {
+      UNREACHABLE();
+    }
+    case kSw64F64x2Floor: {
+      UNREACHABLE();
+    }
+    case kSw64F64x2Trunc: {
+      UNREACHABLE();
+    }
+    case kSw64F64x2NearestInt: {
+      UNREACHABLE();
+    }
+    case kSw64I64x2ReplaceLane: {
+      UNREACHABLE();
+    }
+    case kSw64I64x2Add: {
+      UNREACHABLE();
+    }
+    case kSw64I64x2Sub: {
+      UNREACHABLE();
+    }
+    case kSw64I64x2Mul: {
+      UNREACHABLE();
+    }
+    case kSw64I64x2Neg: {
+      UNREACHABLE();
+    }
+    case kSw64I64x2Shl: {
+      UNREACHABLE();
+    }
+    case kSw64I64x2ShrS: {
+      UNREACHABLE();
+    }
+    case kSw64I64x2ShrU: {
+      UNREACHABLE();
+    }
+    case kSw64F32x4Splat: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ FmoveLow(kScratchReg, i.InputSingleRegister(0));
+      //__ fill_w(i.OutputSimd128Register(), kScratchReg);
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64F32x4ExtractLane: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ copy_u_w(kScratchReg, i.InputSimd128Register(0), i.InputInt8(1));
+      //__ FmoveLow(i.OutputSingleRegister(), kScratchReg);
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64F32x4ReplaceLane: {
+      CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      Simd128Register src = i.InputSimd128Register(0);
+      Simd128Register dst = i.OutputSimd128Register();
+      if (src != dst) {
+        //__ move_v(dst, src);
+	UNREACHABLE();
+      }
+      //__ FmoveLow(kScratchReg, i.InputSingleRegister(2));
+      //__ insert_w(dst, i.InputInt8(1), kScratchReg);
+      break;
+    }
+    case kSw64F32x4SConvertI32x4: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ ffint_s_w(i.OutputSimd128Register(), i.InputSimd128Register(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64F32x4UConvertI32x4: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ ffint_u_w(i.OutputSimd128Register(), i.InputSimd128Register(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I32x4Mul: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ mulv_w(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //          i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I32x4MaxS: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ max_s_w(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //           i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I32x4MinS: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ min_s_w(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //           i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I32x4Eq: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ ceq_w(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //         i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I32x4Ne: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //Simd128Register dst = i.OutputSimd128Register();
+      //__ ceq_w(dst, i.InputSimd128Register(0), i.InputSimd128Register(1));
+      //__ nor_v(dst, dst, dst);
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I32x4Shl: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ slli_w(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //          i.InputInt5(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I32x4ShrS: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ srai_w(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //          i.InputInt5(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I32x4ShrU: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ srli_w(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //          i.InputInt5(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I32x4MaxU: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ max_u_w(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //           i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I32x4MinU: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ min_u_w(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //           i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64S128Select: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //DCHECK(i.OutputSimd128Register() == i.InputSimd128Register(0));
+      //__ bsel_v(i.OutputSimd128Register(), i.InputSimd128Register(2),
+      //          i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64S128AndNot: {
+      UNREACHABLE();
+    }
+    case kSw64F32x4Abs: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ bclri_w(i.OutputSimd128Register(), i.InputSimd128Register(0), 31);
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64F32x4Neg: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ bnegi_w(i.OutputSimd128Register(), i.InputSimd128Register(0), 31);
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64F32x4RecipApprox: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ frcp_w(i.OutputSimd128Register(), i.InputSimd128Register(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64F32x4RecipSqrtApprox: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ frsqrt_w(i.OutputSimd128Register(), i.InputSimd128Register(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64F32x4Add: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ fadd_w(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //          i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64F32x4Sub: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ fsub_w(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //          i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64F32x4Mul: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ fmul_w(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //          i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64F32x4Max: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ fmax_w(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //          i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64F32x4Eq: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ fceq_w(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //          i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64F32x4Ne: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ fcne_w(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //          i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64F32x4Lt: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ fclt_w(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //          i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64F32x4Le: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ fcle_w(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //          i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64F32x4Div: {
+      UNREACHABLE();
+    }
+    case kSw64F32x4Pmin: {
+      UNREACHABLE();
+    }
+    case kSw64F32x4Pmax: {
+      UNREACHABLE();
+    }
+    case kSw64F32x4Ceil: {
+      UNREACHABLE();
+    }
+    case kSw64F32x4Floor: {
+      UNREACHABLE();
+    }
+    case kSw64F32x4Trunc: {
+      UNREACHABLE();
+    }
+    case kSw64F32x4NearestInt: {
+      UNREACHABLE();
+    }
+    case kSw64F32x4Sqrt: {
+      UNREACHABLE();
+    }
+    case kSw64I32x4Abs: {
+      UNREACHABLE();
+    }
+    case kSw64I32x4BitMask: {
+      UNREACHABLE();
+    }
+//    case kSw64I32x4SConvertF32x4: {
+//      CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+//      __ ftrunc_s_w(i.OutputSimd128Register(), i.InputSimd128Register(0));
+//      break;
+//    }
+    case kSw64I32x4UConvertF32x4: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ ftrunc_u_w(i.OutputSimd128Register(), i.InputSimd128Register(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I32x4Neg: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ xor_v(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
+      //__ subv_w(i.OutputSimd128Register(), kSimd128RegZero,
+      //          i.InputSimd128Register(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I32x4GtS: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ clt_s_w(i.OutputSimd128Register(), i.InputSimd128Register(1),
+      //           i.InputSimd128Register(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I32x4GeS: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ cle_s_w(i.OutputSimd128Register(), i.InputSimd128Register(1),
+      //           i.InputSimd128Register(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I32x4GtU: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ clt_u_w(i.OutputSimd128Register(), i.InputSimd128Register(1),
+      //           i.InputSimd128Register(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I32x4GeU: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ cle_u_w(i.OutputSimd128Register(), i.InputSimd128Register(1),
+      //           i.InputSimd128Register(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I16x8Splat: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ fill_h(i.OutputSimd128Register(), i.InputRegister(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I16x8ExtractLaneU: {
+      UNREACHABLE();
+    }
+    case kSw64I16x8ExtractLaneS: {
+      UNREACHABLE();
+    }
+    case kSw64I16x8ReplaceLane: {
+      CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      Simd128Register src = i.InputSimd128Register(0);
+      Simd128Register dst = i.OutputSimd128Register();
+      if (src != dst) {
+        //__ move_v(dst, src);
+	UNREACHABLE();
+      }
+      //__ insert_h(dst, i.InputInt8(1), i.InputRegister(2));
+      break;
+    }
+    case kSw64I16x8Neg: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ xor_v(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
+      //__ subv_h(i.OutputSimd128Register(), kSimd128RegZero,
+      //          i.InputSimd128Register(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I16x8Shl: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ slli_h(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //          i.InputInt4(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I16x8ShrS: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ srai_h(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //          i.InputInt4(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I16x8ShrU: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ srli_h(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //          i.InputInt4(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I16x8Sub: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ subv_h(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //          i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I16x8Mul: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ mulv_h(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //          i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I16x8MaxS: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ max_s_h(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //           i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I16x8MinS: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ min_s_h(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //           i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I16x8Eq: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ ceq_h(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //         i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I16x8AddSaturateS: {
+      UNREACHABLE();
+    }
+    case kSw64I16x8SubSaturateS: {
+      UNREACHABLE();
+    }
+    case kSw64I8x16AddSaturateS: {
+      UNREACHABLE();
+    }
+    case kSw64I8x16SubSaturateS: {
+      UNREACHABLE();
+    }
+    case kSw64I16x8AddSaturateU: {
+      UNREACHABLE();
+    }
+    case kSw64I16x8SubSaturateU: {
+      UNREACHABLE();
+    }
+    case kSw64I8x16AddSaturateU: {
+      UNREACHABLE();
+    }
+    case kSw64I8x16SubSaturateU: {
+      UNREACHABLE();
+    }
+
+    case kSw64I16x8Ne: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //Simd128Register dst = i.OutputSimd128Register();
+      //__ ceq_h(dst, i.InputSimd128Register(0), i.InputSimd128Register(1));
+      //__ nor_v(dst, dst, dst);
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I16x8GtS: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ clt_s_h(i.OutputSimd128Register(), i.InputSimd128Register(1),
+      //           i.InputSimd128Register(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I16x8GeS: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ cle_s_h(i.OutputSimd128Register(), i.InputSimd128Register(1),
+      //           i.InputSimd128Register(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I16x8MaxU: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ max_u_h(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //           i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I16x8MinU: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ min_u_h(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //           i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I16x8GtU: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ clt_u_h(i.OutputSimd128Register(), i.InputSimd128Register(1),
+      //           i.InputSimd128Register(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I16x8GeU: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ cle_u_h(i.OutputSimd128Register(), i.InputSimd128Register(1),
+      //           i.InputSimd128Register(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I16x8RoundingAverageU: {
+      UNREACHABLE();
+    }
+    case kSw64I16x8Abs: {
+      UNREACHABLE();
+    }
+    case kSw64I16x8BitMask: {
+      UNREACHABLE();
+    }
+    case kSw64I8x16Splat: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ fill_b(i.OutputSimd128Register(), i.InputRegister(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I8x16ExtractLaneU: {
+      UNREACHABLE();
+    }
+    case kSw64I8x16ExtractLaneS: {
+      UNREACHABLE();
+    }
+    case kSw64I8x16ReplaceLane: {
+      CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      Simd128Register src = i.InputSimd128Register(0);
+      Simd128Register dst = i.OutputSimd128Register();
+      if (src != dst) {
+        //__ move_v(dst, src);
+	UNREACHABLE();
+      }
+      //__ insert_b(dst, i.InputInt8(1), i.InputRegister(2));
+      break;
+    }
+    case kSw64I8x16Neg: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ xor_v(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
+      //__ subv_b(i.OutputSimd128Register(), kSimd128RegZero,
+      //          i.InputSimd128Register(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I8x16Shl: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ slli_b(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //          i.InputInt3(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I8x16ShrS: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ srai_b(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //          i.InputInt3(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I8x16Sub: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ subv_b(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //          i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I8x16Mul: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ mulv_b(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //          i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I8x16MaxS: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ max_s_b(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //           i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I8x16MinS: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ min_s_b(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //           i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I8x16Eq: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ ceq_b(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //         i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I8x16Ne: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //Simd128Register dst = i.OutputSimd128Register();
+      //__ ceq_b(dst, i.InputSimd128Register(0), i.InputSimd128Register(1));
+      //__ nor_v(dst, dst, dst);
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I8x16GtS: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ clt_s_b(i.OutputSimd128Register(), i.InputSimd128Register(1),
+      //           i.InputSimd128Register(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I8x16GeS: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ cle_s_b(i.OutputSimd128Register(), i.InputSimd128Register(1),
+      //           i.InputSimd128Register(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I8x16ShrU: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ srli_b(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //          i.InputInt3(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I8x16MaxU: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ max_u_b(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //           i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I8x16MinU: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ min_u_b(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //           i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I8x16GtU: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ clt_u_b(i.OutputSimd128Register(), i.InputSimd128Register(1),
+      //           i.InputSimd128Register(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I8x16GeU: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ cle_u_b(i.OutputSimd128Register(), i.InputSimd128Register(1),
+      //           i.InputSimd128Register(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I8x16RoundingAverageU: {
+      UNREACHABLE();
+    }
+    case kSw64I8x16Abs: {
+      UNREACHABLE();
+    }
+    case kSw64I8x16BitMask: {
+      UNREACHABLE();
+    }
+    case kSw64S128And: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ and_v(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //         i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64S128Or: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ or_v(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //        i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64S128Xor: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ xor_v(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //         i.InputSimd128Register(1));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64S128Not: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ nor_v(i.OutputSimd128Register(), i.InputSimd128Register(0),
+      //         i.InputSimd128Register(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64V32x4AnyTrue:
+    case kSw64V16x8AnyTrue:
+    case kSw64V8x16AnyTrue: {
+      CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      Register dst = i.OutputRegister();
+      Label all_false;
+      __ BranchMSA(&all_false, MSA_BRANCH_V, all_zero,
+                   i.InputSimd128Register(0), USE_DELAY_SLOT);
+      __ li(dst, 0l);  // branch delay slot
+      __ li(dst, 1);
+      __ bind(&all_false);
+      break;
+    }
+    case kSw64V32x4AllTrue: {
+      CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      Register dst = i.OutputRegister();
+      Label all_true;
+      __ BranchMSA(&all_true, MSA_BRANCH_W, all_not_zero,
+                   i.InputSimd128Register(0), USE_DELAY_SLOT);
+      __ li(dst, 1);  // branch delay slot
+      __ li(dst, 0l);
+      __ bind(&all_true);
+      break;
+    }
+    case kSw64V16x8AllTrue: {
+      CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      Register dst = i.OutputRegister();
+      Label all_true;
+      __ BranchMSA(&all_true, MSA_BRANCH_H, all_not_zero,
+                   i.InputSimd128Register(0), USE_DELAY_SLOT);
+      __ li(dst, 1);  // branch delay slot
+      __ li(dst, 0l);
+      __ bind(&all_true);
+      break;
+    }
+    case kSw64V8x16AllTrue: {
+      CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      Register dst = i.OutputRegister();
+      Label all_true;
+      __ BranchMSA(&all_true, MSA_BRANCH_B, all_not_zero,
+                   i.InputSimd128Register(0), USE_DELAY_SLOT);
+      __ li(dst, 1);  // branch delay slot
+      __ li(dst, 0l);
+      __ bind(&all_true);
+      break;
+    }
+    case kSw64MsaLd: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ ld_b(i.OutputSimd128Register(), i.MemoryOperand());
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64MsaSt: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ st_b(i.InputSimd128Register(2), i.MemoryOperand());
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64S32x4InterleaveRight: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //Simd128Register dst = i.OutputSimd128Register(),
+      //                src0 = i.InputSimd128Register(0),
+      //                src1 = i.InputSimd128Register(1);
+      // src1 = [7, 6, 5, 4], src0 = [3, 2, 1, 0]
+      // dst = [5, 1, 4, 0]
+      //__ ilvr_w(dst, src1, src0);
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64S32x4InterleaveLeft: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //Simd128Register dst = i.OutputSimd128Register(),
+      //                src0 = i.InputSimd128Register(0),
+      //                src1 = i.InputSimd128Register(1);
+      // src1 = [7, 6, 5, 4], src0 = [3, 2, 1, 0]
+      // dst = [7, 3, 6, 2]
+      //__ ilvl_w(dst, src1, src0);
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64S32x4PackEven: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //Simd128Register dst = i.OutputSimd128Register(),
+      //                src0 = i.InputSimd128Register(0),
+      //                src1 = i.InputSimd128Register(1);
+      // src1 = [7, 6, 5, 4], src0 = [3, 2, 1, 0]
+      // dst = [6, 4, 2, 0]
+      //__ pckev_w(dst, src1, src0);
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64S32x4PackOdd: {
+      UNREACHABLE();
+    }
+    case kSw64S32x4InterleaveEven: {
+      UNREACHABLE();
+    }
+    case kSw64S32x4InterleaveOdd: {
+      UNREACHABLE();
+    }
+    case kSw64S32x4Shuffle: {
+      UNREACHABLE();
+    }
+    case kSw64S16x8InterleaveRight: {
+      UNREACHABLE();
+    }
+    case kSw64S16x8InterleaveLeft: {
+      UNREACHABLE();
+    }
+    case kSw64S16x8PackEven: {
+      UNREACHABLE();
+    }
+    case kSw64S16x8PackOdd: {
+      UNREACHABLE();
+    }
+    case kSw64S16x8InterleaveEven: {
+      UNREACHABLE();
+    }
+    case kSw64S16x8InterleaveOdd: {
+      UNREACHABLE();
+    }
+    case kSw64S16x4Reverse: {
+      UNREACHABLE();
+    }
+    case kSw64S16x2Reverse: {
+      UNREACHABLE();
+    }
+    case kSw64S8x16InterleaveRight: {
+      UNREACHABLE();
+    }
+    case kSw64S8x16InterleaveLeft: {
+      UNREACHABLE();
+    }
+    case kSw64S8x16PackEven: {
+      UNREACHABLE();
+    }
+    case kSw64S8x16PackOdd: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //Simd128Register dst = i.OutputSimd128Register(),
+      //                src0 = i.InputSimd128Register(0),
+      //                src1 = i.InputSimd128Register(1);
+      // src1 = [31, ... 19, 18, 17, 16], src0 = [15, ... 3, 2, 1, 0]
+      // dst = [31, 29, ... 7, 5, 3, 1]
+      //__ pckod_b(dst, src1, src0);
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64S8x16InterleaveEven: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //Simd128Register dst = i.OutputSimd128Register(),
+      //                src0 = i.InputSimd128Register(0),
+      //                src1 = i.InputSimd128Register(1);
+      // src1 = [31, ... 19, 18, 17, 16], src0 = [15, ... 3, 2, 1, 0]
+      // dst = [30, 14, ... 18, 2, 16, 0]
+      //__ ilvev_b(dst, src1, src0);
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64S8x16InterleaveOdd: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //Simd128Register dst = i.OutputSimd128Register(),
+      //                src0 = i.InputSimd128Register(0),
+      //                src1 = i.InputSimd128Register(1);
+      // src1 = [31, ... 19, 18, 17, 16], src0 = [15, ... 3, 2, 1, 0]
+      // dst = [31, 15, ... 19, 3, 17, 1]
+      //__ ilvod_b(dst, src1, src0);
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64S8x16Concat: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //Simd128Register dst = i.OutputSimd128Register();
+      //DCHECK(dst == i.InputSimd128Register(0));
+      //__ sldi_b(dst, i.InputSimd128Register(1), i.InputInt4(2));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I8x16Shuffle: {
+      UNREACHABLE();
+      //if (dst == src0) {
+      //  __ move_v(kSimd128ScratchReg, src0);
+      //  src0 = kSimd128ScratchReg;
+      //} else if (dst == src1) {
+      //  __ move_v(kSimd128ScratchReg, src1);
+      //  src1 = kSimd128ScratchReg;
+      //}
+
+      //int64_t control_low =
+      //    static_cast<int64_t>(i.InputInt32(3)) << 32 | i.InputInt32(2);
+      //int64_t control_hi =
+      //    static_cast<int64_t>(i.InputInt32(5)) << 32 | i.InputInt32(4);
+      //__ li(kScratchReg, control_low);
+      //__ insert_d(dst, 0, kScratchReg);
+      //__ li(kScratchReg, control_hi);
+      //__ insert_d(dst, 1, kScratchReg);
+      //__ vshf_b(dst, src1, src0);
+      break;
+    }
+    case kSw64I8x16Swizzle: {
+      UNREACHABLE();
+    }
+    case kSw64S8x8Reverse: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      // src = [15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]
+      // dst = [8, 9, 10, 11, 12, 13, 14, 15, 0, 1, 2, 3, 4, 5, 6, 7]
+      // [A B C D] => [B A D C]: shf.w imm: 2 3 0 1 = 10110001 = 0xB1
+      // C: [7, 6, 5, 4] => A': [4, 5, 6, 7]: shf.b imm: 00011011 = 0x1B
+      //__ shf_w(kSimd128ScratchReg, i.InputSimd128Register(0), 0xB1);
+      //__ shf_b(i.OutputSimd128Register(), kSimd128ScratchReg, 0x1B);
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64S8x4Reverse: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      // src = [15, 14, ... 3, 2, 1, 0], dst = [12, 13, 14, 15, ... 0, 1, 2, 3]
+      // shf.df imm field: 0 1 2 3 = 00011011 = 0x1B
+      //__ shf_b(i.OutputSimd128Register(), i.InputSimd128Register(0), 0x1B);
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64S8x2Reverse: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      // src = [15, 14, ... 3, 2, 1, 0], dst = [14, 15, 12, 13, ... 2, 3, 0, 1]
+      // shf.df imm field: 2 3 0 1 = 10110001 = 0xB1
+      //__ shf_b(i.OutputSimd128Register(), i.InputSimd128Register(0), 0xB1);
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I32x4SConvertI16x8Low: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //Simd128Register dst = i.OutputSimd128Register();
+      //Simd128Register src = i.InputSimd128Register(0);
+      //__ ilvr_h(kSimd128ScratchReg, src, src);
+      //__ slli_w(dst, kSimd128ScratchReg, 16);
+      //__ srai_w(dst, dst, 16);
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I32x4SConvertI16x8High: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //Simd128Register dst = i.OutputSimd128Register();
+      //Simd128Register src = i.InputSimd128Register(0);
+      //__ ilvl_h(kSimd128ScratchReg, src, src);
+      //__ slli_w(dst, kSimd128ScratchReg, 16);
+      //__ srai_w(dst, dst, 16);
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I32x4UConvertI16x8Low: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ xor_v(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
+      //__ ilvr_h(i.OutputSimd128Register(), kSimd128RegZero,
+      //          i.InputSimd128Register(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I32x4UConvertI16x8High: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ xor_v(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
+      //__ ilvl_h(i.OutputSimd128Register(), kSimd128RegZero,
+      //          i.InputSimd128Register(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I16x8SConvertI8x16Low: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //Simd128Register dst = i.OutputSimd128Register();
+      //Simd128Register src = i.InputSimd128Register(0);
+      //__ ilvr_b(kSimd128ScratchReg, src, src);
+      //__ slli_h(dst, kSimd128ScratchReg, 8);
+      //__ srai_h(dst, dst, 8);
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I16x8SConvertI8x16High: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //Simd128Register dst = i.OutputSimd128Register();
+      //Simd128Register src = i.InputSimd128Register(0);
+      //__ ilvl_b(kSimd128ScratchReg, src, src);
+      //__ slli_h(dst, kSimd128ScratchReg, 8);
+      //__ srai_h(dst, dst, 8);
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I16x8SConvertI32x4: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //Simd128Register dst = i.OutputSimd128Register();
+      //Simd128Register src0 = i.InputSimd128Register(0);
+      //Simd128Register src1 = i.InputSimd128Register(1);
+      //__ sat_s_w(kSimd128ScratchReg, src0, 15);
+      //__ sat_s_w(kSimd128RegZero, src1, 15);  // kSimd128RegZero as scratch
+      //__ pckev_h(dst, kSimd128RegZero, kSimd128ScratchReg);
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I16x8UConvertI32x4: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //Simd128Register dst = i.OutputSimd128Register();
+      //Simd128Register src0 = i.InputSimd128Register(0);
+      //Simd128Register src1 = i.InputSimd128Register(1);
+      //__ sat_u_w(kSimd128ScratchReg, src0, 15);
+      //__ sat_u_w(kSimd128RegZero, src1, 15);  // kSimd128RegZero as scratch
+      //__ pckev_h(dst, kSimd128RegZero, kSimd128ScratchReg);
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I16x8UConvertI8x16Low: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ xor_v(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
+      //__ ilvr_b(i.OutputSimd128Register(), kSimd128RegZero,
+      //          i.InputSimd128Register(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I16x8UConvertI8x16High: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //__ xor_v(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
+      //__ ilvl_b(i.OutputSimd128Register(), kSimd128RegZero,
+      //          i.InputSimd128Register(0));
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I8x16SConvertI16x8: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //Simd128Register dst = i.OutputSimd128Register();
+      //Simd128Register src0 = i.InputSimd128Register(0);
+      //Simd128Register src1 = i.InputSimd128Register(1);
+      //__ sat_s_h(kSimd128ScratchReg, src0, 7);
+      //__ sat_s_h(kSimd128RegZero, src1, 7);  // kSimd128RegZero as scratch
+      //__ pckev_b(dst, kSimd128RegZero, kSimd128ScratchReg);
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I8x16UConvertI16x8: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //Simd128Register dst = i.OutputSimd128Register();
+      //Simd128Register src0 = i.InputSimd128Register(0);
+      //Simd128Register src1 = i.InputSimd128Register(1);
+      //__ sat_u_h(kSimd128ScratchReg, src0, 7);
+      //__ sat_u_h(kSimd128RegZero, src1, 7);  // kSimd128RegZero as scratch
+      //__ pckev_b(dst, kSimd128RegZero, kSimd128ScratchReg);
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64F32x4AddHoriz: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //Simd128Register src0 = i.InputSimd128Register(0);
+      //Simd128Register src1 = i.InputSimd128Register(1);
+      //Simd128Register dst = i.OutputSimd128Register();
+      //__ shf_w(kSimd128ScratchReg, src0, 0xB1);  // 2 3 0 1 : 10110001 : 0xB1
+      //__ shf_w(kSimd128RegZero, src1, 0xB1);     // kSimd128RegZero as scratch
+      //__ fadd_w(kSimd128ScratchReg, kSimd128ScratchReg, src0);
+      //__ fadd_w(kSimd128RegZero, kSimd128RegZero, src1);
+      //__ pckev_w(dst, kSimd128RegZero, kSimd128ScratchReg);
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I32x4AddHoriz: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //Simd128Register src0 = i.InputSimd128Register(0);
+      //Simd128Register src1 = i.InputSimd128Register(1);
+      //Simd128Register dst = i.OutputSimd128Register();
+      //__ hadd_s_d(kSimd128ScratchReg, src0, src0);
+      //__ hadd_s_d(kSimd128RegZero, src1, src1);  // kSimd128RegZero as scratch
+      //__ pckev_w(dst, kSimd128RegZero, kSimd128ScratchReg);
+      //break;
+      UNREACHABLE();
+    }
+    case kSw64I16x8AddHoriz: {
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //Simd128Register src0 = i.InputSimd128Register(0);
+      //Simd128Register src1 = i.InputSimd128Register(1);
+      //Simd128Register dst = i.OutputSimd128Register();
+      //__ hadd_s_w(kSimd128ScratchReg, src0, src0);
+      //__ hadd_s_w(kSimd128RegZero, src1, src1);  // kSimd128RegZero as scratch
+      //__ pckev_h(dst, kSimd128RegZero, kSimd128ScratchReg);
+      //break;
+      UNREACHABLE();
+    }
+    default:
+      UNREACHABLE();
+  }
+  return kSuccess;
+}  // NOLINT(readability/fn_size)
+
+
+#define UNSUPPORTED_COND(opcode, condition)                                    \
+  StdoutStream{} << "Unsupported " << #opcode << " condition: \"" << condition \
+                 << "\"";                                                      \
+  UNIMPLEMENTED();
+
+
+void AssembleBranchToLabels(CodeGenerator* gen, TurboAssembler* tasm,
+                            Instruction* instr, FlagsCondition condition,
+                            Label* tlabel, Label* flabel, bool fallthru) {
+#undef __
+#define __ tasm->
+  Sw64OperandConverter i(gen, instr);
+
+  Condition cc = kNoCondition;
+  // SW64 does not have condition code flags, so compare and branch are
+  // implemented differently than on the other arch's. The compare operations
+  // emit sw64 pseudo-instructions, which are handled here by branch
+  // instructions that do the actual comparison. Essential that the input
+  // registers to compare pseudo-op are not modified before this branch op, as
+  // they are tested here.
+
+  if (instr->arch_opcode() == kSw64Tst) {
+    cc = FlagsConditionToConditionTst(condition);
+    __ Branch(tlabel, cc, kScratchReg, Operand(zero_reg));
+  } else if (instr->arch_opcode() == kSw64Dadd ||
+             instr->arch_opcode() == kSw64Dsub) {
+    cc = FlagsConditionToConditionOvf(condition);
+    __ sral(i.OutputRegister(), 32 ,kScratchReg);
+    __ Sraw(kScratchReg2, i.OutputRegister(), 31);
+    __ Branch(tlabel, cc, kScratchReg2, Operand(kScratchReg));
+  } else if (instr->arch_opcode() == kSw64DaddOvf ||
+             instr->arch_opcode() == kSw64DsubOvf) {
+    switch (condition) {
+      // Overflow occurs if overflow register is negative
+      case kOverflow:
+        __ Branch(tlabel, lt, kScratchReg, Operand(zero_reg));
+        break;
+      case kNotOverflow:
+        __ Branch(tlabel, ge, kScratchReg, Operand(zero_reg));
+        break;
+      default:
+        UNSUPPORTED_COND(instr->arch_opcode(), condition);
+        break;
+    }
+  } else if (instr->arch_opcode() == kSw64MulOvf) {
+    // Overflow occurs if overflow register is not zero
+    switch (condition) {
+      case kOverflow:
+        __ Branch(tlabel, ne, kScratchReg, Operand(zero_reg));
+        break;
+      case kNotOverflow:
+        __ Branch(tlabel, eq, kScratchReg, Operand(zero_reg));
+        break;
+      default:
+        UNSUPPORTED_COND(kSw64MulOvf, condition);
+        break;
+    }
+  } else if (instr->arch_opcode() == kSw64Cmp) {
+    cc = FlagsConditionToConditionCmp(condition);
+    __ Branch(tlabel, cc, i.InputRegister(0), i.InputOperand(1));
+  } else if (instr->arch_opcode() == kArchStackPointerGreaterThan) {
+    cc = FlagsConditionToConditionCmp(condition);
+    Register lhs_register = sp;
+    uint32_t offset;
+    if (gen->ShouldApplyOffsetToStackCheck(instr, &offset)) {
+      lhs_register = i.TempRegister(0);
+      __ Subl(lhs_register, sp, offset);
+    }
+    __ Branch(tlabel, cc, lhs_register, Operand(i.InputRegister(0)));
+  } else if (instr->arch_opcode() == kSw64CmpS ||
+             instr->arch_opcode() == kSw64CmpD) {
+    bool predicate;
+    FlagsConditionToConditionCmpFPU(&predicate, condition);
+    if (predicate) {
+      __ BranchTrueF(tlabel);
+    } else {
+      __ BranchFalseF(tlabel);
+    }
+  } else {
+    PrintF("AssembleArchBranch Unimplemented arch_opcode: %d\n",
+           instr->arch_opcode());
+    UNIMPLEMENTED();
+  }
+  if (!fallthru) __ Branch(flabel);  // no fallthru to flabel.
+#undef __
+#define __ tasm()->
+}
+
+// Assembles branches after an instruction.
+void CodeGenerator::AssembleArchBranch(Instruction* instr, BranchInfo* branch) {
+  Label* tlabel = branch->true_label;
+  Label* flabel = branch->false_label;
+
+  AssembleBranchToLabels(this, tasm(), instr, branch->condition, tlabel, flabel,
+                         branch->fallthru);
+}
+
+void CodeGenerator::AssembleBranchPoisoning(FlagsCondition condition,
+                                            Instruction* instr) {
+  // TODO(jarin) Handle float comparisons (kUnordered[Not]Equal).
+  if (condition == kUnorderedEqual || condition == kUnorderedNotEqual) {
+    return;
+  }
+
+  Sw64OperandConverter i(this, instr);
+  condition = NegateFlagsCondition(condition);
+
+  switch (instr->arch_opcode()) {
+    case kSw64Cmp: {
+      __ LoadZeroOnCondition(kSpeculationPoisonRegister, i.InputRegister(0),
+                             i.InputOperand(1),
+                             FlagsConditionToConditionCmp(condition));
+    }
+      return;
+    case kSw64Tst: {
+      switch (condition) {
+        case kEqual:
+          __ LoadZeroIfConditionZero(kSpeculationPoisonRegister, kScratchReg);
+          break;
+        case kNotEqual:
+          __ LoadZeroIfConditionNotZero(kSpeculationPoisonRegister,
+                                        kScratchReg);
+          break;
+        default:
+          UNREACHABLE();
+      }
+    }
+      return;
+    case kSw64Dadd:
+    case kSw64Dsub: {
+      // Check for overflow creates 1 or 0 for result.
+      __ srll(i.OutputRegister(), 63 ,kScratchReg);
+      __ Srlw(kScratchReg2, i.OutputRegister(), 31);
+      __ xor_ins(kScratchReg, kScratchReg2, kScratchReg2);
+      switch (condition) {
+        case kOverflow:
+          __ LoadZeroIfConditionNotZero(kSpeculationPoisonRegister,
+                                        kScratchReg2);
+          break;
+        case kNotOverflow:
+          __ LoadZeroIfConditionZero(kSpeculationPoisonRegister, kScratchReg2);
+          break;
+        default:
+          UNSUPPORTED_COND(instr->arch_opcode(), condition);
+      }
+    }
+      return;
+    case kSw64DaddOvf:
+    case kSw64DsubOvf: {
+      // Overflow occurs if overflow register is negative
+      __ Cmplt(kScratchReg2, kScratchReg, zero_reg);
+      switch (condition) {
+        case kOverflow:
+          __ LoadZeroIfConditionNotZero(kSpeculationPoisonRegister,
+                                        kScratchReg2);
+          break;
+        case kNotOverflow:
+          __ LoadZeroIfConditionZero(kSpeculationPoisonRegister, kScratchReg2);
+          break;
+        default:
+          UNSUPPORTED_COND(instr->arch_opcode(), condition);
+      }
+    }
+      return;
+    case kSw64MulOvf: {
+      // Overflow occurs if overflow register is not zero
+      switch (condition) {
+        case kOverflow:
+          __ LoadZeroIfConditionNotZero(kSpeculationPoisonRegister,
+                                        kScratchReg);
+          break;
+        case kNotOverflow:
+          __ LoadZeroIfConditionZero(kSpeculationPoisonRegister, kScratchReg);
+          break;
+        default:
+          UNSUPPORTED_COND(instr->arch_opcode(), condition);
+      }
+    }
+      return;
+    case kSw64CmpS:
+    case kSw64CmpD: {
+      bool predicate;
+      FlagsConditionToConditionCmpFPU(&predicate, condition);
+      if (predicate) {
+        __ LoadZeroIfFPUCondition(kSpeculationPoisonRegister);
+      } else {
+        __ LoadZeroIfNotFPUCondition(kSpeculationPoisonRegister);
+      }
+    }
+      return;
+    default:
+      UNREACHABLE();
+  }
+}
+
+#undef UNSUPPORTED_COND
+
+void CodeGenerator::AssembleArchDeoptBranch(Instruction* instr,
+                                            BranchInfo* branch) {
+  AssembleArchBranch(instr, branch);
+}
+
+void CodeGenerator::AssembleArchJump(RpoNumber target) {
+  if (!IsNextInAssemblyOrder(target)) __ Branch(GetLabel(target));
+}
+
+void CodeGenerator::AssembleArchTrap(Instruction* instr,
+                                     FlagsCondition condition) {
+  class OutOfLineTrap final : public OutOfLineCode {
+   public:
+    OutOfLineTrap(CodeGenerator* gen, Instruction* instr)
+        : OutOfLineCode(gen), instr_(instr), gen_(gen) {}
+    void Generate() final {
+      Sw64OperandConverter i(gen_, instr_);
+      TrapId trap_id =
+          static_cast<TrapId>(i.InputInt32(instr_->InputCount() - 1));
+      GenerateCallToTrap(trap_id);
+    }
+
+   private:
+    void GenerateCallToTrap(TrapId trap_id) {
+      if (trap_id == TrapId::kInvalid) {
+        // We cannot test calls to the runtime in cctest/test-run-wasm.
+        // Therefore we emit a call to C here instead of a call to the runtime.
+        // We use the context register as the scratch register, because we do
+        // not have a context here.
+        __ PrepareCallCFunction(0, 0, cp);
+        __ CallCFunction(
+            ExternalReference::wasm_call_trap_callback_for_testing(), 0);
+        __ LeaveFrame(StackFrame::WASM);
+        auto call_descriptor = gen_->linkage()->GetIncomingDescriptor();
+        int pop_count =
+            static_cast<int>(call_descriptor->StackParameterCount());
+        pop_count += (pop_count & 1);  // align
+        __ Drop(pop_count);
+        __ Ret();
+      } else {
+        gen_->AssembleSourcePosition(instr_);
+        // A direct call to a wasm runtime stub defined in this module.
+        // Just encode the stub index. This will be patched when the code
+        // is added to the native module and copied into wasm code space.
+        __ Call(static_cast<Address>(trap_id), RelocInfo::WASM_STUB_CALL);
+        ReferenceMap* reference_map =
+            gen_->zone()->New<ReferenceMap>(gen_->zone());
+        gen_->RecordSafepoint(reference_map, Safepoint::kNoLazyDeopt);
+        if (FLAG_debug_code) {
+          __ halt();//stop(GetAbortReason(AbortReason::kUnexpectedReturnFromWasmTrap));
+        }
+      }
+    }
+    Instruction* instr_;
+    CodeGenerator* gen_;
+  };
+  auto ool = zone()->New<OutOfLineTrap>(this, instr);
+  Label* tlabel = ool->entry();
+  AssembleBranchToLabels(this, tasm(), instr, condition, tlabel, nullptr, true);
+}
+
+// Assembles boolean materializations after an instruction.
+void CodeGenerator::AssembleArchBoolean(Instruction* instr,
+                                        FlagsCondition condition) {
+  Sw64OperandConverter i(this, instr);
+
+  // Materialize a full 32-bit 1 or 0 value. The result register is always the
+  // last output of the instruction.
+  DCHECK_NE(0u, instr->OutputCount());
+  Register result = i.OutputRegister(instr->OutputCount() - 1);
+  Condition cc = kNoCondition;
+  // SW64 does not have condition code flags, so compare and branch are
+  // implemented differently than on the other arch's. The compare operations
+  // emit sw64 pseudo-instructions, which are checked and handled here.
+
+  if (instr->arch_opcode() == kSw64Tst) {
+    cc = FlagsConditionToConditionTst(condition);
+    if (cc == eq) {
+      __ Cmpult(result, kScratchReg, 1);
+    } else {
+      __ Cmpult(result, zero_reg, kScratchReg);
+    }
+    return;
+  } else if (instr->arch_opcode() == kSw64Dadd ||
+             instr->arch_opcode() == kSw64Dsub) {
+    cc = FlagsConditionToConditionOvf(condition);
+    // Check for overflow creates 1 or 0 for result.
+    __ srll(i.OutputRegister(), 63 ,kScratchReg);
+    __ Srlw(kScratchReg2, i.OutputRegister(), 31);
+    __ xor_ins(kScratchReg, kScratchReg2, result);
+    if (cc == eq)  // Toggle result for not overflow.
+      __ xor_ins(result, 1, result);
+    return;
+  } else if (instr->arch_opcode() == kSw64DaddOvf ||
+             instr->arch_opcode() == kSw64DsubOvf) {
+    // Overflow occurs if overflow register is negative
+    __ cmplt(kScratchReg, zero_reg, result);
+  } else if (instr->arch_opcode() == kSw64MulOvf) {
+    // Overflow occurs if overflow register is not zero
+    __ Cmpugt(result, kScratchReg, zero_reg);
+  } else if (instr->arch_opcode() == kSw64Cmp) {
+    cc = FlagsConditionToConditionCmp(condition);
+    switch (cc) {
+      case eq:
+      case ne: {
+        Register left = i.InputRegister(0);
+        Operand right = i.InputOperand(1);
+        if (instr->InputAt(1)->IsImmediate()) {
+          if (is_int16(-right.immediate())) {
+            if (right.immediate() == 0) {
+              if (cc == eq) {
+                __ Cmpult(result, left, 1);
+              } else {
+                __ Cmpult(result, zero_reg, left);
+              }
+            } else {
+              __ Addl(result, left, Operand(-right.immediate()));
+              if (cc == eq) {
+                __ Cmpult(result, result, 1);
+              } else {
+                __ Cmpult(result, zero_reg, result);
+              }
+            }
+          } else {
+            if (is_uint16(right.immediate())) {
+              __ Xor(result, left, right);
+            } else {
+              __ li(kScratchReg, right);
+              __ Xor(result, left, kScratchReg);
+            }
+            if (cc == eq) {
+              __ Cmpult(result, result, 1);
+            } else {
+              __ Cmpult(result, zero_reg, result);
+            }
+          }
+        } else {
+          __ Xor(result, left, right);
+          if (cc == eq) {
+            __ Cmpult(result, result, 1);
+          } else {
+            __ Cmpult(result, zero_reg, result);
+          }
+        }
+      } break;
+      case lt:
+      case ge: {
+        Register left = i.InputRegister(0);
+        Operand right = i.InputOperand(1);
+        __ Cmplt(result, left, right);
+        if (cc == ge) {
+          __ xor_ins(result, 1, result);
+        }
+      } break;
+      case gt:
+      case le: {
+        Register left = i.InputRegister(1);
+        Operand right = i.InputOperand(0);
+        __ Cmplt(result, left, right);
+        if (cc == le) {
+          __ xor_ins(result, 1, result);
+        }
+      } break;
+      case lo:
+      case hs: {
+        Register left = i.InputRegister(0);
+        Operand right = i.InputOperand(1);
+        __ Cmpult(result, left, right);
+        if (cc == hs) {
+          __ xor_ins(result, 1, result);
+        }
+      } break;
+      case hi:
+      case ls: {
+        Register left = i.InputRegister(1);
+        Operand right = i.InputOperand(0);
+        __ Cmpult(result, left, right);
+        if (cc == ls) {
+          __ xor_ins(result, 1, result);
+        }
+      } break;
+      default:
+        UNREACHABLE();
+    }
+    return;
+  } else if (instr->arch_opcode() == kSw64CmpD ||
+             instr->arch_opcode() == kSw64CmpS) {
+      bool predicate;
+      FlagsConditionToConditionCmpFPU(&predicate, condition);
+#ifdef SW64
+      __ li(result, Operand(1));
+      if (predicate) {
+        __ LoadZeroIfNotFPUCondition(result);
+      } else {
+        __ LoadZeroIfFPUCondition(result);
+      }
+#endif
+    return;
+  } else {
+    PrintF("AssembleArchBranch Unimplemented arch_opcode is : %d\n",
+           instr->arch_opcode());
+    TRACE_UNIMPL();
+    UNIMPLEMENTED();
+  }
+}
+
+void CodeGenerator::AssembleArchBinarySearchSwitch(Instruction* instr) {
+  Sw64OperandConverter i(this, instr);
+  Register input = i.InputRegister(0);
+  std::vector<std::pair<int32_t, Label*>> cases;
+  for (size_t index = 2; index < instr->InputCount(); index += 2) {
+    cases.push_back({i.InputInt32(index + 0), GetLabel(i.InputRpo(index + 1))});
+  }
+  AssembleArchBinarySearchSwitchRange(input, i.InputRpo(1), cases.data(),
+                                      cases.data() + cases.size());
+}
+
+void CodeGenerator::AssembleArchTableSwitch(Instruction* instr) {
+  Sw64OperandConverter i(this, instr);
+  Register input = i.InputRegister(0);
+  size_t const case_count = instr->InputCount() - 2;
+
+  __ Branch(GetLabel(i.InputRpo(1)), hs, input, Operand(case_count));
+  __ GenerateSwitchTable(input, case_count, [&i, this](size_t index) {
+    return GetLabel(i.InputRpo(index + 2));
+  });
+}
+
+void CodeGenerator::FinishFrame(Frame* frame) {
+  auto call_descriptor = linkage()->GetIncomingDescriptor();
+
+  const RegList saves_fpu = call_descriptor->CalleeSavedFPRegisters();
+  if (saves_fpu != 0) {
+    int count = base::bits::CountPopulation(saves_fpu);
+    DCHECK_EQ(kNumCalleeSavedFPU, count);
+    frame->AllocateSavedCalleeRegisterSlots(count *
+                                            (kDoubleSize / kSystemPointerSize));
+  }
+
+  const RegList saves = call_descriptor->CalleeSavedRegisters();
+  if (saves != 0) {
+    int count = base::bits::CountPopulation(saves);
+    DCHECK_EQ(kNumCalleeSaved, count + 1);
+    frame->AllocateSavedCalleeRegisterSlots(count);
+  }
+}
+
+void CodeGenerator::AssembleConstructFrame() {
+  auto call_descriptor = linkage()->GetIncomingDescriptor();
+
+  if (frame_access_state()->has_frame()) {
+    if (call_descriptor->IsCFunctionCall()) {
+      if (info()->GetOutputStackFrameType() == StackFrame::C_WASM_ENTRY) {
+        __ StubPrologue(StackFrame::C_WASM_ENTRY);
+        // Reserve stack space for saving the c_entry_fp later.
+        __ Subl(sp, sp, Operand(kSystemPointerSize));
+      } else {
+        __ Push(ra, fp);
+        __ mov(fp, sp);
+      }
+    } else if (call_descriptor->IsJSFunctionCall()) {
+      __ Prologue();
+    } else {
+      __ StubPrologue(info()->GetOutputStackFrameType());
+      if (call_descriptor->IsWasmFunctionCall()) {
+        __ Push(kWasmInstanceRegister);
+      } else if (call_descriptor->IsWasmImportWrapper() ||
+                 call_descriptor->IsWasmCapiFunction()) {
+        // Wasm import wrappers are passed a tuple in the place of the instance.
+        // Unpack the tuple into the instance and the target callable.
+        // This must be done here in the codegen because it cannot be expressed
+        // properly in the graph.
+        __ Ldl(kJSFunctionRegister,
+              FieldMemOperand(kWasmInstanceRegister, Tuple2::kValue2Offset));
+        __ Ldl(kWasmInstanceRegister,
+              FieldMemOperand(kWasmInstanceRegister, Tuple2::kValue1Offset));
+        __ Push(kWasmInstanceRegister);
+        if (call_descriptor->IsWasmCapiFunction()) {
+          // Reserve space for saving the PC later.
+          __ Subl(sp, sp, Operand(kSystemPointerSize));
+        }
+      }
+    }
+  }
+
+  int required_slots =
+      frame()->GetTotalFrameSlotCount() - frame()->GetFixedSlotCount();
+
+  if (info()->is_osr()) {
+    // TurboFan OSR-compiled functions cannot be entered directly.
+    __ Abort(AbortReason::kShouldNotDirectlyEnterOsrFunction);
+
+    // Unoptimized code jumps directly to this entrypoint while the unoptimized
+    // frame is still on the stack. Optimized code uses OSR values directly from
+    // the unoptimized frame. Thus, all that needs to be done is to allocate the
+    // remaining stack slots.
+    if (FLAG_code_comments) __ RecordComment("-- OSR entrypoint --");
+    osr_pc_offset_ = __ pc_offset();
+    required_slots -= osr_helper()->UnoptimizedFrameSlots();
+    ResetSpeculationPoison();
+  }
+
+  const RegList saves = call_descriptor->CalleeSavedRegisters();
+  const RegList saves_fpu = call_descriptor->CalleeSavedFPRegisters();
+
+  if (required_slots > 0) {
+    DCHECK(frame_access_state()->has_frame());
+    if (info()->IsWasm() && required_slots > 128) {
+      // For WebAssembly functions with big frames we have to do the stack
+      // overflow check before we construct the frame. Otherwise we may not
+      // have enough space on the stack to call the runtime for the stack
+      // overflow.
+      Label done;
+
+      // If the frame is bigger than the stack, we throw the stack overflow
+      // exception unconditionally. Thereby we can avoid the integer overflow
+      // check in the condition code.
+      if ((required_slots * kSystemPointerSize) < (FLAG_stack_size * 1024)) {
+        __ Ldl(
+             kScratchReg,
+             FieldMemOperand(kWasmInstanceRegister,
+                             WasmInstanceObject::kRealStackLimitAddressOffset));
+        __ Ldl(kScratchReg, MemOperand(kScratchReg));
+        __ Addl(kScratchReg, kScratchReg,
+                 Operand(required_slots * kSystemPointerSize));
+        __ Branch(&done, uge, sp, Operand(kScratchReg));
+      }
+
+      __ Call(wasm::WasmCode::kWasmStackOverflow, RelocInfo::WASM_STUB_CALL);
+      // We come from WebAssembly, there are no references for the GC.
+      ReferenceMap* reference_map = zone()->New<ReferenceMap>(zone());
+      RecordSafepoint(reference_map, Safepoint::kNoLazyDeopt);
+      if (FLAG_debug_code) {
+        __ halt();
+      }
+
+      __ bind(&done);
+    }
+  }
+
+  const int returns = frame()->GetReturnSlotCount();
+
+  // Skip callee-saved and return slots, which are pushed below.
+  required_slots -= base::bits::CountPopulation(saves);
+  required_slots -= base::bits::CountPopulation(saves_fpu);
+  required_slots -= returns;
+  if (required_slots > 0) {
+    __ Subl(sp, sp, Operand(required_slots * kSystemPointerSize));
+  }
+
+  if (saves_fpu != 0) {
+    // Save callee-saved FPU registers.
+    __ MultiPushFPU(saves_fpu);
+    DCHECK_EQ(kNumCalleeSavedFPU, base::bits::CountPopulation(saves_fpu));
+  }
+
+  if (saves != 0) {
+    // Save callee-saved registers.
+    __ MultiPush(saves);
+    DCHECK_EQ(kNumCalleeSaved, base::bits::CountPopulation(saves) + 1);
+  }
+
+  if (returns != 0) {
+    // Create space for returns.
+    __ Subl(sp, sp, Operand(returns * kSystemPointerSize));
+  }
+
+#ifdef SW64
+  // should consider float INF, which will lead to SIGFPE.
+  __ setfpec1();
+#endif
+}
+
+void CodeGenerator::AssembleReturn(InstructionOperand* pop) {
+  auto call_descriptor = linkage()->GetIncomingDescriptor();
+
+  const int returns = frame()->GetReturnSlotCount();
+  if (returns != 0) {
+    __ Addl(sp, sp, Operand(returns * kSystemPointerSize));
+  }
+
+  // Restore GP registers.
+  const RegList saves = call_descriptor->CalleeSavedRegisters();
+  if (saves != 0) {
+    __ MultiPop(saves);
+  }
+
+  // Restore FPU registers.
+  const RegList saves_fpu = call_descriptor->CalleeSavedFPRegisters();
+  if (saves_fpu != 0) {
+    __ MultiPopFPU(saves_fpu);
+  }
+
+  Sw64OperandConverter g(this, nullptr);
+  if (call_descriptor->IsCFunctionCall()) {
+    AssembleDeconstructFrame();
+  } else if (frame_access_state()->has_frame()) {
+    // Canonicalize JSFunction return sites for now unless they have an variable
+    // number of stack slot pops.
+    if (pop->IsImmediate() && g.ToConstant(pop).ToInt32() == 0) {
+      if (return_label_.is_bound()) {
+        __ Branch(&return_label_);
+        return;
+      } else {
+        __ bind(&return_label_);
+        AssembleDeconstructFrame();
+      }
+    } else {
+      AssembleDeconstructFrame();
+    }
+  }
+  int pop_count = static_cast<int>(call_descriptor->StackParameterCount());
+  if (pop->IsImmediate()) {
+    pop_count += g.ToConstant(pop).ToInt32();
+  } else {
+    Register pop_reg = g.ToRegister(pop);
+    __ slll(pop_reg, kSystemPointerSizeLog2, pop_reg);
+    __ Addl(sp, sp, pop_reg);
+  }
+  if (pop_count != 0) {
+    __ DropAndRet(pop_count);
+  } else {
+    __ Ret();
+  }
+}
+
+void CodeGenerator::FinishCode() {}
+
+void CodeGenerator::PrepareForDeoptimizationExits(int deopt_count) {}
+
+void CodeGenerator::AssembleMove(InstructionOperand* source,
+                                 InstructionOperand* destination) {
+  Sw64OperandConverter g(this, nullptr);
+  // Dispatch on the source and destination operand kinds.  Not all
+  // combinations are possible.
+  if (source->IsRegister()) {
+    DCHECK(destination->IsRegister() || destination->IsStackSlot());
+    Register src = g.ToRegister(source);
+    if (destination->IsRegister()) {
+      __ mov(g.ToRegister(destination), src);
+    } else {
+      __ Stl(src, g.ToMemOperand(destination));
+    }
+  } else if (source->IsStackSlot()) {
+    DCHECK(destination->IsRegister() || destination->IsStackSlot());
+    MemOperand src = g.ToMemOperand(source);
+    if (destination->IsRegister()) {
+      __ Ldl(g.ToRegister(destination), src);
+    } else {
+      Register temp = kScratchReg;
+      __ Ldl(temp, src);
+      __ Stl(temp, g.ToMemOperand(destination));
+    }
+  } else if (source->IsConstant()) {
+    Constant src = g.ToConstant(source);
+    if (destination->IsRegister() || destination->IsStackSlot()) {
+      Register dst =
+          destination->IsRegister() ? g.ToRegister(destination) : kScratchReg;
+      switch (src.type()) {
+        case Constant::kInt32:
+          __ li(dst, Operand(src.ToInt32()));
+          break;
+        case Constant::kFloat32:
+          __ li(dst, Operand::EmbeddedNumber(src.ToFloat32()));
+          break;
+        case Constant::kInt64:
+          if (RelocInfo::IsWasmReference(src.rmode())) {
+            __ li(dst, Operand(src.ToInt64(), src.rmode()));
+          } else {
+            __ li(dst, Operand(src.ToInt64()));
+          }
+          break;
+        case Constant::kFloat64:
+          __ li(dst, Operand::EmbeddedNumber(src.ToFloat64().value()));
+          break;
+        case Constant::kExternalReference:
+          __ li(dst, src.ToExternalReference());
+          break;
+        case Constant::kDelayedStringConstant:
+          __ li(dst, src.ToDelayedStringConstant());
+          break;
+        case Constant::kHeapObject: {
+          Handle<HeapObject> src_object = src.ToHeapObject();
+          RootIndex index;
+          if (IsMaterializableFromRoot(src_object, &index)) {
+            __ LoadRoot(dst, index);
+          } else {
+            __ li(dst, src_object);
+          }
+          break;
+        }
+        case Constant::kCompressedHeapObject:
+          UNREACHABLE();
+        case Constant::kRpoNumber:
+          UNREACHABLE();  // TODO(titzer): loading RPO numbers on sw64.
+          break;
+      }
+      if (destination->IsStackSlot()) __ Stl(dst, g.ToMemOperand(destination));
+    } else if (src.type() == Constant::kFloat32) {
+      if (destination->IsFPStackSlot()) {
+        MemOperand dst = g.ToMemOperand(destination);
+        if (bit_cast<int32_t>(src.ToFloat32()) == 0) {
+          __ Stl(zero_reg, dst);
+        } else {
+          __ li(kScratchReg, Operand(bit_cast<int32_t>(src.ToFloat32())));
+          __ Stl(kScratchReg, dst);
+        }
+      } else {
+        DCHECK(destination->IsFPRegister());
+        FloatRegister dst = g.ToSingleRegister(destination);
+        __ Move(dst, src.ToFloat32());
+      }
+    } else {
+      DCHECK_EQ(Constant::kFloat64, src.type());
+      DoubleRegister dst = destination->IsFPRegister()
+                               ? g.ToDoubleRegister(destination)
+                               : kScratchDoubleReg;
+      __ Move(dst, src.ToFloat64().value());
+      if (destination->IsFPStackSlot()) {
+        __ Fstd(dst, g.ToMemOperand(destination));
+      }
+    }
+  } else if (source->IsFPRegister()) {
+    MachineRepresentation rep = LocationOperand::cast(source)->representation();
+    if (rep == MachineRepresentation::kSimd128) {
+      UNREACHABLE();
+      // CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      // MSARegister src = g.ToSimd128Register(source);
+      // if (destination->IsSimd128Register()) {
+        //MSARegister dst = g.ToSimd128Register(destination);
+        //__ move_v(dst, src);
+      // } else {
+        //DCHECK(destination->IsSimd128StackSlot());
+        //__ st_b(src, g.ToMemOperand(destination));
+      //}
+    } else {
+      FPURegister src = g.ToDoubleRegister(source);
+      if (destination->IsFPRegister()) {
+        FPURegister dst = g.ToDoubleRegister(destination);
+        __ Move(dst, src);
+      } else {
+        DCHECK(destination->IsFPStackSlot());
+        destination->IsDoubleStackSlot() ?
+            __ Fstd(src, g.ToMemOperand(destination)) :
+            __ Fsts(src, g.ToMemOperand(destination));
+      }
+    }
+  } else if (source->IsFPStackSlot()) {
+    DCHECK(destination->IsFPRegister() || destination->IsFPStackSlot());
+    MemOperand src = g.ToMemOperand(source);
+    MachineRepresentation rep = LocationOperand::cast(source)->representation();
+    if (rep == MachineRepresentation::kSimd128) {
+      UNREACHABLE();
+      // CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      // if (destination->IsSimd128Register()) {
+        //__ ld_b(g.ToSimd128Register(destination), src);
+      // } else {
+        //DCHECK(destination->IsSimd128StackSlot());
+        //MSARegister temp = kSimd128ScratchReg;
+        //__ ld_b(temp, src);
+        //__ st_b(temp, g.ToMemOperand(destination));
+      // }
+    } else {
+      if (destination->IsFPRegister()) {
+        source->IsDoubleStackSlot() ?
+            __ Fldd(g.ToDoubleRegister(destination), src) :
+            __ Flds(g.ToFloatRegister(destination), src);
+      } else {
+        DCHECK(destination->IsFPStackSlot());
+        FPURegister temp = kScratchDoubleReg;
+        source->IsDoubleStackSlot() ?  __ Fldd(temp, src) : __ Flds(temp, src);
+        destination->IsDoubleStackSlot() ?
+            __ Fstd(temp, g.ToMemOperand(destination)) :
+            __ Fsts(temp, g.ToMemOperand(destination));
+      }
+    }
+  } else {
+    UNREACHABLE();
+  }
+}
+
+void CodeGenerator::AssembleSwap(InstructionOperand* source,
+                                 InstructionOperand* destination) {
+  Sw64OperandConverter g(this, nullptr);
+  // Dispatch on the source and destination operand kinds.  Not all
+  // combinations are possible.
+  if (source->IsRegister()) {
+    // Register-register.
+    Register temp = kScratchReg;
+    Register src = g.ToRegister(source);
+    if (destination->IsRegister()) {
+      Register dst = g.ToRegister(destination);
+      __ Move(temp, src);
+      __ Move(src, dst);
+      __ Move(dst, temp);
+    } else {
+      DCHECK(destination->IsStackSlot());
+      MemOperand dst = g.ToMemOperand(destination);
+      __ mov(temp, src);
+      __ Ldl(src, dst);
+      __ Stl(temp, dst);
+    }
+  } else if (source->IsStackSlot()) {
+    DCHECK(destination->IsStackSlot());
+    Register temp_0 = kScratchReg;
+    Register temp_1 = kScratchReg2;
+    MemOperand src = g.ToMemOperand(source);
+    MemOperand dst = g.ToMemOperand(destination);
+    __ Ldl(temp_0, src);
+    __ Ldl(temp_1, dst);
+    __ Stl(temp_0, dst);
+    __ Stl(temp_1, src);
+  } else if (source->IsFPRegister()) {
+    MachineRepresentation rep = LocationOperand::cast(source)->representation();
+    if (rep == MachineRepresentation::kSimd128) {
+      UNREACHABLE();
+      // CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      // MSARegister temp = kSimd128ScratchReg;
+      // MSARegister src = g.ToSimd128Register(source);
+      // if (destination->IsSimd128Register()) {
+        //MSARegister dst = g.ToSimd128Register(destination);
+        //__ move_v(temp, src);
+        //__ move_v(src, dst);
+        //__ move_v(dst, temp);
+      // } else {
+        //DCHECK(destination->IsSimd128StackSlot());
+        //MemOperand dst = g.ToMemOperand(destination);
+        //__ move_v(temp, src);
+        //__ ld_b(src, dst);
+        //__ st_b(temp, dst);
+      // }
+    } else {
+      FPURegister temp = kScratchDoubleReg;
+      FPURegister src = g.ToDoubleRegister(source);
+      if (destination->IsFPRegister()) {
+        FPURegister dst = g.ToDoubleRegister(destination);
+        __ Move(temp, src);
+        __ Move(src, dst);
+        __ Move(dst, temp);
+      } else {
+        DCHECK(destination->IsFPStackSlot());
+        MemOperand dst = g.ToMemOperand(destination);
+        __ Move(temp, src);
+        destination->IsDoubleStackSlot() ?  __ Fldd(src, dst) : __ Flds(src, dst);
+        source->IsDoubleRegister() ? __ Fstd(temp, dst) : __ Fsts(temp, dst);
+      }
+    }
+  } else if (source->IsFPStackSlot()) {
+    DCHECK(destination->IsFPStackSlot());
+    Register temp_0 = kScratchReg;
+    MemOperand src0 = g.ToMemOperand(source);
+    MemOperand src1(src0.rm(), src0.offset() + kIntSize);
+    MemOperand dst0 = g.ToMemOperand(destination);
+    MemOperand dst1(dst0.rm(), dst0.offset() + kIntSize);
+    MachineRepresentation rep = LocationOperand::cast(source)->representation();
+    if (rep == MachineRepresentation::kSimd128) {
+      //MemOperand src2(src0.rm(), src0.offset() + 2 * kIntSize);
+      //MemOperand src3(src0.rm(), src0.offset() + 3 * kIntSize);
+      //MemOperand dst2(dst0.rm(), dst0.offset() + 2 * kIntSize);
+      //MemOperand dst3(dst0.rm(), dst0.offset() + 3 * kIntSize);
+      //CpuFeatureScope msa_scope(tasm(), SW64_SIMD);
+      //MSARegister temp_1 = kSimd128ScratchReg;
+      //__ ld_b(temp_1, dst0);  // Save destination in temp_1.
+      //__ Ldw(temp_0, src0);    // Then use temp_0 to copy source to destination.
+      //__ Stw(temp_0, dst0);
+      //__ Ldw(temp_0, src1);
+      //__ Stw(temp_0, dst1);
+      //__ Ldw(temp_0, src2);
+      //__ Stw(temp_0, dst2);
+      //__ Ldw(temp_0, src3);
+      //__ Stw(temp_0, dst3);
+      //__ st_b(temp_1, src0);
+      UNREACHABLE();
+    } else {
+      FPURegister temp_1 = kScratchDoubleReg;
+      __ Fldd(temp_1, dst0);  // Save destination in temp_1.
+      __ Ldw(temp_0, src0);    // Then use temp_0 to copy source to destination.
+      __ Stw(temp_0, dst0);
+      __ Ldw(temp_0, src1);
+      __ Stw(temp_0, dst1);
+      __ Fstd(temp_1, src0);
+    }
+  } else {
+    // No other combinations are possible.
+    UNREACHABLE();
+  }
+}
+
+void CodeGenerator::AssembleJumpTable(Label** targets, size_t target_count) {
+  // On 64-bit SW64 we emit the jump tables inline.
+  UNREACHABLE();
+}
+
+#undef ASSEMBLE_ATOMIC_LOAD_INTEGER
+#undef ASSEMBLE_ATOMIC_STORE_INTEGER
+#undef ASSEMBLE_ATOMIC_BINOP
+#undef ASSEMBLE_ATOMIC_BINOP_EXT
+#undef ASSEMBLE_ATOMIC_EXCHANGE_INTEGER
+#undef ASSEMBLE_ATOMIC_EXCHANGE_INTEGER_EXT
+#undef ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER
+#undef ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT
+#undef ASSEMBLE_IEEE754_BINOP
+#undef ASSEMBLE_IEEE754_UNOP
+
+#undef TRACE_MSG
+#undef TRACE_UNIMPL
+#undef __
+
+}  // namespace compiler
+}  // namespace internal
+}  // namespace v8
diff --git a/src/3rdparty/chromium/v8/src/compiler/backend/sw64/instruction-codes-sw64.h b/src/3rdparty/chromium/v8/src/compiler/backend/sw64/instruction-codes-sw64.h
new file mode 100755
index 0000000000..65df932f13
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/compiler/backend/sw64/instruction-codes-sw64.h
@@ -0,0 +1,427 @@
+// Copyright 2014 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef V8_COMPILER_BACKEND_SW64_INSTRUCTION_CODES_SW64_H_
+#define V8_COMPILER_BACKEND_SW64_INSTRUCTION_CODES_SW64_H_
+
+namespace v8 {
+namespace internal {
+namespace compiler {
+
+// SW64-specific opcodes that specify which assembly sequence to emit.
+// Most opcodes specify a single instruction.
+#define TARGET_ARCH_OPCODE_LIST(V)  \
+  V(Sw64Add)                      \
+  V(Sw64Dadd)                     \
+  V(Sw64DaddOvf)                  \
+  V(Sw64Sub)                      \
+  V(Sw64Dsub)                     \
+  V(Sw64DsubOvf)                  \
+  V(Sw64Mul)                      \
+  V(Sw64MulOvf)                   \
+  V(Sw64MulHigh)                  \
+  V(Sw64DMulHigh)                 \
+  V(Sw64MulHighU)                 \
+  V(Sw64Dmul)                     \
+  V(Sw64Div)                      \
+  V(Sw64Ddiv)                     \
+  V(Sw64DivU)                     \
+  V(Sw64DdivU)                    \
+  V(Sw64Mod)                      \
+  V(Sw64Dmod)                     \
+  V(Sw64ModU)                     \
+  V(Sw64DmodU)                    \
+  V(Sw64And)                      \
+  V(Sw64And32)                    \
+  V(Sw64Or)                       \
+  V(Sw64Or32)                     \
+  V(Sw64Nor)                      \
+  V(Sw64Nor32)                    \
+  V(Sw64Xor)                      \
+  V(Sw64Xor32)                    \
+  V(Sw64Clz)                      \
+  V(Sw64Lsa)                      \
+  V(Sw64Dlsa)                     \
+  V(Sw64Shl)                      \
+  V(Sw64Shr)                      \
+  V(Sw64Sar)                      \
+  V(Sw64Ext)                      \
+  V(Sw64Ins)                      \
+  V(Sw64Dext)                     \
+  V(Sw64Dins)                     \
+  V(Sw64Dclz)                     \
+  V(Sw64Ctz)                      \
+  V(Sw64Dctz)                     \
+  V(Sw64Popcnt)                   \
+  V(Sw64Dpopcnt)                  \
+  V(Sw64Dshl)                     \
+  V(Sw64Dshr)                     \
+  V(Sw64Dsar)                     \
+  V(Sw64Ror)                      \
+  V(Sw64Dror)                     \
+  V(Sw64Mov)                      \
+  V(Sw64Tst)                      \
+  V(Sw64Cmp)                      \
+  V(Sw64CmpS)                     \
+  V(Sw64AddS)                     \
+  V(Sw64SubS)                     \
+  V(Sw64MulS)                     \
+  V(Sw64DivS)                     \
+  V(Sw64ModS)                     \
+  V(Sw64AbsS)                     \
+  V(Sw64NegS)                     \
+  V(Sw64SqrtS)                    \
+  V(Sw64MaxS)                     \
+  V(Sw64MinS)                     \
+  V(Sw64CmpD)                     \
+  V(Sw64AddD)                     \
+  V(Sw64SubD)                     \
+  V(Sw64MulD)                     \
+  V(Sw64DivD)                     \
+  V(Sw64ModD)                     \
+  V(Sw64AbsD)                     \
+  V(Sw64NegD)                     \
+  V(Sw64SqrtD)                    \
+  V(Sw64MaxD)                     \
+  V(Sw64MinD)                     \
+  V(Sw64Float64RoundDown)         \
+  V(Sw64Float64RoundTruncate)     \
+  V(Sw64Float64RoundUp)           \
+  V(Sw64Float64RoundTiesEven)     \
+  V(Sw64Float32RoundDown)         \
+  V(Sw64Float32RoundTruncate)     \
+  V(Sw64Float32RoundUp)           \
+  V(Sw64Float32RoundTiesEven)     \
+  V(Sw64CvtSD)                    \
+  V(Sw64CvtDS)                    \
+  V(Sw64TruncWD)                  \
+  V(Sw64RoundWD)                  \
+  V(Sw64FloorWD)                  \
+  V(Sw64CeilWD)                   \
+  V(Sw64TruncWS)                  \
+  V(Sw64RoundWS)                  \
+  V(Sw64FloorWS)                  \
+  V(Sw64CeilWS)                   \
+  V(Sw64TruncLS)                  \
+  V(Sw64TruncLD)                  \
+  V(Sw64TruncUwD)                 \
+  V(Sw64TruncUwS)                 \
+  V(Sw64TruncUlS)                 \
+  V(Sw64TruncUlD)                 \
+  V(Sw64CvtDW)                    \
+  V(Sw64CvtSL)                    \
+  V(Sw64CvtSW)                    \
+  V(Sw64CvtSUw)                   \
+  V(Sw64CvtSUl)                   \
+  V(Sw64CvtDL)                    \
+  V(Sw64CvtDUw)                   \
+  V(Sw64CvtDUl)                   \
+  V(Sw64Ldb)                      \
+  V(Sw64Ldbu)                     \
+  V(Sw64Stb)                      \
+  V(Sw64Ldh)                      \
+  V(Sw64Uldh)                     \
+  V(Sw64Ldhu)                     \
+  V(Sw64Uldhu)                    \
+  V(Sw64Sth)                      \
+  V(Sw64Usth)                     \
+  V(Sw64Ldl)                      \
+  V(Sw64Uldl)                     \
+  V(Sw64Ldw)                      \
+  V(Sw64Uldw)                     \
+  V(Sw64Ldwu)                     \
+  V(Sw64Uldwu)                    \
+  V(Sw64Stw)                      \
+  V(Sw64Ustw)                     \
+  V(Sw64Stl)                      \
+  V(Sw64Ustl)                     \
+  V(Sw64Flds)                     \
+  V(Sw64Uflds)                    \
+  V(Sw64Fsts)                     \
+  V(Sw64Ufsts)                    \
+  V(Sw64Fldd)                     \
+  V(Sw64Ufldd)                    \
+  V(Sw64Fstd)                     \
+  V(Sw64Ufstd)                    \
+  V(Sw64BitcastDL)                \
+  V(Sw64BitcastLD)                \
+  V(Sw64BitcastSW)      \
+  V(Sw64BitcastWS)      \
+  V(Sw64Float64ExtractLowWord32)  \
+  V(Sw64Float64ExtractHighWord32) \
+  V(Sw64Float64InsertLowWord32)   \
+  V(Sw64Float64InsertHighWord32)  \
+  V(Sw64Float32Max)               \
+  V(Sw64Float64Max)               \
+  V(Sw64Float32Min)               \
+  V(Sw64Float64Min)               \
+  V(Sw64Float64SilenceNaN)        \
+  V(Sw64Push)                     \
+  V(Sw64Peek)                     \
+  V(Sw64StoreToStackSlot)         \
+  V(Sw64ByteSwap64)               \
+  V(Sw64ByteSwap32)               \
+  V(Sw64StackClaim)               \
+  V(Sw64Seb)                      \
+  V(Sw64Seh)                      \
+  V(Sw64Sync)                     \
+  V(Sw64AssertEqual)              \
+  V(Sw64S128Const)                \
+  V(Sw64S128Zero)                 \
+  V(Sw64S128AllOnes)              \
+  V(Sw64I32x4Splat)               \
+  V(Sw64I32x4ExtractLane)         \
+  V(Sw64I32x4ReplaceLane)         \
+  V(Sw64I32x4Add)                 \
+  V(Sw64I32x4AddHoriz)            \
+  V(Sw64I32x4Sub)                 \
+  V(Sw64F64x2Abs)                 \
+  V(Sw64F64x2Neg)                 \
+  V(Sw64F32x4Splat)               \
+  V(Sw64F32x4ExtractLane)         \
+  V(Sw64F32x4ReplaceLane)         \
+  V(Sw64F32x4SConvertI32x4)       \
+  V(Sw64F32x4UConvertI32x4)       \
+  V(Sw64I32x4Mul)                 \
+  V(Sw64I32x4MaxS)                \
+  V(Sw64I32x4MinS)                \
+  V(Sw64I32x4Eq)                  \
+  V(Sw64I32x4Ne)                  \
+  V(Sw64I32x4Shl)                 \
+  V(Sw64I32x4ShrS)                \
+  V(Sw64I32x4ShrU)                \
+  V(Sw64I32x4MaxU)                \
+  V(Sw64I32x4MinU)                \
+  V(Sw64F64x2Sqrt)                         \
+  V(Sw64F64x2Add)                          \
+  V(Sw64F64x2Sub)                          \
+  V(Sw64F64x2Mul)                          \
+  V(Sw64F64x2Div)                          \
+  V(Sw64F64x2Min)                          \
+  V(Sw64F64x2Max)                          \
+  V(Sw64F64x2Eq)                           \
+  V(Sw64F64x2Ne)                           \
+  V(Sw64F64x2Lt)                           \
+  V(Sw64F64x2Le)                           \
+  V(Sw64F64x2Splat)                        \
+  V(Sw64F64x2ExtractLane)                  \
+  V(Sw64F64x2ReplaceLane)                  \
+  V(Sw64F64x2Pmin)                         \
+  V(Sw64F64x2Pmax)                         \
+  V(Sw64F64x2Ceil)                         \
+  V(Sw64F64x2Floor)                        \
+  V(Sw64F64x2Trunc)                        \
+  V(Sw64F64x2NearestInt)                   \
+  V(Sw64I64x2Splat)                        \
+  V(Sw64I64x2ExtractLane)                  \
+  V(Sw64I64x2ReplaceLane)                  \
+  V(Sw64I64x2Add)                          \
+  V(Sw64I64x2Sub)                          \
+  V(Sw64I64x2Mul)                          \
+  V(Sw64I64x2Neg)                          \
+  V(Sw64I64x2Shl)                          \
+  V(Sw64I64x2ShrS)                         \
+  V(Sw64I64x2ShrU)                         \
+  V(Sw64F32x4Abs)                 \
+  V(Sw64F32x4Neg)                 \
+  V(Sw64F32x4Sqrt)                         \
+  V(Sw64F32x4RecipApprox)         \
+  V(Sw64F32x4RecipSqrtApprox)     \
+  V(Sw64F32x4Add)                 \
+  V(Sw64F32x4AddHoriz)            \
+  V(Sw64F32x4Sub)                 \
+  V(Sw64F32x4Mul)                 \
+  V(Sw64F32x4Div)                          \
+  V(Sw64F32x4Max)                 \
+  V(Sw64F32x4Min)                 \
+  V(Sw64F32x4Eq)                  \
+  V(Sw64F32x4Ne)                  \
+  V(Sw64F32x4Lt)                  \
+  V(Sw64F32x4Le)                  \
+  V(Sw64F32x4Pmin)                         \
+  V(Sw64F32x4Pmax)                         \
+  V(Sw64F32x4Ceil)                         \
+  V(Sw64F32x4Floor)                        \
+  V(Sw64F32x4Trunc)                        \
+  V(Sw64F32x4NearestInt)                   \
+  V(Sw64I32x4SConvertF32x4)       \
+  V(Sw64I32x4UConvertF32x4)       \
+  V(Sw64I32x4Neg)                 \
+  V(Sw64I32x4GtS)                 \
+  V(Sw64I32x4GeS)                 \
+  V(Sw64I32x4GtU)                 \
+  V(Sw64I32x4GeU)                 \
+  V(Sw64I32x4Abs)                          \
+  V(Sw64I32x4BitMask)                      \
+  V(Sw64I16x8Splat)               \
+  V(Sw64I16x8ExtractLaneU)                 \
+  V(Sw64I16x8ExtractLaneS)                 \
+  V(Sw64I16x8ReplaceLane)         \
+  V(Sw64I16x8Neg)                 \
+  V(Sw64I16x8Shl)                 \
+  V(Sw64I16x8ShrS)                \
+  V(Sw64I16x8ShrU)                \
+  V(Sw64I16x8Add)                 \
+  V(Sw64I16x8AddSaturateS)        \
+  V(Sw64I16x8AddHoriz)            \
+  V(Sw64I16x8Sub)                 \
+  V(Sw64I16x8SubSaturateS)        \
+  V(Sw64I16x8Mul)                 \
+  V(Sw64I16x8MaxS)                \
+  V(Sw64I16x8MinS)                \
+  V(Sw64I16x8Eq)                  \
+  V(Sw64I16x8Ne)                  \
+  V(Sw64I16x8GtS)                 \
+  V(Sw64I16x8GeS)                 \
+  V(Sw64I16x8AddSaturateU)        \
+  V(Sw64I16x8SubSaturateU)        \
+  V(Sw64I16x8MaxU)                \
+  V(Sw64I16x8MinU)                \
+  V(Sw64I16x8GtU)                 \
+  V(Sw64I16x8GeU)                 \
+  V(Sw64I16x8RoundingAverageU)             \
+  V(Sw64I16x8Abs)                          \
+  V(Sw64I16x8BitMask)                      \
+  V(Sw64I8x16Splat)               \
+  V(Sw64I8x16ExtractLaneU)                 \
+  V(Sw64I8x16ExtractLaneS)                 \
+  V(Sw64I8x16ReplaceLane)         \
+  V(Sw64I8x16Neg)                 \
+  V(Sw64I8x16Shl)                 \
+  V(Sw64I8x16ShrS)                \
+  V(Sw64I8x16Add)                 \
+  V(Sw64I8x16AddSaturateS)        \
+  V(Sw64I8x16Sub)                 \
+  V(Sw64I8x16SubSaturateS)        \
+  V(Sw64I8x16Mul)                 \
+  V(Sw64I8x16MaxS)                \
+  V(Sw64I8x16MinS)                \
+  V(Sw64I8x16Eq)                  \
+  V(Sw64I8x16Ne)                  \
+  V(Sw64I8x16GtS)                 \
+  V(Sw64I8x16GeS)                 \
+  V(Sw64I8x16ShrU)                \
+  V(Sw64I8x16AddSaturateU)        \
+  V(Sw64I8x16SubSaturateU)        \
+  V(Sw64I8x16MaxU)                \
+  V(Sw64I8x16MinU)                \
+  V(Sw64I8x16GtU)                 \
+  V(Sw64I8x16GeU)                 \
+  V(Sw64I8x16RoundingAverageU)             \
+  V(Sw64I8x16Abs)                          \
+  V(Sw64I8x16BitMask)                      \
+  V(Sw64S128And)                  \
+  V(Sw64S128Or)                   \
+  V(Sw64S128Xor)                  \
+  V(Sw64S128Not)                  \
+  V(Sw64S128Select)               \
+  V(Sw64S128AndNot)                        \
+  V(Sw64V32x4AnyTrue)                      \
+  V(Sw64V32x4AllTrue)                      \
+  V(Sw64V16x8AnyTrue)                      \
+  V(Sw64V16x8AllTrue)                      \
+  V(Sw64V8x16AnyTrue)                      \
+  V(Sw64V8x16AllTrue)                      \
+  V(Sw64S32x4InterleaveRight)     \
+  V(Sw64S32x4InterleaveLeft)      \
+  V(Sw64S32x4PackEven)            \
+  V(Sw64S32x4PackOdd)             \
+  V(Sw64S32x4InterleaveEven)      \
+  V(Sw64S32x4InterleaveOdd)       \
+  V(Sw64S32x4Shuffle)             \
+  V(Sw64S16x8InterleaveRight)     \
+  V(Sw64S16x8InterleaveLeft)      \
+  V(Sw64S16x8PackEven)            \
+  V(Sw64S16x8PackOdd)             \
+  V(Sw64S16x8InterleaveEven)      \
+  V(Sw64S16x8InterleaveOdd)       \
+  V(Sw64S16x4Reverse)             \
+  V(Sw64S16x2Reverse)             \
+  V(Sw64S8x16InterleaveRight)     \
+  V(Sw64S8x16InterleaveLeft)      \
+  V(Sw64S8x16PackEven)            \
+  V(Sw64S8x16PackOdd)             \
+  V(Sw64S8x16InterleaveEven)      \
+  V(Sw64S8x16InterleaveOdd)       \
+  V(Sw64I8x16Shuffle)             \
+  V(Sw64I8x16Swizzle)             \
+  V(Sw64S8x16Concat)              \
+  V(Sw64S8x8Reverse)              \
+  V(Sw64S8x4Reverse)              \
+  V(Sw64S8x2Reverse)              \
+  V(Sw64MsaLd)                    \
+  V(Sw64MsaSt)                    \
+  V(Sw64I32x4SConvertI16x8Low)    \
+  V(Sw64I32x4SConvertI16x8High)   \
+  V(Sw64I32x4UConvertI16x8Low)    \
+  V(Sw64I32x4UConvertI16x8High)   \
+  V(Sw64I16x8SConvertI8x16Low)    \
+  V(Sw64I16x8SConvertI8x16High)   \
+  V(Sw64I16x8SConvertI32x4)       \
+  V(Sw64I16x8UConvertI32x4)       \
+  V(Sw64I16x8UConvertI8x16Low)    \
+  V(Sw64I16x8UConvertI8x16High)   \
+  V(Sw64I8x16SConvertI16x8)       \
+  V(Sw64I8x16UConvertI16x8)       \
+  V(Sw64Word64AtomicLoadUint8)    \
+  V(Sw64Word64AtomicLoadUint16)   \
+  V(Sw64Word64AtomicLoadUint32)   \
+  V(Sw64Word64AtomicLoadUint64)   \
+  V(Sw64Word64AtomicStoreWord8)   \
+  V(Sw64Word64AtomicStoreWord16)  \
+  V(Sw64Word64AtomicStoreWord32)  \
+  V(Sw64Word64AtomicStoreWord64)  \
+  V(Sw64Word64AtomicAddUint8)     \
+  V(Sw64Word64AtomicAddUint16)    \
+  V(Sw64Word64AtomicAddUint32)    \
+  V(Sw64Word64AtomicAddUint64)    \
+  V(Sw64Word64AtomicSubUint8)     \
+  V(Sw64Word64AtomicSubUint16)    \
+  V(Sw64Word64AtomicSubUint32)    \
+  V(Sw64Word64AtomicSubUint64)    \
+  V(Sw64Word64AtomicAndUint8)     \
+  V(Sw64Word64AtomicAndUint16)             \
+  V(Sw64Word64AtomicAndUint32)             \
+  V(Sw64Word64AtomicAndUint64)             \
+  V(Sw64Word64AtomicOrUint8)               \
+  V(Sw64Word64AtomicOrUint16)              \
+  V(Sw64Word64AtomicOrUint32)              \
+  V(Sw64Word64AtomicOrUint64)              \
+  V(Sw64Word64AtomicXorUint8)              \
+  V(Sw64Word64AtomicXorUint16)             \
+  V(Sw64Word64AtomicXorUint32)             \
+  V(Sw64Word64AtomicXorUint64)             \
+  V(Sw64Word64AtomicExchangeUint8)         \
+  V(Sw64Word64AtomicExchangeUint16)        \
+  V(Sw64Word64AtomicExchangeUint32)        \
+  V(Sw64Word64AtomicExchangeUint64)        \
+  V(Sw64Word64AtomicCompareExchangeUint8)  \
+  V(Sw64Word64AtomicCompareExchangeUint16) \
+  V(Sw64Word64AtomicCompareExchangeUint32) \
+  V(Sw64Word64AtomicCompareExchangeUint64)
+
+// Addressing modes represent the "shape" of inputs to an instruction.
+// Many instructions support multiple addressing modes. Addressing modes
+// are encoded into the InstructionCode of the instruction and tell the
+// code generator after register allocation which assembler method to call.
+//
+// We use the following local notation for addressing modes:
+//
+// R = register
+// O = register or stack slot
+// D = double register
+// I = immediate (handle, external, int32)
+// MRI = [register + immediate]
+// MRR = [register + register]
+// TODO(plind): Add the new r6 address modes.
+#define TARGET_ADDRESSING_MODE_LIST(V) \
+  V(MRI) /* [%r0 + K] */               \
+  V(MRR) /* [%r0 + %r1] */
+
+
+}  // namespace compiler
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_COMPILER_SW64_INSTRUCTION_CODES_SW64_H_
diff --git a/src/3rdparty/chromium/v8/src/compiler/backend/sw64/instruction-scheduler-sw64.cc b/src/3rdparty/chromium/v8/src/compiler/backend/sw64/instruction-scheduler-sw64.cc
new file mode 100755
index 0000000000..d9fbb9927c
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/compiler/backend/sw64/instruction-scheduler-sw64.cc
@@ -0,0 +1,1558 @@
+// Copyright 2015 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#include "src/codegen/macro-assembler.h"
+#include "src/compiler/backend/instruction-scheduler.h"
+
+namespace v8 {
+namespace internal {
+namespace compiler {
+
+bool InstructionScheduler::SchedulerSupported() { return true; }
+
+int InstructionScheduler::GetTargetInstructionFlags(
+    const Instruction* instr) const {
+  switch(instr->arch_opcode())
+	{
+    case kSw64Add:                       
+    case kSw64Dadd:                      
+    case kSw64DaddOvf:                   
+    case kSw64Sub:                       
+    case kSw64Dsub:                      
+    case kSw64DsubOvf:                   
+    case kSw64Mul:                       
+    case kSw64MulOvf:                    
+    case kSw64MulHigh:                   
+    case kSw64DMulHigh:                  
+    case kSw64MulHighU:                  
+    case kSw64Dmul:                      
+    case kSw64Div:                       
+    case kSw64Ddiv:                      
+    case kSw64DivU:                      
+    case kSw64DdivU:                     
+    case kSw64Mod:                       
+    case kSw64Dmod:                      
+    case kSw64ModU:                      
+    case kSw64DmodU:                     
+    case kSw64And:                       
+    case kSw64And32:                     
+    case kSw64Or:                        
+    case kSw64Or32:                      
+    case kSw64Nor:                       
+    case kSw64Nor32:                     
+    case kSw64Xor:                       
+    case kSw64Xor32:                     
+    case kSw64Clz:                       
+    case kSw64Lsa:                       
+    case kSw64Dlsa:                      
+    case kSw64Shl:                       
+    case kSw64Shr:                       
+    case kSw64Sar:                       
+    case kSw64Ext:                       
+    case kSw64Ins:                       
+    case kSw64Dext:                      
+    case kSw64Dins:                      
+    case kSw64Dclz:                      
+    case kSw64Ctz:                       
+    case kSw64Dctz:
+	    
+    case kSw64Popcnt:                    
+    case kSw64Dpopcnt:                   
+    case kSw64Dshl:                      
+    case kSw64Dshr:                      
+    case kSw64Dsar:
+	    
+    case kSw64Ror:                       
+    case kSw64Dror:                      
+    case kSw64Mov:                       
+    case kSw64Tst:                       
+    case kSw64Cmp:                       
+    case kSw64CmpS:                      
+    case kSw64AddS:                      
+    case kSw64SubS:                      
+    case kSw64MulS:                      
+    case kSw64DivS:                                            
+    case kSw64AbsS:                      
+    case kSw64NegS:                      
+    case kSw64SqrtS:                     
+    case kSw64MaxS:                      
+    case kSw64MinS:                      
+    case kSw64CmpD:                      
+    case kSw64AddD:                      
+    case kSw64SubD:                      
+    case kSw64MulD:                      
+    case kSw64DivD:                                           
+    case kSw64AbsD:                      
+    case kSw64NegD:                      
+    case kSw64SqrtD:                     
+    case kSw64MaxD:                      
+    case kSw64MinD:
+	    
+    case kSw64Float64RoundDown:          
+    case kSw64Float64RoundTruncate:      
+    case kSw64Float64RoundUp:            
+    case kSw64Float64RoundTiesEven:      
+    case kSw64Float32RoundDown:          
+    case kSw64Float32RoundTruncate:      
+    case kSw64Float32RoundUp:            
+    case kSw64Float32RoundTiesEven: 
+    
+    case kSw64CvtSD:                     
+    case kSw64CvtDS:                     
+    case kSw64TruncWD:                   
+    case kSw64RoundWD:                   
+    case kSw64FloorWD:                   
+    case kSw64CeilWD:                    
+    case kSw64TruncWS:                   
+    case kSw64RoundWS:                   
+    case kSw64FloorWS:                   
+    case kSw64CeilWS:
+	    
+    case kSw64TruncLS:                   
+    case kSw64TruncLD:                   
+    case kSw64TruncUwD:                  
+    case kSw64TruncUwS:                  
+    case kSw64TruncUlS:                  
+    case kSw64TruncUlD:
+	    
+    case kSw64CvtDW:                     
+    case kSw64CvtSL:                     
+    case kSw64CvtSW:                     
+    case kSw64CvtSUw:                    
+    case kSw64CvtSUl:                    
+    case kSw64CvtDL:                     
+    case kSw64CvtDUw:                    
+    case kSw64CvtDUl:
+	    
+    case kSw64BitcastDL:                 
+    case kSw64BitcastLD:                 
+    case kSw64BitcastSW:       
+    case kSw64BitcastWS:       
+    case kSw64Float64ExtractLowWord32:   
+    case kSw64Float64ExtractHighWord32:  
+    case kSw64Float64InsertLowWord32:    
+    case kSw64Float64InsertHighWord32:   
+    case kSw64Float32Max:                
+    case kSw64Float64Max:                
+    case kSw64Float32Min:                
+    case kSw64Float64Min:                
+    case kSw64Float64SilenceNaN:
+  	    
+    case kSw64ByteSwap64:                
+    case kSw64ByteSwap32:	    
+	    
+    case kSw64Seb:                       
+    case kSw64Seh:                       
+    case kSw64AssertEqual:  
+/*    case kSw64S128Zero:                  
+    case kSw64I32x4Splat:                
+    case kSw64I32x4ExtractLane:          
+    case kSw64I32x4ReplaceLane:          
+    case kSw64I32x4Add:                  
+    case kSw64I32x4AddHoriz:             
+    case kSw64I32x4Sub:                  
+    case kSw64F32x4Splat:                
+    case kSw64F32x4ExtractLane:          
+    case kSw64F32x4ReplaceLane:          
+    case kSw64F32x4SConvertI32x4:        
+    case kSw64F32x4UConvertI32x4:        
+    case kSw64I32x4Mul:                  
+    case kSw64I32x4MaxS:                 
+    case kSw64I32x4MinS:                 
+    case kSw64I32x4Eq:                   
+    case kSw64I32x4Ne:                   
+    case kSw64I32x4Shl:                  
+    case kSw64I32x4ShrS:                 
+    case kSw64I32x4ShrU:                 
+    case kSw64I32x4MaxU:                 
+    case kSw64I32x4MinU:                 
+    case kSw64F32x4Abs:                  
+    case kSw64F32x4Neg:                  
+    case kSw64F32x4RecipApprox:          
+    case kSw64F32x4RecipSqrtApprox:      
+    case kSw64F32x4Add:                  
+    case kSw64F32x4AddHoriz:             
+    case kSw64F32x4Sub:                  
+    case kSw64F32x4Mul:                  
+    case kSw64F32x4Max:                  
+    case kSw64F32x4Min:                  
+    case kSw64F32x4Eq:                   
+    case kSw64F32x4Ne:                   
+    case kSw64F32x4Lt:                   
+    case kSw64F32x4Le:                   
+    case kSw64I32x4SConvertF32x4:        
+    case kSw64I32x4UConvertF32x4:        
+    case kSw64I32x4Neg:                  
+    case kSw64I32x4GtS:                  
+    case kSw64I32x4GeS:                  
+    case kSw64I32x4GtU:                  
+    case kSw64I32x4GeU:                  
+    case kSw64I16x8Splat:                
+    case kSw64I16x8ExtractLane:          
+    case kSw64I16x8ReplaceLane:          
+    case kSw64I16x8Neg:                  
+    case kSw64I16x8Shl:                  
+    case kSw64I16x8ShrS:                 
+    case kSw64I16x8ShrU:                 
+    case kSw64I16x8Add:                  
+    case kSw64I16x8AddSaturateS:         
+    case kSw64I16x8AddHoriz:             
+    case kSw64I16x8Sub:                  
+    case kSw64I16x8SubSaturateS:         
+    case kSw64I16x8Mul:                  
+    case kSw64I16x8MaxS:                 
+    case kSw64I16x8MinS:                 
+    case kSw64I16x8Eq:                   
+    case kSw64I16x8Ne:                   
+    case kSw64I16x8GtS:                  
+    case kSw64I16x8GeS:                  
+    case kSw64I16x8AddSaturateU:         
+    case kSw64I16x8SubSaturateU:         
+    case kSw64I16x8MaxU:                 
+    case kSw64I16x8MinU:                 
+    case kSw64I16x8GtU:                  
+    case kSw64I16x8GeU:                  
+    case kSw64I8x16Splat:                
+    case kSw64I8x16ExtractLane:          
+    case kSw64I8x16ReplaceLane:          
+    case kSw64I8x16Neg:                  
+    case kSw64I8x16Shl:                  
+    case kSw64I8x16ShrS:                 
+    case kSw64I8x16Add:                  
+    case kSw64I8x16AddSaturateS:         
+    case kSw64I8x16Sub:                  
+    case kSw64I8x16SubSaturateS:         
+    case kSw64I8x16Mul:                  
+    case kSw64I8x16MaxS:                 
+    case kSw64I8x16MinS:                 
+    case kSw64I8x16Eq:                   
+    case kSw64I8x16Ne:                   
+    case kSw64I8x16GtS:                  
+    case kSw64I8x16GeS:                  
+    case kSw64I8x16ShrU:                 
+    case kSw64I8x16AddSaturateU:         
+    case kSw64I8x16SubSaturateU:         
+    case kSw64I8x16MaxU:                 
+    case kSw64I8x16MinU:                 
+    case kSw64I8x16GtU:                  
+    case kSw64I8x16GeU:                  
+    case kSw64S128And:                   
+    case kSw64S128Or:                    
+    case kSw64S128Xor:                   
+    case kSw64S128Not:                   
+    case kSw64S128Select:   */             
+   /* case kSw64S32x4InterleaveRight:      
+    case kSw64S32x4InterleaveLeft:       
+    case kSw64S32x4PackEven:             
+    case kSw64S32x4PackOdd:              
+    case kSw64S32x4InterleaveEven:       
+    case kSw64S32x4InterleaveOdd:        
+    case kSw64S32x4Shuffle:              
+    case kSw64S16x8InterleaveRight:      
+    case kSw64S16x8InterleaveLeft:       
+    case kSw64S16x8PackEven:             
+    case kSw64S16x8PackOdd:              
+    case kSw64S16x8InterleaveEven:       
+    case kSw64S16x8InterleaveOdd:        
+    case kSw64S16x4Reverse:              
+    case kSw64S16x2Reverse:              
+    case kSw64S8x16InterleaveRight:      
+    case kSw64S8x16InterleaveLeft:       
+    case kSw64S8x16PackEven:             
+    case kSw64S8x16PackOdd:              
+    case kSw64S8x16InterleaveEven:       
+    case kSw64S8x16InterleaveOdd:        
+    case kSw64S8x16Shuffle:              
+    case kSw64S8x16Concat:               
+    case kSw64S8x8Reverse:               
+    case kSw64S8x4Reverse:               
+    case kSw64S8x2Reverse:                                    
+    case kSw64MsaSt:                     
+    case kSw64I32x4SConvertI16x8Low:     
+    case kSw64I32x4SConvertI16x8High:    
+    case kSw64I32x4UConvertI16x8Low:     
+    case kSw64I32x4UConvertI16x8High:    
+    case kSw64I16x8SConvertI8x16Low:     
+    case kSw64I16x8SConvertI8x16High:    
+    case kSw64I16x8SConvertI32x4:        
+    case kSw64I16x8UConvertI32x4:        
+    case kSw64I16x8UConvertI8x16Low:     
+    case kSw64I16x8UConvertI8x16High:    
+    case kSw64I8x16SConvertI16x8:        
+    case kSw64I8x16UConvertI16x8:*/
+	   return kNoOpcodeFlags;
+    case kSw64Ldb:                        
+    case kSw64Ldbu:
+    case kSw64Ldh:                        
+    case kSw64Uldh:                       
+    case kSw64Ldhu:                       
+    case kSw64Uldhu:
+    case kSw64Ldl:                        
+    case kSw64Uldl:                       
+    case kSw64Ldw:                        
+    case kSw64Uldw:                       
+    case kSw64Ldwu:                       
+    case kSw64Uldwu: 
+    case kSw64Flds:                      
+    case kSw64Uflds:
+    case kSw64Fldd:                      
+    case kSw64Ufldd: 
+    case kSw64Peek:
+    case kSw64MsaLd:
+    case kSw64Word64AtomicLoadUint8:              
+    case kSw64Word64AtomicLoadUint16:             
+    case kSw64Word64AtomicLoadUint32:             
+    case kSw64Word64AtomicLoadUint64:
+	  return kIsLoadOperation;
+    case kSw64ModS:
+    case kSw64ModD: 
+    case kSw64Stb:                                              
+    case kSw64Sth:                        
+    case kSw64Usth:                                            
+    case kSw64Stw:                        
+    case kSw64Ustw:                       
+    case kSw64Stl:                        
+    case kSw64Ustl:                                            
+    case kSw64Fsts:                      
+    case kSw64Ufsts:                                         
+    case kSw64Fstd:                      
+    case kSw64Ufstd:
+    case kSw64Push:
+    case kSw64Word64AtomicStoreWord8:             
+    case kSw64Word64AtomicStoreWord16:            
+    case kSw64Word64AtomicStoreWord32:            
+    case kSw64Word64AtomicStoreWord64:            
+    case kSw64Word64AtomicAddUint8:               
+    case kSw64Word64AtomicAddUint16:              
+    case kSw64Word64AtomicAddUint32:              
+    case kSw64Word64AtomicAddUint64:              
+    case kSw64Word64AtomicSubUint8:               
+    case kSw64Word64AtomicSubUint16:              
+    case kSw64Word64AtomicSubUint32:              
+    case kSw64Word64AtomicSubUint64:              
+    case kSw64Word64AtomicAndUint8:               
+    case kSw64Word64AtomicAndUint16:              
+    case kSw64Word64AtomicAndUint32:              
+    case kSw64Word64AtomicAndUint64:              
+    case kSw64Word64AtomicOrUint8:                
+    case kSw64Word64AtomicOrUint16:               
+    case kSw64Word64AtomicOrUint32:               
+    case kSw64Word64AtomicOrUint64:               
+    case kSw64Word64AtomicXorUint8:               
+    case kSw64Word64AtomicXorUint16:              
+    case kSw64Word64AtomicXorUint32:              
+    case kSw64Word64AtomicXorUint64:              
+    case kSw64Word64AtomicExchangeUint8:          
+    case kSw64Word64AtomicExchangeUint16:         
+    case kSw64Word64AtomicExchangeUint32:         
+    case kSw64Word64AtomicExchangeUint64:         
+    case kSw64Word64AtomicCompareExchangeUint8:   
+    case kSw64Word64AtomicCompareExchangeUint16:  
+    case kSw64Word64AtomicCompareExchangeUint32:  
+    case kSw64Word64AtomicCompareExchangeUint64:
+    case kSw64StoreToStackSlot:	
+    case kSw64StackClaim:	
+	  return kHasSideEffect;
+    default:
+	  return kHasSideEffect;	
+	}
+}
+
+enum Latency {
+  FSELEQ = 2,
+  FSELNE = 2,
+  FSELLT = 2,
+  FSELLE = 2,
+  FSELGT = 2,
+  FSELGE = 2,
+
+  FCPYS = 2,
+  FCPYSE = 2,
+  FCPYSN = 2,
+
+  SLLOW = 3,  //256 sll
+  SRLOW = 3,  //256 srl 
+
+  BRANCH = 4,  // sw ??
+  RINT_S = 4,  // Estimated.
+  RINT_D = 4,  // Estimated.
+
+  LDBU = 4,
+  LDHU = 4,
+  LDW = 4,
+  LDL = 4,
+  LDL_U = 4,
+  STB = 4,
+  STH = 4,
+  STW = 4,
+  STL = 4,
+
+  IFMOVS = 4,
+  IFMOVD = 4,
+  FIMOVS = 4,
+  FIMOVD = 4,
+
+  FLDS = 4,
+  FLDD = 4,
+  FSTS = 4,
+  FSTD = 4,
+
+  MULW = 4,
+  MULL = 4,
+  UMULH = 4,
+
+  FCVTSD = 4,
+  FCVTDS = 4,
+  FCVTDL = 4,
+  FCVTWL = 4,
+  FCVTLW = 4,
+  FCVTLS = 4,
+  FCVTLD = 4,
+  FCVTDL_Z = 4,
+  FCVTDL_P = 4,
+  FCVTDL_G = 4,
+  FCVTDL_N = 4,
+
+  FMAS = 6,
+  FMAD = 6,
+  FMSS = 6,
+  FMSD = 6,
+  FNMAS = 6,
+  FNMAD = 6,
+  FNMSS = 6,
+  FNMSD = 6,
+
+  FADDS = 6,
+  FADDD = 6,
+  FSUBS = 6,
+  FSUBD = 6,
+  FMULS = 6,
+  FMULD = 6,
+
+  FCMPDEQ = 6,
+  FCMPDLE = 6,
+  FCMPDLT = 6,
+  FCMPDUN = 6,
+
+  FDIVS = 17,
+  FSQRTS = 17,
+  FSQRTD = 31,
+  FDIVD = 32,
+};
+
+int DadduLatency(bool is_operand_register = true) {
+  if (is_operand_register) {
+    return 1;
+  } else {
+    return 2;  // Estimated max.
+  }
+}
+
+int SrlwLatency(bool is_operand_register = true) {
+  if (is_operand_register) {
+    return 4;
+  } else {
+    return 3;  // Estimated max.
+  }
+}
+
+int DsubuLatency(bool is_operand_register = true) {
+  return DadduLatency(is_operand_register);
+}
+
+int AndLatency(bool is_operand_register = true) {
+  return DadduLatency(is_operand_register);
+}
+
+int OrLatency(bool is_operand_register = true) {
+  return DadduLatency(is_operand_register);
+}
+
+int NorLatency(bool is_operand_register = true) {
+  if (is_operand_register) {
+    return 2;
+  } else {
+    return 3;  // Estimated max.
+  }
+}
+
+int XorLatency(bool is_operand_register = true) {
+  return DadduLatency(is_operand_register);
+}
+
+int MulLatency(bool is_operand_register = true) {
+  if (is_operand_register) {
+    return Latency::MULW;
+  } else {
+    return Latency::MULW + 1;
+  }
+}
+
+int DmulLatency(bool is_operand_register = true) {
+  int latency = 0;
+  latency = Latency::MULL;
+  if (!is_operand_register) {
+    latency += 1;
+  }
+  return latency;
+}
+
+//Mulwh =(li +)mull + sral 
+int MulhLatency(bool is_operand_register = true) {
+  int latency = 0;
+  latency = Latency::MULL + 1;
+  if (!is_operand_register) {
+    latency += 1;
+  }
+  return latency;
+}
+
+//Mulhu = zapnot + zapnot + mull 
+int MulhuLatency(bool is_operand_register = true) {
+  int latency = 0;
+  latency = Latency::MULL + 2;
+  if (!is_operand_register) {
+    latency += 1;
+  }
+  return latency;
+}
+
+//Dmulh = (li +)umulh + srll + mull + subl + srll +mull +subl 
+int DMulhLatency(bool is_operand_register = true) {
+  int latency = 0;
+  latency = Latency::MULL + Latency::MULL + 5;
+  if (!is_operand_register) {
+    latency += 1;
+  }
+  return latency;
+}
+
+//Divw = ifmovd*2 + fcvtld*2 + fdivd + fcvtdl_z + fimovd
+int DivLatency(bool is_operand_register = true) {
+  if (is_operand_register) {
+    return Latency::IFMOVD * 2 + Latency::FCVTLD * 2 +  Latency::FDIVD +  Latency::FCVTDL_Z +  Latency::FIMOVD;
+  } else {
+    return Latency::IFMOVD * 2 + Latency::FCVTLD * 2 +  Latency::FDIVD +  Latency::FCVTDL_Z +  Latency::FIMOVD + 1;
+  }
+}
+
+//Divwu = Divw + zapnot * 2
+int DivuLatency(bool is_operand_register = true) {
+  if (is_operand_register) {
+    return Latency::IFMOVD * 2 + Latency::FCVTLD * 2 +  Latency::FDIVD +  Latency::FCVTDL_Z +  Latency::FIMOVD + 2;
+  } else {
+    return Latency::IFMOVD * 2 + Latency::FCVTLD * 2 +  Latency::FDIVD +  Latency::FCVTDL_Z +  Latency::FIMOVD + 2 + 1;
+  }
+}
+
+//Divl = ifmovd * 2 + fcvtld * 2 + fdivd + fcvtdl_z + fimovd
+int DdivLatency(bool is_operand_register = true) {
+  int latency = 0;
+  latency = Latency::IFMOVD * 2 + Latency::FCVTLD * 2 +  Latency::FDIVD +  Latency::FCVTDL_Z +  Latency::FIMOVD;
+  if (!is_operand_register) {
+    latency += 1;
+  }
+  return latency;
+}
+
+//Divlu = Divl
+int DdivuLatency(bool is_operand_register = true) {
+  int latency = 0;
+  latency = Latency::IFMOVD * 2 + Latency::FCVTLD * 2 +  Latency::FDIVD +  Latency::FCVTDL_Z +  Latency::FIMOVD;
+  if (!is_operand_register) {
+    latency += 1;
+  }
+  return latency;
+}
+
+//Modw = Divw + mulw + subw
+int ModLatency(bool is_operand_register = true) {
+  int latency = 0;
+  latency = Latency::IFMOVD * 2 + Latency::FCVTLD * 2 +  Latency::FDIVD +  Latency::FCVTDL_Z +  Latency::FIMOVD + Latency::MULW + DadduLatency(is_operand_register);
+  if (!is_operand_register) {
+    latency += 1;
+  }
+  return latency;
+}
+
+//Modwu = Modw + zapnot * 2
+int ModuLatency(bool is_operand_register = true) {
+  int latency = 0;
+  latency = Latency::IFMOVD * 2 + Latency::FCVTLD * 2 +  Latency::FDIVD +  Latency::FCVTDL_Z +  Latency::FIMOVD + Latency::MULW + DadduLatency(is_operand_register);;
+  if (!is_operand_register) {
+    latency += 1;
+  }
+  return latency;
+}
+
+//Modl = Modw
+int DmodLatency(bool is_operand_register = true) {
+  int latency = 0;
+  latency = Latency::IFMOVD * 2 + Latency::FCVTLD * 2 +  Latency::FDIVD +  Latency::FCVTDL_Z +  Latency::FIMOVD + Latency::MULW + DadduLatency(is_operand_register);
+  if (!is_operand_register) {
+    latency += 1;
+  }
+  return latency;
+}
+
+//Modlu = Modl
+int DmoduLatency(bool is_operand_register = true) {
+  int latency = 0;
+  latency = Latency::IFMOVD * 2 + Latency::FCVTLD * 2 +  Latency::FDIVD +  Latency::FCVTDL_Z +  Latency::FIMOVD + Latency::MULW + DadduLatency(is_operand_register);
+  if (!is_operand_register) {
+    latency += 1;
+  }
+  return latency;
+}
+
+int DlsaLatency() {
+  // Estimated max.
+  return DadduLatency() + 1;
+}
+
+int CallLatency() {
+  // Estimated.
+  return DadduLatency(false) + Latency::BRANCH + 5;
+}
+
+int JumpLatency() {
+  // Estimated max.
+  return 1 + DadduLatency() + Latency::BRANCH + 2;
+}
+
+int SmiUntagLatency() { return 1; }
+
+int PrepareForTailCallLatency() {
+  // Estimated max.
+  return 2 * (DlsaLatency() + DadduLatency(false)) + 2 + Latency::BRANCH +
+         Latency::BRANCH + 2 * DsubuLatency(false) + 2 + Latency::BRANCH + 1;
+}
+
+int AssemblePopArgumentsAdoptFrameLatency() {
+  return 1 + Latency::BRANCH + 1 + SmiUntagLatency() +
+         PrepareForTailCallLatency();
+}
+
+int AssertLatency() { return 1; }
+
+int PrepareCallCFunctionLatency() {
+  int frame_alignment = TurboAssembler::ActivationFrameAlignment();
+  if (frame_alignment > kSystemPointerSize) {
+    return 1 + DsubuLatency(false) + AndLatency(false) + 1;
+  } else {
+    return DsubuLatency(false);
+  }
+}
+
+int AdjustBaseAndOffsetLatency() {
+  return 3;  // Estimated max.
+}
+
+int AlignedMemoryLatency() { return AdjustBaseAndOffsetLatency() + 1; }
+
+int MultiPushLatency() {
+  int latency = DsubuLatency(false);
+  for (int16_t i = kNumRegisters - 1; i >= 0; i--) {
+    latency++;
+  }
+  return latency;
+}
+
+int MultiPushFPULatency() {
+  int latency = DsubuLatency(false);
+  for (int16_t i = kNumRegisters - 1; i >= 0; i--) {
+    latency += 4;
+  }
+  return latency;
+}
+
+int PushCallerSavedLatency(SaveFPRegsMode fp_mode) {
+  int latency = MultiPushLatency();
+  if (fp_mode == kSaveFPRegs) {
+    latency += MultiPushFPULatency();
+  }
+  return latency;
+}
+
+int MultiPopLatency() {
+  int latency = DadduLatency(false);
+  for (int16_t i = 0; i < kNumRegisters; i++) {
+    latency++;
+  }
+  return latency;
+}
+
+int MultiPopFPULatency() {
+  int latency = DadduLatency(false);
+  for (int16_t i = 0; i < kNumRegisters; i++) {
+    latency += 4;
+  }
+  return latency;
+}
+
+int PopCallerSavedLatency(SaveFPRegsMode fp_mode) {
+  int latency = MultiPopLatency();
+  if (fp_mode == kSaveFPRegs) {
+    latency += MultiPopFPULatency();
+  }
+  return latency;
+}
+
+int CallCFunctionHelperLatency() {
+  // Estimated.
+  int latency = AndLatency(false) + Latency::BRANCH + 2 + CallLatency();
+  if (base::OS::ActivationFrameAlignment() > kSystemPointerSize) {
+    latency++;
+  } else {
+    latency += DadduLatency(false);
+  }
+  return latency;
+}
+
+int CallCFunctionLatency() { return 1 + CallCFunctionHelperLatency(); }
+
+int AssembleArchJumpLatency() {
+  // Estimated max.
+  return Latency::BRANCH;
+}
+
+int AssembleArchLookupSwitchLatency(const Instruction* instr) {
+  int latency = 0;
+  for (size_t index = 2; index < instr->InputCount(); index += 2) {
+    latency += 1 + Latency::BRANCH;
+  }
+  return latency + AssembleArchJumpLatency();
+}
+
+int GenerateSwitchTableLatency() {
+  int latency = 0;  
+  latency = DlsaLatency() + 2;
+
+  latency += 2;
+  return latency;
+}
+
+int AssembleArchTableSwitchLatency() {
+  return Latency::BRANCH + GenerateSwitchTableLatency();
+}
+
+int DropAndRetLatency() {
+  // Estimated max.
+  return DadduLatency(false) + JumpLatency();
+}
+
+int AssemblerReturnLatency() {
+  // Estimated max.
+  return DadduLatency(false) + MultiPopLatency() + MultiPopFPULatency() +
+         Latency::BRANCH + DadduLatency() + 1 + DropAndRetLatency();
+}
+
+int TryInlineTruncateDoubleToILatency() {
+  return 2 + Latency::FCVTDL_Z + Latency::FCVTLW + Latency::FIMOVS + 1 + 2 + AndLatency(false) +
+         Latency::BRANCH;
+}
+
+int CallStubDelayedLatency() { return 1 + CallLatency(); }
+
+int TruncateDoubleToIDelayedLatency() {
+  // TODO(sw64): This no longer reflects how TruncateDoubleToI is called.
+  return TryInlineTruncateDoubleToILatency() + 1 + DsubuLatency(false) +
+         4 + CallStubDelayedLatency() + DadduLatency(false) + 1;
+}
+
+int CheckPageFlagLatency() {
+  return AndLatency(false) + AlignedMemoryLatency() + AndLatency(false) +
+         Latency::BRANCH;
+}
+
+int SltuLatency(bool is_operand_register = true) {
+  if (is_operand_register) {
+    return 1;
+  } else {
+    return 2;  // Estimated max.
+  }
+}
+
+int BranchShortHelperR6Latency() {
+  return 2;  // Estimated max.
+}
+
+int BranchShortHelperLatency() {
+  return SltuLatency() + 2;  // Estimated max.
+}
+
+int BranchShortLatency(BranchDelaySlot bdslot = PROTECT) {
+    return BranchShortHelperR6Latency();
+}
+
+int MoveLatency() { return 1; }
+
+int MovToFloatParametersLatency() { return 2 * MoveLatency(); }
+
+int MovFromFloatResultLatency() { return MoveLatency(); }
+
+int DaddOverflowLatency() {
+  // Estimated max.
+  return 6;
+}
+
+int DsubOverflowLatency() {
+  // Estimated max.
+  return 6;
+}
+
+int MulOverflowLatency() {
+  // Estimated max.
+  return MulLatency() + MulhLatency() + 2;
+}
+
+//Clz = addw + sellt + blt + ctlz + ldi + subl
+int ClzLatency() { return 6; }
+
+//Dclz = cttz
+int DclzLatency(){ return 1;}
+
+//Ctz = cttz + ldi + subl + selge
+int CtzLatency() {
+  return 4;
+}
+
+//Dctz = cttz
+int DctzLatency() {
+  return 1;
+}
+
+//popcnt = zapnot + ctpop
+int PopcntLatency() {
+  return 2;
+}
+
+//Dpopcnt = ctpop
+int DpopcntLatency() {
+  return 1;
+}
+
+//Ext TODO
+int ExtLatency(){
+  return 1;
+}
+
+//Ins = li + and_ins + slll * 2 + bic + bis + addw
+int InsLatency(){
+  return 7;
+}
+
+//Dins = Ins - addw
+int DinsLatency(){
+  return 6;
+}
+
+//Ror = and_ins + ldi + subw + and_ins + zapnot + srll + addw + and_ins + slll + addw + bis + addw
+int RorLatency(bool is_operand_register = true){
+  if (is_operand_register) {
+     return 11;
+  } else {
+     return 7;
+  }
+}
+
+int DrorLatency(bool is_operand_register = true){
+  if (is_operand_register) {
+     return 5;
+  } else {
+     return 3;
+  }
+}
+
+int CompareFLatency() { return 1; }
+
+int CompareF32Latency() { return CompareFLatency(); }
+
+int CompareF64Latency() { return CompareFLatency(); }
+
+int CompareIsNanFLatency() { return CompareFLatency(); }
+
+int CompareIsNanF32Latency() { return CompareIsNanFLatency(); }
+
+int CompareIsNanF64Latency() { return CompareIsNanFLatency(); }
+
+int Float64RoundLatency() {
+  return 32;
+}
+
+int Float32RoundLatency() {
+  return 36;
+}
+
+int PushLatency() { return DadduLatency() + AlignedMemoryLatency(); }
+
+int ByteSwapSignedLatency() { return 2; }
+
+int LlLatency(int offset) {
+  bool is_one_instruction = is_int9(offset);
+  if (is_one_instruction) {
+    return 1;
+  } else {
+    return 3;
+  }
+}
+
+int ExtractBitsLatency(bool sign_extend, int size) {
+  int latency = 2;
+  if (sign_extend) {
+    switch (size) {
+      case 8:
+      case 16:
+      case 32:
+        latency += 1;
+        break;
+      default:
+        UNREACHABLE();
+    }
+  }
+  return latency;
+}
+
+int InsertBitsLatency() { return 2 + DsubuLatency(false) + 2; }
+
+int ScLatency(int offset) {
+  bool is_one_instruction = is_int9(offset);
+  if (is_one_instruction) {
+    return 1;
+  } else {
+    return 3;
+  }
+}
+
+int Word32AtomicExchangeLatency(bool sign_extend, int size) {
+  return DadduLatency(false) + 1 + DsubuLatency() + 2 + LlLatency(0) +
+         ExtractBitsLatency(sign_extend, size) + InsertBitsLatency() +
+         ScLatency(0) + BranchShortLatency() + 1;
+}
+
+int Word32AtomicCompareExchangeLatency(bool sign_extend, int size) {
+  return 2 + DsubuLatency() + 2 + LlLatency(0) +
+         ExtractBitsLatency(sign_extend, size) + InsertBitsLatency() +
+         ScLatency(0) + BranchShortLatency() + 1;
+}
+
+
+int InstructionScheduler::GetInstructionLatency(const Instruction* instr) {
+  switch(instr->arch_opcode())
+	{
+	  case kArchCallCodeObject: 
+      case kArchCallWasmFunction: 
+	    return CallLatency(); 
+      case kArchTailCallCodeObjectFromJSFunction:
+      case kArchTailCallCodeObject: {
+	    int latency = 0;
+        if (instr->arch_opcode() == kArchTailCallCodeObjectFromJSFunction) {
+          latency = AssemblePopArgumentsAdoptFrameLatency();
+        }
+        return latency + JumpLatency();
+      }
+      case kArchTailCallWasm: 
+      case kArchTailCallAddress: 
+	    return JumpLatency();
+      case kArchCallJSFunction: {
+		int latency = 0;
+        if (FLAG_debug_code) {
+          latency = 1 + AssertLatency();
+        }
+        return latency + 1 + DadduLatency(false) + CallLatency();
+      }
+      case kArchPrepareCallCFunction: 
+	    return PrepareCallCFunctionLatency();
+      case kArchSaveCallerRegisters: {
+		auto fp_mode =
+          static_cast<SaveFPRegsMode>(MiscField::decode(instr->opcode()));
+        return PushCallerSavedLatency(fp_mode);
+      }
+      case kArchRestoreCallerRegisters: {
+		auto fp_mode =
+          static_cast<SaveFPRegsMode>(MiscField::decode(instr->opcode()));
+        return PopCallerSavedLatency(fp_mode);
+      }
+      case kArchPrepareTailCall:
+		return 2;
+      case kArchCallCFunction: 
+		return CallCFunctionLatency();
+      case kArchJmp:
+		return AssembleArchJumpLatency();
+      case kArchTableSwitch:
+		return AssembleArchTableSwitchLatency();
+    case kArchAbortCSAAssert:
+		return CallLatency() + 1;
+      case kArchDebugBreak:
+		return 1;
+      case kArchComment:
+      case kArchNop:
+      case kArchThrowTerminator:
+      case kArchDeoptimize: 
+		return 0;
+      case kArchRet:
+		return AssemblerReturnLatency();
+      case kArchFramePointer:
+		return 1;
+      case kArchParentFramePointer:
+		return AlignedMemoryLatency();
+      case kArchTruncateDoubleToI:
+		return TruncateDoubleToIDelayedLatency();
+      case kArchStoreWithWriteBarrier: 
+		return DadduLatency() + 1 + CheckPageFlagLatency();
+      case kArchStackSlot: 
+		return DadduLatency(false) + AndLatency(false) + AssertLatency() +
+               DadduLatency(false) + AndLatency(false) + BranchShortLatency() +
+               1 + DsubuLatency() + DadduLatency();
+      case kArchWordPoisonOnSpeculation:
+		return AndLatency();
+      case kIeee754Float64Acos:
+      case kIeee754Float64Acosh:
+      case kIeee754Float64Asin:
+      case kIeee754Float64Asinh:
+      case kIeee754Float64Atan:
+      case kIeee754Float64Atanh:
+      case kIeee754Float64Atan2:
+      case kIeee754Float64Cos:
+      case kIeee754Float64Cosh:
+      case kIeee754Float64Cbrt:
+      case kIeee754Float64Exp:
+      case kIeee754Float64Expm1:
+      case kIeee754Float64Log:
+      case kIeee754Float64Log1p:
+      case kIeee754Float64Log2:
+      case kIeee754Float64Log10:
+      case kIeee754Float64Pow: 
+      case kIeee754Float64Sin:
+      case kIeee754Float64Sinh:
+      case kIeee754Float64Tan:
+      case kIeee754Float64Tanh:
+	    return PrepareCallCFunctionLatency() + MovToFloatParametersLatency() +
+               CallCFunctionLatency() + MovFromFloatResultLatency();
+      case kSw64Add:
+      case kSw64Dadd:
+		return DadduLatency(instr->InputAt(1)->IsRegister());
+      case kSw64DaddOvf:
+		return DaddOverflowLatency();
+      case kSw64Sub:
+      case kSw64Dsub:
+		return DsubuLatency(instr->InputAt(1)->IsRegister());
+      case kSw64DsubOvf:
+		return DsubOverflowLatency();
+      case kSw64Mul:
+		return MulLatency();
+      case kSw64MulOvf:
+		return MulOverflowLatency();
+      case kSw64MulHigh:
+		return MulhLatency();
+      case kSw64MulHighU:
+		return MulhuLatency();
+      case kSw64DMulHigh:
+		return DMulhLatency();
+      case kSw64Div: 
+		return DivLatency(instr->InputAt(1)->IsRegister());
+      case kSw64DivU: 
+		return DivuLatency(instr->InputAt(1)->IsRegister());
+      case kSw64Mod:
+		return ModLatency(instr->InputAt(1)->IsRegister());
+      case kSw64ModU: 
+		return ModuLatency(instr->InputAt(1)->IsRegister());
+      case kSw64Dmul:
+	    return DmulLatency(instr->InputAt(1)->IsRegister());
+      case kSw64Ddiv: 
+		return ModLatency(instr->InputAt(1)->IsRegister());
+      case kSw64DdivU:
+		return DdivLatency(instr->InputAt(1)->IsRegister());
+      case kSw64Dmod: 
+		return DmodLatency(instr->InputAt(1)->IsRegister());
+      case kSw64DmodU: 
+		return DmodLatency(instr->InputAt(1)->IsRegister());
+      case kSw64Dlsa:
+      case kSw64Lsa:
+		return DlsaLatency();
+      case kSw64And:
+		return AndLatency(instr->InputAt(1)->IsRegister());
+      case kSw64And32:{
+		bool is_operand_register = instr->InputAt(1)->IsRegister();
+        int latency = AndLatency(is_operand_register);
+        if (is_operand_register) {
+          return latency + 2;
+        } else {
+          return latency + 1;
+        }
+      }
+      case kSw64Or:
+		return OrLatency(instr->InputAt(1)->IsRegister());
+      case kSw64Or32:{
+    	bool is_operand_register = instr->InputAt(1)->IsRegister();
+        int latency = OrLatency(is_operand_register);
+        if (is_operand_register) {
+          return latency + 2;
+        } else {
+          return latency + 1;
+        }
+      }
+      case kSw64Nor: 
+		return NorLatency(instr->InputAt(1)->IsRegister());
+      case kSw64Nor32:{
+		bool is_operand_register = instr->InputAt(1)->IsRegister();
+        int latency = NorLatency(is_operand_register);
+        if (is_operand_register) {
+          return latency + 2;
+        } else {
+          return latency + 1;
+        }
+      }
+      case kSw64Xor:
+		return XorLatency(instr->InputAt(1)->IsRegister());
+      case kSw64Xor32:{
+    	bool is_operand_register = instr->InputAt(1)->IsRegister();
+        int latency = XorLatency(is_operand_register);
+        if (is_operand_register) {
+          return latency + 2;
+        } else {
+          return latency + 1;
+        }
+      }
+      case kSw64Clz:
+	    return ClzLatency();
+      case kSw64Dclz:
+	    return DclzLatency();
+      case kSw64Ctz: 
+		return CtzLatency();
+      case kSw64Dctz: 
+		return DctzLatency();
+      case kSw64Popcnt: 
+		return PopcntLatency();
+      case kSw64Dpopcnt: 
+		return DpopcntLatency();
+      case kSw64Shl:
+		return 1;
+      case kSw64Shr:
+      case kSw64Sar:
+		return 2;
+      case kSw64Ext:
+      case kSw64Dext: 
+		return ExtLatency();
+      case kSw64Ins:
+		return InsLatency();
+      case kSw64Dins:
+        return DinsLatency();
+      case kSw64Dshl:
+      case kSw64Dshr:
+      case kSw64Dsar:
+		return 1;
+      case kSw64Ror:
+		return RorLatency(instr->InputAt(1)->IsRegister());
+      case kSw64Dror:
+		return DrorLatency(instr->InputAt(1)->IsRegister());
+      case kSw64Tst:
+		return AndLatency(instr->InputAt(1)->IsRegister());
+      case kSw64Mov:
+		return 1;
+      case kSw64CmpS: 
+		return CompareF32Latency();
+      case kSw64AddS:
+      case kSw64SubS:
+      case kSw64MulS:
+      case kSw64DivS:
+		return 1;
+      case kSw64ModS: 
+		return PrepareCallCFunctionLatency() + MovToFloatParametersLatency() +
+               CallCFunctionLatency() + MovFromFloatResultLatency();
+      case kSw64AbsS:
+		return Latency::FCPYS;
+      case kSw64NegS:
+		return 1;
+      case kSw64SqrtS: 
+		return Latency::FSQRTS;
+ //   case kSw64MaxS:
+ //   case kSw64MinS:
+      case kSw64CmpD: 
+		return CompareF64Latency();
+      case kSw64AddD:
+      case kSw64SubD:
+      case kSw64MulD:
+      case kSw64DivD:
+		return 1;
+      case kSw64ModD: 
+		return PrepareCallCFunctionLatency() + MovToFloatParametersLatency() +
+               CallCFunctionLatency() + MovFromFloatResultLatency();
+      case kSw64AbsD:
+		return Latency::FCPYS;
+      case kSw64NegD:
+		return 1;
+      case kSw64SqrtD: 
+		return Latency::FSQRTD;
+//    case kSw64MaxD:
+//    case kSw64MinD: 
+      case kSw64Float64RoundDown: 
+      case kSw64Float64RoundTruncate: 
+      case kSw64Float64RoundUp: 
+      case kSw64Float64RoundTiesEven: 
+	    return Float64RoundLatency(); //TODO
+      case kSw64Float32RoundDown: 
+      case kSw64Float32RoundTruncate: 
+      case kSw64Float32RoundUp: 
+      case kSw64Float32RoundTiesEven: 
+	    return Float32RoundLatency(); //TODO
+    /*  case kSw64Float32Max: 
+		return Float32MaxLatency();
+      case kSw64Float64Max: 
+		return Float64MaxLatency();
+      case kSw64Float32Min: 
+		return Float32MinLatency();
+      case kSw64Float64Min: 
+		return Float64MinLatency();*/
+      case kSw64Float64SilenceNaN:
+		 return Latency::FSUBD;
+      case kSw64CvtSD:
+		return Latency::FCVTDS;
+      case kSw64CvtDS:
+		return Latency::FCVTSD;
+      case kSw64CvtDW: 
+		return Latency::IFMOVD + Latency::FCVTLD;
+      case kSw64CvtSW: 
+		return Latency::IFMOVS + Latency::FCVTWL + Latency::FCVTLS;
+      case kSw64CvtSUw: 
+	    return 1 + Latency::FIMOVS;
+      case kSw64CvtSL: 
+		return Latency::IFMOVD + Latency::FCVTLS;
+      case kSw64CvtDL: 
+		return Latency::IFMOVD + Latency::FCVTLD;
+      case kSw64CvtDUw: 
+		return 1 + Latency::FIMOVS;
+      case kSw64CvtDUl: 
+		return 1 + Latency::FIMOVD;
+      case kSw64CvtSUl:
+		return 1 + Latency::FIMOVD;
+      case kSw64FloorWD: 
+		return Latency::FCVTDL_N + Latency::FCVTLW + Latency::FIMOVS;
+      case kSw64CeilWD: 
+		return Latency::FCVTDL_P + Latency::FCVTLW + Latency::FIMOVS;
+      case kSw64RoundWD: 
+		return Latency::FCVTDL_G + Latency::FCVTLW + Latency::FIMOVS;
+      case kSw64TruncWD: 
+		return Latency::FCVTDL_Z + Latency::FCVTLW + Latency::FIMOVS;
+      case kSw64FloorWS: 
+		return Latency::FCVTSD + Latency::FCVTDL_N + Latency::FCVTLW + Latency::FIMOVS;
+      case kSw64CeilWS: 
+		return Latency::FCVTSD + Latency::FCVTDL_P + Latency::FCVTLW + Latency::FIMOVS;
+      case kSw64RoundWS: 
+		return Latency::FCVTSD + Latency::FCVTDL_G + Latency::FCVTLW + Latency::FIMOVS;
+      case kSw64TruncWS: 
+		return Latency::FCVTSD + Latency::FCVTDL_Z + Latency::FCVTLW + Latency::FIMOVS + 3; //cmplt selne ?
+  //    case kSw64TruncLS: 
+		//return 5; //TODO
+  //    case kSw64TruncLD: 
+		//return 5;
+  //    case kSw64TruncUwD: 
+		//return CompareF64Latency() + 2 * Latency::BRANCH +
+  //             2 * Latency::TRUNC_W_D + Latency::SUB_D + OrLatency() +
+  //             Latency::MTC1 + Latency::MFC1 + Latency::MTHC1 + 1;
+  //    case kSw64TruncUwS: 
+		//return CompareF32Latency() + 2 * Latency::BRANCH +
+  //             2 * Latency::TRUNC_W_S + Latency::SUB_S + OrLatency() +
+  //             Latency::MTC1 + 2 * Latency::MFC1 + 2 + MovzLatency();
+  //    case kSw64TruncUlS: 
+	 //   return TruncUlSLatency();
+  //    case kSw64TruncUlD: 
+		//return TruncUlDLatency();
+	//TODO
+      case kSw64BitcastDL:    // D -> L
+		return 4;
+      case kSw64BitcastLD:
+		return 4;
+      case kSw64BitcastSW:    // W -> S
+      case kSw64BitcastWS:    // S -> W
+		return 4;
+      case kSw64Float64ExtractLowWord32:
+      case kSw64Float64InsertLowWord32:
+		return 12;
+      case kSw64Float64ExtractHighWord32:
+      case kSw64Float64InsertHighWord32: 
+		return 5;
+      case kSw64Seb:
+      case kSw64Seh:
+		return 1;
+      case kSw64Ldbu:
+      case kSw64Ldhu:
+      case kSw64Ldl:
+      case kSw64Uldl:
+      case kSw64Ldw:
+      case kSw64Uldw:
+      case kSw64Flds: 
+      case kSw64Uflds: 
+      case kSw64Fldd:
+      case kSw64Ufldd:
+      case kSw64Stb:
+      case kSw64Sth:
+      case kSw64Stw:
+      case kSw64Ustw:
+      case kSw64Stl:
+      case kSw64Ustl:
+      case kSw64Fsts: 
+      case kSw64Ufsts: 
+      case kSw64Fstd: 
+      case kSw64Ufstd: 
+		return 4;
+
+      case kSw64Ldb:
+      case kSw64Ldh:
+      case kSw64Ldwu:
+      case kSw64Uldwu:
+		return 5;
+
+      case kSw64Uldhu:
+		return 10;
+      case kSw64Uldh:
+		return 11;
+      case kSw64Usth:
+		return 8 + SrlwLatency(instr->InputAt(1)->IsRegister());
+      case kSw64Push: {
+		int latency = 0;
+        if (instr->InputAt(0)->IsFPRegister()) {
+          latency = 4 + DsubuLatency(false);
+        } else {
+          latency = PushLatency();
+        }
+        return latency;
+      }
+      case kSw64Peek: {
+		int latency = 0;
+        if (instr->OutputAt(0)->IsFPRegister()) {
+          latency = Latency::FLDD;
+        } else {
+          latency = AlignedMemoryLatency();
+        }
+	    return latency;
+      }
+      case kSw64StackClaim: 
+		return DsubuLatency(false);
+      case kSw64StoreToStackSlot: {
+		int latency = 0;
+        if (instr->InputAt(0)->IsFPRegister()) {
+          if (instr->InputAt(0)->IsSimd128Register()) {
+            latency = 1;  // Estimated value.
+          } else {
+            latency = 4;
+          }
+        } else {
+          latency = AlignedMemoryLatency();
+        }
+        return latency;
+      }
+      //case kSw64ByteSwap64: 
+      //case kSw64ByteSwap32: 
+      case kWord32AtomicLoadInt8:
+      case kWord32AtomicLoadUint8:
+      case kWord32AtomicLoadInt16:
+      case kWord32AtomicLoadUint16:
+      case kWord32AtomicLoadWord32:
+		return 2;
+      case kSw64Word64AtomicLoadUint16:
+      case kSw64Word64AtomicLoadUint32:
+      case kSw64Word64AtomicLoadUint64:
+      case kWord32AtomicStoreWord8:
+      case kWord32AtomicStoreWord16:
+      case kWord32AtomicStoreWord32:
+		return 3;
+      case kSw64Word64AtomicStoreWord8:
+      case kSw64Word64AtomicStoreWord16:
+      case kSw64Word64AtomicStoreWord32:
+      case kSw64Word64AtomicStoreWord64:
+      case kWord32AtomicExchangeInt8:
+		return Word32AtomicExchangeLatency(true, 8);
+      case kWord32AtomicExchangeUint8:
+		return Word32AtomicExchangeLatency(false, 8);
+      case kWord32AtomicExchangeInt16:
+		return Word32AtomicExchangeLatency(true, 16);
+      case kWord32AtomicExchangeUint16:
+		return Word32AtomicExchangeLatency(false, 16);
+      case kWord32AtomicExchangeWord32:
+		return 2 + LlLatency(0) + 1 + ScLatency(0) + BranchShortLatency() + 1;
+      case kSw64Word64AtomicExchangeUint8:
+      case kSw64Word64AtomicExchangeUint16:
+      case kSw64Word64AtomicExchangeUint32:
+      case kSw64Word64AtomicExchangeUint64:
+      case kWord32AtomicCompareExchangeInt8:
+		return Word32AtomicCompareExchangeLatency(true, 8);
+      case kWord32AtomicCompareExchangeUint8:
+		return Word32AtomicCompareExchangeLatency(false, 8);
+      case kWord32AtomicCompareExchangeInt16:
+		return Word32AtomicCompareExchangeLatency(true, 16);
+      case kWord32AtomicCompareExchangeUint16:
+		return Word32AtomicCompareExchangeLatency(false, 16);
+      case kWord32AtomicCompareExchangeWord32:
+		return 3 + LlLatency(0) + BranchShortLatency() + 1 + ScLatency(0) +
+             BranchShortLatency() + 1;
+    /*  case kSw64Word64AtomicCompareExchangeUint8:
+      case kSw64Word64AtomicCompareExchangeUint16:
+      case kSw64Word64AtomicCompareExchangeUint32:
+      case kSw64Word64AtomicCompareExchangeUint64:*/
+  //case kWord32Atomic##op##Int8:                 \
+  //case kWord32Atomic##op##Uint8:                \
+  //case kWord32Atomic##op##Int16:                \
+  //case kWord32Atomic##op##Uint16:               \
+  //case kWord32Atomic##op##Word32:               \
+  //case kSw64Word64Atomic##op##Uint8:                        \
+  //case kSw64Word64Atomic##op##Uint16:                       \
+  //case kSw64Word64Atomic##op##Uint32:                       \
+  //case kSw64Word64Atomic##op##Uint64:                       
+      case kSw64AssertEqual:
+	    return AssertLatency();
+//    case kSw64S128Zero: {
+//    case kSw64I32x4Splat: {
+//    case kSw64I32x4ExtractLane: {
+//    case kSw64I32x4ReplaceLane: {
+//    case kSw64I32x4Add: {
+//    case kSw64I32x4Sub: {
+//    case kSw64F32x4Splat: {
+//    case kSw64F32x4ExtractLane: {
+//    case kSw64F32x4ReplaceLane: {
+//    case kSw64F32x4SConvertI32x4: {
+//    case kSw64F32x4UConvertI32x4: {
+//    case kSw64I32x4Mul: {
+//    case kSw64I32x4MaxS: {
+//    case kSw64I32x4MinS: {
+//    case kSw64I32x4Eq: {
+//    case kSw64I32x4Ne: {
+//    case kSw64I32x4Shl: {
+//    case kSw64I32x4ShrS: {
+//    case kSw64I32x4ShrU: {
+//    case kSw64I32x4MaxU: {
+//    case kSw64I32x4MinU: {
+//    case kSw64S128Select: {
+//    case kSw64F32x4Abs: {
+//    case kSw64F32x4Neg: {
+//    case kSw64F32x4RecipApprox: {
+//    case kSw64F32x4RecipSqrtApprox: {
+//    case kSw64F32x4Add: {
+//    case kSw64F32x4Sub: {
+//    case kSw64F32x4Mul: {
+//    case kSw64F32x4Max: {
+//    case kSw64F32x4Min: {
+//    case kSw64F32x4Eq: {
+//    case kSw64F32x4Ne: {
+//    case kSw64F32x4Lt: {
+//    case kSw64F32x4Le: {
+////    case kSw64I32x4SConvertF32x4: {
+//    case kSw64I32x4UConvertF32x4: {
+//    case kSw64I32x4Neg: {
+//    case kSw64I32x4GtS: {
+//    case kSw64I32x4GeS: {
+//    case kSw64I32x4GtU: {
+//    case kSw64I32x4GeU: {
+//    case kSw64I16x8Splat: {
+//    case kSw64I16x8ExtractLane: {
+//    case kSw64I16x8ReplaceLane: {
+//    case kSw64I16x8Neg: {
+//    case kSw64I16x8Shl: {
+//    case kSw64I16x8ShrS: {
+//    case kSw64I16x8ShrU: {
+//    case kSw64I16x8Add: {
+//    case kSw64I16x8AddSaturateS: {
+//    case kSw64I16x8Sub: {
+//    case kSw64I16x8SubSaturateS: {
+//    case kSw64I16x8Mul: {
+//    case kSw64I16x8MaxS: {
+//    case kSw64I16x8MinS: {
+//    case kSw64I16x8Eq: {
+//    case kSw64I16x8Ne: {
+//    case kSw64I16x8GtS: {
+//    case kSw64I16x8GeS: {
+//    case kSw64I16x8AddSaturateU: {
+//    case kSw64I16x8SubSaturateU: {
+//    case kSw64I16x8MaxU: {
+//    case kSw64I16x8MinU: {
+//    case kSw64I16x8GtU: {
+//    case kSw64I16x8GeU: {
+//    case kSw64I8x16Splat: {
+//    case kSw64I8x16ExtractLane: {
+//    case kSw64I8x16ReplaceLane: {
+//    case kSw64I8x16Neg: {
+//    case kSw64I8x16Shl: {
+//    case kSw64I8x16ShrS: {
+//    case kSw64I8x16Add: {
+//    case kSw64I8x16AddSaturateS: {
+//    case kSw64I8x16Sub: {
+//    case kSw64I8x16SubSaturateS: {
+//    case kSw64I8x16Mul: {
+//    case kSw64I8x16MaxS: {
+//    case kSw64I8x16MinS: {
+//    case kSw64I8x16Eq: {
+//    case kSw64I8x16Ne: {
+//    case kSw64I8x16GtS: {
+//    case kSw64I8x16GeS: {
+//    case kSw64I8x16ShrU: {
+//    case kSw64I8x16AddSaturateU: {
+//    case kSw64I8x16SubSaturateU: {
+//    case kSw64I8x16MaxU: {
+//    case kSw64I8x16MinU: {
+//    case kSw64I8x16GtU: {
+//    case kSw64I8x16GeU: {
+//    case kSw64S128And: {
+//    case kSw64S128Or: {
+//    case kSw64S128Xor: {
+//    case kSw64S128Not: {
+
+    //case kSw64S1x4AnyTrue:
+    //case kSw64S1x8AnyTrue:
+    //case kSw64S1x16AnyTrue: 
+    //case kSw64S1x4AllTrue: 
+    //case kSw64S1x8AllTrue: 
+    //case kSw64S1x16AllTrue: 
+
+   /* case kSw64MsaLd: {
+    case kSw64MsaSt: {
+    case kSw64S32x4InterleaveRight: {
+    case kSw64S32x4InterleaveLeft: {
+    case kSw64S32x4PackEven: {
+    case kSw64S32x4PackOdd: {
+    case kSw64S32x4InterleaveEven: {
+    case kSw64S32x4InterleaveOdd: {
+    case kSw64S32x4Shuffle: {
+    case kSw64S16x8InterleaveRight: {
+    case kSw64S16x8InterleaveLeft: {
+    case kSw64S16x8PackEven: {
+    case kSw64S16x8PackOdd: {
+    case kSw64S16x8InterleaveEven: {
+    case kSw64S16x8InterleaveOdd: {
+    case kSw64S16x4Reverse: {
+    case kSw64S16x2Reverse: {
+    case kSw64S8x16InterleaveRight: {
+    case kSw64S8x16InterleaveLeft: {
+    case kSw64S8x16PackEven: {
+    case kSw64S8x16PackOdd: {
+    case kSw64S8x16InterleaveEven: {
+    case kSw64S8x16InterleaveOdd: {
+    case kSw64S8x16Concat: {
+    case kSw64S8x16Shuffle: {
+    case kSw64S8x8Reverse: {
+    case kSw64S8x4Reverse: {
+    case kSw64S8x2Reverse: {
+    case kSw64I32x4SConvertI16x8Low: {
+    case kSw64I32x4SConvertI16x8High: {
+    case kSw64I32x4UConvertI16x8Low: {
+    case kSw64I32x4UConvertI16x8High: {
+    case kSw64I16x8SConvertI8x16Low: {
+    case kSw64I16x8SConvertI8x16High: {
+    case kSw64I16x8SConvertI32x4: {
+    case kSw64I16x8UConvertI32x4: {
+    case kSw64I16x8UConvertI8x16Low: {
+    case kSw64I16x8UConvertI8x16High: {
+    case kSw64I8x16SConvertI16x8: {
+    case kSw64I8x16UConvertI16x8: {
+    case kSw64F32x4AddHoriz: 
+    case kSw64I32x4AddHoriz: 
+    case kSw64I16x8AddHoriz: */
+    
+    default:
+		return 1;
+	}
+}
+
+}  // namespace compiler
+}  // namespace internal
+}  // namespace v8
diff --git a/src/3rdparty/chromium/v8/src/compiler/backend/sw64/instruction-selector-sw64.cc b/src/3rdparty/chromium/v8/src/compiler/backend/sw64/instruction-selector-sw64.cc
new file mode 100755
index 0000000000..d152466a7e
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/compiler/backend/sw64/instruction-selector-sw64.cc
@@ -0,0 +1,3291 @@
+// Copyright 2014 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#include "src/base/bits.h"
+#include "src/compiler/backend/instruction-selector-impl.h"
+#include "src/compiler/node-matchers.h"
+#include "src/compiler/node-properties.h"
+
+namespace v8 {
+namespace internal {
+namespace compiler {
+
+#define TRACE_UNIMPL() \
+  PrintF("UNIMPLEMENTED instr_sel: %s at line %d\n", __FUNCTION__, __LINE__)
+
+#define TRACE() PrintF("instr_sel: %s at line %d\n", __FUNCTION__, __LINE__)
+
+
+// Adds Sw64-specific methods for generating InstructionOperands.
+class Sw64OperandGenerator final : public OperandGenerator {
+ public:
+  explicit Sw64OperandGenerator(InstructionSelector* selector)
+      : OperandGenerator(selector) {}
+
+  InstructionOperand UseOperand(Node* node, InstructionCode opcode) {
+    if (CanBeImmediate(node, opcode)) {
+      return UseImmediate(node);
+    }
+    return UseRegister(node);
+  }
+
+  // Use the zero register if the node has the immediate value zero, otherwise
+  // assign a register.
+  InstructionOperand UseRegisterOrImmediateZero(Node* node) {
+    if ((IsIntegerConstant(node) && (GetIntegerConstantValue(node) == 0)) ||
+        (IsFloatConstant(node) &&
+         (bit_cast<int64_t>(GetFloatConstantValue(node)) == 0))) {
+      return UseImmediate(node);
+    }
+    return UseRegister(node);
+  }
+
+  bool IsIntegerConstant(Node* node) {
+    return (node->opcode() == IrOpcode::kInt32Constant) ||
+           (node->opcode() == IrOpcode::kInt64Constant);
+  }
+
+  int64_t GetIntegerConstantValue(Node* node) {
+    if (node->opcode() == IrOpcode::kInt32Constant) {
+      return OpParameter<int32_t>(node->op());
+    }
+    DCHECK_EQ(IrOpcode::kInt64Constant, node->opcode());
+    return OpParameter<int64_t>(node->op());
+  }
+
+  bool IsFloatConstant(Node* node) {
+    return (node->opcode() == IrOpcode::kFloat32Constant) ||
+           (node->opcode() == IrOpcode::kFloat64Constant);
+  }
+
+  double GetFloatConstantValue(Node* node) {
+    if (node->opcode() == IrOpcode::kFloat32Constant) {
+      return OpParameter<float>(node->op());
+    }
+    DCHECK_EQ(IrOpcode::kFloat64Constant, node->opcode());
+    return OpParameter<double>(node->op());
+  }
+
+  bool CanBeImmediate(Node* node, InstructionCode mode) {
+    return IsIntegerConstant(node) &&
+           CanBeImmediate(GetIntegerConstantValue(node), mode);
+  }
+
+  bool CanBeImmediate(int64_t value, InstructionCode opcode) {
+    switch (ArchOpcodeField::decode(opcode)) {
+      case kSw64Shl:
+      case kSw64Sar:
+      case kSw64Shr:
+        return is_uint5(value);
+      case kSw64Dshl:
+      case kSw64Dsar:
+      case kSw64Dshr:
+        return is_uint6(value);
+      case kSw64Add:
+      case kSw64And32:
+      case kSw64And:
+      case kSw64Dadd:
+      case kSw64Or32:
+      case kSw64Or:
+      case kSw64Tst:
+      case kSw64Xor:
+        return is_uint16(value);
+      case kSw64Ldb:
+      case kSw64Ldbu:
+      case kSw64Stb:
+      case kSw64Ldh:
+      case kSw64Ldhu:
+      case kSw64Sth:
+      case kSw64Ldw:
+      case kSw64Stw:
+      case kSw64Ldl:
+      case kSw64Stl:
+      case kSw64Flds:
+      case kSw64Fsts:
+      case kSw64Fldd:
+      case kSw64Fstd:
+        return is_int32(value);
+      default:
+        return is_int16(value);
+    }
+  }
+
+ private:
+  bool ImmediateFitsAddrMode1Instruction(int32_t imm) const {
+    TRACE_UNIMPL();
+    return false;
+  }
+};
+
+
+static void VisitRR(InstructionSelector* selector, ArchOpcode opcode,
+                    Node* node) {
+  Sw64OperandGenerator g(selector);
+  selector->Emit(opcode, g.DefineAsRegister(node),
+                 g.UseRegister(node->InputAt(0)));
+}
+
+static void VisitRRI(InstructionSelector* selector, ArchOpcode opcode,
+                     Node* node) {
+  Sw64OperandGenerator g(selector);
+  int32_t imm = OpParameter<int32_t>(node->op());
+  selector->Emit(opcode, g.DefineAsRegister(node),
+                 g.UseRegister(node->InputAt(0)), g.UseImmediate(imm));
+}
+
+static void VisitSimdShift(InstructionSelector* selector, ArchOpcode opcode,
+                           Node* node) {
+  Sw64OperandGenerator g(selector);
+  if (g.IsIntegerConstant(node->InputAt(1))) {
+    selector->Emit(opcode, g.DefineAsRegister(node),
+                   g.UseRegister(node->InputAt(0)),
+                   g.UseImmediate(node->InputAt(1)));
+  } else {
+    selector->Emit(opcode, g.DefineAsRegister(node),
+                   g.UseRegister(node->InputAt(0)),
+                   g.UseRegister(node->InputAt(1)));
+  }
+}
+
+static void VisitRRIR(InstructionSelector* selector, ArchOpcode opcode,
+                      Node* node) {
+  Sw64OperandGenerator g(selector);
+  int32_t imm = OpParameter<int32_t>(node->op());
+  selector->Emit(opcode, g.DefineAsRegister(node),
+                 g.UseRegister(node->InputAt(0)), g.UseImmediate(imm),
+                 g.UseRegister(node->InputAt(1)));
+}
+
+static void VisitRRR(InstructionSelector* selector, ArchOpcode opcode,
+                     Node* node) {
+  Sw64OperandGenerator g(selector);
+  selector->Emit(opcode, g.DefineAsRegister(node),
+                 g.UseRegister(node->InputAt(0)),
+                 g.UseRegister(node->InputAt(1)));
+}
+
+//static void VisitUniqueRRR(InstructionSelector* selector, ArchOpcode opcode,
+//                           Node* node) {
+//  Sw64OperandGenerator g(selector);
+//  selector->Emit(opcode, g.DefineAsRegister(node),
+//                 g.UseUniqueRegister(node->InputAt(0)),
+//                 g.UseUniqueRegister(node->InputAt(1)));
+//}
+
+void VisitRRRR(InstructionSelector* selector, ArchOpcode opcode, Node* node) {
+  Sw64OperandGenerator g(selector);
+  selector->Emit(
+      opcode, g.DefineSameAsFirst(node), g.UseRegister(node->InputAt(0)),
+      g.UseRegister(node->InputAt(1)), g.UseRegister(node->InputAt(2)));
+}
+
+static void VisitRRO(InstructionSelector* selector, ArchOpcode opcode,
+                     Node* node) {
+  Sw64OperandGenerator g(selector);
+  selector->Emit(opcode, g.DefineAsRegister(node),
+                 g.UseRegister(node->InputAt(0)),
+                 g.UseOperand(node->InputAt(1), opcode));
+}
+
+struct ExtendingLoadMatcher {
+  ExtendingLoadMatcher(Node* node, InstructionSelector* selector)
+      : matches_(false), selector_(selector), base_(nullptr), immediate_(0) {
+    Initialize(node);
+  }
+
+  bool Matches() const { return matches_; }
+
+  Node* base() const {
+    DCHECK(Matches());
+    return base_;
+  }
+  int64_t immediate() const {
+    DCHECK(Matches());
+    return immediate_;
+  }
+  ArchOpcode opcode() const {
+    DCHECK(Matches());
+    return opcode_;
+  }
+
+ private:
+  bool matches_;
+  InstructionSelector* selector_;
+  Node* base_;
+  int64_t immediate_;
+  ArchOpcode opcode_;
+
+  void Initialize(Node* node) {
+    Int64BinopMatcher m(node);
+    // When loading a 64-bit value and shifting by 32, we should
+    // just load and sign-extend the interesting 4 bytes instead.
+    // This happens, for example, when we're loading and untagging SMIs.
+    DCHECK(m.IsWord64Sar());
+    if (m.left().IsLoad() && m.right().Is(32) &&
+        selector_->CanCover(m.node(), m.left().node())) {
+      DCHECK_EQ(selector_->GetEffectLevel(node),
+                selector_->GetEffectLevel(m.left().node()));
+      MachineRepresentation rep =
+          LoadRepresentationOf(m.left().node()->op()).representation();
+      DCHECK_EQ(3, ElementSizeLog2Of(rep));
+      if (rep != MachineRepresentation::kTaggedSigned &&
+          rep != MachineRepresentation::kTaggedPointer &&
+          rep != MachineRepresentation::kTagged &&
+          rep != MachineRepresentation::kWord64) {
+        return;
+      }
+
+      Sw64OperandGenerator g(selector_);
+      Node* load = m.left().node();
+      Node* offset = load->InputAt(1);
+      base_ = load->InputAt(0);
+      opcode_ = kSw64Ldw;
+      if (g.CanBeImmediate(offset, opcode_)) {
+#if defined(V8_TARGET_LITTLE_ENDIAN)
+        immediate_ = g.GetIntegerConstantValue(offset) + 4;
+#elif defined(V8_TARGET_BIG_ENDIAN)
+        immediate_ = g.GetIntegerConstantValue(offset);
+#endif
+        matches_ = g.CanBeImmediate(immediate_, kSw64Ldw);
+      }
+    }
+  }
+};
+
+bool TryEmitExtendingLoad(InstructionSelector* selector, Node* node,
+                          Node* output_node) {
+  ExtendingLoadMatcher m(node, selector);
+  Sw64OperandGenerator g(selector);
+  if (m.Matches()) {
+    InstructionOperand inputs[2];
+    inputs[0] = g.UseRegister(m.base());
+    InstructionCode opcode =
+        m.opcode() | AddressingModeField::encode(kMode_MRI);
+    DCHECK(is_int32(m.immediate()));
+    inputs[1] = g.TempImmediate(static_cast<int32_t>(m.immediate()));
+    InstructionOperand outputs[] = {g.DefineAsRegister(output_node)};
+    selector->Emit(opcode, arraysize(outputs), outputs, arraysize(inputs),
+                   inputs);
+    return true;
+  }
+  return false;
+}
+
+bool TryMatchImmediate(InstructionSelector* selector,
+                       InstructionCode* opcode_return, Node* node,
+                       size_t* input_count_return, InstructionOperand* inputs) {
+  Sw64OperandGenerator g(selector);
+  if (g.CanBeImmediate(node, *opcode_return)) {
+    *opcode_return |= AddressingModeField::encode(kMode_MRI);
+    inputs[0] = g.UseImmediate(node);
+    *input_count_return = 1;
+    return true;
+  }
+  return false;
+}
+
+static void VisitBinop(InstructionSelector* selector, Node* node,
+                       InstructionCode opcode, bool has_reverse_opcode,
+                       InstructionCode reverse_opcode,
+                       FlagsContinuation* cont) {
+  Sw64OperandGenerator g(selector);
+  Int32BinopMatcher m(node);
+  InstructionOperand inputs[2];
+  size_t input_count = 0;
+  InstructionOperand outputs[1];
+  size_t output_count = 0;
+
+  if (TryMatchImmediate(selector, &opcode, m.right().node(), &input_count,
+                        &inputs[1])) {
+    inputs[0] = g.UseRegister(m.left().node());
+    input_count++;
+  } else if (has_reverse_opcode &&
+             TryMatchImmediate(selector, &reverse_opcode, m.left().node(),
+                               &input_count, &inputs[1])) {
+    inputs[0] = g.UseRegister(m.right().node());
+    opcode = reverse_opcode;
+    input_count++;
+  } else {
+    inputs[input_count++] = g.UseRegister(m.left().node());
+    inputs[input_count++] = g.UseOperand(m.right().node(), opcode);
+  }
+
+  if (cont->IsDeoptimize()) {
+    // If we can deoptimize as a result of the binop, we need to make sure that
+    // the deopt inputs are not overwritten by the binop result. One way
+    // to achieve that is to declare the output register as same-as-first.
+    outputs[output_count++] = g.DefineSameAsFirst(node);
+  } else {
+    outputs[output_count++] = g.DefineAsRegister(node);
+  }
+
+  DCHECK_NE(0u, input_count);
+  DCHECK_EQ(1u, output_count);
+  DCHECK_GE(arraysize(inputs), input_count);
+  DCHECK_GE(arraysize(outputs), output_count);
+
+  selector->EmitWithContinuation(opcode, output_count, outputs, input_count,
+                                 inputs, cont);
+}
+
+static void VisitBinop(InstructionSelector* selector, Node* node,
+                       InstructionCode opcode, bool has_reverse_opcode,
+                       InstructionCode reverse_opcode) {
+  FlagsContinuation cont;
+  VisitBinop(selector, node, opcode, has_reverse_opcode, reverse_opcode, &cont);
+}
+
+static void VisitBinop(InstructionSelector* selector, Node* node,
+                       InstructionCode opcode, FlagsContinuation* cont) {
+  VisitBinop(selector, node, opcode, false, kArchNop, cont);
+}
+
+static void VisitBinop(InstructionSelector* selector, Node* node,
+                       InstructionCode opcode) {
+  VisitBinop(selector, node, opcode, false, kArchNop);
+}
+
+void InstructionSelector::VisitStackSlot(Node* node) {
+  StackSlotRepresentation rep = StackSlotRepresentationOf(node->op());
+  int alignment = rep.alignment();
+  int slot = frame_->AllocateSpillSlot(rep.size(), alignment);
+  OperandGenerator g(this);
+
+  Emit(kArchStackSlot, g.DefineAsRegister(node),
+       sequence()->AddImmediate(Constant(slot)),
+       sequence()->AddImmediate(Constant(alignment)), 0, nullptr);
+}
+
+void InstructionSelector::VisitAbortCSAAssert(Node* node) {
+  Sw64OperandGenerator g(this);
+  Emit(kArchAbortCSAAssert, g.NoOutput(), g.UseFixed(node->InputAt(0), a0));
+}
+
+void EmitLoad(InstructionSelector* selector, Node* node, InstructionCode opcode,
+              Node* output = nullptr) {
+  Sw64OperandGenerator g(selector);
+  Node* base = node->InputAt(0);
+  Node* index = node->InputAt(1);
+
+  if (g.CanBeImmediate(index, opcode)) {
+    selector->Emit(opcode | AddressingModeField::encode(kMode_MRI),
+                   g.DefineAsRegister(output == nullptr ? node : output),
+                   g.UseRegister(base), g.UseImmediate(index));
+  } else {
+    InstructionOperand addr_reg = g.TempRegister();
+    selector->Emit(kSw64Dadd | AddressingModeField::encode(kMode_None),
+                   addr_reg, g.UseRegister(index), g.UseRegister(base));
+    // Emit desired load opcode, using temp addr_reg.
+    selector->Emit(opcode | AddressingModeField::encode(kMode_MRI),
+                   g.DefineAsRegister(output == nullptr ? node : output),
+                   addr_reg, g.TempImmediate(0));
+  }
+}
+
+void InstructionSelector::VisitLoadTransform(Node* node) {
+  LoadTransformParameters params = LoadTransformParametersOf(node->op());
+
+  InstructionCode opcode = kArchNop;
+  switch (params.transformation) {
+//SKTODO
+    default:
+      UNIMPLEMENTED();
+  }
+
+  EmitLoad(this, node, opcode);
+}
+
+void InstructionSelector::VisitLoad(Node* node) {
+  LoadRepresentation load_rep = LoadRepresentationOf(node->op());
+
+  InstructionCode opcode = kArchNop;
+  switch (load_rep.representation()) {
+    case MachineRepresentation::kFloat32:
+      opcode = kSw64Flds;
+      break;
+    case MachineRepresentation::kFloat64:
+      opcode = kSw64Fldd;
+      break;
+    case MachineRepresentation::kBit:  // Fall through.
+    case MachineRepresentation::kWord8:
+      opcode = load_rep.IsUnsigned() ? kSw64Ldbu : kSw64Ldb;
+      break;
+    case MachineRepresentation::kWord16:
+      opcode = load_rep.IsUnsigned() ? kSw64Ldhu : kSw64Ldh;
+      break;
+    case MachineRepresentation::kWord32:
+      opcode = load_rep.IsUnsigned() ? kSw64Ldwu : kSw64Ldw;
+      break;
+    case MachineRepresentation::kTaggedSigned:   // Fall through.
+    case MachineRepresentation::kTaggedPointer:  // Fall through.
+    case MachineRepresentation::kTagged:  // Fall through.
+    case MachineRepresentation::kWord64:
+      opcode = kSw64Ldl;
+      break;
+    case MachineRepresentation::kSimd128:
+      opcode = kSw64MsaLd;
+      break;
+    case MachineRepresentation::kCompressedPointer:  // Fall through.
+    case MachineRepresentation::kCompressed:         // Fall through.
+    case MachineRepresentation::kNone:
+      UNREACHABLE();
+  }
+  if (node->opcode() == IrOpcode::kPoisonedLoad) {
+    CHECK_NE(poisoning_level_, PoisoningMitigationLevel::kDontPoison);
+    opcode |= MiscField::encode(kMemoryAccessPoisoned);
+  }
+
+  EmitLoad(this, node, opcode);
+}
+
+void InstructionSelector::VisitPoisonedLoad(Node* node) { VisitLoad(node); }
+
+void InstructionSelector::VisitProtectedLoad(Node* node) {
+  // TODO(eholk)
+  UNIMPLEMENTED();
+}
+
+void InstructionSelector::VisitStore(Node* node) {
+  Sw64OperandGenerator g(this);
+  Node* base = node->InputAt(0);
+  Node* index = node->InputAt(1);
+  Node* value = node->InputAt(2);
+
+  StoreRepresentation store_rep = StoreRepresentationOf(node->op());
+  WriteBarrierKind write_barrier_kind = store_rep.write_barrier_kind();
+  MachineRepresentation rep = store_rep.representation();
+
+  if (FLAG_enable_unconditional_write_barriers && CanBeTaggedPointer(rep)) {
+    write_barrier_kind = kFullWriteBarrier;
+  }
+
+  // TODO(sw64): I guess this could be done in a better way.
+  if (write_barrier_kind != kNoWriteBarrier &&
+      V8_LIKELY(!FLAG_disable_write_barriers)) {
+    DCHECK(CanBeTaggedPointer(rep));
+    InstructionOperand inputs[3];
+    size_t input_count = 0;
+    inputs[input_count++] = g.UseUniqueRegister(base);
+    inputs[input_count++] = g.UseUniqueRegister(index);
+    inputs[input_count++] = g.UseUniqueRegister(value);
+    RecordWriteMode record_write_mode =
+        WriteBarrierKindToRecordWriteMode(write_barrier_kind);
+    InstructionOperand temps[] = {g.TempRegister(), g.TempRegister()};
+    size_t const temp_count = arraysize(temps);
+    InstructionCode code = kArchStoreWithWriteBarrier;
+    code |= MiscField::encode(static_cast<int>(record_write_mode));
+    Emit(code, 0, nullptr, input_count, inputs, temp_count, temps);
+  } else {
+    ArchOpcode opcode = kArchNop;
+    switch (rep) {
+      case MachineRepresentation::kFloat32:
+        opcode = kSw64Fsts;
+        break;
+      case MachineRepresentation::kFloat64:
+        opcode = kSw64Fstd;
+        break;
+      case MachineRepresentation::kBit:  // Fall through.
+      case MachineRepresentation::kWord8:
+        opcode = kSw64Stb;
+        break;
+      case MachineRepresentation::kWord16:
+        opcode = kSw64Sth;
+        break;
+      case MachineRepresentation::kWord32:
+        opcode = kSw64Stw;
+        break;
+      case MachineRepresentation::kTaggedSigned:   // Fall through.
+      case MachineRepresentation::kTaggedPointer:  // Fall through.
+      case MachineRepresentation::kTagged:  // Fall through.
+      case MachineRepresentation::kWord64:
+        opcode = kSw64Stl;
+        break;
+      case MachineRepresentation::kSimd128:
+        opcode = kSw64MsaSt;
+        break;
+      case MachineRepresentation::kCompressedPointer:  // Fall through.
+      case MachineRepresentation::kCompressed:         // Fall through.
+      case MachineRepresentation::kNone:
+        UNREACHABLE();
+        return;
+    }
+
+    if (g.CanBeImmediate(index, opcode)) {
+      Emit(opcode | AddressingModeField::encode(kMode_MRI), g.NoOutput(),
+           g.UseRegister(base), g.UseImmediate(index),
+           g.UseRegisterOrImmediateZero(value));
+    } else {
+      InstructionOperand addr_reg = g.TempRegister();
+      Emit(kSw64Dadd | AddressingModeField::encode(kMode_None), addr_reg,
+           g.UseRegister(index), g.UseRegister(base));
+      // Emit desired store opcode, using temp addr_reg.
+      Emit(opcode | AddressingModeField::encode(kMode_MRI), g.NoOutput(),
+           addr_reg, g.TempImmediate(0), g.UseRegisterOrImmediateZero(value));
+    }
+  }
+}
+
+void InstructionSelector::VisitProtectedStore(Node* node) {
+  // TODO(eholk)
+  UNIMPLEMENTED();
+}
+
+void InstructionSelector::VisitWord32And(Node* node) {
+#ifdef SW64
+  //TODO: It is an optimizaion!
+
+#else
+  Sw64OperandGenerator g(this);
+  Int32BinopMatcher m(node);
+  if (m.left().IsWord32Shr() && CanCover(node, m.left().node()) &&
+      m.right().HasValue()) {
+    uint32_t mask = m.right().Value();
+    uint32_t mask_width = base::bits::CountPopulation(mask);
+    uint32_t mask_msb = base::bits::CountLeadingZeros32(mask);
+    if ((mask_width != 0) && (mask_msb + mask_width == 32)) {
+      // The mask must be contiguous, and occupy the least-significant bits.
+      DCHECK_EQ(0u, base::bits::CountTrailingZeros32(mask));
+
+      // Select Ext for And(Shr(x, imm), mask) where the mask is in the least
+      // significant bits.
+      Int32BinopMatcher mleft(m.left().node());
+      if (mleft.right().HasValue()) {
+        // Any shift value can match; int32 shifts use `value % 32`.
+        uint32_t lsb = mleft.right().Value() & 0x1F;
+
+        // Ext cannot extract bits past the register size, however since
+        // shifting the original value would have introduced some zeros we can
+        // still use Ext with a smaller mask and the remaining bits will be
+        // zeros.
+        if (lsb + mask_width > 32) mask_width = 32 - lsb;
+
+        Emit(kSw64Ext, g.DefineAsRegister(node),
+             g.UseRegister(mleft.left().node()), g.TempImmediate(lsb),
+             g.TempImmediate(mask_width));
+        return;
+      }
+      // Other cases fall through to the normal And operation.
+    }
+  }
+  if (m.right().HasValue()) {
+    uint32_t mask = m.right().Value();
+    uint32_t shift = base::bits::CountPopulation(~mask);
+    uint32_t msb = base::bits::CountLeadingZeros32(~mask);
+    if (shift != 0 && shift != 32 && msb + shift == 32) {
+      // Insert zeros for (x >> K) << K => x & ~(2^K - 1) expression reduction
+      // and remove constant loading of inverted mask.
+      Emit(kSw64Ins, g.DefineSameAsFirst(node),
+           g.UseRegister(m.left().node()), g.TempImmediate(0),
+           g.TempImmediate(shift));
+      return;
+    }
+  }
+#endif
+  VisitBinop(this, node, kSw64And32, true, kSw64And32);
+}
+
+
+void InstructionSelector::VisitWord64And(Node* node) {
+#ifdef SW64
+  //TODO: It is an optimizaion!
+
+#else
+  Sw64OperandGenerator g(this);
+  Int64BinopMatcher m(node);
+  if (m.left().IsWord64Shr() && CanCover(node, m.left().node()) &&
+      m.right().HasValue()) {
+    uint64_t mask = m.right().Value();
+    uint32_t mask_width = base::bits::CountPopulation(mask);
+    uint32_t mask_msb = base::bits::CountLeadingZeros64(mask);
+    if ((mask_width != 0) && (mask_msb + mask_width == 64)) {
+      // The mask must be contiguous, and occupy the least-significant bits.
+      DCHECK_EQ(0u, base::bits::CountTrailingZeros64(mask));
+
+      // Select Dext for And(Shr(x, imm), mask) where the mask is in the least
+      // significant bits.
+      Int64BinopMatcher mleft(m.left().node());
+      if (mleft.right().HasValue()) {
+        // Any shift value can match; int64 shifts use `value % 64`.
+        uint32_t lsb = static_cast<uint32_t>(mleft.right().Value() & 0x3F);
+
+        // Dext cannot extract bits past the register size, however since
+        // shifting the original value would have introduced some zeros we can
+        // still use Dext with a smaller mask and the remaining bits will be
+        // zeros.
+        if (lsb + mask_width > 64) mask_width = 64 - lsb;
+
+        if (lsb == 0 && mask_width == 64) {
+          Emit(kArchNop, g.DefineSameAsFirst(node), g.Use(mleft.left().node()));
+        } else {
+          Emit(kSw64Dext, g.DefineAsRegister(node),
+               g.UseRegister(mleft.left().node()), g.TempImmediate(lsb),
+               g.TempImmediate(static_cast<int32_t>(mask_width)));
+        }
+        return;
+      }
+      // Other cases fall through to the normal And operation.
+    }
+  }
+  if (m.right().HasValue()) {
+    uint64_t mask = m.right().Value();
+    uint32_t shift = base::bits::CountPopulation(~mask);
+    uint32_t msb = base::bits::CountLeadingZeros64(~mask);
+    if (shift != 0 && shift < 32 && msb + shift == 64) {
+      // Insert zeros for (x >> K) << K => x & ~(2^K - 1) expression reduction
+      // and remove constant loading of inverted mask. Dins cannot insert bits
+      // past word size, so shifts smaller than 32 are covered.
+      Emit(kSw64Dins, g.DefineSameAsFirst(node),
+           g.UseRegister(m.left().node()), g.TempImmediate(0),
+           g.TempImmediate(shift));
+      return;
+    }
+  }
+#endif
+  VisitBinop(this, node, kSw64And, true, kSw64And);
+}
+
+
+void InstructionSelector::VisitWord32Or(Node* node) {
+  VisitBinop(this, node, kSw64Or32, true, kSw64Or32);
+}
+
+
+void InstructionSelector::VisitWord64Or(Node* node) {
+  VisitBinop(this, node, kSw64Or, true, kSw64Or);
+}
+
+
+void InstructionSelector::VisitWord32Xor(Node* node) {
+  Int32BinopMatcher m(node);
+  if (m.left().IsWord32Or() && CanCover(node, m.left().node()) &&
+      m.right().Is(-1)) {
+    Int32BinopMatcher mleft(m.left().node());
+    if (!mleft.right().HasValue()) {
+      Sw64OperandGenerator g(this);
+      Emit(kSw64Nor32, g.DefineAsRegister(node),
+           g.UseRegister(mleft.left().node()),
+           g.UseRegister(mleft.right().node()));
+      return;
+    }
+  }
+  if (m.right().Is(-1)) {
+    // Use Nor for bit negation and eliminate constant loading for xori.
+    Sw64OperandGenerator g(this);
+    Emit(kSw64Nor32, g.DefineAsRegister(node), g.UseRegister(m.left().node()),
+         g.TempImmediate(0));
+    return;
+  }
+  VisitBinop(this, node, kSw64Xor32, true, kSw64Xor32);
+}
+
+void InstructionSelector::VisitWord64Xor(Node* node) {
+  Int64BinopMatcher m(node);
+  if (m.left().IsWord64Or() && CanCover(node, m.left().node()) &&
+      m.right().Is(-1)) {
+    Int64BinopMatcher mleft(m.left().node());
+    if (!mleft.right().HasValue()) {
+      Sw64OperandGenerator g(this);
+      Emit(kSw64Nor, g.DefineAsRegister(node),
+           g.UseRegister(mleft.left().node()),
+           g.UseRegister(mleft.right().node()));
+      return;
+    }
+  }
+  if (m.right().Is(-1)) {
+    // Use Nor for bit negation and eliminate constant loading for xori.
+    Sw64OperandGenerator g(this);
+    Emit(kSw64Nor, g.DefineAsRegister(node), g.UseRegister(m.left().node()),
+         g.TempImmediate(0));
+    return;
+  }
+  VisitBinop(this, node, kSw64Xor, true, kSw64Xor);
+}
+
+
+void InstructionSelector::VisitWord32Shl(Node* node) {
+  Int32BinopMatcher m(node);
+  if (m.left().IsWord32And() && CanCover(node, m.left().node()) &&
+      m.right().IsInRange(1, 31)) {
+    Sw64OperandGenerator g(this);
+    Int32BinopMatcher mleft(m.left().node());
+    // Match Word32Shl(Word32And(x, mask), imm) to Shl where the mask is
+    // contiguous, and the shift immediate non-zero.
+    if (mleft.right().HasValue()) {
+      uint32_t mask = mleft.right().Value();
+      uint32_t mask_width = base::bits::CountPopulation(mask);
+      uint32_t mask_msb = base::bits::CountLeadingZeros32(mask);
+      if ((mask_width != 0) && (mask_msb + mask_width == 32)) {
+        uint32_t shift = m.right().Value();
+        DCHECK_EQ(0u, base::bits::CountTrailingZeros32(mask));
+        DCHECK_NE(0u, shift);
+        if ((shift + mask_width) >= 32) {
+          // If the mask is contiguous and reaches or extends beyond the top
+          // bit, only the shift is needed.
+          Emit(kSw64Shl, g.DefineAsRegister(node),
+               g.UseRegister(mleft.left().node()),
+               g.UseImmediate(m.right().node()));
+          return;
+        }
+      }
+    }
+  }
+  VisitRRO(this, kSw64Shl, node);
+}
+
+void InstructionSelector::VisitWord32Shr(Node* node) {
+  Int32BinopMatcher m(node);
+  if (m.left().IsWord32And() && m.right().HasValue()) {
+    uint32_t lsb = m.right().Value() & 0x1F;
+    Int32BinopMatcher mleft(m.left().node());
+    if (mleft.right().HasValue() && mleft.right().Value() != 0) {
+      // Select Ext for Shr(And(x, mask), imm) where the result of the mask is
+      // shifted into the least-significant bits.
+      uint32_t mask = (mleft.right().Value() >> lsb) << lsb;
+      unsigned mask_width = base::bits::CountPopulation(mask);
+      unsigned mask_msb = base::bits::CountLeadingZeros32(mask);
+      if ((mask_msb + mask_width + lsb) == 32) {
+        Sw64OperandGenerator g(this);
+        DCHECK_EQ(lsb, base::bits::CountTrailingZeros32(mask));
+        Emit(kSw64Ext, g.DefineAsRegister(node),
+             g.UseRegister(mleft.left().node()), g.TempImmediate(lsb),
+             g.TempImmediate(mask_width));
+        return;
+      }
+    }
+  }
+  VisitRRO(this, kSw64Shr, node);
+}
+
+void InstructionSelector::VisitWord32Sar(Node* node) {
+  Int32BinopMatcher m(node);
+  if (m.left().IsWord32Shl() && CanCover(node, m.left().node())) {
+    Int32BinopMatcher mleft(m.left().node());
+    if (m.right().HasValue() && mleft.right().HasValue()) {
+      Sw64OperandGenerator g(this);
+      uint32_t sar = m.right().Value();
+      uint32_t shl = mleft.right().Value();
+      if ((sar == shl) && (sar == 16)) {
+        Emit(kSw64Seh, g.DefineAsRegister(node),
+             g.UseRegister(mleft.left().node()));
+        return;
+      } else if ((sar == shl) && (sar == 24)) {
+        Emit(kSw64Seb, g.DefineAsRegister(node),
+             g.UseRegister(mleft.left().node()));
+        return;
+      } else if ((sar == shl) && (sar == 32)) {
+        Emit(kSw64Shl, g.DefineAsRegister(node),
+             g.UseRegister(mleft.left().node()), g.TempImmediate(0));
+        return;
+      }
+    }
+  }
+  VisitRRO(this, kSw64Sar, node);
+}
+
+void InstructionSelector::VisitWord64Shl(Node* node) {
+  Sw64OperandGenerator g(this);
+  Int64BinopMatcher m(node);
+  if ((m.left().IsChangeInt32ToInt64() || m.left().IsChangeUint32ToUint64()) &&
+      m.right().IsInRange(32, 63) && CanCover(node, m.left().node())) {
+    // There's no need to sign/zero-extend to 64-bit if we shift out the upper
+    // 32 bits anyway.
+    Emit(kSw64Dshl, g.DefineSameAsFirst(node),
+         g.UseRegister(m.left().node()->InputAt(0)),
+         g.UseImmediate(m.right().node()));
+    return;
+  }
+  if (m.left().IsWord64And() && CanCover(node, m.left().node()) &&
+      m.right().IsInRange(1, 63)) {
+    // Match Word64Shl(Word64And(x, mask), imm) to Dshl where the mask is
+    // contiguous, and the shift immediate non-zero.
+    Int64BinopMatcher mleft(m.left().node());
+    if (mleft.right().HasValue()) {
+      uint64_t mask = mleft.right().Value();
+      uint32_t mask_width = base::bits::CountPopulation(mask);
+      uint32_t mask_msb = base::bits::CountLeadingZeros64(mask);
+      if ((mask_width != 0) && (mask_msb + mask_width == 64)) {
+        uint64_t shift = m.right().Value();
+        DCHECK_EQ(0u, base::bits::CountTrailingZeros64(mask));
+        DCHECK_NE(0u, shift);
+
+        if ((shift + mask_width) >= 64) {
+          // If the mask is contiguous and reaches or extends beyond the top
+          // bit, only the shift is needed.
+          Emit(kSw64Dshl, g.DefineAsRegister(node),
+               g.UseRegister(mleft.left().node()),
+               g.UseImmediate(m.right().node()));
+          return;
+        }
+      }
+    }
+  }
+  VisitRRO(this, kSw64Dshl, node);
+}
+
+
+void InstructionSelector::VisitWord64Shr(Node* node) {
+  Int64BinopMatcher m(node);
+  if (m.left().IsWord64And() && m.right().HasValue()) {
+    uint32_t lsb = m.right().Value() & 0x3F;
+    Int64BinopMatcher mleft(m.left().node());
+    if (mleft.right().HasValue() && mleft.right().Value() != 0) {
+      // Select Dext for Shr(And(x, mask), imm) where the result of the mask is
+      // shifted into the least-significant bits.
+      uint64_t mask = (mleft.right().Value() >> lsb) << lsb;
+      unsigned mask_width = base::bits::CountPopulation(mask);
+      unsigned mask_msb = base::bits::CountLeadingZeros64(mask);
+      if ((mask_msb + mask_width + lsb) == 64) {
+        Sw64OperandGenerator g(this);
+        DCHECK_EQ(lsb, base::bits::CountTrailingZeros64(mask));
+        Emit(kSw64Dext, g.DefineAsRegister(node),
+             g.UseRegister(mleft.left().node()), g.TempImmediate(lsb),
+             g.TempImmediate(mask_width));
+        return;
+      }
+    }
+  }
+  VisitRRO(this, kSw64Dshr, node);
+}
+
+
+void InstructionSelector::VisitWord64Sar(Node* node) {
+  if (TryEmitExtendingLoad(this, node, node)) return;
+  VisitRRO(this, kSw64Dsar, node);
+}
+
+void InstructionSelector::VisitWord32Rol(Node* node) { UNREACHABLE(); }
+
+void InstructionSelector::VisitWord64Rol(Node* node) { UNREACHABLE(); }
+
+void InstructionSelector::VisitWord32Ror(Node* node) {
+  VisitRRO(this, kSw64Ror, node);
+}
+
+
+void InstructionSelector::VisitWord32Clz(Node* node) {
+  VisitRR(this, kSw64Clz, node);
+}
+
+
+void InstructionSelector::VisitWord32ReverseBits(Node* node) { UNREACHABLE(); }
+
+
+void InstructionSelector::VisitWord64ReverseBits(Node* node) { UNREACHABLE(); }
+
+void InstructionSelector::VisitWord64ReverseBytes(Node* node) {
+  Sw64OperandGenerator g(this);
+  Emit(kSw64ByteSwap64, g.DefineAsRegister(node),
+       g.UseRegister(node->InputAt(0)));
+}
+
+void InstructionSelector::VisitWord32ReverseBytes(Node* node) {
+  Sw64OperandGenerator g(this);
+  Emit(kSw64ByteSwap32, g.DefineAsRegister(node),
+       g.UseRegister(node->InputAt(0)));
+}
+
+void InstructionSelector::VisitSimd128ReverseBytes(Node* node) {
+  UNREACHABLE();
+}
+
+void InstructionSelector::VisitWord32Ctz(Node* node) {
+  Sw64OperandGenerator g(this);
+  Emit(kSw64Ctz, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)));
+}
+
+
+void InstructionSelector::VisitWord64Ctz(Node* node) {
+  Sw64OperandGenerator g(this);
+  Emit(kSw64Dctz, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)));
+}
+
+
+void InstructionSelector::VisitWord32Popcnt(Node* node) {
+  Sw64OperandGenerator g(this);
+  Emit(kSw64Popcnt, g.DefineAsRegister(node),
+       g.UseRegister(node->InputAt(0)));
+}
+
+
+void InstructionSelector::VisitWord64Popcnt(Node* node) {
+  Sw64OperandGenerator g(this);
+  Emit(kSw64Dpopcnt, g.DefineAsRegister(node),
+       g.UseRegister(node->InputAt(0)));
+}
+
+
+void InstructionSelector::VisitWord64Ror(Node* node) {
+  VisitRRO(this, kSw64Dror, node);
+}
+
+
+void InstructionSelector::VisitWord64Clz(Node* node) {
+  VisitRR(this, kSw64Dclz, node);
+}
+
+
+void InstructionSelector::VisitInt32Add(Node* node) {
+  Sw64OperandGenerator g(this);
+  Int32BinopMatcher m(node);
+
+  // Select Lsa for (left + (left_of_right << imm)).
+  if (m.right().opcode() == IrOpcode::kWord32Shl &&
+      CanCover(node, m.left().node()) && CanCover(node, m.right().node())) {
+    Int32BinopMatcher mright(m.right().node());
+      if (mright.right().HasValue() && !m.left().HasValue()) {
+        int32_t shift_value =
+            static_cast<int32_t>(mright.right().Value());
+      Emit(kSw64Lsa, g.DefineAsRegister(node), g.UseRegister(m.left().node()),
+           g.UseRegister(mright.left().node()), g.TempImmediate(shift_value));
+      return;
+    }
+  }
+
+  // Select Lsa for ((left_of_left << imm) + right).
+  if (m.left().opcode() == IrOpcode::kWord32Shl &&
+      CanCover(node, m.right().node()) && CanCover(node, m.left().node())) {
+    Int32BinopMatcher mleft(m.left().node());
+      if (mleft.right().HasValue() && !m.right().HasValue()) {
+        int32_t shift_value = static_cast<int32_t>(mleft.right().Value());
+      Emit(kSw64Lsa, g.DefineAsRegister(node),
+           g.UseRegister(m.right().node()), g.UseRegister(mleft.left().node()),
+           g.TempImmediate(shift_value));
+      return;
+    }
+  }
+  VisitBinop(this, node, kSw64Add, true, kSw64Add);
+}
+
+
+void InstructionSelector::VisitInt64Add(Node* node) {
+  Sw64OperandGenerator g(this);
+  Int64BinopMatcher m(node);
+
+  // Select Dlsa for (left + (left_of_right << imm)).
+  if (m.right().opcode() == IrOpcode::kWord64Shl &&
+      CanCover(node, m.left().node()) && CanCover(node, m.right().node())) {
+    Int64BinopMatcher mright(m.right().node());
+      if (mright.right().HasValue() && !m.left().HasValue()) {
+        int32_t shift_value = static_cast<int32_t>(mright.right().Value());
+      Emit(kSw64Dlsa, g.DefineAsRegister(node),
+           g.UseRegister(m.left().node()), g.UseRegister(mright.left().node()),
+           g.TempImmediate(shift_value));
+      return;
+    }
+  }
+
+  // Select Dlsa for ((left_of_left << imm) + right).
+  if (m.left().opcode() == IrOpcode::kWord64Shl &&
+      CanCover(node, m.right().node()) && CanCover(node, m.left().node())) {
+    Int64BinopMatcher mleft(m.left().node());
+      if (mleft.right().HasValue() && !m.right().HasValue()) {
+        int32_t shift_value = static_cast<int32_t>(mleft.right().Value());
+      Emit(kSw64Dlsa, g.DefineAsRegister(node),
+           g.UseRegister(m.right().node()), g.UseRegister(mleft.left().node()),
+           g.TempImmediate(shift_value));
+      return;
+    }
+  }
+
+  VisitBinop(this, node, kSw64Dadd, true, kSw64Dadd);
+}
+
+
+void InstructionSelector::VisitInt32Sub(Node* node) {
+  VisitBinop(this, node, kSw64Sub);
+}
+
+
+void InstructionSelector::VisitInt64Sub(Node* node) {
+  VisitBinop(this, node, kSw64Dsub);
+}
+
+
+void InstructionSelector::VisitInt32Mul(Node* node) {
+  Sw64OperandGenerator g(this);
+  Int32BinopMatcher m(node);
+  if (m.right().HasValue() && m.right().Value() > 0) {
+    uint32_t value = static_cast<uint32_t>(m.right().Value());
+    if (base::bits::IsPowerOfTwo(value)) {
+      Emit(kSw64Shl | AddressingModeField::encode(kMode_None),
+           g.DefineAsRegister(node), g.UseRegister(m.left().node()),
+           g.TempImmediate(base::bits::WhichPowerOfTwo(value)));
+      return;
+    }
+    if (base::bits::IsPowerOfTwo(value - 1)) {
+      Emit(kSw64Lsa, g.DefineAsRegister(node), g.UseRegister(m.left().node()),
+           g.UseRegister(m.left().node()),
+           g.TempImmediate(base::bits::WhichPowerOfTwo(value - 1)));
+      return;
+    }
+    if (base::bits::IsPowerOfTwo(value + 1)) {
+      InstructionOperand temp = g.TempRegister();
+      Emit(kSw64Shl | AddressingModeField::encode(kMode_None), temp,
+           g.UseRegister(m.left().node()),
+           g.TempImmediate(base::bits::WhichPowerOfTwo(value + 1)));
+      Emit(kSw64Sub | AddressingModeField::encode(kMode_None),
+           g.DefineAsRegister(node), temp, g.UseRegister(m.left().node()));
+      return;
+    }
+  }
+  Node* left = node->InputAt(0);
+  Node* right = node->InputAt(1);
+  if (CanCover(node, left) && CanCover(node, right)) {
+    if (left->opcode() == IrOpcode::kWord64Sar &&
+        right->opcode() == IrOpcode::kWord64Sar) {
+      Int64BinopMatcher leftInput(left), rightInput(right);
+      if (leftInput.right().Is(32) && rightInput.right().Is(32)) {
+        // Combine untagging shifts with Dmul high.
+        Emit(kSw64DMulHigh, g.DefineSameAsFirst(node),
+             g.UseRegister(leftInput.left().node()),
+             g.UseRegister(rightInput.left().node()));
+        return;
+      }
+    }
+  }
+  VisitRRR(this, kSw64Mul, node);
+}
+
+
+void InstructionSelector::VisitInt32MulHigh(Node* node) {
+  VisitRRR(this, kSw64MulHigh, node);
+}
+
+
+void InstructionSelector::VisitUint32MulHigh(Node* node) {
+  VisitRRR(this, kSw64MulHighU, node);
+}
+
+
+void InstructionSelector::VisitInt64Mul(Node* node) {
+  Sw64OperandGenerator g(this);
+  Int64BinopMatcher m(node);
+  // TODO(dusmil): Add optimization for shifts larger than 32.
+  if (m.right().HasValue() && m.right().Value() > 0) {
+    uint32_t value = static_cast<uint32_t>(m.right().Value());
+    if (base::bits::IsPowerOfTwo(value)) {
+      Emit(kSw64Dshl | AddressingModeField::encode(kMode_None),
+           g.DefineAsRegister(node), g.UseRegister(m.left().node()),
+           g.TempImmediate(base::bits::WhichPowerOfTwo(value)));
+      return;
+    }
+    if (base::bits::IsPowerOfTwo(value - 1)) {
+      // Dlsa macro will handle the shifting value out of bound cases.
+      Emit(kSw64Dlsa, g.DefineAsRegister(node),
+           g.UseRegister(m.left().node()), g.UseRegister(m.left().node()),
+           g.TempImmediate(base::bits::WhichPowerOfTwo(value - 1)));
+      return;
+    }
+    if (base::bits::IsPowerOfTwo(value + 1)) {
+      InstructionOperand temp = g.TempRegister();
+      Emit(kSw64Dshl | AddressingModeField::encode(kMode_None), temp,
+           g.UseRegister(m.left().node()),
+           g.TempImmediate(base::bits::WhichPowerOfTwo(value + 1)));
+      Emit(kSw64Dsub | AddressingModeField::encode(kMode_None),
+           g.DefineAsRegister(node), temp, g.UseRegister(m.left().node()));
+      return;
+    }
+  }
+  Emit(kSw64Dmul, g.DefineAsRegister(node), g.UseRegister(m.left().node()),
+       g.UseRegister(m.right().node()));
+}
+
+
+void InstructionSelector::VisitInt32Div(Node* node) {
+  Sw64OperandGenerator g(this);
+  Int32BinopMatcher m(node);
+  Node* left = node->InputAt(0);
+  Node* right = node->InputAt(1);
+  if (CanCover(node, left) && CanCover(node, right)) {
+    if (left->opcode() == IrOpcode::kWord64Sar &&
+        right->opcode() == IrOpcode::kWord64Sar) {
+      Int64BinopMatcher rightInput(right), leftInput(left);
+      if (rightInput.right().Is(32) && leftInput.right().Is(32)) {
+        // Combine both shifted operands with Ddiv.
+        Emit(kSw64Ddiv, g.DefineSameAsFirst(node),
+             g.UseRegister(leftInput.left().node()),
+             g.UseRegister(rightInput.left().node()));
+        return;
+      }
+    }
+  }
+  Emit(kSw64Div, g.DefineSameAsFirst(node), g.UseRegister(m.left().node()),
+       g.UseRegister(m.right().node()));
+}
+
+
+void InstructionSelector::VisitUint32Div(Node* node) {
+  Sw64OperandGenerator g(this);
+  Int32BinopMatcher m(node);
+  Emit(kSw64DivU, g.DefineSameAsFirst(node), g.UseRegister(m.left().node()),
+       g.UseRegister(m.right().node()));
+}
+
+
+void InstructionSelector::VisitInt32Mod(Node* node) {
+  Sw64OperandGenerator g(this);
+  Int32BinopMatcher m(node);
+  Node* left = node->InputAt(0);
+  Node* right = node->InputAt(1);
+  if (CanCover(node, left) && CanCover(node, right)) {
+    if (left->opcode() == IrOpcode::kWord64Sar &&
+        right->opcode() == IrOpcode::kWord64Sar) {
+      Int64BinopMatcher rightInput(right), leftInput(left);
+      if (rightInput.right().Is(32) && leftInput.right().Is(32)) {
+        // Combine both shifted operands with Dmod.
+        Emit(kSw64Dmod, g.DefineSameAsFirst(node),
+             g.UseRegister(leftInput.left().node()),
+             g.UseRegister(rightInput.left().node()));
+        return;
+      }
+    }
+  }
+  Emit(kSw64Mod, g.DefineAsRegister(node), g.UseRegister(m.left().node()),
+       g.UseRegister(m.right().node()));
+}
+
+
+void InstructionSelector::VisitUint32Mod(Node* node) {
+  Sw64OperandGenerator g(this);
+  Int32BinopMatcher m(node);
+  Emit(kSw64ModU, g.DefineAsRegister(node), g.UseRegister(m.left().node()),
+       g.UseRegister(m.right().node()));
+}
+
+
+void InstructionSelector::VisitInt64Div(Node* node) {
+  Sw64OperandGenerator g(this);
+  Int64BinopMatcher m(node);
+  Emit(kSw64Ddiv, g.DefineSameAsFirst(node), g.UseRegister(m.left().node()),
+       g.UseRegister(m.right().node()));
+}
+
+
+void InstructionSelector::VisitUint64Div(Node* node) {
+  Sw64OperandGenerator g(this);
+  Int64BinopMatcher m(node);
+  Emit(kSw64DdivU, g.DefineSameAsFirst(node), g.UseRegister(m.left().node()),
+       g.UseRegister(m.right().node()));
+}
+
+
+void InstructionSelector::VisitInt64Mod(Node* node) {
+  Sw64OperandGenerator g(this);
+  Int64BinopMatcher m(node);
+  Emit(kSw64Dmod, g.DefineAsRegister(node), g.UseRegister(m.left().node()),
+       g.UseRegister(m.right().node()));
+}
+
+
+void InstructionSelector::VisitUint64Mod(Node* node) {
+  Sw64OperandGenerator g(this);
+  Int64BinopMatcher m(node);
+  Emit(kSw64DmodU, g.DefineAsRegister(node), g.UseRegister(m.left().node()),
+       g.UseRegister(m.right().node()));
+}
+
+
+void InstructionSelector::VisitChangeFloat32ToFloat64(Node* node) {
+  VisitRR(this, kSw64CvtDS, node);
+}
+
+
+void InstructionSelector::VisitRoundInt32ToFloat32(Node* node) {
+  VisitRR(this, kSw64CvtSW, node);
+}
+
+
+void InstructionSelector::VisitRoundUint32ToFloat32(Node* node) {
+  VisitRR(this, kSw64CvtSUw, node);
+}
+
+
+void InstructionSelector::VisitChangeInt32ToFloat64(Node* node) {
+  VisitRR(this, kSw64CvtDW, node);
+}
+
+void InstructionSelector::VisitChangeInt64ToFloat64(Node* node) {
+  VisitRR(this, kSw64CvtDL, node);
+}
+
+void InstructionSelector::VisitChangeUint32ToFloat64(Node* node) {
+  VisitRR(this, kSw64CvtDUw, node);
+}
+
+
+void InstructionSelector::VisitTruncateFloat32ToInt32(Node* node) {
+  VisitRR(this, kSw64TruncWS, node);
+}
+
+void InstructionSelector::VisitTruncateFloat32ToUint32(Node* node) {
+  VisitRR(this, kSw64TruncUwS, node);
+}
+
+void InstructionSelector::VisitChangeFloat64ToInt32(Node* node) {
+  Sw64OperandGenerator g(this);
+  Node* value = node->InputAt(0);
+  // Match ChangeFloat64ToInt32(Float64Round##OP) to corresponding instruction
+  // which does rounding and conversion to integer format.
+  if (CanCover(node, value)) {
+    switch (value->opcode()) {
+      case IrOpcode::kFloat64RoundDown:
+        Emit(kSw64FloorWD, g.DefineAsRegister(node),
+             g.UseRegister(value->InputAt(0)));
+        return;
+      case IrOpcode::kFloat64RoundUp:
+        Emit(kSw64CeilWD, g.DefineAsRegister(node),
+             g.UseRegister(value->InputAt(0)));
+        return;
+      case IrOpcode::kFloat64RoundTiesEven:
+        Emit(kSw64RoundWD, g.DefineAsRegister(node),
+             g.UseRegister(value->InputAt(0)));
+        return;
+      case IrOpcode::kFloat64RoundTruncate:
+        Emit(kSw64TruncWD, g.DefineAsRegister(node),
+             g.UseRegister(value->InputAt(0)));
+        return;
+      default:
+        break;
+    }
+    if (value->opcode() == IrOpcode::kChangeFloat32ToFloat64) {
+      Node* next = value->InputAt(0);
+      if (CanCover(value, next)) {
+        // Match ChangeFloat64ToInt32(ChangeFloat32ToFloat64(Float64Round##OP))
+        switch (next->opcode()) {
+          case IrOpcode::kFloat32RoundDown:
+            Emit(kSw64FloorWS, g.DefineAsRegister(node),
+                 g.UseRegister(next->InputAt(0)));
+            return;
+          case IrOpcode::kFloat32RoundUp:
+            Emit(kSw64CeilWS, g.DefineAsRegister(node),
+                 g.UseRegister(next->InputAt(0)));
+            return;
+          case IrOpcode::kFloat32RoundTiesEven:
+            Emit(kSw64RoundWS, g.DefineAsRegister(node),
+                 g.UseRegister(next->InputAt(0)));
+            return;
+          case IrOpcode::kFloat32RoundTruncate:
+            Emit(kSw64TruncWS, g.DefineAsRegister(node),
+                 g.UseRegister(next->InputAt(0)));
+            return;
+          default:
+            Emit(kSw64TruncWS, g.DefineAsRegister(node),
+                 g.UseRegister(value->InputAt(0)));
+            return;
+        }
+      } else {
+        // Match float32 -> float64 -> int32 representation change path.
+        Emit(kSw64TruncWS, g.DefineAsRegister(node),
+             g.UseRegister(value->InputAt(0)));
+        return;
+      }
+    }
+  }
+  VisitRR(this, kSw64TruncWD, node);
+}
+
+void InstructionSelector::VisitChangeFloat64ToInt64(Node* node) {
+  VisitRR(this, kSw64TruncLD, node);
+}
+
+void InstructionSelector::VisitChangeFloat64ToUint32(Node* node) {
+  VisitRR(this, kSw64TruncUwD, node);
+}
+
+void InstructionSelector::VisitChangeFloat64ToUint64(Node* node) {
+  VisitRR(this, kSw64TruncUlD, node);
+}
+
+void InstructionSelector::VisitTruncateFloat64ToUint32(Node* node) {
+  VisitRR(this, kSw64TruncUwD, node);
+}
+
+void InstructionSelector::VisitTruncateFloat64ToInt64(Node* node) {
+  VisitRR(this, kSw64TruncLD, node);
+}
+
+void InstructionSelector::VisitTryTruncateFloat32ToInt64(Node* node) {
+  Sw64OperandGenerator g(this);
+  InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};
+  InstructionOperand outputs[2];
+  size_t output_count = 0;
+  outputs[output_count++] = g.DefineAsRegister(node);
+
+  Node* success_output = NodeProperties::FindProjection(node, 1);
+  if (success_output) {
+    outputs[output_count++] = g.DefineAsRegister(success_output);
+  }
+
+  this->Emit(kSw64TruncLS, output_count, outputs, 1, inputs);
+}
+
+
+void InstructionSelector::VisitTryTruncateFloat64ToInt64(Node* node) {
+  Sw64OperandGenerator g(this);
+  InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};
+  InstructionOperand outputs[2];
+  size_t output_count = 0;
+  outputs[output_count++] = g.DefineAsRegister(node);
+
+  Node* success_output = NodeProperties::FindProjection(node, 1);
+  if (success_output) {
+    outputs[output_count++] = g.DefineAsRegister(success_output);
+  }
+
+  Emit(kSw64TruncLD, output_count, outputs, 1, inputs);
+}
+
+
+void InstructionSelector::VisitTryTruncateFloat32ToUint64(Node* node) {
+  Sw64OperandGenerator g(this);
+  InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};
+  InstructionOperand outputs[2];
+  size_t output_count = 0;
+  outputs[output_count++] = g.DefineAsRegister(node);
+
+  Node* success_output = NodeProperties::FindProjection(node, 1);
+  if (success_output) {
+    outputs[output_count++] = g.DefineAsRegister(success_output);
+  }
+
+  Emit(kSw64TruncUlS, output_count, outputs, 1, inputs);
+}
+
+
+void InstructionSelector::VisitTryTruncateFloat64ToUint64(Node* node) {
+  Sw64OperandGenerator g(this);
+
+  InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};
+  InstructionOperand outputs[2];
+  size_t output_count = 0;
+  outputs[output_count++] = g.DefineAsRegister(node);
+
+  Node* success_output = NodeProperties::FindProjection(node, 1);
+  if (success_output) {
+    outputs[output_count++] = g.DefineAsRegister(success_output);
+  }
+
+  Emit(kSw64TruncUlD, output_count, outputs, 1, inputs);
+}
+
+void InstructionSelector::VisitBitcastWord32ToWord64(Node* node) {
+  UNIMPLEMENTED();
+}
+
+void InstructionSelector::VisitChangeInt32ToInt64(Node* node) {
+  Node* value = node->InputAt(0);
+  if (value->opcode() == IrOpcode::kLoad && CanCover(node, value)) {
+    // Generate sign-extending load.
+    LoadRepresentation load_rep = LoadRepresentationOf(value->op());
+    InstructionCode opcode = kArchNop;
+    switch (load_rep.representation()) {
+      case MachineRepresentation::kBit:  // Fall through.
+      case MachineRepresentation::kWord8:
+        opcode = load_rep.IsUnsigned() ? kSw64Ldbu : kSw64Ldb;
+        break;
+      case MachineRepresentation::kWord16:
+        opcode = load_rep.IsUnsigned() ? kSw64Ldhu : kSw64Ldh;
+        break;
+      case MachineRepresentation::kWord32:
+        opcode = kSw64Ldw;
+        break;
+      default:
+        UNREACHABLE();
+        return;
+    }
+    EmitLoad(this, value, opcode, node);
+  } else {
+    Sw64OperandGenerator g(this);
+    Emit(kSw64Shl, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)),
+         g.TempImmediate(0));
+  }
+}
+
+
+bool InstructionSelector::ZeroExtendsWord32ToWord64NoPhis(Node* node) {
+  DCHECK_NE(node->opcode(), IrOpcode::kPhi);
+  switch (node->opcode()) {
+    // 32-bit operations will write their result in a 64 bit register,
+    // clearing the top 32 bits of the destination register.
+    case IrOpcode::kUint32Div:
+    case IrOpcode::kUint32Mod:
+    case IrOpcode::kUint32MulHigh:
+      return true;
+    case IrOpcode::kLoad: {
+      LoadRepresentation load_rep = LoadRepresentationOf(node->op());
+      if (load_rep.IsUnsigned()) {
+        switch (load_rep.representation()) {
+          case MachineRepresentation::kWord8:
+          case MachineRepresentation::kWord16:
+          case MachineRepresentation::kWord32:
+            return true;
+          default:
+            return false;
+        }
+      }
+      return false;
+    }
+    default:
+      return false;
+  }
+}
+
+void InstructionSelector::VisitChangeUint32ToUint64(Node* node) {
+  Sw64OperandGenerator g(this);
+#ifdef SW64
+  //TODO: It is an optimizaion!
+
+#else
+  Node* value = node->InputAt(0);
+  if (ZeroExtendsWord32ToWord64(value)) {
+    Emit(kArchNop, g.DefineSameAsFirst(node), g.Use(value));
+    return;
+  }
+#endif
+  Emit(kSw64Dext, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)),
+       g.TempImmediate(0), g.TempImmediate(32));
+}
+
+void InstructionSelector::VisitTruncateInt64ToInt32(Node* node) {
+  Sw64OperandGenerator g(this);
+
+  Node* value = node->InputAt(0);
+  if (CanCover(node, value)) {
+    switch (value->opcode()) {
+      case IrOpcode::kWord64Sar: {
+        if (CanCoverTransitively(node, value, value->InputAt(0)) &&
+            TryEmitExtendingLoad(this, value, node)) {
+          return;
+        } else {
+          Int64BinopMatcher m(value);
+          if (m.right().IsInRange(32, 63)) {
+            // After smi untagging no need for truncate. Combine sequence.
+            Emit(kSw64Dsar, g.DefineAsRegister(node),
+                 g.UseRegister(m.left().node()),
+                 g.UseImmediate(m.right().node()));
+            return;
+          }
+        }
+        break;
+      }
+      default:
+        break;
+    }
+  }
+  Emit(kSw64Ext, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)),
+       g.TempImmediate(0), g.TempImmediate(32));
+}
+
+
+void InstructionSelector::VisitTruncateFloat64ToFloat32(Node* node) {
+  Sw64OperandGenerator g(this);
+  Node* value = node->InputAt(0);
+  // Match TruncateFloat64ToFloat32(ChangeInt32ToFloat64) to corresponding
+  // instruction.
+  if (CanCover(node, value) &&
+      value->opcode() == IrOpcode::kChangeInt32ToFloat64) {
+    Emit(kSw64CvtSW, g.DefineAsRegister(node),
+         g.UseRegister(value->InputAt(0)));
+    return;
+  }
+  VisitRR(this, kSw64CvtSD, node);
+}
+
+void InstructionSelector::VisitTruncateFloat64ToWord32(Node* node) {
+  VisitRR(this, kArchTruncateDoubleToI, node);
+}
+
+void InstructionSelector::VisitRoundFloat64ToInt32(Node* node) {
+  VisitRR(this, kSw64TruncWD, node);
+}
+
+void InstructionSelector::VisitRoundInt64ToFloat32(Node* node) {
+  VisitRR(this, kSw64CvtSL, node);
+}
+
+
+void InstructionSelector::VisitRoundInt64ToFloat64(Node* node) {
+  VisitRR(this, kSw64CvtDL, node);
+}
+
+
+void InstructionSelector::VisitRoundUint64ToFloat32(Node* node) {
+  VisitRR(this, kSw64CvtSUl, node);
+}
+
+
+void InstructionSelector::VisitRoundUint64ToFloat64(Node* node) {
+  VisitRR(this, kSw64CvtDUl, node);
+}
+
+
+void InstructionSelector::VisitBitcastFloat32ToInt32(Node* node) {
+#ifdef SW64
+  VisitRR(this, kSw64BitcastWS, node);
+#else
+  VisitRR(this, kSw64Float64ExtractLowWord32, node);
+#endif
+}
+
+
+void InstructionSelector::VisitBitcastFloat64ToInt64(Node* node) {
+  VisitRR(this, kSw64BitcastDL, node);
+}
+
+
+void InstructionSelector::VisitBitcastInt32ToFloat32(Node* node) {
+#ifdef SW64
+  VisitRR(this, kSw64BitcastSW, node);
+#else
+  Sw64OperandGenerator g(this);
+  Emit(kSw64Float64InsertLowWord32, g.DefineAsRegister(node),
+       ImmediateOperand(ImmediateOperand::INLINE, 0),
+       g.UseRegister(node->InputAt(0)));
+#endif
+}
+
+
+void InstructionSelector::VisitBitcastInt64ToFloat64(Node* node) {
+  VisitRR(this, kSw64BitcastLD, node);
+}
+
+
+void InstructionSelector::VisitFloat32Add(Node* node) {
+  // Optimization with Madd.S(z, x, y) is intentionally removed.
+  // See explanation for madd_s in assembler-sw64.cc.
+  VisitRRR(this, kSw64AddS, node);
+}
+
+
+void InstructionSelector::VisitFloat64Add(Node* node) {
+  // Optimization with Madd.D(z, x, y) is intentionally removed.
+  // See explanation for madd_d in assembler-sw64.cc.
+  VisitRRR(this, kSw64AddD, node);
+}
+
+
+void InstructionSelector::VisitFloat32Sub(Node* node) {
+  // Optimization with Msub.S(z, x, y) is intentionally removed.
+  // See explanation for madd_s in assembler-sw64.cc.
+  VisitRRR(this, kSw64SubS, node);
+}
+
+void InstructionSelector::VisitFloat64Sub(Node* node) {
+  // Optimization with Msub.D(z, x, y) is intentionally removed.
+  // See explanation for madd_d in assembler-sw64.cc.
+  VisitRRR(this, kSw64SubD, node);
+}
+
+void InstructionSelector::VisitFloat32Mul(Node* node) {
+  VisitRRR(this, kSw64MulS, node);
+}
+
+
+void InstructionSelector::VisitFloat64Mul(Node* node) {
+  VisitRRR(this, kSw64MulD, node);
+}
+
+
+void InstructionSelector::VisitFloat32Div(Node* node) {
+  VisitRRR(this, kSw64DivS, node);
+}
+
+
+void InstructionSelector::VisitFloat64Div(Node* node) {
+  VisitRRR(this, kSw64DivD, node);
+}
+
+
+void InstructionSelector::VisitFloat64Mod(Node* node) {
+  Sw64OperandGenerator g(this);
+  Emit(kSw64ModD, g.DefineAsFixed(node, f0),
+       g.UseFixed(node->InputAt(0), f16),
+       g.UseFixed(node->InputAt(1), f17))->MarkAsCall();
+}
+
+void InstructionSelector::VisitFloat32Max(Node* node) {
+  Sw64OperandGenerator g(this);
+  Emit(kSw64Float32Max, g.DefineAsRegister(node),
+       g.UseRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)));
+}
+
+void InstructionSelector::VisitFloat64Max(Node* node) {
+  Sw64OperandGenerator g(this);
+  Emit(kSw64Float64Max, g.DefineAsRegister(node),
+       g.UseRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)));
+}
+
+void InstructionSelector::VisitFloat32Min(Node* node) {
+  Sw64OperandGenerator g(this);
+  Emit(kSw64Float32Min, g.DefineAsRegister(node),
+       g.UseRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)));
+}
+
+void InstructionSelector::VisitFloat64Min(Node* node) {
+  Sw64OperandGenerator g(this);
+  Emit(kSw64Float64Min, g.DefineAsRegister(node),
+       g.UseRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)));
+}
+
+
+void InstructionSelector::VisitFloat32Abs(Node* node) {
+  VisitRR(this, kSw64AbsS, node);
+}
+
+
+void InstructionSelector::VisitFloat64Abs(Node* node) {
+  VisitRR(this, kSw64AbsD, node);
+}
+
+void InstructionSelector::VisitFloat32Sqrt(Node* node) {
+  VisitRR(this, kSw64SqrtS, node);
+}
+
+
+void InstructionSelector::VisitFloat64Sqrt(Node* node) {
+  VisitRR(this, kSw64SqrtD, node);
+}
+
+
+void InstructionSelector::VisitFloat32RoundDown(Node* node) {
+  VisitRR(this, kSw64Float32RoundDown, node);
+}
+
+
+void InstructionSelector::VisitFloat64RoundDown(Node* node) {
+  VisitRR(this, kSw64Float64RoundDown, node);
+}
+
+
+void InstructionSelector::VisitFloat32RoundUp(Node* node) {
+  VisitRR(this, kSw64Float32RoundUp, node);
+}
+
+
+void InstructionSelector::VisitFloat64RoundUp(Node* node) {
+  VisitRR(this, kSw64Float64RoundUp, node);
+}
+
+
+void InstructionSelector::VisitFloat32RoundTruncate(Node* node) {
+  VisitRR(this, kSw64Float32RoundTruncate, node);
+}
+
+
+void InstructionSelector::VisitFloat64RoundTruncate(Node* node) {
+  VisitRR(this, kSw64Float64RoundTruncate, node);
+}
+
+
+void InstructionSelector::VisitFloat64RoundTiesAway(Node* node) {
+  UNREACHABLE();
+}
+
+
+void InstructionSelector::VisitFloat32RoundTiesEven(Node* node) {
+  VisitRR(this, kSw64Float32RoundTiesEven, node);
+}
+
+
+void InstructionSelector::VisitFloat64RoundTiesEven(Node* node) {
+  VisitRR(this, kSw64Float64RoundTiesEven, node);
+}
+
+void InstructionSelector::VisitFloat32Neg(Node* node) {
+  VisitRR(this, kSw64NegS, node);
+}
+
+void InstructionSelector::VisitFloat64Neg(Node* node) {
+  VisitRR(this, kSw64NegD, node);
+}
+
+void InstructionSelector::VisitFloat64Ieee754Binop(Node* node,
+                                                   InstructionCode opcode) {
+  Sw64OperandGenerator g(this);
+  Emit(opcode, g.DefineAsFixed(node, f0), g.UseFixed(node->InputAt(0), f2),
+       g.UseFixed(node->InputAt(1), f4))
+      ->MarkAsCall();
+}
+
+void InstructionSelector::VisitFloat64Ieee754Unop(Node* node,
+                                                  InstructionCode opcode) {
+  Sw64OperandGenerator g(this);
+  Emit(opcode, g.DefineAsFixed(node, f0), g.UseFixed(node->InputAt(0), f16))
+      ->MarkAsCall();
+}
+
+void InstructionSelector::EmitPrepareArguments(
+    ZoneVector<PushParameter>* arguments, const CallDescriptor* call_descriptor,
+    Node* node) {
+  Sw64OperandGenerator g(this);
+
+  // Prepare for C function call.
+  if (call_descriptor->IsCFunctionCall()) {
+    Emit(kArchPrepareCallCFunction | MiscField::encode(static_cast<int>(
+                                         call_descriptor->ParameterCount())),
+         0, nullptr, 0, nullptr);
+
+    // Poke any stack arguments.
+    int slot = kCArgSlotCount;
+    for (PushParameter input : (*arguments)) {
+      Emit(kSw64StoreToStackSlot, g.NoOutput(), g.UseRegister(input.node),
+           g.TempImmediate(slot << kSystemPointerSizeLog2));
+      ++slot;
+    }
+  } else {
+    int push_count = static_cast<int>(call_descriptor->StackParameterCount());
+    if (push_count > 0) {
+      // Calculate needed space
+      int stack_size = 0;
+      for (PushParameter input : (*arguments)) {
+        if (input.node) {
+          stack_size += input.location.GetSizeInPointers();
+        }
+      }
+      Emit(kSw64StackClaim, g.NoOutput(),
+           g.TempImmediate(stack_size << kSystemPointerSizeLog2));
+    }
+    for (size_t n = 0; n < arguments->size(); ++n) {
+      PushParameter input = (*arguments)[n];
+      if (input.node) {
+        Emit(kSw64StoreToStackSlot, g.NoOutput(), g.UseRegister(input.node),
+             g.TempImmediate(static_cast<int>(n << kSystemPointerSizeLog2)));
+      }
+    }
+  }
+}
+
+void InstructionSelector::EmitPrepareResults(
+    ZoneVector<PushParameter>* results, const CallDescriptor* call_descriptor,
+    Node* node) {
+  Sw64OperandGenerator g(this);
+
+  int reverse_slot = 1;
+  for (PushParameter output : *results) {
+    if (!output.location.IsCallerFrameSlot()) continue;
+    // Skip any alignment holes in nodes.
+    if (output.node != nullptr) {
+      DCHECK(!call_descriptor->IsCFunctionCall());
+      if (output.location.GetType() == MachineType::Float32()) {
+        MarkAsFloat32(output.node);
+      } else if (output.location.GetType() == MachineType::Float64()) {
+        MarkAsFloat64(output.node);
+      } else if (output.location.GetType() == MachineType::Simd128()) {
+        MarkAsSimd128(output.node);
+      }
+      Emit(kSw64Peek, g.DefineAsRegister(output.node),
+           g.UseImmediate(reverse_slot));
+    }
+    reverse_slot += output.location.GetSizeInPointers();
+  }
+}
+
+bool InstructionSelector::IsTailCallAddressImmediate() { return false; }
+
+int InstructionSelector::GetTempsCountForTailCallFromJSFunction() { return 3; }
+
+void InstructionSelector::VisitUnalignedLoad(Node* node) {
+  LoadRepresentation load_rep = LoadRepresentationOf(node->op());
+  Sw64OperandGenerator g(this);
+  Node* base = node->InputAt(0);
+  Node* index = node->InputAt(1);
+
+  ArchOpcode opcode = kArchNop;
+  switch (load_rep.representation()) {
+    case MachineRepresentation::kFloat32:
+      opcode = kSw64Uflds;
+      break;
+    case MachineRepresentation::kFloat64:
+      opcode = kSw64Ufldd;
+      break;
+    case MachineRepresentation::kWord8:
+      opcode = load_rep.IsUnsigned() ? kSw64Ldbu : kSw64Ldb;
+      break;
+    case MachineRepresentation::kWord16:
+      opcode = load_rep.IsUnsigned() ? kSw64Uldhu : kSw64Uldh;
+      break;
+    case MachineRepresentation::kWord32:
+      opcode = load_rep.IsUnsigned() ? kSw64Uldwu : kSw64Uldw;
+      break;
+    case MachineRepresentation::kTaggedSigned:   // Fall through.
+    case MachineRepresentation::kTaggedPointer:  // Fall through.
+    case MachineRepresentation::kTagged:  // Fall through.
+    case MachineRepresentation::kWord64:
+      opcode = kSw64Uldl;
+      break;
+    case MachineRepresentation::kSimd128:
+      opcode = kSw64MsaLd;
+      break;
+    case MachineRepresentation::kBit:                // Fall through.
+    case MachineRepresentation::kCompressedPointer:  // Fall through.
+    case MachineRepresentation::kCompressed:         // Fall through.
+    case MachineRepresentation::kNone:
+      UNREACHABLE();
+  }
+
+  if (g.CanBeImmediate(index, opcode)) {
+    Emit(opcode | AddressingModeField::encode(kMode_MRI),
+         g.DefineAsRegister(node), g.UseRegister(base), g.UseImmediate(index));
+  } else {
+    InstructionOperand addr_reg = g.TempRegister();
+    Emit(kSw64Dadd | AddressingModeField::encode(kMode_None), addr_reg,
+         g.UseRegister(index), g.UseRegister(base));
+    // Emit desired load opcode, using temp addr_reg.
+    Emit(opcode | AddressingModeField::encode(kMode_MRI),
+         g.DefineAsRegister(node), addr_reg, g.TempImmediate(0));
+  }
+}
+
+void InstructionSelector::VisitUnalignedStore(Node* node) {
+  Sw64OperandGenerator g(this);
+  Node* base = node->InputAt(0);
+  Node* index = node->InputAt(1);
+  Node* value = node->InputAt(2);
+
+  UnalignedStoreRepresentation rep = UnalignedStoreRepresentationOf(node->op());
+  ArchOpcode opcode = kArchNop;
+  switch (rep) {
+    case MachineRepresentation::kFloat32:
+      opcode = kSw64Ufsts;
+      break;
+    case MachineRepresentation::kFloat64:
+      opcode = kSw64Ufstd;
+      break;
+    case MachineRepresentation::kWord8:
+      opcode = kSw64Stb;
+      break;
+    case MachineRepresentation::kWord16:
+      opcode = kSw64Usth;
+      break;
+    case MachineRepresentation::kWord32:
+      opcode = kSw64Ustw;
+      break;
+    case MachineRepresentation::kTaggedSigned:   // Fall through.
+    case MachineRepresentation::kTaggedPointer:  // Fall through.
+    case MachineRepresentation::kTagged:  // Fall through.
+    case MachineRepresentation::kWord64:
+      opcode = kSw64Ustl;
+      break;
+    case MachineRepresentation::kSimd128:
+      opcode = kSw64MsaSt;
+      break;
+    case MachineRepresentation::kBit:                // Fall through.
+    case MachineRepresentation::kCompressedPointer:  // Fall through.
+    case MachineRepresentation::kCompressed:         // Fall through.
+    case MachineRepresentation::kNone:
+      UNREACHABLE();
+  }
+
+  if (g.CanBeImmediate(index, opcode)) {
+    Emit(opcode | AddressingModeField::encode(kMode_MRI), g.NoOutput(),
+         g.UseRegister(base), g.UseImmediate(index),
+         g.UseRegisterOrImmediateZero(value));
+  } else {
+    InstructionOperand addr_reg = g.TempRegister();
+    Emit(kSw64Dadd | AddressingModeField::encode(kMode_None), addr_reg,
+         g.UseRegister(index), g.UseRegister(base));
+    // Emit desired store opcode, using temp addr_reg.
+    Emit(opcode | AddressingModeField::encode(kMode_MRI), g.NoOutput(),
+         addr_reg, g.TempImmediate(0), g.UseRegisterOrImmediateZero(value));
+  }
+}
+
+namespace {
+
+// Shared routine for multiple compare operations.
+static void VisitCompare(InstructionSelector* selector, InstructionCode opcode,
+                         InstructionOperand left, InstructionOperand right,
+                         FlagsContinuation* cont) {
+  selector->EmitWithContinuation(opcode, left, right, cont);
+}
+
+
+// Shared routine for multiple float32 compare operations.
+void VisitFloat32Compare(InstructionSelector* selector, Node* node,
+                         FlagsContinuation* cont) {
+  Sw64OperandGenerator g(selector);
+  Float32BinopMatcher m(node);
+  InstructionOperand lhs, rhs;
+
+  lhs = m.left().IsZero() ? g.UseImmediate(m.left().node())
+                          : g.UseRegister(m.left().node());
+  rhs = m.right().IsZero() ? g.UseImmediate(m.right().node())
+                           : g.UseRegister(m.right().node());
+  VisitCompare(selector, kSw64CmpS, lhs, rhs, cont);
+}
+
+
+// Shared routine for multiple float64 compare operations.
+void VisitFloat64Compare(InstructionSelector* selector, Node* node,
+                         FlagsContinuation* cont) {
+  Sw64OperandGenerator g(selector);
+  Float64BinopMatcher m(node);
+  InstructionOperand lhs, rhs;
+
+  lhs = m.left().IsZero() ? g.UseImmediate(m.left().node())
+                          : g.UseRegister(m.left().node());
+  rhs = m.right().IsZero() ? g.UseImmediate(m.right().node())
+                           : g.UseRegister(m.right().node());
+  VisitCompare(selector, kSw64CmpD, lhs, rhs, cont);
+}
+
+
+// Shared routine for multiple word compare operations.
+void VisitWordCompare(InstructionSelector* selector, Node* node,
+                      InstructionCode opcode, FlagsContinuation* cont,
+                      bool commutative) {
+  Sw64OperandGenerator g(selector);
+  Node* left = node->InputAt(0);
+  Node* right = node->InputAt(1);
+
+  // Match immediates on left or right side of comparison.
+  if (g.CanBeImmediate(right, opcode)) {
+    if (opcode == kSw64Tst) {
+      VisitCompare(selector, opcode, g.UseRegister(left), g.UseImmediate(right),
+                   cont);
+    } else {
+      switch (cont->condition()) {
+        case kEqual:
+        case kNotEqual:
+          if (cont->IsSet()) {
+            VisitCompare(selector, opcode, g.UseRegister(left),
+                         g.UseImmediate(right), cont);
+          } else {
+            VisitCompare(selector, opcode, g.UseRegister(left),
+                         g.UseRegister(right), cont);
+          }
+          break;
+        case kSignedLessThan:
+        case kSignedGreaterThanOrEqual:
+        case kUnsignedLessThan:
+        case kUnsignedGreaterThanOrEqual:
+          VisitCompare(selector, opcode, g.UseRegister(left),
+                       g.UseImmediate(right), cont);
+          break;
+        default:
+          VisitCompare(selector, opcode, g.UseRegister(left),
+                       g.UseRegister(right), cont);
+      }
+    }
+  } else if (g.CanBeImmediate(left, opcode)) {
+    if (!commutative) cont->Commute();
+    if (opcode == kSw64Tst) {
+      VisitCompare(selector, opcode, g.UseRegister(right), g.UseImmediate(left),
+                   cont);
+    } else {
+      switch (cont->condition()) {
+        case kEqual:
+        case kNotEqual:
+          if (cont->IsSet()) {
+            VisitCompare(selector, opcode, g.UseRegister(right),
+                         g.UseImmediate(left), cont);
+          } else {
+            VisitCompare(selector, opcode, g.UseRegister(right),
+                         g.UseRegister(left), cont);
+          }
+          break;
+        case kSignedLessThan:
+        case kSignedGreaterThanOrEqual:
+        case kUnsignedLessThan:
+        case kUnsignedGreaterThanOrEqual:
+          VisitCompare(selector, opcode, g.UseRegister(right),
+                       g.UseImmediate(left), cont);
+          break;
+        default:
+          VisitCompare(selector, opcode, g.UseRegister(right),
+                       g.UseRegister(left), cont);
+      }
+    }
+  } else {
+    VisitCompare(selector, opcode, g.UseRegister(left), g.UseRegister(right),
+                 cont);
+  }
+}
+
+bool IsNodeUnsigned(Node* n) {
+  NodeMatcher m(n);
+
+  if (m.IsLoad() || m.IsUnalignedLoad() || m.IsPoisonedLoad() ||
+      m.IsProtectedLoad() || m.IsWord32AtomicLoad() || m.IsWord64AtomicLoad()) {
+    LoadRepresentation load_rep = LoadRepresentationOf(n->op());
+    return load_rep.IsUnsigned();
+  } else {
+    return m.IsUint32Div() || m.IsUint32LessThan() ||
+           m.IsUint32LessThanOrEqual() || m.IsUint32Mod() ||
+           m.IsUint32MulHigh() || m.IsChangeFloat64ToUint32() ||
+           m.IsTruncateFloat64ToUint32() || m.IsTruncateFloat32ToUint32();
+  }
+}
+
+// Shared routine for multiple word compare operations.
+void VisitFullWord32Compare(InstructionSelector* selector, Node* node,
+                            InstructionCode opcode, FlagsContinuation* cont) {
+  Sw64OperandGenerator g(selector);
+  InstructionOperand leftOp = g.TempRegister();
+  InstructionOperand rightOp = g.TempRegister();
+
+  selector->Emit(kSw64Dshl, leftOp, g.UseRegister(node->InputAt(0)),
+                 g.TempImmediate(32));
+  selector->Emit(kSw64Dshl, rightOp, g.UseRegister(node->InputAt(1)),
+                 g.TempImmediate(32));
+
+  VisitCompare(selector, opcode, leftOp, rightOp, cont);
+}
+
+void VisitOptimizedWord32Compare(InstructionSelector* selector, Node* node,
+                                 InstructionCode opcode,
+                                 FlagsContinuation* cont) {
+  if (FLAG_debug_code) {
+    Sw64OperandGenerator g(selector);
+    InstructionOperand leftOp = g.TempRegister();
+    InstructionOperand rightOp = g.TempRegister();
+    InstructionOperand optimizedResult = g.TempRegister();
+    InstructionOperand fullResult = g.TempRegister();
+    FlagsCondition condition = cont->condition();
+    InstructionCode testOpcode = opcode |
+                                 FlagsConditionField::encode(condition) |
+                                 FlagsModeField::encode(kFlags_set);
+
+    selector->Emit(testOpcode, optimizedResult, g.UseRegister(node->InputAt(0)),
+                   g.UseRegister(node->InputAt(1)));
+
+    selector->Emit(kSw64Dshl, leftOp, g.UseRegister(node->InputAt(0)),
+                   g.TempImmediate(32));
+    selector->Emit(kSw64Dshl, rightOp, g.UseRegister(node->InputAt(1)),
+                   g.TempImmediate(32));
+    selector->Emit(testOpcode, fullResult, leftOp, rightOp);
+
+    selector->Emit(
+        kSw64AssertEqual, g.NoOutput(), optimizedResult, fullResult,
+        g.TempImmediate(
+            static_cast<int>(AbortReason::kUnsupportedNonPrimitiveCompare)));
+  }
+
+  VisitWordCompare(selector, node, opcode, cont, false);
+}
+
+void VisitWord32Compare(InstructionSelector* selector, Node* node,
+                        FlagsContinuation* cont) {
+  // SW64 doesn't support Word32 compare instructions. Instead it relies
+  // that the values in registers are correctly sign-extended and uses
+  // Word64 comparison instead. This behavior is correct in most cases,
+  // but doesn't work when comparing signed with unsigned operands.
+  // We could simulate full Word32 compare in all cases but this would
+  // create an unnecessary overhead since unsigned integers are rarely
+  // used in JavaScript.
+  // The solution proposed here tries to match a comparison of signed
+  // with unsigned operand, and perform full Word32Compare only
+  // in those cases. Unfortunately, the solution is not complete because
+  // it might skip cases where Word32 full compare is needed, so
+  // basically it is a hack.
+  // When call to a host function in simulator, if the function return a
+  // int32 value, the simulator do not sign-extended to int64 because in
+  // simulator we do not know the function whether return a int32 or int64.
+  // so we need do a full word32 compare in this case.
+#ifndef USE_SIMULATOR
+  if (IsNodeUnsigned(node->InputAt(0)) != IsNodeUnsigned(node->InputAt(1))) {
+#else
+  if (IsNodeUnsigned(node->InputAt(0)) != IsNodeUnsigned(node->InputAt(1)) ||
+      node->InputAt(0)->opcode() == IrOpcode::kCall ||
+      node->InputAt(1)->opcode() == IrOpcode::kCall ) {
+#endif
+    VisitFullWord32Compare(selector, node, kSw64Cmp, cont);
+  } else {
+    VisitOptimizedWord32Compare(selector, node, kSw64Cmp, cont);
+  }
+}
+
+
+void VisitWord64Compare(InstructionSelector* selector, Node* node,
+                        FlagsContinuation* cont) {
+  VisitWordCompare(selector, node, kSw64Cmp, cont, false);
+}
+
+
+
+void EmitWordCompareZero(InstructionSelector* selector, Node* value,
+                         FlagsContinuation* cont) {
+  Sw64OperandGenerator g(selector);
+  selector->EmitWithContinuation(kSw64Cmp, g.UseRegister(value),
+                                 g.TempImmediate(0), cont);
+}
+
+void VisitAtomicLoad(InstructionSelector* selector, Node* node,
+                     ArchOpcode opcode) {
+  Sw64OperandGenerator g(selector);
+  Node* base = node->InputAt(0);
+  Node* index = node->InputAt(1);
+  if (g.CanBeImmediate(index, opcode)) {
+    selector->Emit(opcode | AddressingModeField::encode(kMode_MRI),
+                   g.DefineAsRegister(node), g.UseRegister(base),
+                   g.UseImmediate(index));
+  } else {
+    InstructionOperand addr_reg = g.TempRegister();
+    selector->Emit(kSw64Dadd | AddressingModeField::encode(kMode_None),
+                   addr_reg, g.UseRegister(index), g.UseRegister(base));
+    // Emit desired load opcode, using temp addr_reg.
+    selector->Emit(opcode | AddressingModeField::encode(kMode_MRI),
+                   g.DefineAsRegister(node), addr_reg, g.TempImmediate(0));
+  }
+}
+
+void VisitAtomicStore(InstructionSelector* selector, Node* node,
+                      ArchOpcode opcode) {
+  Sw64OperandGenerator g(selector);
+  Node* base = node->InputAt(0);
+  Node* index = node->InputAt(1);
+  Node* value = node->InputAt(2);
+
+  if (g.CanBeImmediate(index, opcode)) {
+    selector->Emit(opcode | AddressingModeField::encode(kMode_MRI),
+                   g.NoOutput(), g.UseRegister(base), g.UseImmediate(index),
+                   g.UseRegisterOrImmediateZero(value));
+  } else {
+    InstructionOperand addr_reg = g.TempRegister();
+    selector->Emit(kSw64Dadd | AddressingModeField::encode(kMode_None),
+                   addr_reg, g.UseRegister(index), g.UseRegister(base));
+    // Emit desired store opcode, using temp addr_reg.
+    selector->Emit(opcode | AddressingModeField::encode(kMode_MRI),
+                   g.NoOutput(), addr_reg, g.TempImmediate(0),
+                   g.UseRegisterOrImmediateZero(value));
+  }
+}
+
+void VisitAtomicExchange(InstructionSelector* selector, Node* node,
+                         ArchOpcode opcode) {
+  Sw64OperandGenerator g(selector);
+  Node* base = node->InputAt(0);
+  Node* index = node->InputAt(1);
+  Node* value = node->InputAt(2);
+
+  AddressingMode addressing_mode = kMode_MRI;
+  InstructionOperand inputs[3];
+  size_t input_count = 0;
+  inputs[input_count++] = g.UseUniqueRegister(base);
+  inputs[input_count++] = g.UseUniqueRegister(index);
+  inputs[input_count++] = g.UseUniqueRegister(value);
+  InstructionOperand outputs[1];
+  outputs[0] = g.UseUniqueRegister(node);
+  InstructionOperand temp[3];
+  temp[0] = g.TempRegister();
+  temp[1] = g.TempRegister();
+  temp[2] = g.TempRegister();
+  InstructionCode code = opcode | AddressingModeField::encode(addressing_mode);
+  selector->Emit(code, 1, outputs, input_count, inputs, 3, temp);
+}
+
+void VisitAtomicCompareExchange(InstructionSelector* selector, Node* node,
+                                ArchOpcode opcode) {
+  Sw64OperandGenerator g(selector);
+  Node* base = node->InputAt(0);
+  Node* index = node->InputAt(1);
+  Node* old_value = node->InputAt(2);
+  Node* new_value = node->InputAt(3);
+
+  AddressingMode addressing_mode = kMode_MRI;
+  InstructionOperand inputs[4];
+  size_t input_count = 0;
+  inputs[input_count++] = g.UseUniqueRegister(base);
+  inputs[input_count++] = g.UseUniqueRegister(index);
+  inputs[input_count++] = g.UseUniqueRegister(old_value);
+  inputs[input_count++] = g.UseUniqueRegister(new_value);
+  InstructionOperand outputs[1];
+  outputs[0] = g.UseUniqueRegister(node);
+  InstructionOperand temp[3];
+  temp[0] = g.TempRegister();
+  temp[1] = g.TempRegister();
+  temp[2] = g.TempRegister();
+  InstructionCode code = opcode | AddressingModeField::encode(addressing_mode);
+  selector->Emit(code, 1, outputs, input_count, inputs, 3, temp);
+}
+
+void VisitAtomicBinop(InstructionSelector* selector, Node* node,
+                      ArchOpcode opcode) {
+  Sw64OperandGenerator g(selector);
+  Node* base = node->InputAt(0);
+  Node* index = node->InputAt(1);
+  Node* value = node->InputAt(2);
+
+  AddressingMode addressing_mode = kMode_MRI;
+  InstructionOperand inputs[3];
+  size_t input_count = 0;
+  inputs[input_count++] = g.UseUniqueRegister(base);
+  inputs[input_count++] = g.UseUniqueRegister(index);
+  inputs[input_count++] = g.UseUniqueRegister(value);
+  InstructionOperand outputs[1];
+  outputs[0] = g.UseUniqueRegister(node);
+  InstructionOperand temps[4];
+  temps[0] = g.TempRegister();
+  temps[1] = g.TempRegister();
+  temps[2] = g.TempRegister();
+  temps[3] = g.TempRegister();
+  InstructionCode code = opcode | AddressingModeField::encode(addressing_mode);
+  selector->Emit(code, 1, outputs, input_count, inputs, 4, temps);
+}
+
+}  // namespace
+
+void InstructionSelector::VisitStackPointerGreaterThan(
+    Node* node, FlagsContinuation* cont) {
+  StackCheckKind kind = StackCheckKindOf(node->op());
+  InstructionCode opcode =
+      kArchStackPointerGreaterThan | MiscField::encode(static_cast<int>(kind));
+
+  Sw64OperandGenerator g(this);
+
+  // No outputs.
+  InstructionOperand* const outputs = nullptr;
+  const int output_count = 0;
+
+  // Applying an offset to this stack check requires a temp register. Offsets
+  // are only applied to the first stack check. If applying an offset, we must
+  // ensure the input and temp registers do not alias, thus kUniqueRegister.
+  InstructionOperand temps[] = {g.TempRegister()};
+  const int temp_count = (kind == StackCheckKind::kJSFunctionEntry ? 1 : 0);
+  const auto register_mode = (kind == StackCheckKind::kJSFunctionEntry)
+                                 ? OperandGenerator::kUniqueRegister
+                                 : OperandGenerator::kRegister;
+
+  Node* const value = node->InputAt(0);
+  InstructionOperand inputs[] = {g.UseRegisterWithMode(value, register_mode)};
+  static constexpr int input_count = arraysize(inputs);
+
+  EmitWithContinuation(opcode, output_count, outputs, input_count, inputs,
+                       temp_count, temps, cont);
+}
+
+// Shared routine for word comparisons against zero.
+void InstructionSelector::VisitWordCompareZero(Node* user, Node* value,
+                                               FlagsContinuation* cont) {
+  // Try to combine with comparisons against 0 by simply inverting the branch.
+  while (CanCover(user, value)) {
+    if (value->opcode() == IrOpcode::kWord32Equal) {
+      Int32BinopMatcher m(value);
+      if (!m.right().Is(0)) break;
+      user = value;
+      value = m.left().node();
+    } else if (value->opcode() == IrOpcode::kWord64Equal) {
+      Int64BinopMatcher m(value);
+      if (!m.right().Is(0)) break;
+      user = value;
+      value = m.left().node();
+    } else {
+      break;
+    }
+
+    cont->Negate();
+  }
+
+  if (CanCover(user, value)) {
+    switch (value->opcode()) {
+      case IrOpcode::kWord32Equal:
+        cont->OverwriteAndNegateIfEqual(kEqual);
+        return VisitWord32Compare(this, value, cont);
+      case IrOpcode::kInt32LessThan:
+        cont->OverwriteAndNegateIfEqual(kSignedLessThan);
+        return VisitWord32Compare(this, value, cont);
+      case IrOpcode::kInt32LessThanOrEqual:
+        cont->OverwriteAndNegateIfEqual(kSignedLessThanOrEqual);
+        return VisitWord32Compare(this, value, cont);
+      case IrOpcode::kUint32LessThan:
+        cont->OverwriteAndNegateIfEqual(kUnsignedLessThan);
+        return VisitWord32Compare(this, value, cont);
+      case IrOpcode::kUint32LessThanOrEqual:
+        cont->OverwriteAndNegateIfEqual(kUnsignedLessThanOrEqual);
+        return VisitWord32Compare(this, value, cont);
+      case IrOpcode::kWord64Equal:
+        cont->OverwriteAndNegateIfEqual(kEqual);
+        return VisitWord64Compare(this, value, cont);
+      case IrOpcode::kInt64LessThan:
+        cont->OverwriteAndNegateIfEqual(kSignedLessThan);
+        return VisitWord64Compare(this, value, cont);
+      case IrOpcode::kInt64LessThanOrEqual:
+        cont->OverwriteAndNegateIfEqual(kSignedLessThanOrEqual);
+        return VisitWord64Compare(this, value, cont);
+      case IrOpcode::kUint64LessThan:
+        cont->OverwriteAndNegateIfEqual(kUnsignedLessThan);
+        return VisitWord64Compare(this, value, cont);
+      case IrOpcode::kUint64LessThanOrEqual:
+        cont->OverwriteAndNegateIfEqual(kUnsignedLessThanOrEqual);
+        return VisitWord64Compare(this, value, cont);
+      case IrOpcode::kFloat32Equal:
+        cont->OverwriteAndNegateIfEqual(kEqual);
+        return VisitFloat32Compare(this, value, cont);
+      case IrOpcode::kFloat32LessThan:
+        cont->OverwriteAndNegateIfEqual(kUnsignedLessThan);
+        return VisitFloat32Compare(this, value, cont);
+      case IrOpcode::kFloat32LessThanOrEqual:
+        cont->OverwriteAndNegateIfEqual(kUnsignedLessThanOrEqual);
+        return VisitFloat32Compare(this, value, cont);
+      case IrOpcode::kFloat64Equal:
+        cont->OverwriteAndNegateIfEqual(kEqual);
+        return VisitFloat64Compare(this, value, cont);
+      case IrOpcode::kFloat64LessThan:
+        cont->OverwriteAndNegateIfEqual(kUnsignedLessThan);
+        return VisitFloat64Compare(this, value, cont);
+      case IrOpcode::kFloat64LessThanOrEqual:
+        cont->OverwriteAndNegateIfEqual(kUnsignedLessThanOrEqual);
+        return VisitFloat64Compare(this, value, cont);
+      case IrOpcode::kProjection:
+        // Check if this is the overflow output projection of an
+        // <Operation>WithOverflow node.
+        if (ProjectionIndexOf(value->op()) == 1u) {
+          // We cannot combine the <Operation>WithOverflow with this branch
+          // unless the 0th projection (the use of the actual value of the
+          // <Operation> is either nullptr, which means there's no use of the
+          // actual value, or was already defined, which means it is scheduled
+          // *AFTER* this branch).
+          Node* const node = value->InputAt(0);
+          Node* const result = NodeProperties::FindProjection(node, 0);
+          if (result == nullptr || IsDefined(result)) {
+            switch (node->opcode()) {
+              case IrOpcode::kInt32AddWithOverflow:
+                cont->OverwriteAndNegateIfEqual(kOverflow);
+                return VisitBinop(this, node, kSw64Dadd, cont);
+              case IrOpcode::kInt32SubWithOverflow:
+                cont->OverwriteAndNegateIfEqual(kOverflow);
+                return VisitBinop(this, node, kSw64Dsub, cont);
+              case IrOpcode::kInt32MulWithOverflow:
+                cont->OverwriteAndNegateIfEqual(kOverflow);
+                return VisitBinop(this, node, kSw64MulOvf, cont);
+              case IrOpcode::kInt64AddWithOverflow:
+                cont->OverwriteAndNegateIfEqual(kOverflow);
+                return VisitBinop(this, node, kSw64DaddOvf, cont);
+              case IrOpcode::kInt64SubWithOverflow:
+                cont->OverwriteAndNegateIfEqual(kOverflow);
+                return VisitBinop(this, node, kSw64DsubOvf, cont);
+              default:
+                break;
+            }
+          }
+        }
+        break;
+      case IrOpcode::kWord32And:
+      case IrOpcode::kWord64And:
+        return VisitWordCompare(this, value, kSw64Tst, cont, true);
+      case IrOpcode::kStackPointerGreaterThan:
+        cont->OverwriteAndNegateIfEqual(kStackPointerGreaterThanCondition);
+        return VisitStackPointerGreaterThan(value, cont);
+      default:
+        break;
+    }
+  }
+
+  // Continuation could not be combined with a compare, emit compare against 0.
+  EmitWordCompareZero(this, value, cont);
+}
+
+void InstructionSelector::VisitSwitch(Node* node, const SwitchInfo& sw) {
+  Sw64OperandGenerator g(this);
+  InstructionOperand value_operand = g.UseRegister(node->InputAt(0));
+
+  // Emit either ArchTableSwitch or ArchBinarySearchSwitch.
+  if (enable_switch_jump_table_ == kEnableSwitchJumpTable) {
+    static const size_t kMaxTableSwitchValueRange = 2 << 16;
+    size_t table_space_cost = 10 + 2 * sw.value_range();
+    size_t table_time_cost = 3;
+    size_t lookup_space_cost = 2 + 2 * sw.case_count();
+    size_t lookup_time_cost = sw.case_count();
+    if (sw.case_count() > 0 &&
+        table_space_cost + 3 * table_time_cost <=
+            lookup_space_cost + 3 * lookup_time_cost &&
+        sw.min_value() > std::numeric_limits<int32_t>::min() &&
+        sw.value_range() <= kMaxTableSwitchValueRange) {
+      InstructionOperand index_operand = value_operand;
+      if (sw.min_value()) {
+        index_operand = g.TempRegister();
+        Emit(kSw64Sub, index_operand, value_operand,
+             g.TempImmediate(sw.min_value()));
+      }
+      // Generate a table lookup.
+      return EmitTableSwitch(sw, index_operand);
+    }
+  }
+
+  // Generate a tree of conditional jumps.
+  return EmitBinarySearchSwitch(sw, value_operand);
+}
+
+void InstructionSelector::VisitWord32Equal(Node* const node) {
+  FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
+  Int32BinopMatcher m(node);
+  if (m.right().Is(0)) {
+    return VisitWordCompareZero(m.node(), m.left().node(), &cont);
+  }
+
+  VisitWord32Compare(this, node, &cont);
+}
+
+
+void InstructionSelector::VisitInt32LessThan(Node* node) {
+  FlagsContinuation cont = FlagsContinuation::ForSet(kSignedLessThan, node);
+  VisitWord32Compare(this, node, &cont);
+}
+
+
+void InstructionSelector::VisitInt32LessThanOrEqual(Node* node) {
+  FlagsContinuation cont =
+      FlagsContinuation::ForSet(kSignedLessThanOrEqual, node);
+  VisitWord32Compare(this, node, &cont);
+}
+
+
+void InstructionSelector::VisitUint32LessThan(Node* node) {
+  FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
+  VisitWord32Compare(this, node, &cont);
+}
+
+
+void InstructionSelector::VisitUint32LessThanOrEqual(Node* node) {
+  FlagsContinuation cont =
+      FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
+  VisitWord32Compare(this, node, &cont);
+}
+
+
+void InstructionSelector::VisitInt32AddWithOverflow(Node* node) {
+  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+    return VisitBinop(this, node, kSw64Dadd, &cont);
+  }
+  FlagsContinuation cont;
+  VisitBinop(this, node, kSw64Dadd, &cont);
+}
+
+
+void InstructionSelector::VisitInt32SubWithOverflow(Node* node) {
+  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+    return VisitBinop(this, node, kSw64Dsub, &cont);
+  }
+  FlagsContinuation cont;
+  VisitBinop(this, node, kSw64Dsub, &cont);
+}
+
+void InstructionSelector::VisitInt32MulWithOverflow(Node* node) {
+  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+    return VisitBinop(this, node, kSw64MulOvf, &cont);
+  }
+  FlagsContinuation cont;
+  VisitBinop(this, node, kSw64MulOvf, &cont);
+}
+
+void InstructionSelector::VisitInt64AddWithOverflow(Node* node) {
+  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+    return VisitBinop(this, node, kSw64DaddOvf, &cont);
+  }
+  FlagsContinuation cont;
+  VisitBinop(this, node, kSw64DaddOvf, &cont);
+}
+
+
+void InstructionSelector::VisitInt64SubWithOverflow(Node* node) {
+  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+    return VisitBinop(this, node, kSw64DsubOvf, &cont);
+  }
+  FlagsContinuation cont;
+  VisitBinop(this, node, kSw64DsubOvf, &cont);
+}
+
+
+void InstructionSelector::VisitWord64Equal(Node* const node) {
+  FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
+  Int64BinopMatcher m(node);
+  if (m.right().Is(0)) {
+    return VisitWordCompareZero(m.node(), m.left().node(), &cont);
+  }
+
+  VisitWord64Compare(this, node, &cont);
+}
+
+
+void InstructionSelector::VisitInt64LessThan(Node* node) {
+  FlagsContinuation cont = FlagsContinuation::ForSet(kSignedLessThan, node);
+  VisitWord64Compare(this, node, &cont);
+}
+
+
+void InstructionSelector::VisitInt64LessThanOrEqual(Node* node) {
+  FlagsContinuation cont =
+      FlagsContinuation::ForSet(kSignedLessThanOrEqual, node);
+  VisitWord64Compare(this, node, &cont);
+}
+
+
+void InstructionSelector::VisitUint64LessThan(Node* node) {
+  FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
+  VisitWord64Compare(this, node, &cont);
+}
+
+
+void InstructionSelector::VisitUint64LessThanOrEqual(Node* node) {
+  FlagsContinuation cont =
+      FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
+  VisitWord64Compare(this, node, &cont);
+}
+
+
+void InstructionSelector::VisitFloat32Equal(Node* node) {
+  FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
+  VisitFloat32Compare(this, node, &cont);
+}
+
+
+void InstructionSelector::VisitFloat32LessThan(Node* node) {
+  FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
+  VisitFloat32Compare(this, node, &cont);
+}
+
+
+void InstructionSelector::VisitFloat32LessThanOrEqual(Node* node) {
+  FlagsContinuation cont =
+      FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
+  VisitFloat32Compare(this, node, &cont);
+}
+
+
+void InstructionSelector::VisitFloat64Equal(Node* node) {
+  FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
+  VisitFloat64Compare(this, node, &cont);
+}
+
+
+void InstructionSelector::VisitFloat64LessThan(Node* node) {
+  FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
+  VisitFloat64Compare(this, node, &cont);
+}
+
+
+void InstructionSelector::VisitFloat64LessThanOrEqual(Node* node) {
+  FlagsContinuation cont =
+      FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
+  VisitFloat64Compare(this, node, &cont);
+}
+
+
+void InstructionSelector::VisitFloat64ExtractLowWord32(Node* node) {
+  VisitRR(this, kSw64Float64ExtractLowWord32, node);
+}
+
+
+void InstructionSelector::VisitFloat64ExtractHighWord32(Node* node) {
+  VisitRR(this, kSw64Float64ExtractHighWord32, node);
+}
+
+void InstructionSelector::VisitFloat64SilenceNaN(Node* node) {
+  VisitRR(this, kSw64Float64SilenceNaN, node);
+}
+
+void InstructionSelector::VisitFloat64InsertLowWord32(Node* node) {
+  Sw64OperandGenerator g(this);
+  Node* left = node->InputAt(0);
+  Node* right = node->InputAt(1);
+  Emit(kSw64Float64InsertLowWord32, g.DefineSameAsFirst(node),
+       g.UseRegister(left), g.UseRegister(right));
+}
+
+
+void InstructionSelector::VisitFloat64InsertHighWord32(Node* node) {
+  Sw64OperandGenerator g(this);
+  Node* left = node->InputAt(0);
+  Node* right = node->InputAt(1);
+  Emit(kSw64Float64InsertHighWord32, g.DefineSameAsFirst(node),
+       g.UseRegister(left), g.UseRegister(right));
+}
+
+void InstructionSelector::VisitMemoryBarrier(Node* node) {
+  Sw64OperandGenerator g(this);
+  Emit(kSw64Sync, g.NoOutput());
+}
+
+void InstructionSelector::VisitWord32AtomicLoad(Node* node) {
+  LoadRepresentation load_rep = LoadRepresentationOf(node->op());
+  ArchOpcode opcode = kArchNop;
+  switch (load_rep.representation()) {
+    case MachineRepresentation::kWord8:
+      opcode =
+          load_rep.IsSigned() ? kWord32AtomicLoadInt8 : kWord32AtomicLoadUint8;
+      break;
+    case MachineRepresentation::kWord16:
+      opcode = load_rep.IsSigned() ? kWord32AtomicLoadInt16
+                                   : kWord32AtomicLoadUint16;
+      break;
+    case MachineRepresentation::kWord32:
+      opcode = kWord32AtomicLoadWord32;
+      break;
+    default:
+      UNREACHABLE();
+  }
+  VisitAtomicLoad(this, node, opcode);
+}
+
+void InstructionSelector::VisitWord32AtomicStore(Node* node) {
+  MachineRepresentation rep = AtomicStoreRepresentationOf(node->op());
+  ArchOpcode opcode = kArchNop;
+  switch (rep) {
+    case MachineRepresentation::kWord8:
+      opcode = kWord32AtomicStoreWord8;
+      break;
+    case MachineRepresentation::kWord16:
+      opcode = kWord32AtomicStoreWord16;
+      break;
+    case MachineRepresentation::kWord32:
+      opcode = kWord32AtomicStoreWord32;
+      break;
+    default:
+      UNREACHABLE();
+  }
+
+  VisitAtomicStore(this, node, opcode);
+}
+
+void InstructionSelector::VisitWord64AtomicLoad(Node* node) {
+  LoadRepresentation load_rep = LoadRepresentationOf(node->op());
+  ArchOpcode opcode = kArchNop;
+  switch (load_rep.representation()) {
+    case MachineRepresentation::kWord8:
+      opcode = kSw64Word64AtomicLoadUint8;
+      break;
+    case MachineRepresentation::kWord16:
+      opcode = kSw64Word64AtomicLoadUint16;
+      break;
+    case MachineRepresentation::kWord32:
+      opcode = kSw64Word64AtomicLoadUint32;
+      break;
+    case MachineRepresentation::kWord64:
+      opcode = kSw64Word64AtomicLoadUint64;
+      break;
+    default:
+      UNREACHABLE();
+  }
+  VisitAtomicLoad(this, node, opcode);
+}
+
+void InstructionSelector::VisitWord64AtomicStore(Node* node) {
+  MachineRepresentation rep = AtomicStoreRepresentationOf(node->op());
+  ArchOpcode opcode = kArchNop;
+  switch (rep) {
+    case MachineRepresentation::kWord8:
+      opcode = kSw64Word64AtomicStoreWord8;
+      break;
+    case MachineRepresentation::kWord16:
+      opcode = kSw64Word64AtomicStoreWord16;
+      break;
+    case MachineRepresentation::kWord32:
+      opcode = kSw64Word64AtomicStoreWord32;
+      break;
+    case MachineRepresentation::kWord64:
+      opcode = kSw64Word64AtomicStoreWord64;
+      break;
+    default:
+      UNREACHABLE();
+  }
+
+  VisitAtomicStore(this, node, opcode);
+}
+
+void InstructionSelector::VisitWord32AtomicExchange(Node* node) {
+  ArchOpcode opcode = kArchNop;
+  MachineType type = AtomicOpType(node->op());
+  if (type == MachineType::Int8()) {
+    opcode = kWord32AtomicExchangeInt8;
+  } else if (type == MachineType::Uint8()) {
+    opcode = kWord32AtomicExchangeUint8;
+  } else if (type == MachineType::Int16()) {
+    opcode = kWord32AtomicExchangeInt16;
+  } else if (type == MachineType::Uint16()) {
+    opcode = kWord32AtomicExchangeUint16;
+  } else if (type == MachineType::Int32() || type == MachineType::Uint32()) {
+    opcode = kWord32AtomicExchangeWord32;
+  } else {
+    UNREACHABLE();
+    return;
+  }
+
+  VisitAtomicExchange(this, node, opcode);
+}
+
+void InstructionSelector::VisitWord64AtomicExchange(Node* node) {
+  ArchOpcode opcode = kArchNop;
+  MachineType type = AtomicOpType(node->op());
+  if (type == MachineType::Uint8()) {
+    opcode = kSw64Word64AtomicExchangeUint8;
+  } else if (type == MachineType::Uint16()) {
+    opcode = kSw64Word64AtomicExchangeUint16;
+  } else if (type == MachineType::Uint32()) {
+    opcode = kSw64Word64AtomicExchangeUint32;
+  } else if (type == MachineType::Uint64()) {
+    opcode = kSw64Word64AtomicExchangeUint64;
+  } else {
+    UNREACHABLE();
+    return;
+  }
+  VisitAtomicExchange(this, node, opcode);
+}
+
+void InstructionSelector::VisitWord32AtomicCompareExchange(Node* node) {
+  ArchOpcode opcode = kArchNop;
+  MachineType type = AtomicOpType(node->op());
+  if (type == MachineType::Int8()) {
+    opcode = kWord32AtomicCompareExchangeInt8;
+  } else if (type == MachineType::Uint8()) {
+    opcode = kWord32AtomicCompareExchangeUint8;
+  } else if (type == MachineType::Int16()) {
+    opcode = kWord32AtomicCompareExchangeInt16;
+  } else if (type == MachineType::Uint16()) {
+    opcode = kWord32AtomicCompareExchangeUint16;
+  } else if (type == MachineType::Int32() || type == MachineType::Uint32()) {
+    opcode = kWord32AtomicCompareExchangeWord32;
+  } else {
+    UNREACHABLE();
+    return;
+  }
+
+  VisitAtomicCompareExchange(this, node, opcode);
+}
+
+void InstructionSelector::VisitWord64AtomicCompareExchange(Node* node) {
+  ArchOpcode opcode = kArchNop;
+  MachineType type = AtomicOpType(node->op());
+  if (type == MachineType::Uint8()) {
+    opcode = kSw64Word64AtomicCompareExchangeUint8;
+  } else if (type == MachineType::Uint16()) {
+    opcode = kSw64Word64AtomicCompareExchangeUint16;
+  } else if (type == MachineType::Uint32()) {
+    opcode = kSw64Word64AtomicCompareExchangeUint32;
+  } else if (type == MachineType::Uint64()) {
+    opcode = kSw64Word64AtomicCompareExchangeUint64;
+  } else {
+    UNREACHABLE();
+    return;
+  }
+  VisitAtomicCompareExchange(this, node, opcode);
+}
+void InstructionSelector::VisitWord32AtomicBinaryOperation(
+    Node* node, ArchOpcode int8_op, ArchOpcode uint8_op, ArchOpcode int16_op,
+    ArchOpcode uint16_op, ArchOpcode word32_op) {
+  ArchOpcode opcode = kArchNop;
+  MachineType type = AtomicOpType(node->op());
+  if (type == MachineType::Int8()) {
+    opcode = int8_op;
+  } else if (type == MachineType::Uint8()) {
+    opcode = uint8_op;
+  } else if (type == MachineType::Int16()) {
+    opcode = int16_op;
+  } else if (type == MachineType::Uint16()) {
+    opcode = uint16_op;
+  } else if (type == MachineType::Int32() || type == MachineType::Uint32()) {
+    opcode = word32_op;
+  } else {
+    UNREACHABLE();
+    return;
+  }
+
+  VisitAtomicBinop(this, node, opcode);
+}
+
+#define VISIT_ATOMIC_BINOP(op)                                   \
+  void InstructionSelector::VisitWord32Atomic##op(Node* node) {  \
+    VisitWord32AtomicBinaryOperation(                            \
+        node, kWord32Atomic##op##Int8, kWord32Atomic##op##Uint8, \
+        kWord32Atomic##op##Int16, kWord32Atomic##op##Uint16,     \
+        kWord32Atomic##op##Word32);                              \
+  }
+VISIT_ATOMIC_BINOP(Add)
+VISIT_ATOMIC_BINOP(Sub)
+VISIT_ATOMIC_BINOP(And)
+VISIT_ATOMIC_BINOP(Or)
+VISIT_ATOMIC_BINOP(Xor)
+#undef VISIT_ATOMIC_BINOP
+
+void InstructionSelector::VisitWord64AtomicBinaryOperation(
+    Node* node, ArchOpcode uint8_op, ArchOpcode uint16_op, ArchOpcode uint32_op,
+    ArchOpcode uint64_op) {
+  ArchOpcode opcode = kArchNop;
+  MachineType type = AtomicOpType(node->op());
+  if (type == MachineType::Uint8()) {
+    opcode = uint8_op;
+  } else if (type == MachineType::Uint16()) {
+    opcode = uint16_op;
+  } else if (type == MachineType::Uint32()) {
+    opcode = uint32_op;
+  } else if (type == MachineType::Uint64()) {
+    opcode = uint64_op;
+  } else {
+    UNREACHABLE();
+    return;
+  }
+  VisitAtomicBinop(this, node, opcode);
+}
+
+#define VISIT_ATOMIC_BINOP(op)                                                 \
+  void InstructionSelector::VisitWord64Atomic##op(Node* node) {                \
+    VisitWord64AtomicBinaryOperation(                                          \
+        node, kSw64Word64Atomic##op##Uint8, kSw64Word64Atomic##op##Uint16, \
+        kSw64Word64Atomic##op##Uint32, kSw64Word64Atomic##op##Uint64);     \
+  }
+VISIT_ATOMIC_BINOP(Add)
+VISIT_ATOMIC_BINOP(Sub)
+VISIT_ATOMIC_BINOP(And)
+VISIT_ATOMIC_BINOP(Or)
+VISIT_ATOMIC_BINOP(Xor)
+#undef VISIT_ATOMIC_BINOP
+
+void InstructionSelector::VisitInt32AbsWithOverflow(Node* node) {
+  UNREACHABLE();
+}
+
+void InstructionSelector::VisitInt64AbsWithOverflow(Node* node) {
+  UNREACHABLE();
+}
+
+#define SIMD_TYPE_LIST(V) \
+  V(F64x2)                \
+  V(F32x4)                \
+  V(I64x2)                \
+  V(I32x4)                \
+  V(I16x8)                \
+  V(I8x16)
+
+#define SIMD_UNOP_LIST(V)                                  \
+  V(F64x2Abs, kSw64F64x2Abs)                             \
+  V(F64x2Neg, kSw64F64x2Neg)                             \
+  V(F64x2Sqrt, kSw64F64x2Sqrt)                           \
+  V(F64x2Ceil, kSw64F64x2Ceil)                           \
+  V(F64x2Floor, kSw64F64x2Floor)                         \
+  V(F64x2Trunc, kSw64F64x2Trunc)                         \
+  V(F64x2NearestInt, kSw64F64x2NearestInt)               \
+  V(I64x2Neg, kSw64I64x2Neg)                             \
+  V(F32x4Sqrt, kSw64F32x4Sqrt)                           \
+  V(F32x4SConvertI32x4, kSw64F32x4SConvertI32x4)         \
+  V(F32x4UConvertI32x4, kSw64F32x4UConvertI32x4)         \
+  V(F32x4Abs, kSw64F32x4Abs)                             \
+  V(F32x4Neg, kSw64F32x4Neg)                             \
+  V(F32x4RecipApprox, kSw64F32x4RecipApprox)             \
+  V(F32x4RecipSqrtApprox, kSw64F32x4RecipSqrtApprox)     \
+  V(F32x4Ceil, kSw64F32x4Ceil)                           \
+  V(F32x4Floor, kSw64F32x4Floor)                         \
+  V(F32x4Trunc, kSw64F32x4Trunc)                         \
+  V(F32x4NearestInt, kSw64F32x4NearestInt)               \
+  V(I32x4SConvertF32x4, kSw64I32x4SConvertF32x4)         \
+  V(I32x4UConvertF32x4, kSw64I32x4UConvertF32x4)         \
+  V(I32x4Neg, kSw64I32x4Neg)                             \
+  V(I32x4SConvertI16x8Low, kSw64I32x4SConvertI16x8Low)   \
+  V(I32x4SConvertI16x8High, kSw64I32x4SConvertI16x8High) \
+  V(I32x4UConvertI16x8Low, kSw64I32x4UConvertI16x8Low)   \
+  V(I32x4UConvertI16x8High, kSw64I32x4UConvertI16x8High) \
+  V(I32x4Abs, kSw64I32x4Abs)                             \
+  V(I32x4BitMask, kSw64I32x4BitMask)                     \
+  V(I16x8Neg, kSw64I16x8Neg)                             \
+  V(I16x8SConvertI8x16Low, kSw64I16x8SConvertI8x16Low)   \
+  V(I16x8SConvertI8x16High, kSw64I16x8SConvertI8x16High) \
+  V(I16x8UConvertI8x16Low, kSw64I16x8UConvertI8x16Low)   \
+  V(I16x8UConvertI8x16High, kSw64I16x8UConvertI8x16High) \
+  V(I16x8Abs, kSw64I16x8Abs)                             \
+  V(I16x8BitMask, kSw64I16x8BitMask)                     \
+  V(I8x16Neg, kSw64I8x16Neg)                             \
+  V(I8x16Abs, kSw64I8x16Abs)                             \
+  V(I8x16BitMask, kSw64I8x16BitMask)                     \
+  V(S128Not, kSw64S128Not)                               \
+  V(V32x4AnyTrue, kSw64V32x4AnyTrue)                     \
+  V(V32x4AllTrue, kSw64V32x4AllTrue)                     \
+  V(V16x8AnyTrue, kSw64V16x8AnyTrue)                     \
+  V(V16x8AllTrue, kSw64V16x8AllTrue)                     \
+  V(V8x16AnyTrue, kSw64V8x16AnyTrue)                     \
+  V(V8x16AllTrue, kSw64V8x16AllTrue)
+
+#define SIMD_SHIFT_OP_LIST(V) \
+  V(I64x2Shl)                 \
+  V(I64x2ShrS)                \
+  V(I64x2ShrU)                \
+  V(I32x4Shl)                 \
+  V(I32x4ShrS)                \
+  V(I32x4ShrU)                \
+  V(I16x8Shl)                 \
+  V(I16x8ShrS)                \
+  V(I16x8ShrU)                \
+  V(I8x16Shl)                 \
+  V(I8x16ShrS)                \
+  V(I8x16ShrU)
+
+#define SIMD_BINOP_LIST(V)                         \
+  V(F64x2Add, kSw64F64x2Add)                           \
+  V(F64x2Sub, kSw64F64x2Sub)                           \
+  V(F64x2Mul, kSw64F64x2Mul)                           \
+  V(F64x2Div, kSw64F64x2Div)                           \
+  V(F64x2Min, kSw64F64x2Min)                           \
+  V(F64x2Max, kSw64F64x2Max)                           \
+  V(F64x2Eq, kSw64F64x2Eq)                             \
+  V(F64x2Ne, kSw64F64x2Ne)                             \
+  V(F64x2Lt, kSw64F64x2Lt)                             \
+  V(F64x2Le, kSw64F64x2Le)                             \
+  V(I64x2Add, kSw64I64x2Add)                           \
+  V(I64x2Sub, kSw64I64x2Sub)                           \
+  V(I64x2Mul, kSw64I64x2Mul)                           \
+  V(F32x4Add, kSw64F32x4Add)                     \
+  V(F32x4AddHoriz, kSw64F32x4AddHoriz)           \
+  V(F32x4Sub, kSw64F32x4Sub)                     \
+  V(F32x4Mul, kSw64F32x4Mul)                     \
+  V(F32x4Div, kSw64F32x4Div)                           \
+  V(F32x4Max, kSw64F32x4Max)                     \
+  V(F32x4Min, kSw64F32x4Min)                     \
+  V(F32x4Eq, kSw64F32x4Eq)                       \
+  V(F32x4Ne, kSw64F32x4Ne)                       \
+  V(F32x4Lt, kSw64F32x4Lt)                       \
+  V(F32x4Le, kSw64F32x4Le)                       \
+  V(I32x4Add, kSw64I32x4Add)                     \
+  V(I32x4AddHoriz, kSw64I32x4AddHoriz)           \
+  V(I32x4Sub, kSw64I32x4Sub)                     \
+  V(I32x4Mul, kSw64I32x4Mul)                     \
+  V(I32x4MaxS, kSw64I32x4MaxS)                   \
+  V(I32x4MinS, kSw64I32x4MinS)                   \
+  V(I32x4MaxU, kSw64I32x4MaxU)                   \
+  V(I32x4MinU, kSw64I32x4MinU)                   \
+  V(I32x4Eq, kSw64I32x4Eq)                       \
+  V(I32x4Ne, kSw64I32x4Ne)                       \
+  V(I32x4GtS, kSw64I32x4GtS)                     \
+  V(I32x4GeS, kSw64I32x4GeS)                     \
+  V(I32x4GtU, kSw64I32x4GtU)                     \
+  V(I32x4GeU, kSw64I32x4GeU)                     \
+  V(I16x8Add, kSw64I16x8Add)                     \
+  V(I16x8AddSaturateS, kSw64I16x8AddSaturateS)                   \
+  V(I16x8AddSaturateU, kSw64I16x8AddSaturateU)                   \
+  V(I16x8AddHoriz, kSw64I16x8AddHoriz)           \
+  V(I16x8Sub, kSw64I16x8Sub)                     \
+  V(I16x8SubSaturateS, kSw64I16x8SubSaturateS)                   \
+  V(I16x8SubSaturateU, kSw64I16x8SubSaturateU)                   \
+  V(I16x8Mul, kSw64I16x8Mul)                     \
+  V(I16x8MaxS, kSw64I16x8MaxS)                   \
+  V(I16x8MinS, kSw64I16x8MinS)                   \
+  V(I16x8MaxU, kSw64I16x8MaxU)                   \
+  V(I16x8MinU, kSw64I16x8MinU)                   \
+  V(I16x8Eq, kSw64I16x8Eq)                       \
+  V(I16x8Ne, kSw64I16x8Ne)                       \
+  V(I16x8GtS, kSw64I16x8GtS)                     \
+  V(I16x8GeS, kSw64I16x8GeS)                     \
+  V(I16x8GtU, kSw64I16x8GtU)                     \
+  V(I16x8GeU, kSw64I16x8GeU)                     \
+  V(I16x8RoundingAverageU, kSw64I16x8RoundingAverageU) \
+  V(I16x8SConvertI32x4, kSw64I16x8SConvertI32x4) \
+  V(I16x8UConvertI32x4, kSw64I16x8UConvertI32x4) \
+  V(I8x16Add, kSw64I8x16Add)                     \
+  V(I8x16AddSaturateS, kSw64I8x16AddSaturateS)                   \
+  V(I8x16AddSaturateU, kSw64I8x16AddSaturateU)                   \
+  V(I8x16Sub, kSw64I8x16Sub)                     \
+  V(I8x16SubSaturateS, kSw64I8x16SubSaturateS)                   \
+  V(I8x16SubSaturateU, kSw64I8x16SubSaturateU)                   \
+  V(I8x16Mul, kSw64I8x16Mul)                     \
+  V(I8x16MaxS, kSw64I8x16MaxS)                   \
+  V(I8x16MinS, kSw64I8x16MinS)                   \
+  V(I8x16MaxU, kSw64I8x16MaxU)                   \
+  V(I8x16MinU, kSw64I8x16MinU)                   \
+  V(I8x16Eq, kSw64I8x16Eq)                       \
+  V(I8x16Ne, kSw64I8x16Ne)                       \
+  V(I8x16GtS, kSw64I8x16GtS)                     \
+  V(I8x16GeS, kSw64I8x16GeS)                     \
+  V(I8x16GtU, kSw64I8x16GtU)                     \
+  V(I8x16GeU, kSw64I8x16GeU)                     \
+  V(I8x16RoundingAverageU, kSw64I8x16RoundingAverageU) \
+  V(I8x16SConvertI16x8, kSw64I8x16SConvertI16x8) \
+  V(I8x16UConvertI16x8, kSw64I8x16UConvertI16x8) \
+  V(S128And, kSw64S128And)                       \
+  V(S128Or, kSw64S128Or)                         \
+  V(S128Xor, kSw64S128Xor)                             \
+  V(S128AndNot, kSw64S128AndNot)
+
+//SKTODO
+void InstructionSelector::VisitS128Const(Node* node) {
+  Sw64OperandGenerator g(this);
+  static const int kUint32Immediates = kSimd128Size / sizeof(uint32_t);
+  uint32_t val[kUint32Immediates];
+  memcpy(val, S128ImmediateParameterOf(node->op()).data(), kSimd128Size);
+  // If all bytes are zeros or ones, avoid emitting code for generic constants
+  bool all_zeros = !(val[0] || val[1] || val[2] || val[3]);
+  bool all_ones = val[0] == UINT32_MAX && val[1] == UINT32_MAX &&
+                  val[2] == UINT32_MAX && val[3] == UINT32_MAX;
+  InstructionOperand dst = g.DefineAsRegister(node);
+  if (all_zeros) {
+    Emit(kSw64S128Zero, dst);
+  } else if (all_ones) {
+    Emit(kSw64S128AllOnes, dst);
+  } else {
+    Emit(kSw64S128Const, dst, g.UseImmediate(val[0]), g.UseImmediate(val[1]),
+         g.UseImmediate(val[2]), g.UseImmediate(val[3]));
+  }
+}
+
+void InstructionSelector::VisitS128Zero(Node* node) {
+  Sw64OperandGenerator g(this);
+  Emit(kSw64S128Zero, g.DefineAsRegister(node));
+}
+
+#define SIMD_VISIT_SPLAT(Type)                               \
+  void InstructionSelector::Visit##Type##Splat(Node* node) { \
+    VisitRR(this, kSw64##Type##Splat, node);               \
+  }
+SIMD_TYPE_LIST(SIMD_VISIT_SPLAT)
+#undef SIMD_VISIT_SPLAT
+
+#define SIMD_VISIT_EXTRACT_LANE(Type, Sign)                              \
+  void InstructionSelector::Visit##Type##ExtractLane##Sign(Node* node) { \
+    VisitRRI(this, kSw64##Type##ExtractLane##Sign, node);              \
+  }
+SIMD_VISIT_EXTRACT_LANE(F64x2, )
+SIMD_VISIT_EXTRACT_LANE(F32x4, )
+SIMD_VISIT_EXTRACT_LANE(I64x2, )
+SIMD_VISIT_EXTRACT_LANE(I32x4, )
+SIMD_VISIT_EXTRACT_LANE(I16x8, U)
+SIMD_VISIT_EXTRACT_LANE(I16x8, S)
+SIMD_VISIT_EXTRACT_LANE(I8x16, U)
+SIMD_VISIT_EXTRACT_LANE(I8x16, S)
+#undef SIMD_VISIT_EXTRACT_LANE
+
+#define SIMD_VISIT_REPLACE_LANE(Type)                              \
+  void InstructionSelector::Visit##Type##ReplaceLane(Node* node) { \
+    VisitRRIR(this, kSw64##Type##ReplaceLane, node);             \
+  }
+SIMD_TYPE_LIST(SIMD_VISIT_REPLACE_LANE)
+#undef SIMD_VISIT_REPLACE_LANE
+
+#define SIMD_VISIT_UNOP(Name, instruction)            \
+  void InstructionSelector::Visit##Name(Node* node) { \
+    VisitRR(this, instruction, node);                 \
+  }
+SIMD_UNOP_LIST(SIMD_VISIT_UNOP)
+#undef SIMD_VISIT_UNOP
+
+#define SIMD_VISIT_SHIFT_OP(Name)                     \
+  void InstructionSelector::Visit##Name(Node* node) { \
+    VisitSimdShift(this, kSw64##Name, node);        \
+  }
+SIMD_SHIFT_OP_LIST(SIMD_VISIT_SHIFT_OP)
+#undef SIMD_VISIT_SHIFT_OP
+
+#define SIMD_VISIT_BINOP(Name, instruction)           \
+  void InstructionSelector::Visit##Name(Node* node) { \
+    VisitRRR(this, instruction, node);                \
+  }
+SIMD_BINOP_LIST(SIMD_VISIT_BINOP)
+#undef SIMD_VISIT_BINOP
+
+void InstructionSelector::VisitS128Select(Node* node) {
+  VisitRRRR(this, kSw64S128Select, node);
+}
+
+namespace {
+
+struct ShuffleEntry {
+  uint8_t shuffle[kSimd128Size];
+  ArchOpcode opcode;
+};
+
+static const ShuffleEntry arch_shuffles[] = {
+    {{0, 1, 2, 3, 16, 17, 18, 19, 4, 5, 6, 7, 20, 21, 22, 23},
+     kSw64S32x4InterleaveRight},
+    {{8, 9, 10, 11, 24, 25, 26, 27, 12, 13, 14, 15, 28, 29, 30, 31},
+     kSw64S32x4InterleaveLeft},
+    {{0, 1, 2, 3, 8, 9, 10, 11, 16, 17, 18, 19, 24, 25, 26, 27},
+     kSw64S32x4PackEven},
+    {{4, 5, 6, 7, 12, 13, 14, 15, 20, 21, 22, 23, 28, 29, 30, 31},
+     kSw64S32x4PackOdd},
+    {{0, 1, 2, 3, 16, 17, 18, 19, 8, 9, 10, 11, 24, 25, 26, 27},
+     kSw64S32x4InterleaveEven},
+    {{4, 5, 6, 7, 20, 21, 22, 23, 12, 13, 14, 15, 28, 29, 30, 31},
+     kSw64S32x4InterleaveOdd},
+
+    {{0, 1, 16, 17, 2, 3, 18, 19, 4, 5, 20, 21, 6, 7, 22, 23},
+     kSw64S16x8InterleaveRight},
+    {{8, 9, 24, 25, 10, 11, 26, 27, 12, 13, 28, 29, 14, 15, 30, 31},
+     kSw64S16x8InterleaveLeft},
+    {{0, 1, 4, 5, 8, 9, 12, 13, 16, 17, 20, 21, 24, 25, 28, 29},
+     kSw64S16x8PackEven},
+    {{2, 3, 6, 7, 10, 11, 14, 15, 18, 19, 22, 23, 26, 27, 30, 31},
+     kSw64S16x8PackOdd},
+    {{0, 1, 16, 17, 4, 5, 20, 21, 8, 9, 24, 25, 12, 13, 28, 29},
+     kSw64S16x8InterleaveEven},
+    {{2, 3, 18, 19, 6, 7, 22, 23, 10, 11, 26, 27, 14, 15, 30, 31},
+     kSw64S16x8InterleaveOdd},
+    {{6, 7, 4, 5, 2, 3, 0, 1, 14, 15, 12, 13, 10, 11, 8, 9},
+     kSw64S16x4Reverse},
+    {{2, 3, 0, 1, 6, 7, 4, 5, 10, 11, 8, 9, 14, 15, 12, 13},
+     kSw64S16x2Reverse},
+
+    {{0, 16, 1, 17, 2, 18, 3, 19, 4, 20, 5, 21, 6, 22, 7, 23},
+     kSw64S8x16InterleaveRight},
+    {{8, 24, 9, 25, 10, 26, 11, 27, 12, 28, 13, 29, 14, 30, 15, 31},
+     kSw64S8x16InterleaveLeft},
+    {{0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30},
+     kSw64S8x16PackEven},
+    {{1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31},
+     kSw64S8x16PackOdd},
+    {{0, 16, 2, 18, 4, 20, 6, 22, 8, 24, 10, 26, 12, 28, 14, 30},
+     kSw64S8x16InterleaveEven},
+    {{1, 17, 3, 19, 5, 21, 7, 23, 9, 25, 11, 27, 13, 29, 15, 31},
+     kSw64S8x16InterleaveOdd},
+    {{7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8},
+     kSw64S8x8Reverse},
+    {{3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12},
+     kSw64S8x4Reverse},
+    {{1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14},
+     kSw64S8x2Reverse}};
+
+bool TryMatchArchShuffle(const uint8_t* shuffle, const ShuffleEntry* table,
+                         size_t num_entries, bool is_swizzle,
+                         ArchOpcode* opcode) {
+  uint8_t mask = is_swizzle ? kSimd128Size - 1 : 2 * kSimd128Size - 1;
+  for (size_t i = 0; i < num_entries; ++i) {
+    const ShuffleEntry& entry = table[i];
+    int j = 0;
+    for (; j < kSimd128Size; ++j) {
+      if ((entry.shuffle[j] & mask) != (shuffle[j] & mask)) {
+        break;
+      }
+    }
+    if (j == kSimd128Size) {
+      *opcode = entry.opcode;
+      return true;
+    }
+  }
+  return false;
+}
+
+}  // namespace
+
+void InstructionSelector::VisitI8x16Shuffle(Node* node) {
+  uint8_t shuffle[kSimd128Size];
+  bool is_swizzle;
+  CanonicalizeShuffle(node, shuffle, &is_swizzle);
+  uint8_t shuffle32x4[4];
+  ArchOpcode opcode;
+  if (TryMatchArchShuffle(shuffle, arch_shuffles, arraysize(arch_shuffles),
+                          is_swizzle, &opcode)) {
+    VisitRRR(this, opcode, node);
+    return;
+  }
+  Node* input0 = node->InputAt(0);
+  Node* input1 = node->InputAt(1);
+  uint8_t offset;
+  Sw64OperandGenerator g(this);
+  if (wasm::SimdShuffle::TryMatchConcat(shuffle, &offset)) {
+    Emit(kSw64S8x16Concat, g.DefineSameAsFirst(node), g.UseRegister(input1),
+         g.UseRegister(input0), g.UseImmediate(offset));
+    return;
+  }
+  if (wasm::SimdShuffle::TryMatch32x4Shuffle(shuffle, shuffle32x4)) {
+    Emit(kSw64S32x4Shuffle, g.DefineAsRegister(node), g.UseRegister(input0),
+         g.UseRegister(input1),
+         g.UseImmediate(wasm::SimdShuffle::Pack4Lanes(shuffle32x4)));
+    return;
+  }
+  Emit(kSw64I8x16Shuffle, g.DefineAsRegister(node), g.UseRegister(input0),
+       g.UseRegister(input1),
+       g.UseImmediate(wasm::SimdShuffle::Pack4Lanes(shuffle)),
+       g.UseImmediate(wasm::SimdShuffle::Pack4Lanes(shuffle + 4)),
+       g.UseImmediate(wasm::SimdShuffle::Pack4Lanes(shuffle + 8)),
+       g.UseImmediate(wasm::SimdShuffle::Pack4Lanes(shuffle + 12)));
+}
+
+void InstructionSelector::VisitI8x16Swizzle(Node* node) {
+  Sw64OperandGenerator g(this);
+  InstructionOperand temps[] = {g.TempSimd128Register()};
+  // We don't want input 0 or input 1 to be the same as output, since we will
+  // modify output before do the calculation.
+  Emit(kSw64I8x16Swizzle, g.DefineAsRegister(node),
+       g.UseUniqueRegister(node->InputAt(0)),
+       g.UseUniqueRegister(node->InputAt(1)), arraysize(temps), temps);
+}
+
+void InstructionSelector::VisitSignExtendWord8ToInt32(Node* node) {
+  Sw64OperandGenerator g(this);
+  Emit(kSw64Seb, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)));
+}
+
+void InstructionSelector::VisitSignExtendWord16ToInt32(Node* node) {
+  Sw64OperandGenerator g(this);
+  Emit(kSw64Seh, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)));
+}
+
+void InstructionSelector::VisitSignExtendWord8ToInt64(Node* node) {
+  Sw64OperandGenerator g(this);
+  Emit(kSw64Seb, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)));
+}
+
+void InstructionSelector::VisitSignExtendWord16ToInt64(Node* node) {
+  Sw64OperandGenerator g(this);
+  Emit(kSw64Seh, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)));
+}
+
+void InstructionSelector::VisitSignExtendWord32ToInt64(Node* node) {
+  Sw64OperandGenerator g(this);
+  Emit(kSw64Shl, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)),
+       g.TempImmediate(0));
+}
+
+void InstructionSelector::VisitF32x4Pmin(Node* node) {
+  UNREACHABLE();
+//  VisitUniqueRRR(this, kSw64F32x4Pmin, node);
+}
+
+void InstructionSelector::VisitF32x4Pmax(Node* node) {
+  UNREACHABLE();
+//  VisitUniqueRRR(this, kSw64F32x4Pmax, node);
+}
+
+void InstructionSelector::VisitF64x2Pmin(Node* node) {
+  UNREACHABLE();
+//  VisitUniqueRRR(this, kSw64F64x2Pmin, node);
+}
+
+void InstructionSelector::VisitF64x2Pmax(Node* node) {
+  UNREACHABLE();
+//  VisitUniqueRRR(this, kSw64F64x2Pmax, node);
+}
+
+// static
+MachineOperatorBuilder::Flags
+InstructionSelector::SupportedMachineOperatorFlags() {
+  MachineOperatorBuilder::Flags flags = MachineOperatorBuilder::kNoFlags;
+  return flags | MachineOperatorBuilder::kWord32Ctz |
+         MachineOperatorBuilder::kWord64Ctz |
+         MachineOperatorBuilder::kWord32Popcnt |
+         MachineOperatorBuilder::kWord64Popcnt |
+         MachineOperatorBuilder::kWord32ShiftIsSafe |
+         MachineOperatorBuilder::kInt32DivIsSafe |
+         MachineOperatorBuilder::kUint32DivIsSafe |
+         MachineOperatorBuilder::kFloat64RoundDown |
+         MachineOperatorBuilder::kFloat32RoundDown |
+         MachineOperatorBuilder::kFloat64RoundUp |
+         MachineOperatorBuilder::kFloat32RoundUp |
+         MachineOperatorBuilder::kFloat64RoundTruncate |
+         MachineOperatorBuilder::kFloat32RoundTruncate |
+         MachineOperatorBuilder::kFloat64RoundTiesEven |
+         MachineOperatorBuilder::kFloat32RoundTiesEven;
+}
+
+// static
+MachineOperatorBuilder::AlignmentRequirements
+InstructionSelector::AlignmentRequirements() {
+  if (kArchVariant == kSw64r3) {
+    return MachineOperatorBuilder::AlignmentRequirements::
+        FullUnalignedAccessSupport();
+  } else {
+    DCHECK_EQ(kSw64r2, kArchVariant);
+    return MachineOperatorBuilder::AlignmentRequirements::
+        NoUnalignedAccessSupport();
+  }
+}
+
+#undef SIMD_BINOP_LIST
+#undef SIMD_SHIFT_OP_LIST
+#undef SIMD_UNOP_LIST
+#undef SIMD_TYPE_LIST
+#undef TRACE_UNIMPL
+#undef TRACE
+
+}  // namespace compiler
+}  // namespace internal
+}  // namespace v8
diff --git a/src/3rdparty/chromium/v8/src/compiler/backend/sw64/unwinding-info-writer-sw64.cc b/src/3rdparty/chromium/v8/src/compiler/backend/sw64/unwinding-info-writer-sw64.cc
new file mode 100755
index 0000000000..2eb6c1851c
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/compiler/backend/sw64/unwinding-info-writer-sw64.cc
@@ -0,0 +1,108 @@
+// Copyright 2016 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#include "src/compiler/backend/sw64/unwinding-info-writer-sw64.h"
+#include "src/compiler/backend/instruction.h"
+
+namespace v8 {
+namespace internal {
+namespace compiler {
+
+// TODO(v8:10026): When using CFI, we need to generate unwinding info to tell
+// the unwinder that return addresses are signed.
+
+void UnwindingInfoWriter::BeginInstructionBlock(int pc_offset,
+                                                const InstructionBlock* block) {
+  if (!enabled()) return;
+
+  block_will_exit_ = false;
+
+  DCHECK_LT(block->rpo_number().ToInt(),
+            static_cast<int>(block_initial_states_.size()));
+  const BlockInitialState* initial_state =
+      block_initial_states_[block->rpo_number().ToInt()];
+  if (!initial_state) return;
+  if (initial_state->saved_lr_ != saved_lr_) {
+    eh_frame_writer_.AdvanceLocation(pc_offset);
+    if (initial_state->saved_lr_) {
+      eh_frame_writer_.RecordRegisterSavedToStack(ra, kSystemPointerSize);
+      eh_frame_writer_.RecordRegisterSavedToStack(fp, 0);
+    } else {
+      eh_frame_writer_.RecordRegisterFollowsInitialRule(ra);
+    }
+    saved_lr_ = initial_state->saved_lr_;
+  }
+}
+
+void UnwindingInfoWriter::EndInstructionBlock(const InstructionBlock* block) {
+  if (!enabled() || block_will_exit_) return;
+
+  for (const RpoNumber& successor : block->successors()) {
+    int successor_index = successor.ToInt();
+    DCHECK_LT(successor_index, static_cast<int>(block_initial_states_.size()));
+    const BlockInitialState* existing_state =
+        block_initial_states_[successor_index];
+
+    // If we already had an entry for this BB, check that the values are the
+    // same we are trying to insert.
+    if (existing_state) {
+      DCHECK_EQ(existing_state->saved_lr_, saved_lr_);
+    } else {
+      block_initial_states_[successor_index] =
+          zone_->New<BlockInitialState>(saved_lr_);
+    }
+  }
+}
+
+void UnwindingInfoWriter::MarkFrameConstructed(int at_pc) {
+  if (!enabled()) return;
+
+  // Regardless of the type of frame constructed, the relevant part of the
+  // layout is always the one in the diagram:
+  //
+  // |   ....   |         higher addresses
+  // +----------+               ^
+  // |    RA    |               |            |
+  // +----------+               |            |
+  // | saved FP |               |            |
+  // +----------+ <-- FP                     v
+  // |   ....   |                       stack growth
+  //
+  // The RA is pushed on the stack, and we can record this fact at the end of
+  // the construction, since the RA itself is not modified in the process.
+  eh_frame_writer_.AdvanceLocation(at_pc);
+  eh_frame_writer_.RecordRegisterSavedToStack(ra, kSystemPointerSize);
+  eh_frame_writer_.RecordRegisterSavedToStack(fp, 0);
+  saved_lr_ = true;
+}
+
+void UnwindingInfoWriter::MarkFrameDeconstructed(int at_pc) {
+  if (!enabled()) return;
+
+  // The lr is restored by the last operation in LeaveFrame().
+  eh_frame_writer_.AdvanceLocation(at_pc);
+  eh_frame_writer_.RecordRegisterFollowsInitialRule(ra);
+  saved_lr_ = false;
+}
+
+void UnwindingInfoWriter::MarkLinkRegisterOnTopOfStack(int pc_offset,
+                                                       const Register& sp) {
+  if (!enabled()) return;
+
+  eh_frame_writer_.AdvanceLocation(pc_offset);
+  eh_frame_writer_.SetBaseAddressRegisterAndOffset(sp, 0);
+  eh_frame_writer_.RecordRegisterSavedToStack(ra, 0);
+}
+
+void UnwindingInfoWriter::MarkPopLinkRegisterFromTopOfStack(int pc_offset) {
+  if (!enabled()) return;
+
+  eh_frame_writer_.AdvanceLocation(pc_offset);
+  eh_frame_writer_.SetBaseAddressRegisterAndOffset(fp, 0);
+  eh_frame_writer_.RecordRegisterFollowsInitialRule(ra);
+}
+
+}  // namespace compiler
+}  // namespace internal
+}  // namespace v8
diff --git a/src/3rdparty/chromium/v8/src/compiler/backend/sw64/unwinding-info-writer-sw64.h b/src/3rdparty/chromium/v8/src/compiler/backend/sw64/unwinding-info-writer-sw64.h
new file mode 100755
index 0000000000..ea702773ea
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/compiler/backend/sw64/unwinding-info-writer-sw64.h
@@ -0,0 +1,73 @@
+// Copyright 2016 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef V8_COMPILER_BACKEND_SW64_UNWINDING_INFO_WRITER_SW64_H_
+#define V8_COMPILER_BACKEND_SW64_UNWINDING_INFO_WRITER_SW64_H_
+
+#include "src/diagnostics/eh-frame.h"
+#include "src/flags/flags.h"
+
+namespace v8 {
+namespace internal {
+namespace compiler {
+
+class InstructionBlock;
+
+class UnwindingInfoWriter {
+ public:
+  explicit UnwindingInfoWriter(Zone* zone)
+      : zone_(zone),
+        eh_frame_writer_(zone),
+        saved_lr_(false),
+        block_will_exit_(false),
+        block_initial_states_(zone) {
+    if (enabled()) eh_frame_writer_.Initialize();
+  }
+
+  void SetNumberOfInstructionBlocks(int number) {
+    if (enabled()) block_initial_states_.resize(number);
+  }
+
+  void BeginInstructionBlock(int pc_offset, const InstructionBlock* block);
+  void EndInstructionBlock(const InstructionBlock* block);
+
+  void MarkLinkRegisterOnTopOfStack(int pc_offset, const Register& sp);
+  void MarkPopLinkRegisterFromTopOfStack(int pc_offset);
+
+  void MarkFrameConstructed(int at_pc);
+  void MarkFrameDeconstructed(int at_pc);
+
+  void MarkBlockWillExit() { block_will_exit_ = true; }
+
+  void Finish(int code_size) {
+    if (enabled()) eh_frame_writer_.Finish(code_size);
+  }
+
+  EhFrameWriter* eh_frame_writer() {
+    return enabled() ? &eh_frame_writer_ : nullptr;
+  }
+
+ private:
+  bool enabled() const { return FLAG_perf_prof_unwinding_info; }
+
+  class BlockInitialState : public ZoneObject {
+   public:
+    explicit BlockInitialState(bool saved_lr) : saved_lr_(saved_lr) {}
+
+    bool saved_lr_;
+  };
+
+  Zone* zone_;
+  EhFrameWriter eh_frame_writer_;
+  bool saved_lr_;
+  bool block_will_exit_;
+
+  ZoneVector<const BlockInitialState*> block_initial_states_;
+};
+
+}  // namespace compiler
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_COMPILER_BACKEND_SW64_UNWINDING_INFO_WRITER_SW64_H_
diff --git a/src/3rdparty/chromium/v8/src/compiler/backend/unwinding-info-writer.h b/src/3rdparty/chromium/v8/src/compiler/backend/unwinding-info-writer.h
index a288e219a8..d5380e502f 100644
--- a/src/3rdparty/chromium/v8/src/compiler/backend/unwinding-info-writer.h
+++ b/src/3rdparty/chromium/v8/src/compiler/backend/unwinding-info-writer.h
@@ -17,6 +17,8 @@
 #include "src/compiler/backend/s390/unwinding-info-writer-s390.h"
 #elif V8_TARGET_ARCH_PPC || V8_TARGET_ARCH_PPC64
 #include "src/compiler/backend/ppc/unwinding-info-writer-ppc.h"
+#elif V8_TARGET_ARCH_SW64
+#include "src/compiler/backend/sw64/unwinding-info-writer-sw64.h"
 #else
 
 // Placeholder for unsupported architectures.
diff --git a/src/3rdparty/chromium/v8/src/compiler/c-linkage.cc b/src/3rdparty/chromium/v8/src/compiler/c-linkage.cc
index af467f2bb1..49b6cd942a 100644
--- a/src/3rdparty/chromium/v8/src/compiler/c-linkage.cc
+++ b/src/3rdparty/chromium/v8/src/compiler/c-linkage.cc
@@ -100,6 +100,16 @@ namespace {
 #define CALLEE_SAVE_FP_REGISTERS \
   f20.bit() | f22.bit() | f24.bit() | f26.bit() | f28.bit() | f30.bit()
 
+#elif V8_TARGET_ARCH_SW64
+// ===========================================================================
+// == sw64 ===================================================================
+// ===========================================================================
+#define PARAM_REGISTERS a0, a1, a2, a3, a4, a5
+#define CALLEE_SAVE_REGISTERS                                                  \
+  s0.bit() | s1.bit() | s2.bit() | s3.bit() | s4.bit() | s5.bit()
+#define CALLEE_SAVE_FP_REGISTERS \
+  f2.bit() | f3.bit() | f4.bit() | f5.bit() | f6.bit() | f7.bit() |            \
+      f8.bit() | f9.bit()
 #elif V8_TARGET_ARCH_PPC64
 // ===========================================================================
 // == ppc & ppc64 ============================================================
diff --git a/src/3rdparty/chromium/v8/src/debug/debug-evaluate.cc b/src/3rdparty/chromium/v8/src/debug/debug-evaluate.cc
index c7d0a890c4..0c54b1a141 100644
--- a/src/3rdparty/chromium/v8/src/debug/debug-evaluate.cc
+++ b/src/3rdparty/chromium/v8/src/debug/debug-evaluate.cc
@@ -1068,6 +1068,8 @@ void DebugEvaluate::VerifyTransitiveBuiltins(Isolate* isolate) {
   // MIPS64 doesn't have PC relative code currently.
   // TODO(mips): Add PC relative code to MIPS64.
   USE(sanity_check);
+#elif defined(V8_TARGET_ARCH_SW64)
+  USE(sanity_check);
 #else
   CHECK(sanity_check);
 #endif
diff --git a/src/3rdparty/chromium/v8/src/debug/sw64/debug-sw64.cc b/src/3rdparty/chromium/v8/src/debug/sw64/debug-sw64.cc
new file mode 100755
index 0000000000..56748b5dec
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/debug/sw64/debug-sw64.cc
@@ -0,0 +1,57 @@
+// Copyright 2012 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#if V8_TARGET_ARCH_SW64
+
+#include "src/debug/debug.h"
+
+#include "src/codegen/macro-assembler.h"
+#include "src/debug/liveedit.h"
+#include "src/execution/frames-inl.h"
+
+namespace v8 {
+namespace internal {
+
+#define __ ACCESS_MASM(masm)
+
+void DebugCodegen::GenerateHandleDebuggerStatement(MacroAssembler* masm) {
+  {
+    FrameScope scope(masm, StackFrame::INTERNAL);
+    __ CallRuntime(Runtime::kHandleDebuggerStatement, 0);
+  }
+  __ MaybeDropFrames();
+
+  // Return to caller.
+  __ Ret();
+}
+
+void DebugCodegen::GenerateFrameDropperTrampoline(MacroAssembler* masm) {
+  // Frame is being dropped:
+  // - Drop to the target frame specified by a1.
+  // - Look up current function on the frame.
+  // - Leave the frame.
+  // - Restart the frame by calling the function.
+  __ mov(fp, a1);
+  __ Ldl(a1, MemOperand(fp, StandardFrameConstants::kFunctionOffset));
+
+  // Pop return address and frame.
+  __ LeaveFrame(StackFrame::INTERNAL);
+
+  __ Ldl(a0, FieldMemOperand(a1, JSFunction::kSharedFunctionInfoOffset));
+  __ Ldhu(a0,
+        FieldMemOperand(a0, SharedFunctionInfo::kFormalParameterCountOffset));
+  __ mov(a2, a0);
+
+  __ InvokeFunction(a1, a2, a0, JUMP_FUNCTION);
+}
+
+
+const bool LiveEdit::kFrameDropperSupported = true;
+
+#undef __
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_TARGET_ARCH_SW64
diff --git a/src/3rdparty/chromium/v8/src/deoptimizer/sw64/deoptimizer-sw64.cc b/src/3rdparty/chromium/v8/src/deoptimizer/sw64/deoptimizer-sw64.cc
new file mode 100644
index 0000000000..335a6fe354
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/deoptimizer/sw64/deoptimizer-sw64.cc
@@ -0,0 +1,250 @@
+// Copyright 2011 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#include "src/codegen/macro-assembler.h"
+#include "src/codegen/register-configuration.h"
+#include "src/codegen/safepoint-table.h"
+#include "src/deoptimizer/deoptimizer.h"
+
+namespace v8 {
+namespace internal {
+
+const bool Deoptimizer::kSupportsFixedDeoptExitSizes = false;
+const int Deoptimizer::kNonLazyDeoptExitSize = 0;
+const int Deoptimizer::kLazyDeoptExitSize = 0;
+
+#define __ masm->
+
+// This code tries to be close to ia32 code so that any changes can be
+// easily ported.
+void Deoptimizer::GenerateDeoptimizationEntries(MacroAssembler* masm,
+                                                Isolate* isolate,
+                                                DeoptimizeKind deopt_kind) {
+  NoRootArrayScope no_root_array(masm);
+
+  // Unlike on ARM we don't save all the registers, just the useful ones.
+  // For the rest, there are gaps on the stack, so the offsets remain the same.
+  const int kNumberOfRegisters = Register::kNumRegisters;
+
+  RegList restored_regs = kJSCallerSaved | kCalleeSaved;
+  RegList saved_regs = restored_regs | sp.bit() | ra.bit();
+
+  const int kDoubleRegsSize = kDoubleSize * DoubleRegister::kNumRegisters;
+
+  // Save all double FPU registers before messing with them.
+  __ Subl(sp, sp, Operand(kDoubleRegsSize));
+  const RegisterConfiguration* config = RegisterConfiguration::Default();
+  for (int i = 0; i < config->num_allocatable_double_registers(); ++i) {
+    int code = config->GetAllocatableDoubleCode(i);
+    const DoubleRegister fpu_reg = DoubleRegister::from_code(code);
+    int offset = code * kDoubleSize;
+    __ Fstd(fpu_reg, MemOperand(sp, offset));
+  }
+
+  // Push saved_regs (needed to populate FrameDescription::registers_).
+  // Leave gaps for other registers.
+  __ Subl(sp, sp, kNumberOfRegisters * kPointerSize);
+  for (int16_t i = kNumberOfRegisters - 1; i >= 0; i--) {
+    if ((saved_regs & (1 << i)) != 0) {
+      __ Stl(ToRegister(i), MemOperand(sp, kPointerSize * i));
+    }
+  }
+
+  __ li(a2, Operand(ExternalReference::Create(
+                IsolateAddressId::kCEntryFPAddress, isolate)));
+  __ Stl(fp, MemOperand(a2));
+
+  const int kSavedRegistersAreaSize =
+      (kNumberOfRegisters * kPointerSize) + kDoubleRegsSize;
+
+  // Get the bailout is passed as kRootRegister by the caller.
+  __ mov(a2, kRootRegister);
+
+  // Get the address of the location in the code object (a3) (return
+  // address for lazy deoptimization) and compute the fp-to-sp delta in
+  // register a4.
+  __ mov(a3, ra);
+  __ Addl(a4, sp, Operand(kSavedRegistersAreaSize));
+
+  __ Subl(a4, fp, a4);
+
+  // Allocate a new deoptimizer object.
+  __ PrepareCallCFunction(6, a5);
+  // Pass six arguments, according to n64 ABI.
+  __ mov(a0, zero_reg);
+  Label context_check;
+  __ Ldl(a1, MemOperand(fp, CommonFrameConstants::kContextOrFrameTypeOffset));
+  __ JumpIfSmi(a1, &context_check);
+  __ Ldl(a0, MemOperand(fp, StandardFrameConstants::kFunctionOffset));
+  __ bind(&context_check);
+  __ li(a1, Operand(static_cast<int>(deopt_kind)));
+  // a2: bailout id already loaded.
+  // a3: code address or 0 already loaded.
+  // a4: already has fp-to-sp delta.
+  __ li(a5, Operand(ExternalReference::isolate_address(isolate)));
+
+  // Call Deoptimizer::New().
+  {
+    AllowExternalCallThatCantCauseGC scope(masm);
+    __ CallCFunction(ExternalReference::new_deoptimizer_function(), 6);
+  }
+
+  // Preserve "deoptimizer" object in register v0 and get the input
+  // frame descriptor pointer to a1 (deoptimizer->input_);
+  // Move deopt-obj to a0 for call to Deoptimizer::ComputeOutputFrames() below.
+  __ mov(a0, v0);
+  __ Ldl(a1, MemOperand(v0, Deoptimizer::input_offset()));
+
+  // Copy core registers into FrameDescription::registers_[kNumRegisters].
+  DCHECK_EQ(Register::kNumRegisters, kNumberOfRegisters);
+  for (int i = 0; i < kNumberOfRegisters; i++) {
+    int offset = (i * kPointerSize) + FrameDescription::registers_offset();
+    if ((saved_regs & (1 << i)) != 0) {
+      __ Ldl(a2, MemOperand(sp, i * kPointerSize));
+      __ Stl(a2, MemOperand(a1, offset));
+    } else if (FLAG_debug_code) {
+      __ li(a2, kDebugZapValue);
+      __ Stl(a2, MemOperand(a1, offset));
+    }
+  }
+
+  int double_regs_offset = FrameDescription::double_registers_offset();
+  // Copy FPU registers to
+  // double_registers_[DoubleRegister::kNumAllocatableRegisters]
+  for (int i = 0; i < config->num_allocatable_double_registers(); ++i) {
+    int code = config->GetAllocatableDoubleCode(i);
+    int dst_offset = code * kDoubleSize + double_regs_offset;
+    int src_offset =
+        code * kDoubleSize + kNumberOfRegisters * kPointerSize;
+    __ Fldd(f0, MemOperand(sp, src_offset));
+    __ Fstd(f0, MemOperand(a1, dst_offset));
+  }
+
+  // Remove the saved registers from the stack.
+  __ Addl(sp, sp, Operand(kSavedRegistersAreaSize));
+
+  // Compute a pointer to the unwinding limit in register a2; that is
+  // the first stack slot not part of the input frame.
+  __ Ldl(a2, MemOperand(a1, FrameDescription::frame_size_offset()));
+  __ Addl(a2, a2, sp);
+
+  // Unwind the stack down to - but not including - the unwinding
+  // limit and copy the contents of the activation frame to the input
+  // frame description.
+  __ Addl(a3, a1, Operand(FrameDescription::frame_content_offset()));
+  Label pop_loop;
+  Label pop_loop_header;
+  __ BranchShort(&pop_loop_header);
+  __ bind(&pop_loop);
+  __ pop(a4);
+  __ Stl(a4, MemOperand(a3, 0));
+  __ addl(a3, sizeof(uint64_t), a3);
+  __ bind(&pop_loop_header);
+  __ BranchShort(&pop_loop, ne, a2, Operand(sp));
+  // Compute the output frame in the deoptimizer.
+  __ push(a0);  // Preserve deoptimizer object across call.
+  // a0: deoptimizer object; a1: scratch.
+  __ PrepareCallCFunction(1, a1);
+  // Call Deoptimizer::ComputeOutputFrames().
+  {
+    AllowExternalCallThatCantCauseGC scope(masm);
+    __ CallCFunction(ExternalReference::compute_output_frames_function(), 1);
+  }
+  __ pop(a0);  // Restore deoptimizer object (class Deoptimizer).
+
+  __ Ldl(sp, MemOperand(a0, Deoptimizer::caller_frame_top_offset()));
+
+  // Replace the current (input) frame with the output frames.
+  Label outer_push_loop, inner_push_loop,
+      outer_loop_header, inner_loop_header;
+  // Outer loop state: a4 = current "FrameDescription** output_",
+  // a1 = one past the last FrameDescription**.
+  __ Ldw(a1, MemOperand(a0, Deoptimizer::output_count_offset()));
+  __ Ldl(a4, MemOperand(a0, Deoptimizer::output_offset()));  // a4 is output_.
+  __ s8addl(a1, a4, a1);  DCHECK_EQ(kPointerSizeLog2, 3);
+  __ BranchShort(&outer_loop_header);
+  __ bind(&outer_push_loop);
+  // Inner loop state: a2 = current FrameDescription*, a3 = loop index.
+  __ Ldl(a2, MemOperand(a4, 0));  // output_[ix]
+  __ Ldl(a3, MemOperand(a2, FrameDescription::frame_size_offset()));
+  __ BranchShort(&inner_loop_header);
+  __ bind(&inner_push_loop);
+  __ Subl(a3, a3, Operand(sizeof(uint64_t)));
+  __ Addl(t9, a2, Operand(a3));
+  __ Ldl(t10, MemOperand(t9, FrameDescription::frame_content_offset()));
+  __ push(t10);
+  __ bind(&inner_loop_header);
+  __ BranchShort(&inner_push_loop, ne, a3, Operand(zero_reg));
+
+  __ Addl(a4, a4, Operand(kPointerSize));
+  __ bind(&outer_loop_header);
+  __ BranchShort(&outer_push_loop, lt, a4, Operand(a1));
+
+  __ Ldl(a1, MemOperand(a0, Deoptimizer::input_offset()));
+  for (int i = 0; i < config->num_allocatable_double_registers(); ++i) {
+    int code = config->GetAllocatableDoubleCode(i);
+    const DoubleRegister fpu_reg = DoubleRegister::from_code(code);
+    int src_offset = code * kDoubleSize + double_regs_offset;
+    __ Fldd(fpu_reg, MemOperand(a1, src_offset));
+  }
+
+  // Push pc and continuation from the last output frame.
+  __ Ldl(t9, MemOperand(a2, FrameDescription::pc_offset()));
+  __ push(t9);
+  __ Ldl(t9, MemOperand(a2, FrameDescription::continuation_offset()));
+  __ push(t9);
+
+  // Technically restoring 'at' should work unless zero_reg is also restored
+  // but it's safer to check for this.
+  DCHECK(!(at.bit() & restored_regs));
+  // Restore the registers from the last output frame.
+  __ mov(at, a2);
+  for (int i = kNumberOfRegisters - 1; i >= 0; i--) {
+    int offset = (i * kPointerSize) + FrameDescription::registers_offset();
+    if ((restored_regs & (1 << i)) != 0) {
+      __ Ldl(ToRegister(i), MemOperand(at, offset));
+    }
+  }
+
+  __ pop(at);  // Get continuation, leave pc on stack.
+  __ pop(ra);
+  __ Jump(at);
+  __ halt();//stop("Unreachable.");
+}
+
+
+// Maximum size of a table entry generated below.
+#ifdef _SW64_ARCH_SW64R3
+const int Deoptimizer::table_entry_size_ = 2 * kInstrSize;
+#else
+const int Deoptimizer::table_entry_size_ = 3 * kInstrSize;
+#endif
+
+Float32 RegisterValues::GetFloatRegister(unsigned n) const {
+  return Float32::FromBits(
+      static_cast<uint32_t>(double_registers_[n].get_bits()));
+}
+
+void FrameDescription::SetCallerPc(unsigned offset, intptr_t value) {
+  SetFrameSlot(offset, value);
+}
+
+
+void FrameDescription::SetCallerFp(unsigned offset, intptr_t value) {
+  SetFrameSlot(offset, value);
+}
+
+
+void FrameDescription::SetCallerConstantPool(unsigned offset, intptr_t value) {
+  // No embedded constant pool support.
+  UNREACHABLE();
+}
+
+void FrameDescription::SetPc(intptr_t pc) { pc_ = pc; }
+
+#undef __
+
+
+}  // namespace internal
+}  // namespace v8
diff --git a/src/3rdparty/chromium/v8/src/diagnostics/gdb-jit.cc b/src/3rdparty/chromium/v8/src/diagnostics/gdb-jit.cc
index c9180fbaf5..65d409bc0b 100644
--- a/src/3rdparty/chromium/v8/src/diagnostics/gdb-jit.cc
+++ b/src/3rdparty/chromium/v8/src/diagnostics/gdb-jit.cc
@@ -1081,6 +1081,8 @@ class DebugInfoSection : public DebugSection {
       w->Write<uint8_t>(DW_OP_reg31);  // The frame pointer is here on PPC64.
 #elif V8_TARGET_ARCH_S390
       w->Write<uint8_t>(DW_OP_reg11);  // The frame pointer's here on S390.
+#elif V8_TARGET_ARCH_SW64
+      UNIMPLEMENTED();
 #else
 #error Unsupported target architecture.
 #endif
diff --git a/src/3rdparty/chromium/v8/src/diagnostics/perf-jit.h b/src/3rdparty/chromium/v8/src/diagnostics/perf-jit.h
index dbe78ddf2d..e80f8b311d 100644
--- a/src/3rdparty/chromium/v8/src/diagnostics/perf-jit.h
+++ b/src/3rdparty/chromium/v8/src/diagnostics/perf-jit.h
@@ -86,6 +86,7 @@ class PerfJitLogger : public CodeEventLogger {
   static const uint32_t kElfMachARM64 = 183;
   static const uint32_t kElfMachS390x = 22;
   static const uint32_t kElfMachPPC64 = 21;
+  static const uint32_t kElfMachSW64 = 0x9916;
 
   uint32_t GetElfMach() {
 #if V8_TARGET_ARCH_IA32
@@ -104,6 +105,8 @@ class PerfJitLogger : public CodeEventLogger {
     return kElfMachS390x;
 #elif V8_TARGET_ARCH_PPC64
     return kElfMachPPC64;
+#elif V8_TARGET_ARCH_SW64
+    return kElfMachSW64;
 #else
     UNIMPLEMENTED();
     return 0;
diff --git a/src/3rdparty/chromium/v8/src/diagnostics/sw64/disasm-sw64.cc b/src/3rdparty/chromium/v8/src/diagnostics/sw64/disasm-sw64.cc
new file mode 100755
index 0000000000..ff7e4bd7e9
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/diagnostics/sw64/disasm-sw64.cc
@@ -0,0 +1,3439 @@
+// Copyright 2012 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+// A Disassembler object is used to disassemble a block of code instruction by
+// instruction. The default implementation of the NameConverter object can be
+// overriden to modify register names or to do symbol lookup on addresses.
+//
+// The example below will disassemble a block of code and print it to stdout.
+//
+//   NameConverter converter;
+//   Disassembler d(converter);
+//   for (byte* pc = begin; pc < end;) {
+//     v8::internal::EmbeddedVector<char, 256> buffer;
+//     byte* prev_pc = pc;
+//     pc += d.InstructionDecode(buffer, pc);
+//     printf("%p    %08x      %s\n",
+//            prev_pc, *reinterpret_cast<int32_t*>(prev_pc), buffer);
+//   }
+//
+// The Disassembler class also has a convenience method to disassemble a block
+// of code into a FILE*, meaning that the above functionality could also be
+// achieved by just calling Disassembler::Disassemble(stdout, begin, end);
+
+#include <assert.h>
+#include <stdarg.h>
+#include <stdio.h>
+#include <string.h>
+
+#ifdef SW64
+# include <pthread.h>
+# include <dlfcn.h>
+#endif
+
+#if V8_TARGET_ARCH_SW64
+
+#include "src/base/platform/platform.h"
+#include "src/codegen/macro-assembler.h"
+#include "src/codegen/sw64/constants-sw64.h"
+#include "src/diagnostics/disasm.h"
+
+namespace v8 {
+namespace internal {
+
+//------------------------------------------------------------------------------
+
+// Decoder decodes and disassembles instructions into an output buffer.
+// It uses the converter to convert register names and call destinations into
+// more informative description.
+class Decoder {
+ public:
+  Decoder(const disasm::NameConverter& converter,
+          v8::internal::Vector<char> out_buffer)
+      : converter_(converter), out_buffer_(out_buffer), out_buffer_pos_(0) {
+    out_buffer_[out_buffer_pos_] = '\0';
+  }
+
+  ~Decoder() {}
+
+  // Writes one disassembled instruction into 'buffer' (0-terminated).
+  // Returns the length of the disassembled machine instruction in bytes.
+  int InstructionDecode(byte* instruction);
+#ifdef SW64
+  // address decode_instructions(address start, address end);
+  byte* decode_instructions(byte* start, byte* end);
+  // tries to load library and return whether it succedded.
+  static bool load_library();
+
+ private:
+  // this is the type of the dll entry point:
+  typedef void* (*decode_func)(void* start, void* end,
+                               void* (*event_callback)(void*, const char*, void*),
+                               void* event_stream,
+                               int (*printf_callback)(void*, const char*, ...),
+                               void* printf_stream,
+                               const char* options);
+  // points to the library.
+  static void*    _library;
+  // bailout
+  static bool     _tried_to_load_library;
+  // points to the decode function.
+  static decode_func _decode_instructions;
+#endif
+
+ private:
+  // Bottleneck functions to print into the out_buffer.
+  void PrintChar(const char ch);
+  void Print(const char* str);
+
+  // Printing of common values.
+  void PrintRegister(int reg);
+  void PrintFPURegister(int freg);
+  void PrintMSARegister(int wreg);
+  void PrintFPUStatusRegister(int freg);
+  void PrintMSAControlRegister(int creg);
+  void PrintRs(Instruction* instr);
+  void PrintRt(Instruction* instr);
+  void PrintRd(Instruction* instr);
+  void PrintFs(Instruction* instr);
+  void PrintFt(Instruction* instr);
+  void PrintFd(Instruction* instr);
+  void PrintSa(Instruction* instr);
+  void PrintLsaSa(Instruction* instr);
+  void PrintSd(Instruction* instr);
+  void PrintSs1(Instruction* instr);
+  void PrintSs2(Instruction* instr);
+  void PrintSs3(Instruction* instr);
+  void PrintSs4(Instruction* instr);
+  void PrintSs5(Instruction* instr);
+  void PrintBc(Instruction* instr);
+  void PrintCc(Instruction* instr);
+  void PrintFunction(Instruction* instr);
+  void PrintSecondaryField(Instruction* instr);
+  void PrintUImm9(Instruction* instr);
+  void PrintSImm9(Instruction* instr);
+  void PrintUImm16(Instruction* instr);
+  void PrintSImm16(Instruction* instr);
+  void PrintXImm16(Instruction* instr);
+  void PrintPCImm16(Instruction* instr, int delta_pc, int n_bits);
+  void PrintXImm18(Instruction* instr);
+  void PrintSImm18(Instruction* instr);
+  void PrintXImm19(Instruction* instr);
+  void PrintSImm19(Instruction* instr);
+  void PrintXImm21(Instruction* instr);
+  void PrintSImm21(Instruction* instr);
+  void PrintPCImm21(Instruction* instr, int delta_pc, int n_bits);
+  void PrintXImm26(Instruction* instr);
+  void PrintSImm26(Instruction* instr);
+  void PrintPCImm26(Instruction* instr, int delta_pc, int n_bits);
+  void PrintPCImm26(Instruction* instr);
+  void PrintCode(Instruction* instr);   // For break and trap instructions.
+  void PrintFormat(Instruction* instr);  // For floating format postfix.
+  void PrintBp2(Instruction* instr);
+  void PrintBp3(Instruction* instr);
+  void PrintMsaDataFormat(Instruction* instr);
+  void PrintMsaXImm8(Instruction* instr);
+  void PrintMsaImm8(Instruction* instr);
+  void PrintMsaImm5(Instruction* instr);
+  void PrintMsaSImm5(Instruction* instr);
+  void PrintMsaSImm10(Instruction* instr, bool is_mi10 = false);
+  void PrintMsaImmBit(Instruction* instr);
+  void PrintMsaImmElm(Instruction* instr);
+  void PrintMsaCopy(Instruction* instr);
+  // Printing of instruction name.
+  void PrintInstructionName(Instruction* instr);
+
+  // Handle formatting of instructions and their options.
+  int FormatRegister(Instruction* instr, const char* option);
+  int FormatFPURegister(Instruction* instr, const char* option);
+  int FormatMSARegister(Instruction* instr, const char* option);
+  int FormatOption(Instruction* instr, const char* option);
+  void Format(Instruction* instr, const char* format);
+  void Unknown(Instruction* instr);
+  int DecodeBreakInstr(Instruction* instr);
+
+  // Each of these functions decodes one particular instruction type.
+  bool DecodeTypeRegisterRsType(Instruction* instr);
+  void DecodeTypeRegisterSRsType(Instruction* instr);
+  void DecodeTypeRegisterDRsType(Instruction* instr);
+  void DecodeTypeRegisterLRsType(Instruction* instr);
+  void DecodeTypeRegisterWRsType(Instruction* instr);
+  void DecodeTypeRegisterSPECIAL(Instruction* instr);
+  void DecodeTypeRegisterSPECIAL2(Instruction* instr);
+  void DecodeTypeRegisterSPECIAL3(Instruction* instr);
+  void DecodeTypeRegisterCOP1(Instruction* instr);
+  void DecodeTypeRegisterCOP1X(Instruction* instr);
+  int DecodeTypeRegister(Instruction* instr);
+
+  void DecodeTypeImmediateCOP1(Instruction* instr);
+  void DecodeTypeImmediateREGIMM(Instruction* instr);
+  void DecodeTypeImmediateSPECIAL3(Instruction* instr);
+  void DecodeTypeImmediate(Instruction* instr);
+
+  void DecodeTypeJump(Instruction* instr);
+
+  void DecodeTypeMsaI8(Instruction* instr);
+  void DecodeTypeMsaI5(Instruction* instr);
+  void DecodeTypeMsaI10(Instruction* instr);
+  void DecodeTypeMsaELM(Instruction* instr);
+  void DecodeTypeMsaBIT(Instruction* instr);
+  void DecodeTypeMsaMI10(Instruction* instr);
+  void DecodeTypeMsa3R(Instruction* instr);
+  void DecodeTypeMsa3RF(Instruction* instr);
+  void DecodeTypeMsaVec(Instruction* instr);
+  void DecodeTypeMsa2R(Instruction* instr);
+  void DecodeTypeMsa2RF(Instruction* instr);
+
+#ifdef SW64 //jzy 20150213
+  void SwDecodeTypeSyscall(Instruction* instr);
+  void SwDecodeTypeTransferance(Instruction* instr);
+  void SwDecodeTypeStorage(Instruction* instr);
+  void SwDecodeTypeSimpleCalculation(Instruction* instr);
+  void SwDecodeTypeCompositeCalculation(Instruction* instr);
+  int  SwGetInstructionRange(const char* format, int& hi, int& lo);
+
+  void SwPrintRa(Instruction* instr);
+  void SwPrintRb(Instruction* instr);
+  int  SwPrintRc(Instruction* instr, const char* format);
+  void SwPrintRd(Instruction* instr);
+  void SwPrintFa(Instruction* instr);
+  void SwPrintFb(Instruction* instr);
+  int  SwPrintFc(Instruction* instr, const char* format);
+  void SwPrintFd(Instruction* instr);
+
+  //Print unsigned immediate value
+  int  SwPrintImm(Instruction* instr, const char* format);
+
+  //Print signed immediate value
+  int  SwPrintDisp(Instruction* instr, const char* format);
+  int  SwPrintDispTransfer(Instruction* instr, const char* format);//ld 20150320
+
+  //Print int to hex
+  int SwPrintHex(Instruction* instr,const char* format);//cjq 20150319
+  
+
+private:
+  void SwDecodeTypeCompositeCalculationInteger(Instruction* instr);
+  void SwDecodeTypeCompositeCalculationFloatintPoint(Instruction* instr);
+#endif
+
+
+  const disasm::NameConverter& converter_;
+  v8::internal::Vector<char> out_buffer_;
+  int out_buffer_pos_;
+  byte* instr_pc_; //ld 20150323;
+  
+  DISALLOW_COPY_AND_ASSIGN(Decoder);
+};
+
+
+// Support for assertions in the Decoder formatting functions.
+#define STRING_STARTS_WITH(string, compare_string) \
+  (strncmp(string, compare_string, strlen(compare_string)) == 0)
+
+
+// Append the ch to the output buffer.
+void Decoder::PrintChar(const char ch) {
+  out_buffer_[out_buffer_pos_++] = ch;
+}
+
+
+// Append the str to the output buffer.
+void Decoder::Print(const char* str) {
+  char cur = *str++;
+  while (cur != '\0' && (out_buffer_pos_ < (out_buffer_.length() - 1))) {
+    PrintChar(cur);
+    cur = *str++;
+  }
+  out_buffer_[out_buffer_pos_] = 0;
+}
+
+
+// Print the register name according to the active name converter.
+void Decoder::PrintRegister(int reg) {
+  Print(converter_.NameOfCPURegister(reg));
+}
+
+
+void Decoder::PrintRs(Instruction* instr) {
+  int reg = instr->RsValue();
+  PrintRegister(reg);
+}
+
+
+void Decoder::PrintRt(Instruction* instr) {
+  int reg = instr->RtValue();
+  PrintRegister(reg);
+}
+
+
+void Decoder::PrintRd(Instruction* instr) {
+  int reg = instr->RdValue();
+  PrintRegister(reg);
+}
+
+
+// Print the FPUregister name according to the active name converter.
+void Decoder::PrintFPURegister(int freg) {
+  Print(converter_.NameOfXMMRegister(freg));
+}
+
+void Decoder::PrintMSARegister(int wreg) { Print(MSARegisters::Name(wreg)); }
+
+void Decoder::PrintFPUStatusRegister(int freg) {
+  switch (freg) {
+    case kFCSRRegister:
+      Print("FCSR");
+      break;
+    default:
+      Print(converter_.NameOfXMMRegister(freg));
+  }
+}
+
+void Decoder::PrintMSAControlRegister(int creg) {
+  switch (creg) {
+    case kMSAIRRegister:
+      Print("MSAIR");
+      break;
+    case kMSACSRRegister:
+      Print("MSACSR");
+      break;
+    default:
+      Print("no_msacreg");
+  }
+}
+
+void Decoder::PrintFs(Instruction* instr) {
+  int freg = instr->RsValue();
+  PrintFPURegister(freg);
+}
+
+
+void Decoder::PrintFt(Instruction* instr) {
+  int freg = instr->RtValue();
+  PrintFPURegister(freg);
+}
+
+
+void Decoder::PrintFd(Instruction* instr) {
+  int freg = instr->RdValue();
+  PrintFPURegister(freg);
+}
+
+
+// Print the integer value of the sa field.
+void Decoder::PrintSa(Instruction* instr) {
+  int sa = instr->SaValue();
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "%d", sa);
+}
+
+
+// Print the integer value of the sa field of a lsa instruction.
+void Decoder::PrintLsaSa(Instruction* instr) {
+  int sa = instr->LsaSaValue() + 1;
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "%d", sa);
+}
+
+
+// Print the integer value of the rd field, when it is not used as reg.
+void Decoder::PrintSd(Instruction* instr) {
+  int sd = instr->RdValue();
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "%d", sd);
+}
+
+// Print the integer value of ext/dext/dextu size from the msbd field.
+void Decoder::PrintSs1(Instruction* instr) {
+  int msbd = instr->RdValue();
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "%d", msbd + 1);
+}
+
+// Print the integer value of ins/dins/dinsu size from the msb and lsb fields
+// (for dinsu it is msbminus32 and lsbminus32 fields).
+void Decoder::PrintSs2(Instruction* instr) {
+  int msb = instr->RdValue();
+  int lsb = instr->SaValue();
+  out_buffer_pos_ +=
+      SNPrintF(out_buffer_ + out_buffer_pos_, "%d", msb - lsb + 1);
+}
+
+// Print the integer value of dextm size from the msbdminus32 field.
+void Decoder::PrintSs3(Instruction* instr) {
+  int msbdminus32 = instr->RdValue();
+  out_buffer_pos_ +=
+      SNPrintF(out_buffer_ + out_buffer_pos_, "%d", msbdminus32 + 32 + 1);
+}
+
+// Print the integer value of dinsm size from the msbminus32 and lsb fields.
+void Decoder::PrintSs4(Instruction* instr) {
+  int msbminus32 = instr->RdValue();
+  int lsb = instr->SaValue();
+  out_buffer_pos_ +=
+      SNPrintF(out_buffer_ + out_buffer_pos_, "%d", msbminus32 + 32 - lsb + 1);
+}
+
+// Print the integer value of dextu/dinsu pos from the lsbminus32 field.
+void Decoder::PrintSs5(Instruction* instr) {
+  int lsbminus32 = instr->SaValue();
+  out_buffer_pos_ +=
+      SNPrintF(out_buffer_ + out_buffer_pos_, "%d", lsbminus32 + 32);
+}
+
+
+// Print the integer value of the cc field for the bc1t/f instructions.
+void Decoder::PrintBc(Instruction* instr) {
+  int cc = instr->FBccValue();
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "%d", cc);
+}
+
+
+// Print the integer value of the cc field for the FP compare instructions.
+void Decoder::PrintCc(Instruction* instr) {
+  int cc = instr->FCccValue();
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "cc(%d)", cc);
+}
+
+// Print 9-bit unsigned immediate value.
+void Decoder::PrintUImm9(Instruction* instr) {
+  int32_t imm = instr->Imm9Value();
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "%u", imm);
+}
+
+// Print 9-bit signed immediate value.
+void Decoder::PrintSImm9(Instruction* instr) {
+  int32_t imm = ((instr->Imm9Value()) << 23) >> 23;
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "%d", imm);
+}
+
+// Print 16-bit unsigned immediate value.
+void Decoder::PrintUImm16(Instruction* instr) {
+  int32_t imm = instr->Imm16Value();
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "%u", imm);
+}
+
+
+// Print 16-bit signed immediate value.
+void Decoder::PrintSImm16(Instruction* instr) {
+  int32_t imm =
+      ((instr->Imm16Value()) << (32 - kImm16Bits)) >> (32 - kImm16Bits);
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "%d", imm);
+}
+
+
+// Print 16-bit hexa immediate value.
+void Decoder::PrintXImm16(Instruction* instr) {
+  int32_t imm = instr->Imm16Value();
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "0x%x", imm);
+}
+
+
+// Print absoulte address for 16-bit offset or immediate value.
+// The absolute address is calculated according following expression:
+//      PC + delta_pc + (offset << n_bits)
+void Decoder::PrintPCImm16(Instruction* instr, int delta_pc, int n_bits) {
+  int16_t offset = instr->Imm16Value();
+  out_buffer_pos_ +=
+      SNPrintF(out_buffer_ + out_buffer_pos_, "%s",
+               converter_.NameOfAddress(reinterpret_cast<byte*>(instr) +
+                                        delta_pc + (offset << n_bits)));
+}
+
+
+// Print 18-bit signed immediate value.
+void Decoder::PrintSImm18(Instruction* instr) {
+  int32_t imm =
+      ((instr->Imm18Value()) << (32 - kImm18Bits)) >> (32 - kImm18Bits);
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "%d", imm);
+}
+
+
+// Print 18-bit hexa immediate value.
+void Decoder::PrintXImm18(Instruction* instr) {
+  int32_t imm = instr->Imm18Value();
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "0x%x", imm);
+}
+
+
+// Print 19-bit hexa immediate value.
+void Decoder::PrintXImm19(Instruction* instr) {
+  int32_t imm = instr->Imm19Value();
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "0x%x", imm);
+}
+
+
+// Print 19-bit signed immediate value.
+void Decoder::PrintSImm19(Instruction* instr) {
+  int32_t imm19 = instr->Imm19Value();
+  // set sign
+  imm19 <<= (32 - kImm19Bits);
+  imm19 >>= (32 - kImm19Bits);
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "%d", imm19);
+}
+
+
+// Print 21-bit immediate value.
+void Decoder::PrintXImm21(Instruction* instr) {
+  uint32_t imm = instr->Imm21Value();
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "0x%x", imm);
+}
+
+
+// Print 21-bit signed immediate value.
+void Decoder::PrintSImm21(Instruction* instr) {
+  int32_t imm21 = instr->Imm21Value();
+  // set sign
+  imm21 <<= (32 - kImm21Bits);
+  imm21 >>= (32 - kImm21Bits);
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "%d", imm21);
+}
+
+
+// Print absoulte address for 21-bit offset or immediate value.
+// The absolute address is calculated according following expression:
+//      PC + delta_pc + (offset << n_bits)
+void Decoder::PrintPCImm21(Instruction* instr, int delta_pc, int n_bits) {
+  int32_t imm21 = instr->Imm21Value();
+  // set sign
+  imm21 <<= (32 - kImm21Bits);
+  imm21 >>= (32 - kImm21Bits);
+  out_buffer_pos_ +=
+      SNPrintF(out_buffer_ + out_buffer_pos_, "%s",
+               converter_.NameOfAddress(reinterpret_cast<byte*>(instr) +
+                                        delta_pc + (imm21 << n_bits)));
+}
+
+
+// Print 26-bit hex immediate value.
+void Decoder::PrintXImm26(Instruction* instr) {
+  uint64_t target = static_cast<uint64_t>(instr->Imm26Value())
+                    << kImmFieldShift;
+  target = (reinterpret_cast<uint64_t>(instr) & ~0xFFFFFFF) | target;
+  out_buffer_pos_ +=
+      SNPrintF(out_buffer_ + out_buffer_pos_, "0x%" PRIx64, target);
+}
+
+
+// Print 26-bit signed immediate value.
+void Decoder::PrintSImm26(Instruction* instr) {
+  int32_t imm26 = instr->Imm26Value();
+  // set sign
+  imm26 <<= (32 - kImm26Bits);
+  imm26 >>= (32 - kImm26Bits);
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "%d", imm26);
+}
+
+
+// Print absoulte address for 26-bit offset or immediate value.
+// The absolute address is calculated according following expression:
+//      PC + delta_pc + (offset << n_bits)
+void Decoder::PrintPCImm26(Instruction* instr, int delta_pc, int n_bits) {
+  int32_t imm26 = instr->Imm26Value();
+  // set sign
+  imm26 <<= (32 - kImm26Bits);
+  imm26 >>= (32 - kImm26Bits);
+  out_buffer_pos_ +=
+      SNPrintF(out_buffer_ + out_buffer_pos_, "%s",
+               converter_.NameOfAddress(reinterpret_cast<byte*>(instr) +
+                                        delta_pc + (imm26 << n_bits)));
+}
+
+
+// Print absoulte address for 26-bit offset or immediate value.
+// The absolute address is calculated according following expression:
+//      PC[GPRLEN-1 .. 28] || instr_index26 || 00
+void Decoder::PrintPCImm26(Instruction* instr) {
+  int32_t imm26 = instr->Imm26Value();
+  uint64_t pc_mask = ~0xFFFFFFF;
+  uint64_t pc = ((uint64_t)(instr + 1) & pc_mask) | (imm26 << 2);
+  out_buffer_pos_ +=
+      SNPrintF(out_buffer_ + out_buffer_pos_, "%s",
+               converter_.NameOfAddress((reinterpret_cast<byte*>(pc))));
+}
+
+
+void Decoder::PrintBp2(Instruction* instr) {
+  int bp2 = instr->Bp2Value();
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "%d", bp2);
+}
+
+
+void Decoder::PrintBp3(Instruction* instr) {
+  int bp3 = instr->Bp3Value();
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "%d", bp3);
+}
+
+
+// Print 26-bit immediate value.
+void Decoder::PrintCode(Instruction* instr) {
+  if (instr->OpcodeFieldRaw() != SPECIAL)
+    return;  // Not a break or trap instruction.
+  switch (instr->FunctionFieldRaw()) {
+    case BREAK: {
+      int32_t code = instr->Bits(25, 6);
+      out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_,
+                                  "0x%05x (%d)", code, code);
+      break;
+                }
+    case TGE:
+    case TGEU:
+    case TLT:
+    case TLTU:
+    case TEQ:
+    case TNE: {
+      int32_t code = instr->Bits(15, 6);
+      out_buffer_pos_ +=
+          SNPrintF(out_buffer_ + out_buffer_pos_, "0x%03x", code);
+      break;
+    }
+    default:  // Not a break or trap instruction.
+    break;
+  }
+}
+
+void Decoder::PrintMsaXImm8(Instruction* instr) {
+  int32_t imm = instr->MsaImm8Value();
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "0x%x", imm);
+}
+
+void Decoder::PrintMsaImm8(Instruction* instr) {
+  int32_t imm = instr->MsaImm8Value();
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "%u", imm);
+}
+
+void Decoder::PrintMsaImm5(Instruction* instr) {
+  int32_t imm = instr->MsaImm5Value();
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "%u", imm);
+}
+
+void Decoder::PrintMsaSImm5(Instruction* instr) {
+  int32_t imm = instr->MsaImm5Value();
+  imm <<= (32 - kMsaImm5Bits);
+  imm >>= (32 - kMsaImm5Bits);
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "%d", imm);
+}
+
+void Decoder::PrintMsaSImm10(Instruction* instr, bool is_mi10) {
+  int32_t imm = is_mi10 ? instr->MsaImmMI10Value() : instr->MsaImm10Value();
+  imm <<= (32 - kMsaImm10Bits);
+  imm >>= (32 - kMsaImm10Bits);
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "%d", imm);
+}
+
+void Decoder::PrintMsaImmBit(Instruction* instr) {
+  int32_t m = instr->MsaBitMValue();
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "%u", m);
+}
+
+void Decoder::PrintMsaImmElm(Instruction* instr) {
+  int32_t n = instr->MsaElmNValue();
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "%u", n);
+}
+
+void Decoder::PrintMsaCopy(Instruction* instr) {
+  int32_t rd = instr->WdValue();
+  int32_t ws = instr->WsValue();
+  int32_t n = instr->MsaElmNValue();
+  out_buffer_pos_ +=
+      SNPrintF(out_buffer_ + out_buffer_pos_, "%s, %s[%u]",
+               converter_.NameOfCPURegister(rd), MSARegisters::Name(ws), n);
+}
+
+void Decoder::PrintFormat(Instruction* instr) {
+  char formatLetter = ' ';
+  switch (instr->RsFieldRaw()) {
+    case S:
+      formatLetter = 's';
+      break;
+    case D:
+      formatLetter = 'd';
+      break;
+    case W:
+      formatLetter = 'w';
+      break;
+    case L:
+      formatLetter = 'l';
+      break;
+    default:
+      UNREACHABLE();
+  }
+  PrintChar(formatLetter);
+}
+
+void Decoder::PrintMsaDataFormat(Instruction* instr) {
+  DCHECK(instr->IsMSAInstr());
+  char df = ' ';
+  if (instr->IsMSABranchInstr()) {
+    switch (instr->RsFieldRaw()) {
+      case BZ_V:
+      case BNZ_V:
+        df = 'v';
+        break;
+      case BZ_B:
+      case BNZ_B:
+        df = 'b';
+        break;
+      case BZ_H:
+      case BNZ_H:
+        df = 'h';
+        break;
+      case BZ_W:
+      case BNZ_W:
+        df = 'w';
+        break;
+      case BZ_D:
+      case BNZ_D:
+        df = 'd';
+        break;
+      default:
+        UNREACHABLE();
+        break;
+    }
+  } else {
+    char DF[] = {'b', 'h', 'w', 'd'};
+    switch (instr->MSAMinorOpcodeField()) {
+      case kMsaMinorI5:
+      case kMsaMinorI10:
+      case kMsaMinor3R:
+        df = DF[instr->Bits(22, 21)];
+        break;
+      case kMsaMinorMI10:
+        df = DF[instr->Bits(1, 0)];
+        break;
+      case kMsaMinorBIT:
+        df = DF[instr->MsaBitDf()];
+        break;
+      case kMsaMinorELM:
+        df = DF[instr->MsaElmDf()];
+        break;
+      case kMsaMinor3RF: {
+        uint32_t opcode = instr->InstructionBits() & kMsa3RFMask;
+        switch (opcode) {
+          case FEXDO:
+          case FTQ:
+          case MUL_Q:
+          case MADD_Q:
+          case MSUB_Q:
+          case MULR_Q:
+          case MADDR_Q:
+          case MSUBR_Q:
+            df = DF[1 + instr->Bit(21)];
+            break;
+          default:
+            df = DF[2 + instr->Bit(21)];
+            break;
+        }
+      } break;
+      case kMsaMinor2R:
+        df = DF[instr->Bits(17, 16)];
+        break;
+      case kMsaMinor2RF:
+        df = DF[2 + instr->Bit(16)];
+        break;
+      default:
+        UNREACHABLE();
+        break;
+    }
+  }
+
+  PrintChar(df);
+}
+
+// Printing of instruction name.
+void Decoder::PrintInstructionName(Instruction* instr) {
+}
+
+
+// Handle all register based formatting in this function to reduce the
+// complexity of FormatOption.
+int Decoder::FormatRegister(Instruction* instr, const char* format) {
+  DCHECK_EQ(format[0], 'r');
+  if (format[1] == 's') {  // 'rs: Rs register.
+    int reg = instr->RsValue();
+    PrintRegister(reg);
+    return 2;
+  } else if (format[1] == 't') {  // 'rt: rt register.
+    int reg = instr->RtValue();
+    PrintRegister(reg);
+    return 2;
+  } else if (format[1] == 'd') {  // 'rd: rd register.
+    int reg = instr->RdValue();
+    PrintRegister(reg);
+    return 2;
+  }
+  UNREACHABLE();
+}
+
+
+// Handle all FPUregister based formatting in this function to reduce the
+// complexity of FormatOption.
+int Decoder::FormatFPURegister(Instruction* instr, const char* format) {
+  DCHECK_EQ(format[0], 'f');
+  if ((CTC1 == instr->RsFieldRaw()) || (CFC1 == instr->RsFieldRaw())) {
+    if (format[1] == 's') {  // 'fs: fs register.
+      int reg = instr->FsValue();
+      PrintFPUStatusRegister(reg);
+      return 2;
+    } else if (format[1] == 't') {  // 'ft: ft register.
+      int reg = instr->FtValue();
+      PrintFPUStatusRegister(reg);
+      return 2;
+    } else if (format[1] == 'd') {  // 'fd: fd register.
+      int reg = instr->FdValue();
+      PrintFPUStatusRegister(reg);
+      return 2;
+    } else if (format[1] == 'r') {  // 'fr: fr register.
+      int reg = instr->FrValue();
+      PrintFPUStatusRegister(reg);
+      return 2;
+    }
+  } else {
+    if (format[1] == 's') {  // 'fs: fs register.
+      int reg = instr->FsValue();
+      PrintFPURegister(reg);
+      return 2;
+    } else if (format[1] == 't') {  // 'ft: ft register.
+      int reg = instr->FtValue();
+      PrintFPURegister(reg);
+      return 2;
+    } else if (format[1] == 'd') {  // 'fd: fd register.
+      int reg = instr->FdValue();
+      PrintFPURegister(reg);
+      return 2;
+    } else if (format[1] == 'r') {  // 'fr: fr register.
+      int reg = instr->FrValue();
+      PrintFPURegister(reg);
+      return 2;
+    }
+  }
+  UNREACHABLE();
+}
+
+// Handle all MSARegister based formatting in this function to reduce the
+// complexity of FormatOption.
+int Decoder::FormatMSARegister(Instruction* instr, const char* format) {
+  DCHECK_EQ(format[0], 'w');
+  if (format[1] == 's') {
+    int reg = instr->WsValue();
+    PrintMSARegister(reg);
+    return 2;
+  } else if (format[1] == 't') {
+    int reg = instr->WtValue();
+    PrintMSARegister(reg);
+    return 2;
+  } else if (format[1] == 'd') {
+    int reg = instr->WdValue();
+    PrintMSARegister(reg);
+    return 2;
+  }
+
+  UNREACHABLE();
+}
+
+// FormatOption takes a formatting string and interprets it based on
+// the current instructions. The format string points to the first
+// character of the option string (the option escape has already been
+// consumed by the caller.)  FormatOption returns the number of
+// characters that were consumed from the formatting string.
+#ifdef SW64//jzy 20150213:TODO
+int Decoder::FormatOption(Instruction* instr, const char* format) {
+    switch (format[0]) {
+        case 'r': {
+            switch (format[1]) {
+                case 'a' : 
+                    SwPrintRa(instr);
+                    break;
+                case 'b': 
+                    SwPrintRb(instr);
+                    break;
+                case 'c':
+                    return SwPrintRc(instr, format);
+                case 'd':
+                    SwPrintRd(instr);
+                    break;
+            }
+            return 2;
+        }
+
+        case 'f': {
+            switch (format[1]) {
+                case 'a' : 
+                    SwPrintFa(instr);
+                    break;
+                case 'b': 
+                    SwPrintFb(instr);
+                    break;
+                case 'c':
+                    return SwPrintFc(instr, format);
+                case 'd':
+                    SwPrintFd(instr);
+                    break;
+            }
+            return 2;
+        }
+
+        case 'i': 
+            return SwPrintImm(instr, format);
+        case 'd':  
+            return SwPrintDisp(instr, format);
+        case 't'://ld 20150323
+            return SwPrintDispTransfer(instr,format);
+        //modified by cjq 20150318
+        case '0':
+            return SwPrintHex(instr, format);
+    
+    }
+    UNREACHABLE();
+    return -1;
+}
+#else
+int Decoder::FormatOption(Instruction* instr, const char* format) {
+  switch (format[0]) {
+    case 'c': {   // 'code for break or trap instructions.
+      DCHECK(STRING_STARTS_WITH(format, "code"));
+      PrintCode(instr);
+      return 4;
+    }
+    case 'i': {   // 'imm16u or 'imm26.
+      if (format[3] == '1') {
+        if (format[4] == '6') {
+          DCHECK(STRING_STARTS_WITH(format, "imm16"));
+          switch (format[5]) {
+            case 's':
+              DCHECK(STRING_STARTS_WITH(format, "imm16s"));
+              PrintSImm16(instr);
+              break;
+            case 'u':
+              DCHECK(STRING_STARTS_WITH(format, "imm16u"));
+              PrintSImm16(instr);
+              break;
+            case 'x':
+              DCHECK(STRING_STARTS_WITH(format, "imm16x"));
+              PrintXImm16(instr);
+              break;
+            case 'p': {  // The PC relative address.
+              DCHECK(STRING_STARTS_WITH(format, "imm16p"));
+              int delta_pc = 0;
+              int n_bits = 0;
+              switch (format[6]) {
+                case '4': {
+                  DCHECK(STRING_STARTS_WITH(format, "imm16p4"));
+                  delta_pc = 4;
+                  switch (format[8]) {
+                    case '2':
+                      DCHECK(STRING_STARTS_WITH(format, "imm16p4s2"));
+                      n_bits = 2;
+                      PrintPCImm16(instr, delta_pc, n_bits);
+                      return 9;
+                  }
+                }
+              }
+            }
+          }
+          return 6;
+        } else if (format[4] == '8') {
+          DCHECK(STRING_STARTS_WITH(format, "imm18"));
+          switch (format[5]) {
+            case 's':
+              DCHECK(STRING_STARTS_WITH(format, "imm18s"));
+              PrintSImm18(instr);
+              break;
+            case 'x':
+              DCHECK(STRING_STARTS_WITH(format, "imm18x"));
+              PrintXImm18(instr);
+              break;
+          }
+          return 6;
+        } else if (format[4] == '9') {
+          DCHECK(STRING_STARTS_WITH(format, "imm19"));
+          switch (format[5]) {
+            case 's':
+              DCHECK(STRING_STARTS_WITH(format, "imm19s"));
+              PrintSImm19(instr);
+              break;
+            case 'x':
+              DCHECK(STRING_STARTS_WITH(format, "imm19x"));
+              PrintXImm19(instr);
+              break;
+          }
+          return 6;
+        } else if (format[4] == '0' && format[5] == 's') {
+          DCHECK(STRING_STARTS_WITH(format, "imm10s"));
+          if (format[6] == '1') {
+            DCHECK(STRING_STARTS_WITH(format, "imm10s1"));
+            PrintMsaSImm10(instr, false);
+          } else if (format[6] == '2') {
+            DCHECK(STRING_STARTS_WITH(format, "imm10s2"));
+            PrintMsaSImm10(instr, true);
+          }
+          return 7;
+        }
+      } else if (format[3] == '2' && format[4] == '1') {
+        DCHECK(STRING_STARTS_WITH(format, "imm21"));
+        switch (format[5]) {
+          case 's':
+            DCHECK(STRING_STARTS_WITH(format, "imm21s"));
+            PrintSImm21(instr);
+            break;
+          case 'x':
+            DCHECK(STRING_STARTS_WITH(format, "imm21x"));
+            PrintXImm21(instr);
+            break;
+          case 'p': {  // The PC relative address.
+            DCHECK(STRING_STARTS_WITH(format, "imm21p"));
+            int delta_pc = 0;
+            int n_bits = 0;
+            switch (format[6]) {
+              case '4': {
+                DCHECK(STRING_STARTS_WITH(format, "imm21p4"));
+                delta_pc = 4;
+                switch (format[8]) {
+                  case '2':
+                    DCHECK(STRING_STARTS_WITH(format, "imm21p4s2"));
+                    n_bits = 2;
+                    PrintPCImm21(instr, delta_pc, n_bits);
+                    return 9;
+                }
+              }
+            }
+          }
+        }
+        return 6;
+      } else if (format[3] == '2' && format[4] == '6') {
+        DCHECK(STRING_STARTS_WITH(format, "imm26"));
+        switch (format[5]) {
+          case 's':
+            DCHECK(STRING_STARTS_WITH(format, "imm26s"));
+            PrintSImm26(instr);
+            break;
+          case 'x':
+            DCHECK(STRING_STARTS_WITH(format, "imm26x"));
+            PrintXImm26(instr);
+            break;
+          case 'p': {  // The PC relative address.
+            DCHECK(STRING_STARTS_WITH(format, "imm26p"));
+            int delta_pc = 0;
+            int n_bits = 0;
+            switch (format[6]) {
+              case '4': {
+                DCHECK(STRING_STARTS_WITH(format, "imm26p4"));
+                delta_pc = 4;
+                switch (format[8]) {
+                  case '2':
+                    DCHECK(STRING_STARTS_WITH(format, "imm26p4s2"));
+                    n_bits = 2;
+                    PrintPCImm26(instr, delta_pc, n_bits);
+                    return 9;
+                }
+              }
+            }
+          }
+          case 'j': {  // Absolute address for jump instructions.
+            DCHECK(STRING_STARTS_WITH(format, "imm26j"));
+            PrintPCImm26(instr);
+            break;
+          }
+        }
+        return 6;
+      } else if (format[3] == '5') {
+        DCHECK(STRING_STARTS_WITH(format, "imm5"));
+        if (format[4] == 'u') {
+          DCHECK(STRING_STARTS_WITH(format, "imm5u"));
+          PrintMsaImm5(instr);
+        } else if (format[4] == 's') {
+          DCHECK(STRING_STARTS_WITH(format, "imm5s"));
+          PrintMsaSImm5(instr);
+        }
+        return 5;
+      } else if (format[3] == '8') {
+        DCHECK(STRING_STARTS_WITH(format, "imm8"));
+        PrintMsaImm8(instr);
+        return 4;
+      } else if (format[3] == '9') {
+        DCHECK(STRING_STARTS_WITH(format, "imm9"));
+        if (format[4] == 'u') {
+          DCHECK(STRING_STARTS_WITH(format, "imm9u"));
+          PrintUImm9(instr);
+        } else if (format[4] == 's') {
+          DCHECK(STRING_STARTS_WITH(format, "imm9s"));
+          PrintSImm9(instr);
+        }
+        return 5;
+      } else if (format[3] == 'b') {
+        DCHECK(STRING_STARTS_WITH(format, "immb"));
+        PrintMsaImmBit(instr);
+        return 4;
+      } else if (format[3] == 'e') {
+        DCHECK(STRING_STARTS_WITH(format, "imme"));
+        PrintMsaImmElm(instr);
+        return 4;
+      }
+      UNREACHABLE();
+    }
+    case 'r': {   // 'r: registers.
+      return FormatRegister(instr, format);
+    }
+    case 'f': {   // 'f: FPUregisters.
+      return FormatFPURegister(instr, format);
+    }
+    case 'w': {  // 'w: MSA Register
+      return FormatMSARegister(instr, format);
+    }
+    case 's': {   // 'sa.
+      switch (format[1]) {
+        case 'a':
+          if (format[2] == '2') {
+            DCHECK(STRING_STARTS_WITH(format, "sa2"));  // 'sa2
+            PrintLsaSa(instr);
+            return 3;
+          } else {
+            DCHECK(STRING_STARTS_WITH(format, "sa"));
+            PrintSa(instr);
+            return 2;
+          }
+          break;
+        case 'd': {
+          DCHECK(STRING_STARTS_WITH(format, "sd"));
+          PrintSd(instr);
+          return 2;
+        }
+        case 's': {
+          if (format[2] == '1') {
+            DCHECK(STRING_STARTS_WITH(format, "ss1"));  // ext, dext, dextu size
+            PrintSs1(instr);
+          } else if (format[2] == '2') {
+            DCHECK(STRING_STARTS_WITH(format, "ss2"));  // ins, dins, dinsu size
+            PrintSs2(instr);
+          } else if (format[2] == '3') {
+            DCHECK(STRING_STARTS_WITH(format, "ss3"));  // dextm size
+            PrintSs3(instr);
+          } else if (format[2] == '4') {
+            DCHECK(STRING_STARTS_WITH(format, "ss4"));  // dinsm size
+            PrintSs4(instr);
+          } else {
+            DCHECK(STRING_STARTS_WITH(format, "ss5"));  // dextu, dinsu pos
+            PrintSs5(instr);
+          }
+          return 3;
+        }
+      }
+    }
+    case 'b': {
+      switch (format[1]) {
+        case 'c': {  // 'bc - Special for bc1 cc field.
+          DCHECK(STRING_STARTS_WITH(format, "bc"));
+          PrintBc(instr);
+          return 2;
+        }
+        case 'p': {
+          switch (format[2]) {
+            case '2': {  // 'bp2
+              DCHECK(STRING_STARTS_WITH(format, "bp2"));
+              PrintBp2(instr);
+              return 3;
+            }
+            case '3': {  // 'bp3
+              DCHECK(STRING_STARTS_WITH(format, "bp3"));
+              PrintBp3(instr);
+              return 3;
+            }
+          }
+        }
+      }
+    }
+    case 'C': {   // 'Cc - Special for c.xx.d cc field.
+      DCHECK(STRING_STARTS_WITH(format, "Cc"));
+      PrintCc(instr);
+      return 2;
+    }
+    case 't':
+      if (instr->IsMSAInstr()) {
+        PrintMsaDataFormat(instr);
+      } else {
+        PrintFormat(instr);
+      }
+      return 1;
+  }
+  UNREACHABLE();
+}
+#endif
+
+
+// Format takes a formatting string for a whole instruction and prints it into
+// the output buffer. All escaped options are handed to FormatOption to be
+// parsed further.
+void Decoder::Format(Instruction* instr, const char* format) {
+  char cur = *format++;
+  while ((cur != 0) && (out_buffer_pos_ < (out_buffer_.length() - 1))) {
+    if (cur == '\'') {  // Single quote is used as the formatting escape.
+      format += FormatOption(instr, format);
+    } else {
+      out_buffer_[out_buffer_pos_++] = cur;
+    }
+    cur = *format++;
+  }
+  out_buffer_[out_buffer_pos_]  = '\0';
+}
+
+
+// For currently unimplemented decodings the disassembler calls Unknown(instr)
+// which will just print "unknown" of the instruction bits.
+void Decoder::Unknown(Instruction* instr) {
+  Format(instr, "unknown");
+}
+
+
+int Decoder::DecodeBreakInstr(Instruction* instr) {
+  // This is already known to be BREAK instr, just extract the code.
+  if (instr->Bits(25, 6) == static_cast<int>(kMaxStopCode)) {
+    // This is stop(msg).
+    Format(instr, "break, code: 'code");
+    out_buffer_pos_ += SNPrintF(
+        out_buffer_ + out_buffer_pos_, "\n%p       %08" PRIx64,
+        static_cast<void*>(reinterpret_cast<int32_t*>(instr + kInstrSize)),
+        reinterpret_cast<uint64_t>(
+            *reinterpret_cast<char**>(instr + kInstrSize)));
+    // Size 3: the break_ instr, plus embedded 64-bit char pointer.
+    return 3 * kInstrSize;
+  } else {
+    Format(instr, "break, code: 'code");
+    return kInstrSize;
+  }
+}
+
+
+bool Decoder::DecodeTypeRegisterRsType(Instruction* instr) {
+  switch (instr->FunctionFieldRaw()) {
+    case RINT:
+      Format(instr, "rint.'t    'fd, 'fs");
+      break;
+//    case SEL:
+//      Format(instr, "sel.'t      'fd, 'fs, 'ft");
+//      break;
+//    case SELEQZ_C:
+//      Format(instr, "seleqz.'t    'fd, 'fs, 'ft");
+//      break;
+//    case SELNEZ_C:
+//      Format(instr, "selnez.'t    'fd, 'fs, 'ft");
+//      break;
+//    case MOVZ_C:
+//      Format(instr, "movz.'t    'fd, 'fs, 'rt");
+//      break;
+//    case MOVN_C:
+//      Format(instr, "movn.'t    'fd, 'fs, 'rt");
+//      break;
+//    case MOVF:
+//      if (instr->Bit(16)) {
+//        Format(instr, "movt.'t    'fd, 'fs, 'Cc");
+//      } else {
+//        Format(instr, "movf.'t    'fd, 'fs, 'Cc");
+//      }
+//      break;
+    case MIN:
+      Format(instr, "min.'t    'fd, 'fs, 'ft");
+      break;
+    case MAX:
+      Format(instr, "max.'t    'fd, 'fs, 'ft");
+      break;
+//    case MINA:
+//      Format(instr, "mina.'t   'fd, 'fs, 'ft");
+//      break;
+//    case MAXA:
+//      Format(instr, "maxa.'t   'fd, 'fs, 'ft");
+//      break;
+    case ADD_D:
+      Format(instr, "add.'t   'fd, 'fs, 'ft");
+      break;
+    case SUB_D:
+      Format(instr, "sub.'t   'fd, 'fs, 'ft");
+      break;
+    case MUL_D:
+      Format(instr, "mul.'t   'fd, 'fs, 'ft");
+      break;
+    case DIV_D:
+      Format(instr, "div.'t   'fd, 'fs, 'ft");
+      break;
+    case ABS_D:
+      Format(instr, "abs.'t   'fd, 'fs");
+      break;
+    case MOV_D:
+      Format(instr, "mov.'t   'fd, 'fs");
+      break;
+    case NEG_D:
+      Format(instr, "neg.'t   'fd, 'fs");
+      break;
+    case SQRT_D:
+      Format(instr, "sqrt.'t  'fd, 'fs");
+      break;
+    case RECIP_D:
+      Format(instr, "recip.'t  'fd, 'fs");
+      break;
+    case RSQRT_D:
+      Format(instr, "rsqrt.'t  'fd, 'fs");
+      break;
+    case CVT_W_D:
+      Format(instr, "cvt.w.'t 'fd, 'fs");
+      break;
+    case CVT_L_D:
+      Format(instr, "cvt.l.'t 'fd, 'fs");
+      break;
+    case TRUNC_W_D:
+      Format(instr, "trunc.w.'t 'fd, 'fs");
+      break;
+    case TRUNC_L_D:
+      Format(instr, "trunc.l.'t 'fd, 'fs");
+      break;
+    case ROUND_W_D:
+      Format(instr, "round.w.'t 'fd, 'fs");
+      break;
+    case ROUND_L_D:
+      Format(instr, "round.l.'t 'fd, 'fs");
+      break;
+    case FLOOR_W_D:
+      Format(instr, "floor.w.'t 'fd, 'fs");
+      break;
+    case FLOOR_L_D:
+      Format(instr, "floor.l.'t 'fd, 'fs");
+      break;
+    case CEIL_W_D:
+      Format(instr, "ceil.w.'t 'fd, 'fs");
+      break;
+    case CEIL_L_D:
+      Format(instr, "ceil.l.'t 'fd, 'fs");
+      break;
+    case CLASS_D:
+      Format(instr, "class.'t 'fd, 'fs");
+      break;
+    case CVT_S_D:
+      Format(instr, "cvt.s.'t 'fd, 'fs");
+      break;
+    case C_F_D:
+      Format(instr, "c.f.'t   'fs, 'ft, 'Cc");
+      break;
+    case C_UN_D:
+      Format(instr, "c.un.'t  'fs, 'ft, 'Cc");
+      break;
+    case C_EQ_D:
+      Format(instr, "c.eq.'t  'fs, 'ft, 'Cc");
+      break;
+    case C_UEQ_D:
+      Format(instr, "c.ueq.'t 'fs, 'ft, 'Cc");
+      break;
+    case C_OLT_D:
+      Format(instr, "c.olt.'t 'fs, 'ft, 'Cc");
+      break;
+    case C_ULT_D:
+      Format(instr, "c.ult.'t 'fs, 'ft, 'Cc");
+      break;
+    case C_OLE_D:
+      Format(instr, "c.ole.'t 'fs, 'ft, 'Cc");
+      break;
+    case C_ULE_D:
+      Format(instr, "c.ule.'t 'fs, 'ft, 'Cc");
+      break;
+    default:
+      return false;
+  }
+  return true;
+}
+
+
+void Decoder::DecodeTypeRegisterSRsType(Instruction* instr) {
+  if (!DecodeTypeRegisterRsType(instr)) {
+    switch (instr->FunctionFieldRaw()) {
+      case CVT_D_S:
+        Format(instr, "cvt.d.'t 'fd, 'fs");
+        break;
+      case MADDF_S:
+        Format(instr, "maddf.s  'fd, 'fs, 'ft");
+        break;
+      case MSUBF_S:
+        Format(instr, "msubf.s  'fd, 'fs, 'ft");
+        break;
+      default:
+        Format(instr, "unknown.cop1.'t");
+        break;
+    }
+  }
+}
+
+
+void Decoder::DecodeTypeRegisterDRsType(Instruction* instr) {
+  if (!DecodeTypeRegisterRsType(instr)) {
+    switch (instr->FunctionFieldRaw()) {
+      case MADDF_D:
+        Format(instr, "maddf.d  'fd, 'fs, 'ft");
+        break;
+      case MSUBF_D:
+        Format(instr, "msubf.d  'fd, 'fs, 'ft");
+        break;
+      default:
+        Format(instr, "unknown.cop1.'t");
+        break;
+    }
+  }
+}
+
+
+void Decoder::DecodeTypeRegisterLRsType(Instruction* instr) {
+  switch (instr->FunctionFieldRaw()) {
+    case CVT_D_L:
+      Format(instr, "cvt.d.l 'fd, 'fs");
+      break;
+    case CVT_S_L:
+      Format(instr, "cvt.s.l 'fd, 'fs");
+      break;
+    case CMP_AF:
+      Format(instr, "cmp.af.d  'fd,  'fs, 'ft");
+      break;
+    case CMP_UN:
+      Format(instr, "cmp.un.d  'fd,  'fs, 'ft");
+      break;
+    case CMP_EQ:
+      Format(instr, "cmp.eq.d  'fd,  'fs, 'ft");
+      break;
+    case CMP_UEQ:
+      Format(instr, "cmp.ueq.d  'fd,  'fs, 'ft");
+      break;
+    case CMP_LT:
+      Format(instr, "cmp.lt.d  'fd,  'fs, 'ft");
+      break;
+    case CMP_ULT:
+      Format(instr, "cmp.ult.d  'fd,  'fs, 'ft");
+      break;
+    case CMP_LE:
+      Format(instr, "cmp.le.d  'fd,  'fs, 'ft");
+      break;
+    case CMP_ULE:
+      Format(instr, "cmp.ule.d  'fd,  'fs, 'ft");
+      break;
+    case CMP_OR:
+      Format(instr, "cmp.or.d  'fd,  'fs, 'ft");
+      break;
+    case CMP_UNE:
+      Format(instr, "cmp.une.d  'fd,  'fs, 'ft");
+      break;
+    case CMP_NE:
+      Format(instr, "cmp.ne.d  'fd,  'fs, 'ft");
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+
+void Decoder::DecodeTypeRegisterWRsType(Instruction* instr) {
+  switch (instr->FunctionValue()) {
+    case CVT_S_W:  // Convert word to float (single).
+      Format(instr, "cvt.s.w 'fd, 'fs");
+      break;
+    case CVT_D_W:  // Convert word to double.
+      Format(instr, "cvt.d.w 'fd, 'fs");
+      break;
+    case CMP_AF:
+      Format(instr, "cmp.af.s    'fd, 'fs, 'ft");
+      break;
+    case CMP_UN:
+      Format(instr, "cmp.un.s    'fd, 'fs, 'ft");
+      break;
+    case CMP_EQ:
+      Format(instr, "cmp.eq.s    'fd, 'fs, 'ft");
+      break;
+    case CMP_UEQ:
+      Format(instr, "cmp.ueq.s   'fd, 'fs, 'ft");
+      break;
+    case CMP_LT:
+      Format(instr, "cmp.lt.s    'fd, 'fs, 'ft");
+      break;
+    case CMP_ULT:
+      Format(instr, "cmp.ult.s   'fd, 'fs, 'ft");
+      break;
+    case CMP_LE:
+      Format(instr, "cmp.le.s    'fd, 'fs, 'ft");
+      break;
+    case CMP_ULE:
+      Format(instr, "cmp.ule.s   'fd, 'fs, 'ft");
+      break;
+    case CMP_OR:
+      Format(instr, "cmp.or.s    'fd, 'fs, 'ft");
+      break;
+    case CMP_UNE:
+      Format(instr, "cmp.une.s   'fd, 'fs, 'ft");
+      break;
+    case CMP_NE:
+      Format(instr, "cmp.ne.s    'fd, 'fs, 'ft");
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+
+void Decoder::DecodeTypeRegisterCOP1(Instruction* instr) {
+  switch (instr->RsFieldRaw()) {
+//    case MFC1:
+//      Format(instr, "mfc1    'rt, 'fs");
+//      break;
+//    case DMFC1:
+//      Format(instr, "dmfc1    'rt, 'fs");
+//      break;
+//    case MFHC1:
+//      Format(instr, "mfhc1   'rt, 'fs");
+//      break;
+//    case MTC1:
+//      Format(instr, "mtc1    'rt, 'fs");
+//      break;
+//    case DMTC1:
+//      Format(instr, "dmtc1    'rt, 'fs");
+//      break;
+    // These are called "fs" too, although they are not FPU registers.
+    case CTC1:
+      Format(instr, "ctc1    'rt, 'fs");
+      break;
+    case CFC1:
+      Format(instr, "cfc1    'rt, 'fs");
+      break;
+//    case MTHC1:
+//      Format(instr, "mthc1   'rt, 'fs");
+//      break;
+    case S:
+      DecodeTypeRegisterSRsType(instr);
+      break;
+    case D:
+      DecodeTypeRegisterDRsType(instr);
+      break;
+    case W:
+      DecodeTypeRegisterWRsType(instr);
+      break;
+    case L:
+      DecodeTypeRegisterLRsType(instr);
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+
+void Decoder::DecodeTypeRegisterCOP1X(Instruction* instr) {
+  switch (instr->FunctionFieldRaw()) {
+    case MADD_S:
+      Format(instr, "madd.s  'fd, 'fr, 'fs, 'ft");
+      break;
+    case MADD_D:
+      Format(instr, "madd.d  'fd, 'fr, 'fs, 'ft");
+      break;
+    case MSUB_S:
+      Format(instr, "msub.s  'fd, 'fr, 'fs, 'ft");
+      break;
+    case MSUB_D:
+      Format(instr, "msub.d  'fd, 'fr, 'fs, 'ft");
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+
+void Decoder::DecodeTypeRegisterSPECIAL(Instruction* instr) {
+  switch (instr->FunctionFieldRaw()) {
+#if 0
+    case DIV:  // @Sw64r3 == DIV_MOD.
+      if (kArchVariant != kSw64r3) {
+        Format(instr, "div     'rs, 'rt");
+      } else {
+        if (instr->SaValue() == DIV_OP) {
+          Format(instr, "div    'rd, 'rs, 'rt");
+        } else {
+          Format(instr, "mod    'rd, 'rs, 'rt");
+        }
+      }
+      break;
+    case DDIV:  // @Sw64r3 == D_DIV_MOD.
+      if (kArchVariant != kSw64r3) {
+        Format(instr, "ddiv    'rs, 'rt");
+      } else {
+        if (instr->SaValue() == DIV_OP) {
+          Format(instr, "ddiv   'rd, 'rs, 'rt");
+        } else {
+          Format(instr, "dmod   'rd, 'rs, 'rt");
+        }
+      }
+      break;
+    case DIVU:  // @Sw64r3 == DIV_MOD_U.
+      if (kArchVariant != kSw64r3) {
+        Format(instr, "divu    'rs, 'rt");
+      } else {
+        if (instr->SaValue() == DIV_OP) {
+          Format(instr, "divu   'rd, 'rs, 'rt");
+        } else {
+          Format(instr, "modu   'rd, 'rs, 'rt");
+        }
+      }
+      break;
+    case DDIVU:  // @Sw64r3 == D_DIV_MOD_U.
+      if (kArchVariant != kSw64r3) {
+        Format(instr, "ddivu   'rs, 'rt");
+      } else {
+        if (instr->SaValue() == DIV_OP) {
+          Format(instr, "ddivu  'rd, 'rs, 'rt");
+        } else {
+          Format(instr, "dmodu  'rd, 'rs, 'rt");
+        }
+      }
+      break;
+#endif
+    case ADD:
+      Format(instr, "add     'rd, 'rs, 'rt");
+      break;
+    case DADD:
+      Format(instr, "dadd    'rd, 'rs, 'rt");
+      break;
+    case SUB:
+      Format(instr, "sub     'rd, 'rs, 'rt");
+      break;
+    case DSUB:
+      Format(instr, "dsub    'rd, 'rs, 'rt");
+      break;
+    case AND:
+      Format(instr, "and     'rd, 'rs, 'rt");
+      break;
+    case OR:
+      if (0 == instr->RsValue()) {
+        Format(instr, "mov     'rd, 'rt");
+      } else if (0 == instr->RtValue()) {
+        Format(instr, "mov     'rd, 'rs");
+      } else {
+        Format(instr, "or      'rd, 'rs, 'rt");
+      }
+      break;
+    case XOR:
+      Format(instr, "xor     'rd, 'rs, 'rt");
+      break;
+    case NOR:
+      Format(instr, "nor     'rd, 'rs, 'rt");
+      break;
+    case MOVCI:
+      if (instr->Bit(16)) {
+        Format(instr, "movt    'rd, 'rs, 'bc");
+      } else {
+        Format(instr, "movf    'rd, 'rs, 'bc");
+      }
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+
+void Decoder::DecodeTypeRegisterSPECIAL2(Instruction* instr) {
+  switch (instr->FunctionFieldRaw()) {
+    case MUL:
+      Format(instr, "mul     'rd, 'rs, 'rt");
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+
+void Decoder::DecodeTypeRegisterSPECIAL3(Instruction* instr) {
+  switch (instr->FunctionFieldRaw()) {
+#if 0
+    case EXT: {
+      Format(instr, "ext     'rt, 'rs, 'sa, 'ss1");
+      break;
+    }
+    case DEXT: {
+      Format(instr, "dext    'rt, 'rs, 'sa, 'ss1");
+      break;
+    }
+    case DEXTM: {
+      Format(instr, "dextm   'rt, 'rs, 'sa, 'ss3");
+      break;
+    }
+    case DEXTU: {
+      Format(instr, "dextu   'rt, 'rs, 'ss5, 'ss1");
+      break;
+    }
+    case INS: {
+      Format(instr, "ins     'rt, 'rs, 'sa, 'ss2");
+      break;
+    }
+    case DINS: {
+      Format(instr, "dins    'rt, 'rs, 'sa, 'ss2");
+      break;
+    }
+    case DINSM: {
+      Format(instr, "dinsm   'rt, 'rs, 'sa, 'ss4");
+      break;
+    }
+    case DINSU: {
+      Format(instr, "dinsu   'rt, 'rs, 'ss5, 'ss2");
+      break;
+    }
+#endif
+    case BSHFL: {
+      int sa = instr->SaFieldRaw() >> kSaShift;
+      switch (sa) {
+        case BITSWAP: {
+          Format(instr, "bitswap 'rd, 'rt");
+          break;
+        }
+        default: {
+          sa >>= kBp2Bits;
+          switch (sa) {
+            case ALIGN: {
+              Format(instr, "align  'rd, 'rs, 'rt, 'bp2");
+              break;
+            }
+            default:
+              UNREACHABLE();
+              break;
+          }
+          break;
+        }
+      }
+      break;
+    }
+    case DBSHFL: {
+      int sa = instr->SaFieldRaw() >> kSaShift;
+      switch (sa) {
+        case DBITSWAP: {
+          switch (instr->SaFieldRaw() >> kSaShift) {
+            case DBITSWAP_SA:
+              Format(instr, "dbitswap 'rd, 'rt");
+              break;
+            default:
+              UNREACHABLE();
+              break;
+          }
+          break;
+        }
+        default: {
+          sa >>= kBp3Bits;
+          switch (sa) {
+            case DALIGN: {
+              Format(instr, "dalign  'rd, 'rs, 'rt, 'bp3");
+              break;
+            }
+            default:
+              UNREACHABLE();
+              break;
+          }
+          break;
+        }
+      }
+      break;
+    }
+    default:
+      UNREACHABLE();
+  }
+}
+
+
+int Decoder::DecodeTypeRegister(Instruction* instr) {
+  switch (instr->OpcodeFieldRaw()) {
+    case COP1:  // Coprocessor instructions.
+      DecodeTypeRegisterCOP1(instr);
+      break;
+    case COP1X:
+      DecodeTypeRegisterCOP1X(instr);
+      break;
+    case SPECIAL:
+      switch (instr->FunctionFieldRaw()) {
+        case BREAK:
+          return DecodeBreakInstr(instr);
+        default:
+          DecodeTypeRegisterSPECIAL(instr);
+          break;
+      }
+      break;
+    case SPECIAL2:
+      DecodeTypeRegisterSPECIAL2(instr);
+      break;
+    case SPECIAL3:
+      DecodeTypeRegisterSPECIAL3(instr);
+      break;
+    case MSA:
+      switch (instr->MSAMinorOpcodeField()) {
+        case kMsaMinor3R:
+          DecodeTypeMsa3R(instr);
+          break;
+        case kMsaMinor3RF:
+          DecodeTypeMsa3RF(instr);
+          break;
+        case kMsaMinorVEC:
+          DecodeTypeMsaVec(instr);
+          break;
+        case kMsaMinor2R:
+          DecodeTypeMsa2R(instr);
+          break;
+        case kMsaMinor2RF:
+          DecodeTypeMsa2RF(instr);
+          break;
+        case kMsaMinorELM:
+          DecodeTypeMsaELM(instr);
+          break;
+        default:
+          UNREACHABLE();
+      }
+      break;
+    default:
+      UNREACHABLE();
+  }
+  return kInstrSize;
+}
+
+
+void Decoder::DecodeTypeImmediateCOP1(Instruction* instr) {
+  switch (instr->RsFieldRaw()) {
+    case BC1:
+      if (instr->FBtrueValue()) {
+        Format(instr, "bc1t    'bc, 'imm16u -> 'imm16p4s2");
+      } else {
+        Format(instr, "bc1f    'bc, 'imm16u -> 'imm16p4s2");
+      }
+      break;
+    case BC1EQZ:
+      Format(instr, "bc1eqz    'ft, 'imm16u -> 'imm16p4s2");
+      break;
+    case BC1NEZ:
+      Format(instr, "bc1nez    'ft, 'imm16u -> 'imm16p4s2");
+      break;
+    case BZ_V:
+    case BZ_B:
+    case BZ_H:
+    case BZ_W:
+    case BZ_D:
+      Format(instr, "bz.'t  'wt, 'imm16s -> 'imm16p4s2");
+      break;
+    case BNZ_V:
+    case BNZ_B:
+    case BNZ_H:
+    case BNZ_W:
+    case BNZ_D:
+      Format(instr, "bnz.'t  'wt, 'imm16s -> 'imm16p4s2");
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+
+void Decoder::DecodeTypeImmediateREGIMM(Instruction* instr) {
+  switch (instr->RtFieldRaw()) {
+    case BLTZ:
+      Format(instr, "bltz    'rs, 'imm16u -> 'imm16p4s2");
+      break;
+    case BLTZAL:
+      Format(instr, "bltzal  'rs, 'imm16u -> 'imm16p4s2");
+      break;
+    case BGEZ:
+      Format(instr, "bgez    'rs, 'imm16u -> 'imm16p4s2");
+      break;
+    case BGEZAL: {
+      if (instr->RsValue() == 0)
+        Format(instr, "bal     'imm16s -> 'imm16p4s2");
+      else
+        Format(instr, "bgezal  'rs, 'imm16u -> 'imm16p4s2");
+      break;
+    }
+    case BGEZALL:
+      Format(instr, "bgezall 'rs, 'imm16u -> 'imm16p4s2");
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+void Decoder::DecodeTypeImmediateSPECIAL3(Instruction* instr) {
+  switch (instr->FunctionFieldRaw()) {
+    case LL_R6: {
+      if (kArchVariant == kSw64r3) {
+        Format(instr, "ll     'rt, 'imm9s('rs)");
+      } else {
+        Unknown(instr);
+      }
+      break;
+    }
+    case LLD_R6: {
+      if (kArchVariant == kSw64r3) {
+        Format(instr, "lld     'rt, 'imm9s('rs)");
+      } else {
+        Unknown(instr);
+      }
+      break;
+    }
+    case SC_R6: {
+      if (kArchVariant == kSw64r3) {
+        Format(instr, "sc     'rt, 'imm9s('rs)");
+      } else {
+        Unknown(instr);
+      }
+      break;
+    }
+    case SCD_R6: {
+      if (kArchVariant == kSw64r3) {
+        Format(instr, "scd     'rt, 'imm9s('rs)");
+      } else {
+        Unknown(instr);
+      }
+      break;
+    }
+    default:
+      UNREACHABLE();
+  }
+}
+
+void Decoder::DecodeTypeImmediate(Instruction* instr) {
+  switch (instr->OpcodeFieldRaw()) {
+    case COP1:
+      DecodeTypeImmediateCOP1(instr);
+      break;  // Case COP1.
+    // ------------- REGIMM class.
+    case REGIMM:
+      DecodeTypeImmediateREGIMM(instr);
+      break;  // Case REGIMM.
+    // ------------- Branch instructions.
+    case BEQ:
+      Format(instr, "beq     'rs, 'rt, 'imm16u -> 'imm16p4s2");
+      break;
+    case BC:
+      Format(instr, "bc      'imm26s -> 'imm26p4s2");
+      break;
+    case BALC:
+      Format(instr, "balc    'imm26s -> 'imm26p4s2");
+      break;
+    case BNE:
+      Format(instr, "bne     'rs, 'rt, 'imm16u -> 'imm16p4s2");
+      break;
+    case BLEZ:
+      if ((instr->RtValue() == 0) && (instr->RsValue() != 0)) {
+        Format(instr, "blez    'rs, 'imm16u -> 'imm16p4s2");
+      } else if ((instr->RtValue() != instr->RsValue()) &&
+                 (instr->RsValue() != 0) && (instr->RtValue() != 0)) {
+        Format(instr, "bgeuc   'rs, 'rt, 'imm16u -> 'imm16p4s2");
+      } else if ((instr->RtValue() == instr->RsValue()) &&
+                 (instr->RtValue() != 0)) {
+        Format(instr, "bgezalc 'rs, 'imm16u -> 'imm16p4s2");
+      } else if ((instr->RsValue() == 0) && (instr->RtValue() != 0)) {
+        Format(instr, "blezalc 'rt, 'imm16u -> 'imm16p4s2");
+      } else {
+        UNREACHABLE();
+      }
+      break;
+    case BGTZ:
+      if ((instr->RtValue() == 0) && (instr->RsValue() != 0)) {
+        Format(instr, "bgtz    'rs, 'imm16u -> 'imm16p4s2");
+      } else if ((instr->RtValue() != instr->RsValue()) &&
+                 (instr->RsValue() != 0) && (instr->RtValue() != 0)) {
+        Format(instr, "bltuc   'rs, 'rt, 'imm16u -> 'imm16p4s2");
+      } else if ((instr->RtValue() == instr->RsValue()) &&
+                 (instr->RtValue() != 0)) {
+        Format(instr, "bltzalc 'rt, 'imm16u -> 'imm16p4s2");
+      } else if ((instr->RsValue() == 0) && (instr->RtValue() != 0)) {
+        Format(instr, "bgtzalc 'rt, 'imm16u -> 'imm16p4s2");
+      } else {
+        UNREACHABLE();
+      }
+      break;
+    case BLEZL:
+      if ((instr->RtValue() == instr->RsValue()) && (instr->RtValue() != 0)) {
+        Format(instr, "bgezc    'rt, 'imm16u -> 'imm16p4s2");
+      } else if ((instr->RtValue() != instr->RsValue()) &&
+                 (instr->RsValue() != 0) && (instr->RtValue() != 0)) {
+        Format(instr, "bgec     'rs, 'rt, 'imm16u -> 'imm16p4s2");
+      } else if ((instr->RsValue() == 0) && (instr->RtValue() != 0)) {
+        Format(instr, "blezc    'rt, 'imm16u -> 'imm16p4s2");
+      } else {
+        UNREACHABLE();
+      }
+      break;
+    case BGTZL:
+      if ((instr->RtValue() == instr->RsValue()) && (instr->RtValue() != 0)) {
+        Format(instr, "bltzc    'rt, 'imm16u -> 'imm16p4s2");
+      } else if ((instr->RtValue() != instr->RsValue()) &&
+                 (instr->RsValue() != 0) && (instr->RtValue() != 0)) {
+        Format(instr, "bltc    'rs, 'rt, 'imm16u -> 'imm16p4s2");
+      } else if ((instr->RsValue() == 0) && (instr->RtValue() != 0)) {
+        Format(instr, "bgtzc    'rt, 'imm16u -> 'imm16p4s2");
+      } else {
+        UNREACHABLE();
+      }
+      break;
+    case POP66:
+      if (instr->RsValue() == JIC) {
+        Format(instr, "jic     'rt, 'imm16s");
+      } else {
+        Format(instr, "beqzc   'rs, 'imm21s -> 'imm21p4s2");
+      }
+      break;
+    case POP76:
+      if (instr->RsValue() == JIALC) {
+        Format(instr, "jialc   'rt, 'imm16s");
+      } else {
+        Format(instr, "bnezc   'rs, 'imm21s -> 'imm21p4s2");
+      }
+      break;
+    // ------------- Arithmetic instructions.
+    case ADDI:
+      if (kArchVariant != kSw64r3) {
+        Format(instr, "addi    'rt, 'rs, 'imm16s");
+      } else {
+        int rs_reg = instr->RsValue();
+        int rt_reg = instr->RtValue();
+        // Check if BOVC, BEQZALC or BEQC instruction.
+        if (rs_reg >= rt_reg) {
+          Format(instr, "bovc  'rs, 'rt, 'imm16s -> 'imm16p4s2");
+        } else {
+          DCHECK_GT(rt_reg, 0);
+          if (rs_reg == 0) {
+            Format(instr, "beqzalc 'rt, 'imm16s -> 'imm16p4s2");
+          } else {
+            Format(instr, "beqc    'rs, 'rt, 'imm16s -> 'imm16p4s2");
+          }
+        }
+      }
+      break;
+    case DADDI:
+      if (kArchVariant != kSw64r3) {
+        Format(instr, "daddi   'rt, 'rs, 'imm16s");
+      } else {
+        int rs_reg = instr->RsValue();
+        int rt_reg = instr->RtValue();
+        // Check if BNVC, BNEZALC or BNEC instruction.
+        if (rs_reg >= rt_reg) {
+          Format(instr, "bnvc  'rs, 'rt, 'imm16s -> 'imm16p4s2");
+        } else {
+          DCHECK_GT(rt_reg, 0);
+          if (rs_reg == 0) {
+            Format(instr, "bnezalc 'rt, 'imm16s -> 'imm16p4s2");
+          } else {
+            Format(instr, "bnec  'rs, 'rt, 'imm16s -> 'imm16p4s2");
+          }
+        }
+      }
+      break;
+    case PCREL: {
+      int32_t imm21 = instr->Imm21Value();
+      // rt field: 5-bits checking
+      uint8_t rt = (imm21 >> kImm16Bits);
+      switch (rt) {
+        case ALUIPC:
+          Format(instr, "aluipc  'rs, 'imm16s");
+          break;
+        case AUIPC:
+          Format(instr, "auipc   'rs, 'imm16s");
+          break;
+        default: {
+          // rt field: checking of the most significant 3-bits
+          rt = (imm21 >> kImm18Bits);
+          switch (rt) {
+            case LDPC:
+              Format(instr, "ldpc    'rs, 'imm18s");
+              break;
+            default: {
+              // rt field: checking of the most significant 2-bits
+              rt = (imm21 >> kImm19Bits);
+              switch (rt) {
+                case LWUPC:
+                  Format(instr, "lwupc   'rs, 'imm19s");
+                  break;
+                case LWPC:
+                  Format(instr, "lwpc    'rs, 'imm19s");
+                  break;
+                case ADDIUPC:
+                  Format(instr, "addiupc 'rs, 'imm19s");
+                  break;
+                default:
+                  UNREACHABLE();
+                  break;
+              }
+              break;
+            }
+          }
+          break;
+        }
+      }
+      break;
+    }
+    case SPECIAL3:
+      DecodeTypeImmediateSPECIAL3(instr);
+      break;
+    case MSA:
+      switch (instr->MSAMinorOpcodeField()) {
+        case kMsaMinorI8:
+          DecodeTypeMsaI8(instr);
+          break;
+        case kMsaMinorI5:
+          DecodeTypeMsaI5(instr);
+          break;
+        case kMsaMinorI10:
+          DecodeTypeMsaI10(instr);
+          break;
+        case kMsaMinorELM:
+          DecodeTypeMsaELM(instr);
+          break;
+        case kMsaMinorBIT:
+          DecodeTypeMsaBIT(instr);
+          break;
+        case kMsaMinorMI10:
+          DecodeTypeMsaMI10(instr);
+          break;
+        default:
+          UNREACHABLE();
+          break;
+      }
+      break;
+    default:
+      printf("a 0x%x \n", instr->OpcodeFieldRaw());
+      UNREACHABLE();
+  }
+}
+
+
+void Decoder::DecodeTypeJump(Instruction* instr) {
+  switch (instr->OpcodeFieldRaw()) {
+    default:
+      UNREACHABLE();
+  }
+}
+
+void Decoder::DecodeTypeMsaI8(Instruction* instr) {
+  uint32_t opcode = instr->InstructionBits() & kMsaI8Mask;
+
+  switch (opcode) {
+    default:
+      UNREACHABLE();
+  }
+}
+
+void Decoder::DecodeTypeMsaI5(Instruction* instr) {
+  uint32_t opcode = instr->InstructionBits() & kMsaI5Mask;
+
+  switch (opcode) {
+    default:
+      UNREACHABLE();
+  }
+}
+
+void Decoder::DecodeTypeMsaI10(Instruction* instr) {
+  uint32_t opcode = instr->InstructionBits() & kMsaI5Mask;
+  if (opcode == LDI) {
+    Format(instr, "ldi.'t  'wd, 'imm10s1");
+  } else {
+    UNREACHABLE();
+  }
+}
+
+void Decoder::DecodeTypeMsaELM(Instruction* instr) {
+  uint32_t opcode = instr->InstructionBits() & kMsaELMMask;
+  switch (opcode) {
+    case SLDI:
+      if (instr->Bits(21, 16) == 0x3E) {
+        Format(instr, "ctcmsa  ");
+        PrintMSAControlRegister(instr->WdValue());
+        Print(", ");
+        PrintRegister(instr->WsValue());
+      } else {
+        Format(instr, "sldi.'t  'wd, 'ws['imme]");
+      }
+      break;
+    case SPLATI:
+      if (instr->Bits(21, 16) == 0x3E) {
+        Format(instr, "cfcmsa  ");
+        PrintRegister(instr->WdValue());
+        Print(", ");
+        PrintMSAControlRegister(instr->WsValue());
+      } else {
+        Format(instr, "splati.'t  'wd, 'ws['imme]");
+      }
+      break;
+    case COPY_S:
+      if (instr->Bits(21, 16) == 0x3E) {
+        Format(instr, "move.v  'wd, 'ws");
+      } else {
+        Format(instr, "copy_s.'t  ");
+        PrintMsaCopy(instr);
+      }
+      break;
+    case COPY_U:
+      Format(instr, "copy_u.'t  ");
+      PrintMsaCopy(instr);
+      break;
+    case INSERT:
+      Format(instr, "insert.'t  'wd['imme], ");
+      PrintRegister(instr->WsValue());
+      break;
+    case INSVE:
+      Format(instr, "insve.'t  'wd['imme], 'ws[0]");
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+void Decoder::DecodeTypeMsaBIT(Instruction* instr) {
+  uint32_t opcode = instr->InstructionBits() & kMsaBITMask;
+
+  switch (opcode) {
+    case SLLI:
+      Format(instr, "slli.'t  'wd, 'ws, 'immb");
+      break;
+    case SRAI:
+      Format(instr, "srai.'t  'wd, 'ws, 'immb");
+      break;
+    case SRLI:
+      Format(instr, "srli.'t  'wd, 'ws, 'immb");
+      break;
+    case BCLRI:
+      Format(instr, "bclri.'t  'wd, 'ws, 'immb");
+      break;
+    case BSETI:
+      Format(instr, "bseti.'t  'wd, 'ws, 'immb");
+      break;
+    case BNEGI:
+      Format(instr, "bnegi.'t  'wd, 'ws, 'immb");
+      break;
+    case BINSLI:
+      Format(instr, "binsli.'t  'wd, 'ws, 'immb");
+      break;
+    case BINSRI:
+      Format(instr, "binsri.'t  'wd, 'ws, 'immb");
+      break;
+    case SAT_S:
+      Format(instr, "sat_s.'t  'wd, 'ws, 'immb");
+      break;
+    case SAT_U:
+      Format(instr, "sat_u.'t  'wd, 'ws, 'immb");
+      break;
+    case SRARI:
+      Format(instr, "srari.'t  'wd, 'ws, 'immb");
+      break;
+    case SRLRI:
+      Format(instr, "srlri.'t  'wd, 'ws, 'immb");
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+void Decoder::DecodeTypeMsaMI10(Instruction* instr) {
+  uint32_t opcode = instr->InstructionBits() & kMsaMI10Mask;
+  if (opcode == MSA_LD) {
+    Format(instr, "ld.'t  'wd, 'imm10s2(");
+    PrintRegister(instr->WsValue());
+    Print(")");
+  } else if (opcode == MSA_ST) {
+    Format(instr, "st.'t  'wd, 'imm10s2(");
+    PrintRegister(instr->WsValue());
+    Print(")");
+  } else {
+    UNREACHABLE();
+  }
+}
+
+void Decoder::DecodeTypeMsa3R(Instruction* instr) {
+  uint32_t opcode = instr->InstructionBits() & kMsa3RMask;
+  switch (opcode) {
+    case SLD:
+      Format(instr, "sld.'t  'wd, 'ws['rt]");
+      break;
+    case SPLAT:
+      Format(instr, "splat.'t  'wd, 'ws['rt]");
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+void Decoder::DecodeTypeMsa3RF(Instruction* instr) {
+  uint32_t opcode = instr->InstructionBits() & kMsa3RFMask;
+  switch (opcode) {
+    default:
+      UNREACHABLE();
+  }
+}
+
+void Decoder::DecodeTypeMsaVec(Instruction* instr) {
+  uint32_t opcode = instr->InstructionBits() & kMsaVECMask;
+  switch (opcode) {
+    case AND_V:
+      Format(instr, "and.v  'wd, 'ws, 'wt");
+      break;
+    case OR_V:
+      Format(instr, "or.v  'wd, 'ws, 'wt");
+      break;
+    case NOR_V:
+      Format(instr, "nor.v  'wd, 'ws, 'wt");
+      break;
+    case XOR_V:
+      Format(instr, "xor.v  'wd, 'ws, 'wt");
+      break;
+    case BMNZ_V:
+      Format(instr, "bmnz.v  'wd, 'ws, 'wt");
+      break;
+    case BMZ_V:
+      Format(instr, "bmz.v  'wd, 'ws, 'wt");
+      break;
+    case BSEL_V:
+      Format(instr, "bsel.v  'wd, 'ws, 'wt");
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+void Decoder::DecodeTypeMsa2R(Instruction* instr) {
+  uint32_t opcode = instr->InstructionBits() & kMsa2RMask;
+  switch (opcode) {
+    case FILL: {
+      Format(instr, "fill.'t  'wd, ");
+      PrintRegister(instr->WsValue());  // rs value is in ws field
+    } break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+void Decoder::DecodeTypeMsa2RF(Instruction* instr) {
+  uint32_t opcode = instr->InstructionBits() & kMsa2RFMask;
+  switch (opcode) {
+    default:
+      UNREACHABLE();
+  }
+}
+
+
+#ifdef SW64 //jzy 20150213
+
+#define OP(x)           (((x) & 0x3F) << 26)
+#define OPR(oo,ff)      (OP(oo) | (((ff) & 0xFF) << 5))
+
+void Decoder::SwPrintRa(Instruction* instr){
+    int reg = instr->SwRaValue();
+    PrintRegister(reg);
+}
+void Decoder::SwPrintRb(Instruction* instr){
+    int reg = instr->SwRbValue();
+    PrintRegister(reg);
+}
+
+int Decoder::SwPrintRc(Instruction* instr, const char* format) {
+    int len, hi, lo;
+    len = SwGetInstructionRange(format, hi, lo);
+    int reg = instr->SwRcValue(hi, lo);
+    PrintRegister(reg);
+    return len;
+}
+
+void Decoder::SwPrintRd(Instruction* instr){
+    int reg = instr->SwRdValue();
+    PrintRegister(reg);
+}
+void Decoder::SwPrintFa(Instruction* instr){
+    int reg = instr->SwFaValue();
+    PrintFPURegister(reg);
+}
+void Decoder::SwPrintFb(Instruction* instr){
+    int reg = instr->SwFbValue();
+    PrintFPURegister(reg);
+}
+int  Decoder::SwPrintFc(Instruction* instr, const char* format){
+    int len, hi, lo;
+    len = SwGetInstructionRange(format, hi, lo);
+    int reg = instr->SwFcValue(hi, lo);
+    PrintFPURegister(reg);
+    return len;
+}
+void Decoder::SwPrintFd(Instruction* instr) {
+    int reg = instr->SwFdValue();
+    PrintFPURegister(reg);
+}
+
+
+int Decoder::SwPrintDisp(Instruction* instr, const char* format) {
+    int32_t len, hi, lo;
+    len = SwGetInstructionRange(format, hi, lo);
+
+    int32_t imm = instr->SwImmOrDispFieldValue(hi, lo);
+    out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "%d", imm);
+
+    return len;
+}
+//for transfer ld 20150320
+int Decoder::SwPrintDispTransfer(Instruction* instr, const char* format) {
+    int32_t len, hi, lo;
+    len = SwGetInstructionRange(format, hi, lo);
+
+    int32_t imm = instr->SwImmOrDispFieldValue(hi, lo);
+    //int64_t imm_transfer = imm*4+Instruction::kInstrSize+(*reinterpret_cast<int64_t*>(&instr_pc_));
+    void* imm_transfer = imm+1+reinterpret_cast<int32_t*>(instr_pc_);  //ld 20150429
+    out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, " %p", imm_transfer);
+
+    return len;
+}
+
+
+int Decoder::SwPrintImm(Instruction* instr, const char* format) {
+    int32_t len, hi, lo;
+    len = SwGetInstructionRange(format, hi, lo);
+
+    int32_t imm = instr->SwImmOrDispFieldRaw(hi, lo) >> lo;
+    out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "%u", imm);
+
+    return len;
+}
+
+
+//modified by cjq 
+/**change int to hex
+ * return length of format string
+ */
+int Decoder::SwPrintHex(Instruction* instr, const char* format){
+    int32_t len, hi, lo;
+    len = SwGetInstructionRange(format, hi, lo);
+              
+    int32_t imm = instr->SwImmOrDispFieldRaw(hi, lo) >> lo;
+    out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_, "0x%x", imm);
+
+    return len;
+}
+
+
+//imm(15-12) disp(25-0) retrun length(format)
+int Decoder::SwGetInstructionRange(const char* format, int& hi, int& lo) {
+    const char* p = format;
+    p = strstr(format, "(");
+    DCHECK('(' == *p );
+    hi = atoi(++p);
+    p = strstr(format, "-");
+    DCHECK('-' == *p );
+    lo = atoi(++p);
+    p = strstr(format, ")");
+    DCHECK(')' == *p );
+    return (int)(p-format+1);
+}
+
+
+/** modified by cjq 
+ *  *  add function to parse 'sys_call'
+ *   */
+void Decoder::SwDecodeTypeSyscall(Instruction* instr){
+    if (instr->OpcodeFieldValue() == op_sys_call){
+        return Format(instr, "sys_call '0x(25-0)");
+    }
+}
+
+void Decoder::SwDecodeTypeTransferance(Instruction* instr){//ld 20150319
+ switch(instr->OpcodeFieldValue()){
+        case op_br:
+            Format(instr,"br  'ra, 'tr_disp(20-0)");//ld 20150320
+            break;
+        case op_bsr:
+            Format(instr,"bsr 'ra, 'tr_disp(20-0)");
+            break;
+        case op_beq:
+            Format(instr,"beq 'ra, 'tr_disp(20-0)");
+            break;
+        case op_bne:
+            Format(instr,"bne 'ra, 'tr_disp(20-0)");
+            break;
+        case op_blt:
+            Format(instr,"blt 'ra, 'tr_disp(20-0)");
+            break;
+        case op_ble:
+            Format(instr,"ble 'ra, 'tr_disp(20-0)");
+            break;
+        case op_bgt:
+            Format(instr,"bgt 'ra, 'tr_disp(20-0)");
+            break;
+        case op_bge:
+            Format(instr,"bge 'ra, 'tr_disp(20-0)");
+            break;
+        case op_blbc:
+            Format(instr,"blbc 'ra, 'tr_disp(20-0)");
+            break;
+        case op_blbs:
+            Format(instr,"blbs 'ra, 'tr_disp(20-0)");
+            break;
+        case op_fbeq:
+            Format(instr,"fbeq 'fa, 'tr_disp(20-0)");
+            break;
+        case op_fbne:
+            Format(instr,"fbne 'fa, 'tr_disp(20-0)");
+            break;
+        case op_fblt:
+            Format(instr,"fblt 'fa, 'tr_disp(20-0)");
+            break;
+        case op_fble:
+            Format(instr,"fble 'fa, 'tr_disp(20-0)");
+            break;
+        case op_fbgt:
+            Format(instr,"fbgt 'fa, 'tr_disp(20-0)");
+            break;
+        case op_fbge:
+            Format(instr,"fbge 'fa, 'tr_disp(20-0)");
+            break;
+        default:
+            printf("a 0x%x \n", instr->OpcodeFieldRaw());
+            UNREACHABLE();
+    }
+}
+void Decoder::SwDecodeTypeStorage(Instruction* instr){
+    int opcode_func_raw;
+    switch (instr->OpcodeFieldValue()) {
+        case op_call:
+            Format(instr, "call 'ra, ('rb)");
+            break;
+        case op_ret:
+            Format(instr, "ret 'ra, ('rb)");
+            break;
+        case op_jmp:
+            Format(instr, "jmp 'ra, ('rb)");
+            break;
+
+        case op_ldbu:
+            Format(instr, "ldbu 'ra, 'disp(15-0)('rb)");
+            break;
+        case op_ldhu:
+            Format(instr, "ldhu 'ra, 'disp(15-0)('rb)");
+            break;
+        case op_ldw:
+            Format(instr, "ldw 'ra, 'disp(15-0)('rb)");
+            break;
+        case op_ldl:
+            Format(instr, "ldl 'ra, 'disp(15-0)('rb)");
+            break;
+        case op_ldl_u:
+            Format(instr, "ldl_u 'ra, 'disp(15-0)('rb)");
+            break;
+        case op_stb:
+            Format(instr, "stb 'ra, 'disp(15-0)('rb)");
+            break;
+        case op_sth:
+            Format(instr, "sth 'ra, 'disp(15-0)('rb)");
+            break;
+        case op_stw:
+            Format(instr, "stw 'ra, 'disp(15-0)('rb)");
+            break;
+        case op_stl:
+            Format(instr, "stl 'ra, 'disp(15-0)('rb)");
+            break;
+        case op_stl_u:
+            Format(instr, "stl_u 'ra, 'disp(15-0)('rb)");
+            break;
+        case op_ldi:
+            Format(instr, "ldi 'ra, 'disp(15-0)('rb)");
+            break;
+        case op_ldih:
+            Format(instr, "ldih 'ra, 'disp(15-0)('rb)");
+            break;
+
+        case op_flds:
+            Format(instr, "flds 'fa, 'disp(15-0)('rb)");
+            break;
+        case op_fldd:
+            Format(instr, "fldd 'fa, 'disp(15-0)('rb)");
+            break;
+        case op_fsts:
+            Format(instr, "fsts 'fa, 'disp(15-0)('rb)");
+            break;
+        case op_fstd:
+            Format(instr, "fstd 'fa, 'disp(15-0)('rb)");
+            break;
+
+        case op_ldwe:
+        case op_ldse:
+        case op_ldde:
+        case op_vlds:
+        case op_vldd:
+        case op_vsts:
+        case op_vstd:
+            UNIMPLEMENTED_SW64();
+            break;
+
+        case OP(0x08):
+        case OP(0x06): //
+            opcode_func_raw = instr->SwFunctionFieldRaw(15, 0) | instr->OpcodeFieldRaw();
+            switch (opcode_func_raw) {
+            case op_lldw:
+                Format(instr, "lldw  'ra, 'disp(11-0)('rb)");
+                break;
+            case op_lldl:
+                Format(instr, "lldl  'ra, 'disp(11-0)('rb)");
+                break;
+            case op_ldw_inc:
+                Format(instr, "ldw_inc  'ra, 'disp(11-0)('rb)");
+                break;
+            case op_ldl_inc:
+                Format(instr, "ldl_inc  'ra, 'disp(11-0)('rb)");
+                break;
+            case op_ldw_dec:
+                Format(instr, "ldw_dec  'ra, 'disp(11-0)('rb)");
+                break;
+            case op_ldl_dec:
+                Format(instr, "ldl_dec  'ra, 'disp(11-0)('rb)");
+                break;
+            case op_ldw_set:
+                Format(instr, "ldw_set  'ra, 'disp(11-0)('rb)");
+                break;
+            case op_ldl_set:
+                Format(instr, "ldl_set  'ra, 'disp(11-0)('rb)");
+                break;
+            case op_lstw:
+                Format(instr, "lstw  'ra, 'disp(11-0)('rb)");
+                break;
+            case op_lstl:
+                Format(instr, "lstl  'ra, 'disp(11-0)('rb)");
+                break;
+
+            case op_memb:
+                Format(instr, "memb");
+                break;
+            case op_rtc:
+                Format(instr, "rtc 'ra, 'rb");
+                break;
+            case op_rcid:
+                Format(instr, "rcid 'ra");
+                break;
+            case op_halt:
+                Format(instr, "halt");
+                break;
+            case op_rd_f:
+                Format(instr, "rd_f 'ra");
+                break;
+            case op_wr_f:
+                Format(instr, "wr_f, 'ra");
+                break;
+            default:
+                UNIMPLEMENTED_SW64();
+            }
+            break;
+
+        default:
+            printf("a 0x%x \n", instr->OpcodeFieldRaw());
+            UNREACHABLE();
+    }
+}
+void Decoder::SwDecodeTypeSimpleCalculation(Instruction* instr){
+    int simple_calculation_op = instr->SwFunctionFieldRaw(12, 5) | instr->OpcodeFieldValue();
+    switch (simple_calculation_op) {
+        case op_addw:
+            Format(instr, "addw 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_subw:
+            Format(instr, "subw 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_s4addw:
+            Format(instr, "s4addw 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_s4subw:
+            Format(instr, "s4subw 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_s8addw:
+            Format(instr, "s8addw 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_s8subw:
+            Format(instr, "s8subw 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_addl:
+            Format(instr, "addl 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_subl:
+            Format(instr, "subl 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_s4addl:
+            Format(instr, "s4addl 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_s4subl:
+            Format(instr, "s4subl 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_s8addl:
+            Format(instr, "s8addl 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_s8subl:
+            Format(instr, "s8subl 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_mulw:
+            Format(instr, "mulw 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_mull:
+            Format(instr, "mull 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_umulh:
+            Format(instr, "umulh 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_cmpeq:
+            Format(instr, "cmpeq 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_cmplt:
+            Format(instr, "cmplt 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_cmple:
+            Format(instr, "cmple 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_cmpult:
+            Format(instr, "cmpult 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_cmpule:
+            Format(instr, "cmpule 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_and:
+            Format(instr, "and 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_bic:
+            Format(instr, "bic 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_bis: //case op_or:
+            Format(instr, "or 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_ornot:
+            Format(instr, "ornot 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_xor:
+            Format(instr, "xor 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_eqv:
+            Format(instr, "eqv 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_inslb:
+            Format(instr, "inslb 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_inslh:
+            Format(instr, "inslh 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_inslw:
+            Format(instr, "inslw 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_insll:
+            Format(instr, "insll 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_inshb:
+            Format(instr, "inshb 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_inshh:
+            Format(instr, "inshh 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_inshw:
+            Format(instr, "inshw 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_inshl:
+            Format(instr, "inshl 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_slll:
+            Format(instr, "slll 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_srll:
+            Format(instr, "srll 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_sral:
+            Format(instr, "sral 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_extlb:
+            Format(instr, "extlb 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_extlh:
+            Format(instr, "extlh 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_extlw:
+            Format(instr, "extlw 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_extll:
+            Format(instr, "extll 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_exthb:
+            Format(instr, "exthb 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_exthh:
+            Format(instr, "exthh 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_exthw:
+            Format(instr, "exthw 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_exthl:
+            Format(instr, "exthl 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_ctpop:
+            Format(instr, "ctpop 'rb, 'rc(4-0)");
+            break;
+        case op_ctlz:
+            Format(instr, "ctlz 'rb, 'rc(4-0)");
+            break;
+        case op_cttz:
+            Format(instr, "cttz 'rb, 'rc(4-0)");
+            break;
+        case op_masklb:
+            Format(instr, "masklb 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_masklh:
+            Format(instr, "masklh 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_masklw:
+            Format(instr, "masklw 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_maskll:
+            Format(instr, "maskll 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_maskhb:
+            Format(instr, "maskhb 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_maskhh:
+            Format(instr, "maskhh 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_maskhw:
+            Format(instr, "maskhw 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_maskhl:
+            Format(instr, "maskhl 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_zap:
+            Format(instr, "zap 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_zapnot:
+            Format(instr, "zapnot 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_sextb:
+            Format(instr, "sextb 'rb, 'rc(4-0)");
+            break;
+        case op_sexth:
+            Format(instr, "sexth 'rb, 'rc(4-0)");
+            break;
+        case op_cmpgeb:
+            Format(instr, "cmpgeb 'ra, 'rb, 'rc(4-0)");
+            break;
+        case op_fimovs:
+            Format(instr, "fimovs 'fa, 'rc(4-0)");
+            break;
+        case op_fimovd:
+            Format(instr, "fimovd 'fa, 'rc(4-0)");
+            break;
+
+        case op_addw_l:  //?            
+            Format(instr, "addw 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_subw_l:
+            Format(instr, "subw 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_s4addw_l:
+            Format(instr, "s4addw 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_s4subw_l:
+            Format(instr, "s4subw 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_s8addw_l:
+            Format(instr, "s8addw 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_s8subw_l:
+            Format(instr, "s8subw 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_addl_l:
+            Format(instr, "addl 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_subl_l:
+            Format(instr, "subl 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_s4addl_l:
+            Format(instr, "s4addl 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_s4subl_l:
+            Format(instr, "s4subl 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_s8addl_l:
+            Format(instr, "s8addl 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_s8subl_l:
+            Format(instr, "s8subl 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_mulw_l:
+            Format(instr, "mulw 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_mull_l:
+            Format(instr, "mull 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_umulh_l:
+            Format(instr, "umulh 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_cmpeq_l:
+            Format(instr, "cmpeq 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_cmplt_l:
+            Format(instr, "cmplt 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_cmple_l:
+            Format(instr, "cmple 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_cmpult_l:
+            Format(instr, "cmpult 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_cmpule_l:
+            Format(instr, "cmpule 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_and_l:
+            Format(instr, "and 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_bic_l:
+            Format(instr, "bic 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_bis_l: //case op_or_l:
+            Format(instr, "or 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_ornot_l:
+            Format(instr, "ornot 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_xor_l:
+            Format(instr, "xor 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_eqv_l:
+            Format(instr, "eqv 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_inslb_l:
+            Format(instr, "inslb 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_inslh_l:
+            Format(instr, "inslh 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_inslw_l:
+            Format(instr, "inslw 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_insll_l:
+            Format(instr, "insll 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_inshb_l:
+            Format(instr, "inshb 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_inshh_l:
+            Format(instr, "inshh 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_inshw_l:
+            Format(instr, "inshw 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_inshl_l:
+            Format(instr, "inshl 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_slll_l:
+            Format(instr, "slll 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_srll_l:
+            Format(instr, "srll 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_sral_l:
+            Format(instr, "sral 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_extlb_l:
+            Format(instr, "extlb 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_extlh_l:
+            Format(instr, "extlh 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_extlw_l:
+            Format(instr, "extlw 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_extll_l:
+            Format(instr, "extll 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_exthb_l:
+            Format(instr, "exthb 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_exthh_l:
+            Format(instr, "exthh 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_exthw_l:
+            Format(instr, "exthw 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_exthl_l:
+            Format(instr, "exthl 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_masklb_l:
+            Format(instr, "masklb 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_masklh_l:
+            Format(instr, "masklh 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_masklw_l:
+            Format(instr, "masklw 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_maskll_l:
+            Format(instr, "maskll 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_maskhb_l:
+            Format(instr, "maskhb 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_maskhh_l:
+            Format(instr, "maskhh 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_maskhw_l:
+            Format(instr, "maskhw 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_maskhl_l:
+            Format(instr, "maskhl 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_zap_l:
+            Format(instr, "zap 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_zapnot_l:
+            Format(instr, "zapnot 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_sextb_l:
+            Format(instr, "sextb 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_sexth_l:
+            Format(instr, "sexth 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_cmpgeb_l:
+            Format(instr, "cmpgeb 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+
+        case op_fadds: //?            
+            Format(instr, "fadds 'fa, 'fb, 'fc(4-0)");
+            break;
+        case op_faddd:
+            Format(instr, "faddd 'fa, 'fb, 'fc(4-0)");
+            break;
+        case op_fsubs:
+            Format(instr, "fsubs 'fa, 'fb, 'fc(4-0)");
+            break;
+        case op_fsubd:
+            Format(instr, "fsubd 'fa, 'fb, 'fc(4-0)");
+            break;
+        case op_fmuls:
+            Format(instr, "fmuls 'fa, 'fb, 'fc(4-0)");
+            break;
+        case op_fmuld:
+            Format(instr, "fmuld 'fa, 'fb, 'fc(4-0)");
+            break;
+        case op_fdivs:
+            Format(instr, "fdivs 'fa, 'fb, 'fc(4-0)");
+            break;
+        case op_fdivd:
+            Format(instr, "fdivd 'fa, 'fb, 'fc(4-0)");
+            break;
+        case op_fsqrts:
+            Format(instr, "fsqrts 'fb, 'fc(4-0)");
+            break;
+        case op_fsqrtd:
+            Format(instr, "fsqrtd 'fb, 'fc(4-0)");
+            break;
+        case op_fcmpeq:
+            Format(instr, "fcmpeq 'fa, 'fb, 'fc(4-0)");
+            break;
+        case op_fcmple:
+            Format(instr, "fcmple 'fa, 'fb, 'fc(4-0)");
+            break;
+        case op_fcmplt:
+            Format(instr, "fcmplt 'fa, 'fb, 'fc(4-0)");
+            break;
+        case op_fcmpun:
+            Format(instr, "fcmpun 'fa, 'fb, 'fc(4-0)");
+            break;
+        case op_fcvtsd:
+            Format(instr, "fcvtsd 'fb, 'fc(4-0)");
+            break;
+        case op_fcvtds:
+            Format(instr, "fcvtds 'fb, 'fc(4-0)");
+            break;
+        case op_fcvtdl_g:
+            Format(instr, "fcvtdl_g 'fb, 'fc(4-0)");
+            break;
+        case op_fcvtdl_p:
+            Format(instr, "fcvtdl_p 'fb, 'fc(4-0)");
+            break;
+        case op_fcvtdl_z:
+            Format(instr, "fcvtdl_z 'fb, 'fc(4-0)");
+            break;
+        case op_fcvtdl_n:
+            Format(instr, "fcvtdl_n 'fb, 'fc(4-0)");
+            break;
+        case op_fcvtdl:
+            Format(instr, "fcvtdl 'fb, 'fc(4-0)");
+            break;
+        case op_fcvtwl:
+            Format(instr, "fcvtwl 'fb, 'fc(4-0)");
+            break;
+        case op_fcvtlw:
+            Format(instr, "fcvtlw 'fb, 'fc(4-0)");
+            break;
+        case op_fcvtls:
+            Format(instr, "fcvtls 'fb, 'fc(4-0)");
+            break;
+        case op_fcvtld:
+            Format(instr, "fcvtld 'fb, 'fc(4-0)");
+            break;
+        case op_fcpys:
+            Format(instr, "fcpys 'fa, 'fb, 'fc(4-0)");
+            break;
+        case op_fcpyse:
+            Format(instr, "fcpyse 'fa, 'fb, 'fc(4-0)");
+            break;
+        case op_fcpysn:
+            Format(instr, "fcpysn 'fa, 'fb, 'fc(4-0)");
+            break;
+        case op_ifmovs:
+            Format(instr, "ifmovs 'ra, 'fc(4-0)");
+            break;
+        case op_ifmovd:
+            Format(instr, "ifmovd 'ra, 'fc(4-0)");
+            break;
+        case op_rfpcr:
+            Format(instr, "rfpcr 'fa, FPCR");
+            break;
+        case op_wfpcr:
+            Format(instr, "wfpcr 'fa, FPCR");
+            break;
+        case op_setfpec0:
+            Format(instr, "setfpec0");
+            break;
+        case op_setfpec1:
+            Format(instr, "setfpec1");
+            break;
+        case op_setfpec2:
+            Format(instr, "setfpec2");
+            break;
+        case op_setfpec3:
+            Format(instr, "setfpec3");
+            break;
+
+    default:
+        printf("a 0x%x \n", instr->OpcodeFieldRaw());
+        UNREACHABLE();
+    }
+
+}
+void Decoder::SwDecodeTypeCompositeCalculation(Instruction* instr){
+    switch (instr->OpcodeFieldValue()) {
+        case OP(0x11)://
+        case OP(0x13)://
+            SwDecodeTypeCompositeCalculationInteger(instr);
+            break;
+        case OP(0x19)://
+            SwDecodeTypeCompositeCalculationFloatintPoint(instr);
+            break;
+    }
+}
+
+void Decoder::SwDecodeTypeCompositeCalculationInteger(Instruction* instr) {
+    int composite_calculation_op = instr->SwFunctionFieldRaw(12, 10) | instr->OpcodeFieldValue();
+    switch (composite_calculation_op) {
+        case op_seleq:
+            Format(instr, "seleq 'ra, 'rb, 'rc(9-5), 'rd");
+            break;
+        case op_selge:
+            Format(instr, "selge 'ra, 'rb, 'rc(9-5), 'rd");
+            break;
+        case op_selgt:
+            Format(instr, "selgt 'ra, 'rb, 'rc(9-5), 'rd");
+            break;
+        case op_selle:
+            Format(instr, "selle 'ra, 'rb, 'rc(9-5), 'rd");
+            break;
+        case op_sellt:
+            Format(instr, "sellt 'ra, 'rb, 'rc(9-5), 'rd");
+            break;
+        case op_selne:
+            Format(instr, "selne 'ra, 'rb, 'rc(9-5), 'rd");
+            break;
+        case op_sellbc:
+            Format(instr, "sellbc 'ra, 'rb, 'rc(9-5), 'rd");
+            break;
+        case op_sellbs:
+            Format(instr, "sellbs 'ra, 'rb, 'rc(9-5), 'rd");
+            break;
+        case op_seleq_l:
+            Format(instr, "seleq 'ra, 'imm(20-13), 'rc(9-5), 'rd");
+            break;
+        case op_selge_l:
+            Format(instr, "selge 'ra, 'imm(20-13), 'rc(9-5), 'rd");
+            break;
+        case op_selgt_l:
+            Format(instr, "selgt 'ra, 'imm(20-13), 'rc(9-5), 'rd");
+            break;
+        case op_selle_l:
+            Format(instr, "selle 'ra, 'imm(20-13), 'rc(9-5), 'rd");
+            break;
+        case op_sellt_l:
+            Format(instr, "sellt 'ra, 'imm(20-13), 'rc(9-5), 'rd");
+            break;
+        case op_selne_l:
+            Format(instr, "selne 'ra, 'imm(20-13), 'rc(9-5), 'rd");
+            break;
+        case op_sellbc_l:
+            Format(instr, "sellbc 'ra, 'imm(20-13), 'rc(9-5), 'rd");
+            break;
+        case op_sellbs_l:
+            Format(instr, "sellbs 'ra, 'imm(20-13), 'rc(9-5), 'rd");
+            break;
+
+        default:
+            printf("a 0x%x \n", instr->OpcodeFieldRaw());
+            UNREACHABLE();
+    }
+}
+
+void Decoder::SwDecodeTypeCompositeCalculationFloatintPoint(Instruction* instr) {
+    int composite_fp_calculation_op = instr->SwFunctionFieldRaw(15, 10) | instr->OpcodeFieldValue();
+    switch (composite_fp_calculation_op) {
+        case op_fmas:
+            Format(instr, "fmas 'fa, 'fb, 'fc(9-5), 'fd");
+            break;
+        case op_fmad:
+            Format(instr, "fmad 'fa, 'fb, 'fc(9-5), 'fd");
+            break;
+        case op_fmss:
+            Format(instr, "fmss 'fa, 'fb, 'fc(9-5), 'fd");
+            break;
+        case op_fmsd:
+            Format(instr, "fmsd 'fa, 'fb, 'fc(9-5), 'fd");
+            break;
+        case op_fnmas:
+            Format(instr, "fnmas 'fa, 'fb, 'fc(9-5), 'fd");
+            break;
+        case op_fnmad:
+            Format(instr, "fnmad 'fa, 'fb, 'fc(9-5), 'fd");
+            break;
+        case op_fnmss:
+            Format(instr, "fnmss 'fa, 'fb, 'fc(9-5), 'fd");
+            break;
+        case op_fnmsd:
+            Format(instr, "fnmsd 'fa, 'fb, 'fc(9-5), 'fd");
+            break;
+        case op_fseleq:
+            Format(instr, "fseleq 'fa, 'fb, 'fc(9-5), 'fd");
+            break;
+        case op_fselne:
+            Format(instr, "fselne 'fa, 'fb, 'fc(9-5), 'fd");
+            break;
+        case op_fsellt:
+            Format(instr, "fsellt 'fa, 'fb, 'fc(9-5), 'fd");
+            break;
+        case op_fselle:
+            Format(instr, "fselle 'fa, 'fb, 'fc(9-5), 'fd");
+            break;
+        case op_fselgt:
+            Format(instr, "fselgt 'fa, 'fb, 'fc(9-5), 'fd");
+            break;
+        case op_fselge:
+            Format(instr, "fselge 'fa, 'fb, 'fc(9-5), 'fd");
+            break;
+
+        default:
+            printf("a 0x%x \n", instr->OpcodeFieldRaw());
+            UNREACHABLE();
+
+    }
+}
+
+#undef OP
+#undef OPR
+#endif
+
+
+// Disassemble the instruction at *instr_ptr into the output buffer.
+// All instructions are one word long, except for the simulator
+// pseudo-instruction stop(msg). For that one special case, we return
+// size larger than one kInstrSize.
+int Decoder::InstructionDecode(byte* instr_ptr) {
+  Instruction* instr = Instruction::At(instr_ptr);
+  instr_pc_ = instr_ptr; //ld 20150323
+  // Print raw instruction bytes.
+  out_buffer_pos_ += SNPrintF(out_buffer_ + out_buffer_pos_,
+                              "%08x       ",
+                              instr->InstructionBits());
+  switch (instr->InstructionType()) {
+#ifdef SW64
+    case Instruction::kSwStorageType: {
+      SwDecodeTypeStorage(instr);
+      break;
+    }
+    case Instruction::kSwSimpleCalculationType: {
+      SwDecodeTypeSimpleCalculation(instr);
+      break;
+    }
+    case Instruction::kSwTransferanceType:{
+      SwDecodeTypeTransferance(instr);//ld 20150319
+      break;
+    }
+    case Instruction::kSwCompositeCalculationType: {
+      SwDecodeTypeCompositeCalculation(instr);
+      break;
+    }
+//cjq 20150317:TODO
+    case Instruction::kSwSyscallType:{
+      SwDecodeTypeSyscall(instr);
+      break;
+    }
+    case Instruction::kSwSimulatorTrap:{
+      Format(instr, "op_trap '0x(25-0)");
+      break;
+    }
+#endif
+    default: {
+      Format(instr, "UNSUPPORTED");
+      UNSUPPORTED_SW64();
+    }
+  }
+  return kInstrSize;
+}
+
+byte* Decoder::decode_instructions(byte* start, byte* end) {
+
+  // decode a series of instructions and return the end of the last instruction
+
+  return (byte*)
+    (*Decoder::_decode_instructions)(start, end,
+                                     NULL, (void*) this,
+                                     NULL, (void*) this,
+                                     NULL /*options()*/);
+}
+
+#ifdef SW64
+void*       Decoder::_library               = NULL;
+bool        Decoder::_tried_to_load_library = false;
+
+// This routine is in the shared library:
+Decoder::decode_func Decoder::_decode_instructions = NULL;
+
+//static const char hsdis_library_name[] = "hsdis-sw64";
+static const char decode_instructions_name[] = "decode_instructions";
+
+/* Used to protect dlsym() calls */
+static pthread_mutex_t dl_mutex;
+
+void* dll_load(const char *filename, char *ebuf, int ebuflen) {
+  void* result= dlopen(filename, RTLD_LAZY);
+  if (result != NULL) {
+    // Successful loading
+    return result;
+  }
+
+  return NULL;
+}
+
+void* dll_lookup(void* handle, const char* name) {
+  pthread_mutex_lock(&dl_mutex);
+  void* res = dlsym(handle, name);
+  pthread_mutex_unlock(&dl_mutex);
+  return res;
+}
+
+bool Decoder::load_library() {
+  if (_decode_instructions != NULL) {
+    // Already succeeded.
+    return true;
+  }
+
+  if (_tried_to_load_library) {
+    // Do not try twice.
+    // To force retry in debugger: assign _tried_to_load_library=0
+    return false;
+  }
+
+  // Try to load it.
+  //// v8::internal::Decoder d(converter_, buffer);
+  char ebuf[1024];
+  char buf[4096];
+  // Find the disassembler shared library.
+  // 4. hsdis-<arch>.so  (using LD_LIBRARY_PATH)
+  if (_library == NULL) {
+    // 4. hsdis-<arch>.so  (using LD_LIBRARY_PATH)
+    strcpy(&buf[0], "./hsdis-sw64.so");
+    _library = dll_load(buf, ebuf, sizeof ebuf);
+  }
+  if (_library != NULL) {
+    _decode_instructions = decode_func((byte*) (dll_lookup(_library, decode_instructions_name)) );
+  }
+  _tried_to_load_library = true;
+
+  if (_decode_instructions == NULL) {
+    v8::internal::PrintF("Could not load %s; %s\n", buf, ((_library != NULL) ? "entry point is missing"
+                                                                           : "library not loadable"));
+    return false;
+  }
+
+  // Success.
+  v8::internal::PrintF("Loaded disassembler from %s\n", buf);
+  return true;
+}
+#endif
+
+}  // namespace internal
+}  // namespace v8
+
+
+//------------------------------------------------------------------------------
+
+namespace disasm {
+
+const char* NameConverter::NameOfAddress(byte* addr) const {
+  v8::internal::SNPrintF(tmp_buffer_, "%p", static_cast<void*>(addr));
+  return tmp_buffer_.begin();
+}
+
+
+const char* NameConverter::NameOfConstant(byte* addr) const {
+  return NameOfAddress(addr);
+}
+
+
+const char* NameConverter::NameOfCPURegister(int reg) const {
+  return v8::internal::Registers::Name(reg);
+}
+
+
+const char* NameConverter::NameOfXMMRegister(int reg) const {
+  return v8::internal::FPURegisters::Name(reg);
+}
+
+
+const char* NameConverter::NameOfByteCPURegister(int reg) const {
+  UNREACHABLE();  // SW64 does not have the concept of a byte register.
+  return "nobytereg";
+}
+
+
+const char* NameConverter::NameInCode(byte* addr) const {
+  // The default name converter is called for unknown code. So we will not try
+  // to access any memory.
+  return "";
+}
+
+
+//------------------------------------------------------------------------------
+
+int Disassembler::InstructionDecode(v8::internal::Vector<char> buffer,
+                                    byte* instruction) {
+  v8::internal::Decoder d(converter_, buffer);
+#if 0 //def SW64  //20181029
+  if( d.load_library() ) {
+    d.decode_instructions(instruction, instruction);
+  } 
+  return 4 /*Instruction::kInstrSize*/;
+#else
+  return d.InstructionDecode(instruction);
+#endif
+}
+
+
+// The SW64 assembler does not currently use constant pools.
+int Disassembler::ConstantPoolSizeAt(byte* instruction) {
+  return -1;
+}
+
+
+void Disassembler::Disassemble(FILE* f, byte* begin, byte* end,
+                               UnimplementedOpcodeAction unimplemented_action) {
+  NameConverter converter;
+  Disassembler d(converter, unimplemented_action);
+  for (byte* pc = begin; pc < end;) {
+    v8::internal::EmbeddedVector<char, 128> buffer;
+    buffer[0] = '\0';
+    byte* prev_pc = pc;
+    pc += d.InstructionDecode(buffer, pc);
+    v8::internal::PrintF(f, "%p    %08x      %s\n", static_cast<void*>(prev_pc),
+                         *reinterpret_cast<int32_t*>(prev_pc), buffer.begin());
+  }
+}
+
+#undef STRING_STARTS_WITH
+
+}  // namespace disasm
+
+#endif  // V8_TARGET_ARCH_SW64
diff --git a/src/3rdparty/chromium/v8/src/execution/frame-constants.h b/src/3rdparty/chromium/v8/src/execution/frame-constants.h
index 6c037451a2..91955bc1b7 100644
--- a/src/3rdparty/chromium/v8/src/execution/frame-constants.h
+++ b/src/3rdparty/chromium/v8/src/execution/frame-constants.h
@@ -356,6 +356,8 @@ inline static int FrameSlotToFPOffset(int slot) {
 #include "src/execution/mips64/frame-constants-mips64.h"  // NOLINT
 #elif V8_TARGET_ARCH_S390
 #include "src/execution/s390/frame-constants-s390.h"  // NOLINT
+#elif V8_TARGET_ARCH_SW64
+#include "src/execution/sw64/frame-constants-sw64.h"  // NOLINT
 #else
 #error Unsupported target architecture.
 #endif
diff --git a/src/3rdparty/chromium/v8/src/execution/simulator.h b/src/3rdparty/chromium/v8/src/execution/simulator.h
index 74763474c6..ec692a9618 100644
--- a/src/3rdparty/chromium/v8/src/execution/simulator.h
+++ b/src/3rdparty/chromium/v8/src/execution/simulator.h
@@ -26,6 +26,8 @@
 #include "src/execution/mips64/simulator-mips64.h"
 #elif V8_TARGET_ARCH_S390
 #include "src/execution/s390/simulator-s390.h"
+#elif V8_TARGET_ARCH_SW64
+#include "src/execution/sw64/simulator-sw64.h"
 #else
 #error Unsupported target architecture.
 #endif
diff --git a/src/3rdparty/chromium/v8/src/execution/sw64/frame-constants-sw64.cc b/src/3rdparty/chromium/v8/src/execution/sw64/frame-constants-sw64.cc
new file mode 100755
index 0000000000..d52a4e3e52
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/execution/sw64/frame-constants-sw64.cc
@@ -0,0 +1,32 @@
+// Copyright 2011 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#if V8_TARGET_ARCH_SW64
+
+#include "src/codegen/sw64/assembler-sw64-inl.h"
+#include "src/execution/frame-constants.h"
+#include "src/execution/frames.h"
+
+#include "src/execution/sw64/frame-constants-sw64.h"
+
+namespace v8 {
+namespace internal {
+
+Register JavaScriptFrame::fp_register() { return v8::internal::fp; }
+Register JavaScriptFrame::context_register() { return cp; }
+Register JavaScriptFrame::constant_pool_pointer_register() { UNREACHABLE(); }
+
+int InterpreterFrameConstants::RegisterStackSlotCount(int register_count) {
+  return register_count;
+}
+
+int BuiltinContinuationFrameConstants::PaddingSlotCount(int register_count) {
+  USE(register_count);
+  return 0;
+}
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_TARGET_ARCH_SW64
diff --git a/src/3rdparty/chromium/v8/src/execution/sw64/frame-constants-sw64.h b/src/3rdparty/chromium/v8/src/execution/sw64/frame-constants-sw64.h
new file mode 100755
index 0000000000..96b1f2ac63
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/execution/sw64/frame-constants-sw64.h
@@ -0,0 +1,78 @@
+// Copyright 2011 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef V8_EXECUTION_SW64_FRAME_CONSTANTS_SW64_H_
+#define V8_EXECUTION_SW64_FRAME_CONSTANTS_SW64_H_
+
+#include "src/base/bits.h"
+#include "src/base/macros.h"
+#include "src/execution/frame-constants.h"
+
+namespace v8 {
+namespace internal {
+
+class EntryFrameConstants : public AllStatic {
+ public:
+  // This is the offset to where JSEntry pushes the current value of
+  // Isolate::c_entry_fp onto the stack.
+  static constexpr int kCallerFPOffset = -3 * kSystemPointerSize;
+};
+
+class WasmCompileLazyFrameConstants : public TypedFrameConstants {
+ public:
+  static constexpr int kNumberOfSavedGpParamRegs = 6;
+  static constexpr int kNumberOfSavedFpParamRegs = 6;
+  static constexpr int kNumberOfSavedAllParamRegs = 12;
+
+  // FP-relative.
+  // See Generate_WasmCompileLazy in builtins-sw64.cc.
+  static constexpr int kWasmInstanceOffset =
+      TYPED_FRAME_PUSHED_VALUE_OFFSET(kNumberOfSavedAllParamRegs);
+  static constexpr int kFixedFrameSizeFromFp =
+      TypedFrameConstants::kFixedFrameSizeFromFp +
+      kNumberOfSavedGpParamRegs * kPointerSize +
+      kNumberOfSavedFpParamRegs * kDoubleSize;
+};
+
+// Frame constructed by the {WasmDebugBreak} builtin.
+// After pushing the frame type marker, the builtin pushes all Liftoff cache
+// registers (see liftoff-assembler-defs.h).
+class WasmDebugBreakFrameConstants : public TypedFrameConstants {
+ public:
+  // {a0, a1, a2, a3, a4, a5, t0, t1, t2, t3, t4, t9, t10, s5, v0}
+  static constexpr uint32_t kPushedGpRegs = 0b1101111110100000000111111;
+  // {f0, f1, f2, f3, f4, f5, f6, f7, f8, f9, f10, f11, f12, f13, f14, f15, f16, f17, f18, f19, f20, f21,
+  // f22, f23, f24, f25, f26}
+  static constexpr uint32_t kPushedFpRegs = 0b111111111111111111111111111;
+
+  static constexpr int kNumPushedGpRegisters =
+      base::bits::CountPopulation(kPushedGpRegs);
+  static constexpr int kNumPushedFpRegisters =
+      base::bits::CountPopulation(kPushedFpRegs);
+
+  static constexpr int kLastPushedGpRegisterOffset =
+      -kFixedFrameSizeFromFp - kNumPushedGpRegisters * kSystemPointerSize;
+  static constexpr int kLastPushedFpRegisterOffset =
+      kLastPushedGpRegisterOffset - kNumPushedFpRegisters * kDoubleSize;
+
+  // Offsets are fp-relative.
+  static int GetPushedGpRegisterOffset(int reg_code) {
+    DCHECK_NE(0, kPushedGpRegs & (1 << reg_code));
+    uint32_t lower_regs = kPushedGpRegs & ((uint32_t{1} << reg_code) - 1);
+    return kLastPushedGpRegisterOffset +
+           base::bits::CountPopulation(lower_regs) * kSystemPointerSize;
+  }
+
+  static int GetPushedFpRegisterOffset(int reg_code) {
+    DCHECK_NE(0, kPushedFpRegs & (1 << reg_code));
+    uint32_t lower_regs = kPushedFpRegs & ((uint32_t{1} << reg_code) - 1);
+    return kLastPushedFpRegisterOffset +
+           base::bits::CountPopulation(lower_regs) * kDoubleSize;
+  }
+};
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_SW64_FRAME_CONSTANTS_SW64_H_
diff --git a/src/3rdparty/chromium/v8/src/execution/sw64/simulator-sw64.cc b/src/3rdparty/chromium/v8/src/execution/sw64/simulator-sw64.cc
new file mode 100755
index 0000000000..fe50a90026
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/execution/sw64/simulator-sw64.cc
@@ -0,0 +1,8980 @@
+// Copyright 2011 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#include "src/execution/sw64/simulator-sw64.h"
+
+// Only build the simulator if not compiling for real SW64 hardware.
+#if defined(USE_SIMULATOR)
+
+#include <limits.h>
+#include <stdarg.h>
+#include <stdlib.h>
+#include <cmath>
+
+#include "src/base/bits.h"
+#include "src/codegen/assembler-inl.h"
+#include "src/codegen/macro-assembler.h"
+#include "src/codegen/sw64/constants-sw64.h"
+#include "src/diagnostics/disasm.h"
+#include "src/heap/combined-heap.h"
+#include "src/runtime/runtime-utils.h"
+#include "src/utils/ostreams.h"
+#include "src/utils/vector.h"
+
+#define IsSw64SoftFloatABI 0
+enum {
+    SW64_OVI0_BIT = 57,
+    SW64_INE0_BIT = 56,
+    SW64_UNF0_BIT = 55,
+    SW64_OVF0_BIT = 54,
+    SW64_DZE0_BIT = 53,
+    SW64_INV0_BIT = 52,
+};
+
+namespace v8 {
+namespace internal {
+
+// Util functions.
+inline bool HaveSameSign(int64_t a, int64_t b) { return ((a ^ b) >= 0); }
+
+uint32_t get_fcsr_condition_bit(uint32_t cc) {
+  if (cc == 0) {
+    return 23;
+  } else {
+    return 24 + cc;
+  }
+}
+
+
+//static int64_t MultiplyHighSigned(int64_t u, int64_t v) {
+//  uint64_t u0, v0, w0;
+//  int64_t u1, v1, w1, w2, t;
+
+//  u0 = u & 0xFFFFFFFFL;
+//  u1 = u >> 32;
+//  v0 = v & 0xFFFFFFFFL;
+//  v1 = v >> 32;
+
+//  w0 = u0 * v0;
+//  t = u1 * v0 + (w0 >> 32);
+//  w1 = t & 0xFFFFFFFFL;
+//  w2 = t >> 32;
+//  w1 = u0 * v1 + w1;
+
+//  return u1 * v1 + w2 + (w1 >> 32);
+//}
+
+
+// This macro provides a platform independent use of sscanf. The reason for
+// SScanF not being implemented in a platform independent was through
+// ::v8::internal::OS in the same way as SNPrintF is that the Windows C Run-Time
+// Library does not provide vsscanf.
+#define SScanF sscanf  // NOLINT
+
+// The Sw64Debugger class is used by the simulator while debugging simulated
+// code.
+class Sw64Debugger {
+ public:
+  explicit Sw64Debugger(Simulator* sim) : sim_(sim) { }
+
+  void Stop(Instruction* instr);
+  void Debug();
+  // Print all registers with a nice formatting.
+  void PrintAllRegs();
+  void PrintAllRegsIncludingFPU();
+
+ private:
+  // We set the breakpoint code to 0xFFFFF to easily recognize it.
+  static const Instr kBreakpointInstr = op_trap | BREAK;
+  static const Instr kNopInstr =  0x0;
+
+  Simulator* sim_;
+
+  int64_t GetRegisterValue(int regnum);
+  int64_t GetFPURegisterValue(int regnum);
+  float GetFPURegisterValueFloat(int regnum);
+  double GetFPURegisterValueDouble(int regnum);
+  bool GetValue(const char* desc, int64_t* value);
+
+  // Set or delete a breakpoint. Returns true if successful.
+  bool SetBreakpoint(Instruction* breakpc);
+  bool DeleteBreakpoint(Instruction* breakpc);
+
+  // Undo and redo all breakpoints. This is needed to bracket disassembly and
+  // execution to skip past breakpoints when run from the debugger.
+  void UndoBreakpoints();
+  void RedoBreakpoints();
+};
+
+inline void UNSUPPORTED() { printf("Sim: Unsupported instruction.\n"); }
+
+void Sw64Debugger::Stop(Instruction* instr) {
+  // Get the stop code.
+  uint32_t code = instr->Bits(25, 6);
+  PrintF("Simulator hit (%u)\n", code);
+  Debug();
+}
+
+int64_t Sw64Debugger::GetRegisterValue(int regnum) {
+  if (regnum == kNumSimuRegisters) {
+    return sim_->get_pc();
+  } else {
+    return sim_->get_register(regnum);
+  }
+}
+
+
+int64_t Sw64Debugger::GetFPURegisterValue(int regnum) {
+  if (regnum == kNumFPURegisters) {
+    return sim_->get_pc();
+  } else {
+    return sim_->get_fpu_register(regnum);
+  }
+}
+
+
+float Sw64Debugger::GetFPURegisterValueFloat(int regnum) {
+  if (regnum == kNumFPURegisters) {
+    return sim_->get_pc();
+  } else {
+    return sim_->get_fpu_register_float(regnum);
+  }
+}
+
+
+double Sw64Debugger::GetFPURegisterValueDouble(int regnum) {
+  if (regnum == kNumFPURegisters) {
+    return sim_->get_pc();
+  } else {
+    return sim_->get_fpu_register_double(regnum);
+  }
+}
+
+
+bool Sw64Debugger::GetValue(const char* desc, int64_t* value) {
+  int regnum = Registers::Number(desc);
+  int fpuregnum = FPURegisters::Number(desc);
+
+  if (regnum != kInvalidRegister) {
+    *value = GetRegisterValue(regnum);
+    return true;
+  } else if (fpuregnum != kInvalidFPURegister) {
+    *value = GetFPURegisterValue(fpuregnum);
+    return true;
+  } else if (strncmp(desc, "0x", 2) == 0) {
+    return SScanF(desc + 2, "%" SCNx64,
+                  reinterpret_cast<uint64_t*>(value)) == 1;
+  } else {
+    return SScanF(desc, "%" SCNu64, reinterpret_cast<uint64_t*>(value)) == 1;
+  }
+  return false;
+}
+
+
+bool Sw64Debugger::SetBreakpoint(Instruction* breakpc) {
+  // Check if a breakpoint can be set. If not return without any side-effects.
+  if (sim_->break_pc_ != nullptr) {
+    return false;
+  }
+
+  // Set the breakpoint.
+  sim_->break_pc_ = breakpc;
+  sim_->break_instr_ = breakpc->InstructionBits();
+  // Not setting the breakpoint instruction in the code itself. It will be set
+  // when the debugger shell continues.
+  return true;
+}
+
+
+bool Sw64Debugger::DeleteBreakpoint(Instruction* breakpc) {
+  if (sim_->break_pc_ != nullptr) {
+    sim_->break_pc_->SetInstructionBits(sim_->break_instr_);
+  }
+
+  sim_->break_pc_ = nullptr;
+  sim_->break_instr_ = 0;
+  return true;
+}
+
+
+void Sw64Debugger::UndoBreakpoints() {
+  if (sim_->break_pc_ != nullptr) {
+    sim_->break_pc_->SetInstructionBits(sim_->break_instr_);
+  }
+}
+
+
+void Sw64Debugger::RedoBreakpoints() {
+  if (sim_->break_pc_ != nullptr) {
+    mprotect((void*)((uint64_t)sim_->break_pc_ & ~((uint64_t)4095)), 4096, PROT_WRITE);
+    sim_->break_pc_->SetInstructionBits(kBreakpointInstr);
+  }
+}
+
+
+void Sw64Debugger::PrintAllRegs() {
+#define REG_INFO(n) Registers::Name(n), GetRegisterValue(n), GetRegisterValue(n)
+
+  PrintF("\n");
+  // at, v0, a0.
+  PrintF("%3s: 0x%016" PRIx64 " %14" PRId64 "\t%3s: 0x%016" PRIx64 " %14" PRId64
+         "\t%3s: 0x%016" PRIx64 " %14" PRId64 "\n",
+         REG_INFO(1), REG_INFO(2), REG_INFO(4));
+  // v1, a1.
+  PrintF("%34s\t%3s: 0x%016" PRIx64 "  %14" PRId64 " \t%3s: 0x%016" PRIx64
+         "  %14" PRId64 " \n",
+         "", REG_INFO(3), REG_INFO(5));
+  // a2.
+  PrintF("%34s\t%34s\t%3s: 0x%016" PRIx64 "  %14" PRId64 " \n", "", "",
+         REG_INFO(6));
+  // a3.
+  PrintF("%34s\t%34s\t%3s: 0x%016" PRIx64 "  %14" PRId64 " \n", "", "",
+         REG_INFO(7));
+  PrintF("\n");
+  // a4-t3, s0-s7
+  for (int i = 0; i < 8; i++) {
+    PrintF("%3s: 0x%016" PRIx64 "  %14" PRId64 " \t%3s: 0x%016" PRIx64
+           "  %14" PRId64 " \n",
+           REG_INFO(8 + i), REG_INFO(16 + i));
+  }
+  PrintF("\n");
+  // t8, k0, LO.
+  PrintF("%3s: 0x%016" PRIx64 "  %14" PRId64 " \t%3s: 0x%016" PRIx64
+         "  %14" PRId64 " \t%3s: 0x%016" PRIx64 "  %14" PRId64 " \n",
+         REG_INFO(24), REG_INFO(26), REG_INFO(32));
+  // t9, k1, HI.
+  PrintF("%3s: 0x%016" PRIx64 "  %14" PRId64 " \t%3s: 0x%016" PRIx64
+         "  %14" PRId64 " \t%3s: 0x%016" PRIx64 "  %14" PRId64 " \n",
+         REG_INFO(25), REG_INFO(27), REG_INFO(33));
+  // sp, fp, gp.
+  PrintF("%3s: 0x%016" PRIx64 "  %14" PRId64 " \t%3s: 0x%016" PRIx64
+         "  %14" PRId64 " \t%3s: 0x%016" PRIx64 "  %14" PRId64 " \n",
+         REG_INFO(29), REG_INFO(30), REG_INFO(28));
+  // pc.
+  PrintF("%3s: 0x%016" PRIx64 "  %14" PRId64 " \t%3s: 0x%016" PRIx64
+         "  %14" PRId64 " \n",
+         REG_INFO(31), REG_INFO(34));
+
+#undef REG_INFO
+#undef FPU_REG_INFO
+}
+
+
+void Sw64Debugger::PrintAllRegsIncludingFPU() {
+#define FPU_REG_INFO(n) FPURegisters::Name(n), \
+        GetFPURegisterValue(n), \
+        GetFPURegisterValueDouble(n)
+
+  PrintAllRegs();
+
+  PrintF("\n\n");
+  // f0, f1, f2, ... f31.
+  // TODO(plind): consider printing 2 columns for space efficiency.
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(0));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(1));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(2));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(3));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(4));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(5));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(6));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(7));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(8));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(9));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(10));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(11));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(12));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(13));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(14));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(15));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(16));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(17));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(18));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(19));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(20));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(21));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(22));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(23));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(24));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(25));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(26));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(27));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(28));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(29));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(30));
+  PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n", FPU_REG_INFO(31));
+
+#undef REG_INFO
+#undef FPU_REG_INFO
+}
+
+
+void Sw64Debugger::Debug() {
+  intptr_t last_pc = -1;
+  bool done = false;
+
+#define COMMAND_SIZE 63
+#define ARG_SIZE 255
+
+#define STR(a) #a
+#define XSTR(a) STR(a)
+
+  char cmd[COMMAND_SIZE + 1];
+  char arg1[ARG_SIZE + 1];
+  char arg2[ARG_SIZE + 1];
+  char* argv[3] = { cmd, arg1, arg2 };
+
+  // Make sure to have a proper terminating character if reaching the limit.
+  cmd[COMMAND_SIZE] = 0;
+  arg1[ARG_SIZE] = 0;
+  arg2[ARG_SIZE] = 0;
+
+  // Undo all set breakpoints while running in the debugger shell. This will
+  // make them invisible to all commands.
+  UndoBreakpoints();
+
+  while (!done && (sim_->get_pc() != Simulator::end_sim_pc)) {
+    if (last_pc != sim_->get_pc()) {
+      disasm::NameConverter converter;
+      disasm::Disassembler dasm(converter);
+      // Use a reasonably large buffer.
+      v8::internal::EmbeddedVector<char, 256> buffer;
+      dasm.InstructionDecode(buffer, reinterpret_cast<byte*>(sim_->get_pc()));
+      PrintF("  0x%016" PRIx64 "   %s\n", sim_->get_pc(), buffer.begin());
+      last_pc = sim_->get_pc();
+    }
+    char* line = ReadLine("sim> ");
+    if (line == nullptr) {
+      break;
+    } else {
+      char* last_input = sim_->last_debugger_input();
+      if (strcmp(line, "\n") == 0 && last_input != nullptr) {
+        line = last_input;
+      } else {
+        // Ownership is transferred to sim_;
+        sim_->set_last_debugger_input(line);
+      }
+      // Use sscanf to parse the individual parts of the command line. At the
+      // moment no command expects more than two parameters.
+      int argc = SScanF(line,
+                        "%" XSTR(COMMAND_SIZE) "s "
+                        "%" XSTR(ARG_SIZE) "s "
+                        "%" XSTR(ARG_SIZE) "s",
+                        cmd, arg1, arg2);
+      if ((strcmp(cmd, "si") == 0) || (strcmp(cmd, "stepi") == 0)) {
+        Instruction* instr = reinterpret_cast<Instruction*>(sim_->get_pc());
+        if (!(instr->IsTrap()) ||
+            instr->InstructionBits() == rtCallRedirInstr) {
+          sim_->InstructionDecode(
+              reinterpret_cast<Instruction*>(sim_->get_pc()));
+        } else {
+          // Allow si to jump over generated breakpoints.
+          PrintF("/!\\ Jumping over generated breakpoint.\n");
+          sim_->set_pc(sim_->get_pc() + kInstrSize);
+        }
+      } else if ((strcmp(cmd, "c") == 0) || (strcmp(cmd, "cont") == 0)) {
+        // Execute the one instruction we broke at with breakpoints disabled.
+        sim_->InstructionDecode(reinterpret_cast<Instruction*>(sim_->get_pc()));
+        // Leave the debugger shell.
+        done = true;
+      } else if ((strcmp(cmd, "p") == 0) || (strcmp(cmd, "print") == 0)) {
+        if (argc == 2) {
+          int64_t value;
+          double dvalue;
+          if (strcmp(arg1, "all") == 0) {
+            PrintAllRegs();
+          } else if (strcmp(arg1, "allf") == 0) {
+            PrintAllRegsIncludingFPU();
+          } else {
+            int regnum = Registers::Number(arg1);
+            int fpuregnum = FPURegisters::Number(arg1);
+
+            if (regnum != kInvalidRegister) {
+              value = GetRegisterValue(regnum);
+              PrintF("%s: 0x%08" PRIx64 "  %" PRId64 "  \n", arg1, value,
+                     value);
+            } else if (fpuregnum != kInvalidFPURegister) {
+              value = GetFPURegisterValue(fpuregnum);
+              dvalue = GetFPURegisterValueDouble(fpuregnum);
+              PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n",
+                     FPURegisters::Name(fpuregnum), value, dvalue);
+            } else {
+              PrintF("%s unrecognized\n", arg1);
+            }
+          }
+        } else {
+          if (argc == 3) {
+            if (strcmp(arg2, "single") == 0) {
+              int64_t value;
+              float fvalue;
+              int fpuregnum = FPURegisters::Number(arg1);
+
+              if (fpuregnum != kInvalidFPURegister) {
+                value = GetFPURegisterValue(fpuregnum);
+                value &= 0xFFFFFFFFUL;
+                fvalue = GetFPURegisterValueFloat(fpuregnum);
+                PrintF("%s: 0x%08" PRIx64 "  %11.4e\n", arg1, value, fvalue);
+              } else {
+                PrintF("%s unrecognized\n", arg1);
+              }
+            } else {
+              PrintF("print <fpu register> single\n");
+            }
+          } else {
+            PrintF("print <register> or print <fpu register> single\n");
+          }
+        }
+      } else if ((strcmp(cmd, "po") == 0)
+                 || (strcmp(cmd, "printobject") == 0)) {
+        if (argc == 2) {
+          int64_t value;
+          StdoutStream os;
+          if (GetValue(arg1, &value)) {
+            Object obj(value);
+            os << arg1 << ": \n";
+#ifdef DEBUG
+            obj.Print(os);
+            os << "\n";
+#else
+            os << Brief(obj) << "\n";
+#endif
+          } else {
+            os << arg1 << " unrecognized\n";
+          }
+        } else {
+          PrintF("printobject <value>\n");
+        }
+      } else if (strcmp(cmd, "stack") == 0 || strcmp(cmd, "mem") == 0 ||
+                 strcmp(cmd, "dump") == 0) {
+        int64_t* cur = nullptr;
+        int64_t* end = nullptr;
+        int next_arg = 1;
+
+        if (strcmp(cmd, "stack") == 0) {
+          cur = reinterpret_cast<int64_t*>(sim_->get_register(Simulator::sp));
+        } else {  // Command "mem".
+          int64_t value;
+          if (!GetValue(arg1, &value)) {
+            PrintF("%s unrecognized\n", arg1);
+            continue;
+          }
+          cur = reinterpret_cast<int64_t*>(value);
+          next_arg++;
+        }
+
+        int64_t words;
+        if (argc == next_arg) {
+          words = 10;
+        } else {
+          if (!GetValue(argv[next_arg], &words)) {
+            words = 10;
+          }
+        }
+        end = cur + words;
+
+        bool skip_obj_print = (strcmp(cmd, "dump") == 0);
+        while (cur < end) {
+          PrintF("  0x%012" PRIxPTR " :  0x%016" PRIx64 "  %14" PRId64 " ",
+                 reinterpret_cast<intptr_t>(cur), *cur, *cur);
+          Object obj(*cur);
+          Heap* current_heap = sim_->isolate_->heap();
+          if (!skip_obj_print) {
+            if (obj.IsSmi() ||
+                IsValidHeapObject(current_heap, HeapObject::cast(obj))) {
+            PrintF(" (");
+            if (obj.IsSmi()) {
+              PrintF("smi %d", Smi::ToInt(obj));
+            } else {
+                obj.ShortPrint();
+            }
+            PrintF(")");
+          }
+          }
+          PrintF("\n");
+          cur++;
+        }
+
+      } else if ((strcmp(cmd, "disasm") == 0) ||
+                 (strcmp(cmd, "dpc") == 0) ||
+                 (strcmp(cmd, "di") == 0)) {
+        disasm::NameConverter converter;
+        disasm::Disassembler dasm(converter);
+        // Use a reasonably large buffer.
+        v8::internal::EmbeddedVector<char, 256> buffer;
+
+        byte* cur = nullptr;
+        byte* end = nullptr;
+
+        if (argc == 1) {
+          cur = reinterpret_cast<byte*>(sim_->get_pc());
+          end = cur + (10 * kInstrSize);
+        } else if (argc == 2) {
+          int regnum = Registers::Number(arg1);
+          if (regnum != kInvalidRegister || strncmp(arg1, "0x", 2) == 0) {
+            // The argument is an address or a register name.
+            int64_t value;
+            if (GetValue(arg1, &value)) {
+              cur = reinterpret_cast<byte*>(value);
+              // Disassemble 10 instructions at <arg1>.
+              end = cur + (10 * kInstrSize);
+            }
+          } else {
+            // The argument is the number of instructions.
+            int64_t value;
+            if (GetValue(arg1, &value)) {
+              cur = reinterpret_cast<byte*>(sim_->get_pc());
+              // Disassemble <arg1> instructions.
+              end = cur + (value * kInstrSize);
+            }
+          }
+        } else {
+          int64_t value1;
+          int64_t value2;
+          if (GetValue(arg1, &value1) && GetValue(arg2, &value2)) {
+            cur = reinterpret_cast<byte*>(value1);
+            end = cur + (value2 * kInstrSize);
+          }
+        }
+
+        while (cur < end) {
+          dasm.InstructionDecode(buffer, cur);
+          PrintF("  0x%08" PRIxPTR "   %s\n", reinterpret_cast<intptr_t>(cur),
+                 buffer.begin());
+          cur += kInstrSize;
+        }
+      } else if (strcmp(cmd, "gdb") == 0) {
+        PrintF("relinquishing control to gdb\n");
+        v8::base::OS::DebugBreak();
+        PrintF("regaining control from gdb\n");
+      } else if (strcmp(cmd, "break") == 0) {
+        if (argc == 2) {
+          int64_t value;
+          if (GetValue(arg1, &value)) {
+            if (!SetBreakpoint(reinterpret_cast<Instruction*>(value))) {
+              PrintF("setting breakpoint failed\n");
+            }
+          } else {
+            PrintF("%s unrecognized\n", arg1);
+          }
+        } else {
+          PrintF("break <address>\n");
+        }
+      } else if (strcmp(cmd, "del") == 0) {
+        if (!DeleteBreakpoint(nullptr)) {
+          PrintF("deleting breakpoint failed\n");
+        }
+      } else if (strcmp(cmd, "flags") == 0) {
+        PrintF("No flags on SW64 !\n");
+      } else if (strcmp(cmd, "stop") == 0) {
+        int64_t value;
+        intptr_t stop_pc = sim_->get_pc() -
+            2 * kInstrSize;
+        Instruction* stop_instr = reinterpret_cast<Instruction*>(stop_pc);
+        Instruction* msg_address =
+          reinterpret_cast<Instruction*>(stop_pc +
+              kInstrSize);
+        if ((argc == 2) && (strcmp(arg1, "unstop") == 0)) {
+          // Remove the current stop.
+          if (sim_->IsStopInstruction(stop_instr)) {
+            stop_instr->SetInstructionBits(kNopInstr);
+            msg_address->SetInstructionBits(kNopInstr);
+          } else {
+            PrintF("Not at debugger stop.\n");
+          }
+        } else if (argc == 3) {
+          // Print information about all/the specified breakpoint(s).
+          if (strcmp(arg1, "info") == 0) {
+            if (strcmp(arg2, "all") == 0) {
+              PrintF("Stop information:\n");
+              for (uint32_t i = kMaxWatchpointCode + 1;
+                   i <= kMaxStopCode;
+                   i++) {
+                sim_->PrintStopInfo(i);
+              }
+            } else if (GetValue(arg2, &value)) {
+              sim_->PrintStopInfo(value);
+            } else {
+              PrintF("Unrecognized argument.\n");
+            }
+          } else if (strcmp(arg1, "enable") == 0) {
+            // Enable all/the specified breakpoint(s).
+            if (strcmp(arg2, "all") == 0) {
+              for (uint32_t i = kMaxWatchpointCode + 1;
+                   i <= kMaxStopCode;
+                   i++) {
+                sim_->EnableStop(i);
+              }
+            } else if (GetValue(arg2, &value)) {
+              sim_->EnableStop(value);
+            } else {
+              PrintF("Unrecognized argument.\n");
+            }
+          } else if (strcmp(arg1, "disable") == 0) {
+            // Disable all/the specified breakpoint(s).
+            if (strcmp(arg2, "all") == 0) {
+              for (uint32_t i = kMaxWatchpointCode + 1;
+                   i <= kMaxStopCode;
+                   i++) {
+                sim_->DisableStop(i);
+              }
+            } else if (GetValue(arg2, &value)) {
+              sim_->DisableStop(value);
+            } else {
+              PrintF("Unrecognized argument.\n");
+            }
+          }
+        } else {
+          PrintF("Wrong usage. Use help command for more information.\n");
+        }
+      } else if ((strcmp(cmd, "stat") == 0) || (strcmp(cmd, "st") == 0)) {
+        // Print registers and disassemble.
+        PrintAllRegs();
+        PrintF("\n");
+
+        disasm::NameConverter converter;
+        disasm::Disassembler dasm(converter);
+        // Use a reasonably large buffer.
+        v8::internal::EmbeddedVector<char, 256> buffer;
+
+        byte* cur = nullptr;
+        byte* end = nullptr;
+
+        if (argc == 1) {
+          cur = reinterpret_cast<byte*>(sim_->get_pc());
+          end = cur + (10 * kInstrSize);
+        } else if (argc == 2) {
+          int64_t value;
+          if (GetValue(arg1, &value)) {
+            cur = reinterpret_cast<byte*>(value);
+            // no length parameter passed, assume 10 instructions
+            end = cur + (10 * kInstrSize);
+          }
+        } else {
+          int64_t value1;
+          int64_t value2;
+          if (GetValue(arg1, &value1) && GetValue(arg2, &value2)) {
+            cur = reinterpret_cast<byte*>(value1);
+            end = cur + (value2 * kInstrSize);
+          }
+        }
+
+        while (cur < end) {
+          dasm.InstructionDecode(buffer, cur);
+          PrintF("  0x%08" PRIxPTR "   %s\n", reinterpret_cast<intptr_t>(cur),
+                 buffer.begin());
+          cur += kInstrSize;
+        }
+      } else if ((strcmp(cmd, "h") == 0) || (strcmp(cmd, "help") == 0)) {
+        PrintF("cont\n");
+        PrintF("  continue execution (alias 'c')\n");
+        PrintF("stepi\n");
+        PrintF("  step one instruction (alias 'si')\n");
+        PrintF("print <register>\n");
+        PrintF("  print register content (alias 'p')\n");
+        PrintF("  use register name 'all' to print all registers\n");
+        PrintF("printobject <register>\n");
+        PrintF("  print an object from a register (alias 'po')\n");
+        PrintF("stack [<words>]\n");
+        PrintF("  dump stack content, default dump 10 words)\n");
+        PrintF("mem <address> [<words>]\n");
+        PrintF("  dump memory content, default dump 10 words)\n");
+        PrintF("dump [<words>]\n");
+        PrintF(
+            "  dump memory content without pretty printing JS objects, default "
+            "dump 10 words)\n");
+        PrintF("flags\n");
+        PrintF("  print flags\n");
+        PrintF("disasm [<instructions>]\n");
+        PrintF("disasm [<address/register>]\n");
+        PrintF("disasm [[<address/register>] <instructions>]\n");
+        PrintF("  disassemble code, default is 10 instructions\n");
+        PrintF("  from pc (alias 'di')\n");
+        PrintF("gdb\n");
+        PrintF("  enter gdb\n");
+        PrintF("break <address>\n");
+        PrintF("  set a break point on the address\n");
+        PrintF("del\n");
+        PrintF("  delete the breakpoint\n");
+        PrintF("stop feature:\n");
+        PrintF("  Description:\n");
+        PrintF("    Stops are debug instructions inserted by\n");
+        PrintF("    the Assembler::stop() function.\n");
+        PrintF("    When hitting a stop, the Simulator will\n");
+        PrintF("    stop and and give control to the Debugger.\n");
+        PrintF("    All stop codes are watched:\n");
+        PrintF("    - They can be enabled / disabled: the Simulator\n");
+        PrintF("       will / won't stop when hitting them.\n");
+        PrintF("    - The Simulator keeps track of how many times they \n");
+        PrintF("      are met. (See the info command.) Going over a\n");
+        PrintF("      disabled stop still increases its counter. \n");
+        PrintF("  Commands:\n");
+        PrintF("    stop info all/<code> : print infos about number <code>\n");
+        PrintF("      or all stop(s).\n");
+        PrintF("    stop enable/disable all/<code> : enables / disables\n");
+        PrintF("      all or number <code> stop(s)\n");
+        PrintF("    stop unstop\n");
+        PrintF("      ignore the stop instruction at the current location\n");
+        PrintF("      from now on\n");
+      } else {
+        PrintF("Unknown command: %s\n", cmd);
+      }
+    }
+  }
+
+  // Add all the breakpoints back to stop execution and enter the debugger
+  // shell when hit.
+  RedoBreakpoints();
+
+#undef COMMAND_SIZE
+#undef ARG_SIZE
+
+#undef STR
+#undef XSTR
+}
+
+bool Simulator::ICacheMatch(void* one, void* two) {
+  DCHECK_EQ(reinterpret_cast<intptr_t>(one) & CachePage::kPageMask, 0);
+  DCHECK_EQ(reinterpret_cast<intptr_t>(two) & CachePage::kPageMask, 0);
+  return one == two;
+}
+
+
+static uint32_t ICacheHash(void* key) {
+  return static_cast<uint32_t>(reinterpret_cast<uintptr_t>(key)) >> 2;
+}
+
+
+static bool AllOnOnePage(uintptr_t start, size_t size) {
+  intptr_t start_page = (start & ~CachePage::kPageMask);
+  intptr_t end_page = ((start + size) & ~CachePage::kPageMask);
+  return start_page == end_page;
+}
+
+
+void Simulator::set_last_debugger_input(char* input) {
+  DeleteArray(last_debugger_input_);
+  last_debugger_input_ = input;
+}
+
+void Simulator::SetRedirectInstruction(Instruction* instruction) {
+  instruction->SetInstructionBits(rtCallRedirInstr);
+}
+
+void Simulator::FlushICache(base::CustomMatcherHashMap* i_cache,
+                            void* start_addr, size_t size) {
+  int64_t start = reinterpret_cast<int64_t>(start_addr);
+  int64_t intra_line = (start & CachePage::kLineMask);
+  start -= intra_line;
+  size += intra_line;
+  size = ((size - 1) | CachePage::kLineMask) + 1;
+  int offset = (start & CachePage::kPageMask);
+  while (!AllOnOnePage(start, size - 1)) {
+    int bytes_to_flush = CachePage::kPageSize - offset;
+    FlushOnePage(i_cache, start, bytes_to_flush);
+    start += bytes_to_flush;
+    size -= bytes_to_flush;
+    DCHECK_EQ((int64_t)0, start & CachePage::kPageMask);
+    offset = 0;
+  }
+  if (size != 0) {
+    FlushOnePage(i_cache, start, size);
+  }
+}
+
+CachePage* Simulator::GetCachePage(base::CustomMatcherHashMap* i_cache,
+                                   void* page) {
+  base::HashMap::Entry* entry = i_cache->LookupOrInsert(page, ICacheHash(page));
+  if (entry->value == nullptr) {
+    CachePage* new_page = new CachePage();
+    entry->value = new_page;
+  }
+  return reinterpret_cast<CachePage*>(entry->value);
+}
+
+
+// Flush from start up to and not including start + size.
+void Simulator::FlushOnePage(base::CustomMatcherHashMap* i_cache,
+                             intptr_t start, size_t size) {
+  DCHECK_LE(size, CachePage::kPageSize);
+  DCHECK(AllOnOnePage(start, size - 1));
+  DCHECK_EQ(start & CachePage::kLineMask, 0);
+  DCHECK_EQ(size & CachePage::kLineMask, 0);
+  void* page = reinterpret_cast<void*>(start & (~CachePage::kPageMask));
+  int offset = (start & CachePage::kPageMask);
+  CachePage* cache_page = GetCachePage(i_cache, page);
+  char* valid_bytemap = cache_page->ValidityByte(offset);
+  memset(valid_bytemap, CachePage::LINE_INVALID, size >> CachePage::kLineShift);
+}
+
+void Simulator::CheckICache(base::CustomMatcherHashMap* i_cache,
+                            Instruction* instr) {
+  int64_t address = reinterpret_cast<int64_t>(instr);
+  void* page = reinterpret_cast<void*>(address & (~CachePage::kPageMask));
+  void* line = reinterpret_cast<void*>(address & (~CachePage::kLineMask));
+  int offset = (address & CachePage::kPageMask);
+  CachePage* cache_page = GetCachePage(i_cache, page);
+  char* cache_valid_byte = cache_page->ValidityByte(offset);
+  bool cache_hit = (*cache_valid_byte == CachePage::LINE_VALID);
+  char* cached_line = cache_page->CachedData(offset & ~CachePage::kLineMask);
+  if (cache_hit) {
+    // Check that the data in memory matches the contents of the I-cache.
+    CHECK_EQ(0, memcmp(reinterpret_cast<void*>(instr),
+                       cache_page->CachedData(offset),
+                       kInstrSize));
+  } else {
+    // Cache miss.  Load memory into the cache.
+    memcpy(cached_line, line, CachePage::kLineLength);
+    *cache_valid_byte = CachePage::LINE_VALID;
+  }
+}
+
+
+Simulator::Simulator(Isolate* isolate) : isolate_(isolate) {
+  // Set up simulator support first. Some of this information is needed to
+  // setup the architecture state.
+  stack_size_ = FLAG_sim_stack_size * KB;
+  stack_ = reinterpret_cast<char*>(malloc(stack_size_));
+  pc_modified_ = false;
+  icount_ = 0;
+  break_count_ = 0;
+  break_pc_ = nullptr;
+  break_instr_ = 0;
+
+  lock_valid = 0;
+  lock_flag = 0;
+  lock_success = 0;
+  lock_register_padd = 0;
+  lock_register_flag = 0;
+
+  // Set up architecture state.
+  // All registers are initialized to zero to start with.
+  for (int i = 0; i < kNumSimuRegisters; i++) {
+    registers_[i] = 0;
+  }
+  for (int i = 0; i < kNumFPURegisters; i++) {
+    FPUregisters_[2 * i] = 0;
+    FPUregisters_[2 * i + 1] = 0;  // upper part for MSA ASE
+  }
+
+  if (kArchVariant == kSw64r3) {
+    FCSR_ = kFCSRNaN2008FlagMask;
+    MSACSR_ = 0;
+  } else {
+    FCSR_ = 0;
+  }
+
+  // The sp is initialized to point to the bottom (high address) of the
+  // allocated stack area. To be safe in potential stack underflows we leave
+  // some buffer below.
+  registers_[sp] = reinterpret_cast<int64_t>(stack_) + stack_size_ - 64;
+  // The ra and pc are initialized to a known bad value that will cause an
+  // access violation if the simulator ever tries to execute it.
+  registers_[pc] = bad_ra;
+  registers_[ra] = bad_ra;
+
+  last_debugger_input_ = nullptr;
+}
+
+
+Simulator::~Simulator() {     free(stack_); }
+
+
+// Get the active Simulator for the current thread.
+Simulator* Simulator::current(Isolate* isolate) {
+  v8::internal::Isolate::PerIsolateThreadData* isolate_data =
+       isolate->FindOrAllocatePerThreadDataForThisThread();
+  DCHECK_NOT_NULL(isolate_data);
+
+  Simulator* sim = isolate_data->simulator();
+  if (sim == nullptr) {
+    // TODO(146): delete the simulator object when a thread/isolate goes away.
+    sim = new Simulator(isolate);
+    isolate_data->set_simulator(sim);
+  }
+  return sim;
+}
+
+// Sets the register in the architecture state. It will also deal with updating
+// Simulator internal state for special registers such as PC.
+void Simulator::set_register(int reg, int64_t value) {
+  DCHECK((reg >= 0) && (reg < kNumSimuRegisters));
+  if (reg == pc) {
+    pc_modified_ = true;
+  }
+
+  // Zero register always holds 0.
+  registers_[reg] = (reg == zero_reg) ? 0 : value;
+}
+
+
+void Simulator::set_dw_register(int reg, const int* dbl) {
+  DCHECK((reg >= 0) && (reg < kNumSimuRegisters));
+  registers_[reg] = dbl[1];
+  registers_[reg] = registers_[reg] << 32;
+  registers_[reg] += dbl[0];
+}
+
+
+void Simulator::set_fpu_register(int fpureg, int64_t value) {
+  DCHECK((fpureg >= 0) && (fpureg < kNumFPURegisters));
+  FPUregisters_[fpureg * 2] = value;
+}
+
+
+void Simulator::set_fpu_register_word(int fpureg, int32_t value) {
+  // Set ONLY lower 32-bits, leaving upper bits untouched.
+  DCHECK((fpureg >= 0) && (fpureg < kNumFPURegisters));
+  int32_t* pword;
+  if (kArchEndian == kLittle) {
+    pword = reinterpret_cast<int32_t*>(&FPUregisters_[fpureg * 2]);
+  } else {
+    pword = reinterpret_cast<int32_t*>(&FPUregisters_[fpureg * 2]) + 1;
+  }
+  *pword = value;
+}
+
+
+void Simulator::set_fpu_register_hi_word(int fpureg, int32_t value) {
+  // Set ONLY upper 32-bits, leaving lower bits untouched.
+  DCHECK((fpureg >= 0) && (fpureg < kNumFPURegisters));
+  int32_t* phiword;
+  if (kArchEndian == kLittle) {
+    phiword = (reinterpret_cast<int32_t*>(&FPUregisters_[fpureg * 2])) + 1;
+  } else {
+    phiword = reinterpret_cast<int32_t*>(&FPUregisters_[fpureg * 2]);
+  }
+  *phiword = value;
+}
+
+
+void Simulator::set_fpu_register_float(int fpureg, float value) {
+  DCHECK((fpureg >= 0) && (fpureg < kNumFPURegisters));
+  *bit_cast<float*>(&FPUregisters_[fpureg * 2]) = value;
+}
+
+
+void Simulator::set_fpu_register_double(int fpureg, double value) {
+  DCHECK((fpureg >= 0) && (fpureg < kNumFPURegisters));
+  *bit_cast<double*>(&FPUregisters_[fpureg * 2]) = value;
+}
+
+
+// Get the register from the architecture state. This function does handle
+// the special case of accessing the PC register.
+int64_t Simulator::get_register(int64_t reg) const {
+  DCHECK((reg >= 0) && (reg < kNumSimuRegisters));
+  if (reg == zero_reg)
+    return 0;
+  else
+    return registers_[reg] + ((reg == pc) ? Instruction::kPCReadOffset : 0);
+}
+
+
+double Simulator::get_double_from_register_pair(int reg) {
+  // TODO(plind): bad ABI stuff, refactor or remove.
+  DCHECK((reg >= 0) && (reg < kNumSimuRegisters) && ((reg % 2) == 0));
+
+  double dm_val = 0.0;
+  // Read the bits from the unsigned integer register_[] array
+  // into the double precision floating point value and return it.
+  char buffer[sizeof(registers_[0])];
+  memcpy(buffer, &registers_[reg], sizeof(registers_[0]));
+  memcpy(&dm_val, buffer, sizeof(registers_[0]));
+  return(dm_val);
+}
+
+
+int64_t Simulator::get_fpu_register(int fpureg) const {
+  DCHECK((fpureg >= 0) && (fpureg < kNumFPURegisters));
+  return FPUregisters_[fpureg * 2];
+}
+
+
+int32_t Simulator::get_fpu_register_word(int fpureg) const {
+  DCHECK((fpureg >= 0) && (fpureg < kNumFPURegisters));
+  return static_cast<int32_t>(FPUregisters_[fpureg * 2] & 0xFFFFFFFF);
+}
+
+
+int32_t Simulator::get_fpu_register_signed_word(int fpureg) const {
+  DCHECK((fpureg >= 0) && (fpureg < kNumFPURegisters));
+  return static_cast<int32_t>(FPUregisters_[fpureg * 2] & 0xFFFFFFFF);
+}
+
+
+int32_t Simulator::get_fpu_register_hi_word(int fpureg) const {
+  DCHECK((fpureg >= 0) && (fpureg < kNumFPURegisters));
+  return static_cast<int32_t>((FPUregisters_[fpureg * 2] >> 32) & 0xFFFFFFFF);
+}
+
+
+float Simulator::get_fpu_register_float(int fpureg) const {
+  DCHECK((fpureg >= 0) && (fpureg < kNumFPURegisters));
+  return *bit_cast<float*>(const_cast<int64_t*>(&FPUregisters_[fpureg * 2]));
+}
+
+
+double Simulator::get_fpu_register_double(int fpureg) const {
+  DCHECK((fpureg >= 0) && (fpureg < kNumFPURegisters));
+  return *bit_cast<double*>(&FPUregisters_[fpureg * 2]);
+}
+
+template <typename T>
+void Simulator::get_msa_register(int wreg, T* value) {
+  DCHECK((wreg >= 0) && (wreg < kNumMSARegisters));
+  memcpy(value, FPUregisters_ + wreg * 2, kSimd128Size);
+}
+
+template <typename T>
+void Simulator::set_msa_register(int wreg, const T* value) {
+  DCHECK((wreg >= 0) && (wreg < kNumMSARegisters));
+  memcpy(FPUregisters_ + wreg * 2, value, kSimd128Size);
+}
+
+// Runtime FP routines take up to two double arguments and zero
+// or one integer arguments. All are constructed here,
+// from a0-a3 or f12 and f13 (n64), or f14 (O32).
+void Simulator::GetFpArgs(double* x, double* y, int32_t* z) {
+  if (!IsSw64SoftFloatABI) {
+    *x = get_fpu_register_double(f16);
+    *y = get_fpu_register_double(f17);
+    *z = static_cast<int32_t>(get_register(a1));
+  } else {
+  // TODO(plind): bad ABI stuff, refactor or remove.
+    // We use a char buffer to get around the strict-aliasing rules which
+    // otherwise allow the compiler to optimize away the copy.
+    char buffer[sizeof(*x)];
+    int32_t* reg_buffer = reinterpret_cast<int32_t*>(buffer);
+
+    // Registers a0 and a1 -> x.
+    reg_buffer[0] = get_register(a0);
+    reg_buffer[1] = get_register(a1);
+    memcpy(x, buffer, sizeof(buffer));
+    // Registers a2 and a3 -> y.
+    reg_buffer[0] = get_register(a2);
+    reg_buffer[1] = get_register(a3);
+    memcpy(y, buffer, sizeof(buffer));
+    // Register 2 -> z.
+    reg_buffer[0] = get_register(a2);
+    memcpy(z, buffer, sizeof(*z));
+  }
+}
+
+
+// The return value is either in v0/v1 or f0.
+void Simulator::SetFpResult(const double& result) {
+    set_fpu_register_double(f0, result);
+}
+
+
+// Helper functions for setting and testing the FCSR register's bits.
+void Simulator::set_fcsr_bit(uint32_t cc, bool value) {
+  if (value) {
+    FCSR_ |= (1LL << cc);
+  } else {
+    FCSR_ &= ~(1LL << cc);
+  }
+}
+
+bool Simulator::test_fcsr_bit(uint32_t cc) { return FCSR_ & (1 << cc); }
+
+void Simulator::set_fcsr_rounding_mode(FPURoundingMode mode) {
+  FCSR_ |= mode & kFPURoundingModeMask;
+}
+
+void Simulator::set_msacsr_rounding_mode(FPURoundingMode mode) {
+  MSACSR_ |= mode & kFPURoundingModeMask;
+}
+
+unsigned int Simulator::get_fcsr_rounding_mode() {
+  return FCSR_ & kFPURoundingModeMask;
+}
+
+unsigned int Simulator::get_msacsr_rounding_mode() {
+  return MSACSR_ & kFPURoundingModeMask;
+}
+
+// Sets the rounding error codes in FCSR based on the result of the rounding.
+// Returns true if the operation was invalid.
+bool Simulator::set_fcsr_round_error(double original, double rounded) {
+  bool ret = false;
+  double max_int32 = std::numeric_limits<int32_t>::max();
+  double min_int32 = std::numeric_limits<int32_t>::min();
+
+  if (!std::isfinite(original) || !std::isfinite(rounded)) {
+    set_fcsr_bit(kFCSRInvalidOpFlagBit, true);
+    ret = true;
+  }
+
+  if (original != rounded) {
+    set_fcsr_bit(kFCSRInexactFlagBit, true);
+  }
+
+  if (rounded < DBL_MIN && rounded > -DBL_MIN && rounded != 0) {
+    set_fcsr_bit(kFCSRUnderflowFlagBit, true);
+    ret = true;
+  }
+
+  if (rounded > max_int32 || rounded < min_int32) {
+    set_fcsr_bit(kFCSROverflowFlagBit, true);
+    // The reference is not really clear but it seems this is required:
+    set_fcsr_bit(kFCSRInvalidOpFlagBit, true);
+    ret = true;
+  }
+
+  return ret;
+}
+
+// Sets the rounding error codes in FCSR based on the result of the rounding.
+// Returns true if the operation was invalid.
+bool Simulator::set_fcsr_round64_error(double original, double rounded) {
+  bool ret = false;
+  // The value of INT64_MAX (2^63-1) can't be represented as double exactly,
+  // loading the most accurate representation into max_int64, which is 2^63.
+  double max_int64 = std::numeric_limits<int64_t>::max();
+  double min_int64 = std::numeric_limits<int64_t>::min();
+
+  if (!std::isfinite(original) || !std::isfinite(rounded)) {
+    set_fcsr_bit(kFCSRInvalidOpFlagBit, true);
+    ret = true;
+  }
+
+  if (original != rounded) {
+    set_fcsr_bit(kFCSRInexactFlagBit, true);
+  }
+
+  if (rounded < DBL_MIN && rounded > -DBL_MIN && rounded != 0) {
+    set_fcsr_bit(kFCSRUnderflowFlagBit, true);
+    ret = true;
+  }
+
+  if (rounded >= max_int64 || rounded < min_int64) {
+    set_fcsr_bit(kFCSROverflowFlagBit, true);
+    // The reference is not really clear but it seems this is required:
+    set_fcsr_bit(kFCSRInvalidOpFlagBit, true);
+    ret = true;
+  }
+
+  return ret;
+}
+
+// Sets the rounding error codes in FCSR based on the result of the rounding.
+// Returns true if the operation was invalid.
+bool Simulator::set_fcsr_round_error(float original, float rounded) {
+  bool ret = false;
+  double max_int32 = std::numeric_limits<int32_t>::max();
+  double min_int32 = std::numeric_limits<int32_t>::min();
+
+  if (!std::isfinite(original) || !std::isfinite(rounded)) {
+    set_fcsr_bit(kFCSRInvalidOpFlagBit, true);
+    ret = true;
+  }
+
+  if (original != rounded) {
+    set_fcsr_bit(kFCSRInexactFlagBit, true);
+  }
+
+  if (rounded < FLT_MIN && rounded > -FLT_MIN && rounded != 0) {
+    set_fcsr_bit(kFCSRUnderflowFlagBit, true);
+    ret = true;
+  }
+
+  if (rounded > max_int32 || rounded < min_int32) {
+    set_fcsr_bit(kFCSROverflowFlagBit, true);
+    // The reference is not really clear but it seems this is required:
+    set_fcsr_bit(kFCSRInvalidOpFlagBit, true);
+    ret = true;
+  }
+
+  return ret;
+}
+
+void Simulator::set_fpu_register_word_invalid_result(float original,
+                                                     float rounded) {
+  if (FCSR_ & kFCSRNaN2008FlagMask) {
+    double max_int32 = std::numeric_limits<int32_t>::max();
+    double min_int32 = std::numeric_limits<int32_t>::min();
+    if (std::isnan(original)) {
+      set_fpu_register_word(fd_reg(), 0);
+    } else if (rounded > max_int32) {
+      set_fpu_register_word(fd_reg(), kFPUInvalidResult);
+    } else if (rounded < min_int32) {
+      set_fpu_register_word(fd_reg(), kFPUInvalidResultNegative);
+    } else {
+      UNREACHABLE();
+    }
+  } else {
+    set_fpu_register_word(fd_reg(), kFPUInvalidResult);
+  }
+}
+
+
+void Simulator::set_fpu_register_invalid_result(float original, float rounded) {
+  if (FCSR_ & kFCSRNaN2008FlagMask) {
+    double max_int32 = std::numeric_limits<int32_t>::max();
+    double min_int32 = std::numeric_limits<int32_t>::min();
+    if (std::isnan(original)) {
+      set_fpu_register(fd_reg(), 0);
+    } else if (rounded > max_int32) {
+      set_fpu_register(fd_reg(), kFPUInvalidResult);
+    } else if (rounded < min_int32) {
+      set_fpu_register(fd_reg(), kFPUInvalidResultNegative);
+    } else {
+      UNREACHABLE();
+    }
+  } else {
+    set_fpu_register(fd_reg(), kFPUInvalidResult);
+  }
+}
+
+
+void Simulator::set_fpu_register_invalid_result64(float original,
+                                                  float rounded) {
+  if (FCSR_ & kFCSRNaN2008FlagMask) {
+    // The value of INT64_MAX (2^63-1) can't be represented as double exactly,
+    // loading the most accurate representation into max_int64, which is 2^63.
+    double max_int64 = std::numeric_limits<int64_t>::max();
+    double min_int64 = std::numeric_limits<int64_t>::min();
+    if (std::isnan(original)) {
+      set_fpu_register(fd_reg(), 0);
+    } else if (rounded >= max_int64) {
+      set_fpu_register(fd_reg(), kFPU64InvalidResult);
+    } else if (rounded < min_int64) {
+      set_fpu_register(fd_reg(), kFPU64InvalidResultNegative);
+    } else {
+      UNREACHABLE();
+    }
+  } else {
+    set_fpu_register(fd_reg(), kFPU64InvalidResult);
+  }
+}
+
+
+void Simulator::set_fpu_register_word_invalid_result(double original,
+                                                     double rounded) {
+  if (FCSR_ & kFCSRNaN2008FlagMask) {
+    double max_int32 = std::numeric_limits<int32_t>::max();
+    double min_int32 = std::numeric_limits<int32_t>::min();
+    if (std::isnan(original)) {
+      set_fpu_register_word(fd_reg(), 0);
+    } else if (rounded > max_int32) {
+      set_fpu_register_word(fd_reg(), kFPUInvalidResult);
+    } else if (rounded < min_int32) {
+      set_fpu_register_word(fd_reg(), kFPUInvalidResultNegative);
+    } else {
+      UNREACHABLE();
+    }
+  } else {
+    set_fpu_register_word(fd_reg(), kFPUInvalidResult);
+  }
+}
+
+
+void Simulator::set_fpu_register_invalid_result(double original,
+                                                double rounded) {
+  if (FCSR_ & kFCSRNaN2008FlagMask) {
+    double max_int32 = std::numeric_limits<int32_t>::max();
+    double min_int32 = std::numeric_limits<int32_t>::min();
+    if (std::isnan(original)) {
+      set_fpu_register(fd_reg(), 0);
+    } else if (rounded > max_int32) {
+      set_fpu_register(fd_reg(), kFPUInvalidResult);
+    } else if (rounded < min_int32) {
+      set_fpu_register(fd_reg(), kFPUInvalidResultNegative);
+    } else {
+      UNREACHABLE();
+    }
+  } else {
+    set_fpu_register(fd_reg(), kFPUInvalidResult);
+  }
+}
+
+
+void Simulator::set_fpu_register_invalid_result64(double original,
+                                                  double rounded) {
+  if (FCSR_ & kFCSRNaN2008FlagMask) {
+    // The value of INT64_MAX (2^63-1) can't be represented as double exactly,
+    // loading the most accurate representation into max_int64, which is 2^63.
+    double max_int64 = std::numeric_limits<int64_t>::max();
+    double min_int64 = std::numeric_limits<int64_t>::min();
+    if (std::isnan(original)) {
+      set_fpu_register(fd_reg(), 0);
+    } else if (rounded >= max_int64) {
+      set_fpu_register(fd_reg(), kFPU64InvalidResult);
+    } else if (rounded < min_int64) {
+      set_fpu_register(fd_reg(), kFPU64InvalidResultNegative);
+    } else {
+      UNREACHABLE();
+    }
+  } else {
+    set_fpu_register(fd_reg(), kFPU64InvalidResult);
+  }
+}
+
+
+// Sets the rounding error codes in FCSR based on the result of the rounding.
+// Returns true if the operation was invalid.
+bool Simulator::set_fcsr_round64_error(float original, float rounded) {
+  bool ret = false;
+  // The value of INT64_MAX (2^63-1) can't be represented as double exactly,
+  // loading the most accurate representation into max_int64, which is 2^63.
+  double max_int64 = std::numeric_limits<int64_t>::max();
+  double min_int64 = std::numeric_limits<int64_t>::min();
+
+  if (!std::isfinite(original) || !std::isfinite(rounded)) {
+    set_fcsr_bit(kFCSRInvalidOpFlagBit, true);
+    ret = true;
+  }
+
+  if (original != rounded) {
+    set_fcsr_bit(kFCSRInexactFlagBit, true);
+  }
+
+  if (rounded < FLT_MIN && rounded > -FLT_MIN && rounded != 0) {
+    set_fcsr_bit(kFCSRUnderflowFlagBit, true);
+    ret = true;
+  }
+
+  if (rounded >= max_int64 || rounded < min_int64) {
+    set_fcsr_bit(kFCSROverflowFlagBit, true);
+    // The reference is not really clear but it seems this is required:
+    set_fcsr_bit(kFCSRInvalidOpFlagBit, true);
+    ret = true;
+  }
+
+  return ret;
+}
+
+
+// For cvt instructions only
+void Simulator::round_according_to_fcsr(double toRound, double* rounded,
+                                        int32_t* rounded_int, double fs) {
+  // 0 RN (round to nearest): Round a result to the nearest
+  // representable value; if the result is exactly halfway between
+  // two representable values, round to zero. Behave like round_w_d.
+
+  // 1 RZ (round toward zero): Round a result to the closest
+  // representable value whose absolute value is less than or
+  // equal to the infinitely accurate result. Behave like trunc_w_d.
+
+  // 2 RP (round up, or toward +infinity): Round a result to the
+  // next representable value up. Behave like ceil_w_d.
+
+  // 3 RN (round down, or toward infinity): Round a result to
+  // the next representable value down. Behave like floor_w_d.
+  switch (FCSR_ & 3) {
+    case kRoundToNearest:
+      *rounded = std::floor(fs + 0.5);
+      *rounded_int = static_cast<int32_t>(*rounded);
+      if ((*rounded_int & 1) != 0 && *rounded_int - fs == 0.5) {
+        // If the number is halfway between two integers,
+        // round to the even one.
+        *rounded_int -= 1;
+        *rounded -= 1.;
+      }
+      break;
+    case kRoundToZero:
+      *rounded = trunc(fs);
+      *rounded_int = static_cast<int32_t>(*rounded);
+      break;
+    case kRoundToPlusInf:
+      *rounded = std::ceil(fs);
+      *rounded_int = static_cast<int32_t>(*rounded);
+      break;
+    case kRoundToMinusInf:
+      *rounded = std::floor(fs);
+      *rounded_int = static_cast<int32_t>(*rounded);
+      break;
+  }
+}
+
+void Simulator::round64_according_to_fcsr(double toRound, double* rounded,
+                                          int64_t* rounded_int, double fs) {
+  // 0 RN (round to nearest): Round a result to the nearest
+  // representable value; if the result is exactly halfway between
+  // two representable values, round to zero. Behave like round_w_d.
+
+  // 1 RZ (round toward zero): Round a result to the closest
+  // representable value whose absolute value is less than or.
+  // equal to the infinitely accurate result. Behave like trunc_w_d.
+
+  // 2 RP (round up, or toward +infinity): Round a result to the
+  // next representable value up. Behave like ceil_w_d.
+
+  // 3 RN (round down, or toward infinity): Round a result to
+  // the next representable value down. Behave like floor_w_d.
+  switch (FCSR_ & 3) {
+    case kRoundToNearest:
+      *rounded = std::floor(fs + 0.5);
+      *rounded_int = static_cast<int64_t>(*rounded);
+      if ((*rounded_int & 1) != 0 && *rounded_int - fs == 0.5) {
+        // If the number is halfway between two integers,
+        // round to the even one.
+        *rounded_int -= 1;
+        *rounded -= 1.;
+      }
+      break;
+    case kRoundToZero:
+      *rounded = trunc(fs);
+      *rounded_int = static_cast<int64_t>(*rounded);
+      break;
+    case kRoundToPlusInf:
+      *rounded = std::ceil(fs);
+      *rounded_int = static_cast<int64_t>(*rounded);
+      break;
+    case kRoundToMinusInf:
+      *rounded = std::floor(fs);
+      *rounded_int = static_cast<int64_t>(*rounded);
+      break;
+  }
+}
+
+
+// for cvt instructions only
+void Simulator::round_according_to_fcsr(float toRound, float* rounded,
+                                        int32_t* rounded_int, float fs) {
+  // 0 RN (round to nearest): Round a result to the nearest
+  // representable value; if the result is exactly halfway between
+  // two representable values, round to zero. Behave like round_w_d.
+
+  // 1 RZ (round toward zero): Round a result to the closest
+  // representable value whose absolute value is less than or
+  // equal to the infinitely accurate result. Behave like trunc_w_d.
+
+  // 2 RP (round up, or toward +infinity): Round a result to the
+  // next representable value up. Behave like ceil_w_d.
+
+  // 3 RN (round down, or toward infinity): Round a result to
+  // the next representable value down. Behave like floor_w_d.
+  switch (FCSR_ & 3) {
+    case kRoundToNearest:
+      *rounded = std::floor(fs + 0.5);
+      *rounded_int = static_cast<int32_t>(*rounded);
+      if ((*rounded_int & 1) != 0 && *rounded_int - fs == 0.5) {
+        // If the number is halfway between two integers,
+        // round to the even one.
+        *rounded_int -= 1;
+        *rounded -= 1.f;
+      }
+      break;
+    case kRoundToZero:
+      *rounded = trunc(fs);
+      *rounded_int = static_cast<int32_t>(*rounded);
+      break;
+    case kRoundToPlusInf:
+      *rounded = std::ceil(fs);
+      *rounded_int = static_cast<int32_t>(*rounded);
+      break;
+    case kRoundToMinusInf:
+      *rounded = std::floor(fs);
+      *rounded_int = static_cast<int32_t>(*rounded);
+      break;
+  }
+}
+
+void Simulator::round64_according_to_fcsr(float toRound, float* rounded,
+                                          int64_t* rounded_int, float fs) {
+  // 0 RN (round to nearest): Round a result to the nearest
+  // representable value; if the result is exactly halfway between
+  // two representable values, round to zero. Behave like round_w_d.
+
+  // 1 RZ (round toward zero): Round a result to the closest
+  // representable value whose absolute value is less than or.
+  // equal to the infinitely accurate result. Behave like trunc_w_d.
+
+  // 2 RP (round up, or toward +infinity): Round a result to the
+  // next representable value up. Behave like ceil_w_d.
+
+  // 3 RN (round down, or toward infinity): Round a result to
+  // the next representable value down. Behave like floor_w_d.
+  switch (FCSR_ & 3) {
+    case kRoundToNearest:
+      *rounded = std::floor(fs + 0.5);
+      *rounded_int = static_cast<int64_t>(*rounded);
+      if ((*rounded_int & 1) != 0 && *rounded_int - fs == 0.5) {
+        // If the number is halfway between two integers,
+        // round to the even one.
+        *rounded_int -= 1;
+        *rounded -= 1.f;
+      }
+      break;
+    case kRoundToZero:
+      *rounded = trunc(fs);
+      *rounded_int = static_cast<int64_t>(*rounded);
+      break;
+    case kRoundToPlusInf:
+      *rounded = std::ceil(fs);
+      *rounded_int = static_cast<int64_t>(*rounded);
+      break;
+    case kRoundToMinusInf:
+      *rounded = std::floor(fs);
+      *rounded_int = static_cast<int64_t>(*rounded);
+      break;
+  }
+}
+
+template <typename T_fp, typename T_int>
+void Simulator::round_according_to_msacsr(T_fp toRound, T_fp* rounded,
+                                          T_int* rounded_int) {
+  // 0 RN (round to nearest): Round a result to the nearest
+  // representable value; if the result is exactly halfway between
+  // two representable values, round to zero. Behave like round_w_d.
+
+  // 1 RZ (round toward zero): Round a result to the closest
+  // representable value whose absolute value is less than or
+  // equal to the infinitely accurate result. Behave like trunc_w_d.
+
+  // 2 RP (round up, or toward +infinity): Round a result to the
+  // next representable value up. Behave like ceil_w_d.
+
+  // 3 RN (round down, or toward infinity): Round a result to
+  // the next representable value down. Behave like floor_w_d.
+  switch (get_msacsr_rounding_mode()) {
+    case kRoundToNearest:
+      *rounded = std::floor(toRound + 0.5);
+      *rounded_int = static_cast<T_int>(*rounded);
+      if ((*rounded_int & 1) != 0 && *rounded_int - toRound == 0.5) {
+        // If the number is halfway between two integers,
+        // round to the even one.
+        *rounded_int -= 1;
+        *rounded -= 1.;
+      }
+      break;
+    case kRoundToZero:
+      *rounded = trunc(toRound);
+      *rounded_int = static_cast<T_int>(*rounded);
+      break;
+    case kRoundToPlusInf:
+      *rounded = std::ceil(toRound);
+      *rounded_int = static_cast<T_int>(*rounded);
+      break;
+    case kRoundToMinusInf:
+      *rounded = std::floor(toRound);
+      *rounded_int = static_cast<T_int>(*rounded);
+      break;
+  }
+}
+
+// Raw access to the PC register.
+void Simulator::set_pc(int64_t value) {
+  pc_modified_ = true;
+  registers_[pc] = value;
+}
+
+
+bool Simulator::has_bad_pc() const {
+  return ((registers_[pc] == bad_ra) || (registers_[pc] == end_sim_pc));
+}
+
+
+// Raw access to the PC register without the special adjustment when reading.
+int64_t Simulator::get_pc() const {
+  return registers_[pc];
+}
+
+
+// The SW64 cannot do unaligned reads and writes.  On some SW64 platforms an
+// interrupt is caused.  On others it does a funky rotation thing.  For now we
+// simply disallow unaligned reads, but at some point we may want to move to
+// emulating the rotate behaviour.  Note that simulator runs have the runtime
+// system running directly on the host system and only generated code is
+// executed in the simulator.  Since the host is typically IA32 we will not
+// get the correct SW64-like behaviour on unaligned accesses.
+
+// TODO(plind): refactor this messy debug code when we do unaligned access.
+void Simulator::DieOrDebug() {
+  if ((1)) {  // Flag for this was removed.
+    Sw64Debugger dbg(this);
+    dbg.Debug();
+  } else {
+    base::OS::Abort();
+  }
+}
+
+void Simulator::TraceRegWr(int64_t value, TraceType t) {
+  if (::v8::internal::FLAG_trace_sim) {
+    union {
+      int64_t fmt_int64;
+      int32_t fmt_int32[2];
+      float fmt_float[2];
+      double fmt_double;
+    } v;
+    v.fmt_int64 = value;
+
+    switch (t) {
+      case WORD:
+        SNPrintF(trace_buf_, "%016" PRIx64 "    (%" PRId64 ")    int32:%" PRId32
+                             " uint32:%" PRIu32,
+                 v.fmt_int64, icount_, v.fmt_int32[0], v.fmt_int32[0]);
+        break;
+      case DWORD:
+        SNPrintF(trace_buf_, "%016" PRIx64 "    (%" PRId64 ")    int64:%" PRId64
+                             " uint64:%" PRIu64,
+                 value, icount_, value, value);
+        break;
+      case FLOAT:
+        SNPrintF(trace_buf_, "%016" PRIx64 "    (%" PRId64 ")    flt:%e",
+                 v.fmt_int64, icount_, v.fmt_float[0]);
+        break;
+      case DOUBLE:
+        SNPrintF(trace_buf_, "%016" PRIx64 "    (%" PRId64 ")    dbl:%e",
+                 v.fmt_int64, icount_, v.fmt_double);
+        break;
+      case FLOAT_DOUBLE:
+        SNPrintF(trace_buf_, "%016" PRIx64 "    (%" PRId64 ")    flt:%e dbl:%e",
+                 v.fmt_int64, icount_, v.fmt_float[0], v.fmt_double);
+        break;
+      case WORD_DWORD:
+        SNPrintF(trace_buf_,
+                 "%016" PRIx64 "    (%" PRId64 ")    int32:%" PRId32
+                 " uint32:%" PRIu32 " int64:%" PRId64 " uint64:%" PRIu64,
+                 v.fmt_int64, icount_, v.fmt_int32[0], v.fmt_int32[0],
+                 v.fmt_int64, v.fmt_int64);
+        break;
+      default:
+        UNREACHABLE();
+    }
+  }
+}
+
+template <typename T>
+void Simulator::TraceMSARegWr(T* value, TraceType t) {
+  if (::v8::internal::FLAG_trace_sim) {
+    union {
+      uint8_t b[16];
+      uint16_t h[8];
+      uint32_t w[4];
+      uint64_t d[2];
+      float f[4];
+      double df[2];
+    } v;
+    memcpy(v.b, value, kSimd128Size);
+    switch (t) {
+      case BYTE:
+        SNPrintF(trace_buf_,
+                 "LO: %016" PRIx64 "  HI: %016" PRIx64 "    (%" PRIu64 ")",
+                 v.d[0], v.d[1], icount_);
+        break;
+      case HALF:
+        SNPrintF(trace_buf_,
+                 "LO: %016" PRIx64 "  HI: %016" PRIx64 "    (%" PRIu64 ")",
+                 v.d[0], v.d[1], icount_);
+        break;
+      case WORD:
+        SNPrintF(trace_buf_,
+                 "LO: %016" PRIx64 "  HI: %016" PRIx64 "    (%" PRIu64
+                 ")    int32[0..3]:%" PRId32 "  %" PRId32 "  %" PRId32
+                 "  %" PRId32,
+                 v.d[0], v.d[1], icount_, v.w[0], v.w[1], v.w[2], v.w[3]);
+        break;
+      case DWORD:
+        SNPrintF(trace_buf_,
+                 "LO: %016" PRIx64 "  HI: %016" PRIx64 "    (%" PRIu64 ")",
+                 v.d[0], v.d[1], icount_);
+        break;
+      case FLOAT:
+        SNPrintF(trace_buf_,
+                 "LO: %016" PRIx64 "  HI: %016" PRIx64 "    (%" PRIu64
+                 ")    flt[0..3]:%e  %e  %e  %e",
+                 v.d[0], v.d[1], icount_, v.f[0], v.f[1], v.f[2], v.f[3]);
+        break;
+      case DOUBLE:
+        SNPrintF(trace_buf_,
+                 "LO: %016" PRIx64 "  HI: %016" PRIx64 "    (%" PRIu64
+                 ")    dbl[0..1]:%e  %e",
+                 v.d[0], v.d[1], icount_, v.df[0], v.df[1]);
+        break;
+      default:
+        UNREACHABLE();
+    }
+  }
+}
+
+template <typename T>
+void Simulator::TraceMSARegWr(T* value) {
+  if (::v8::internal::FLAG_trace_sim) {
+    union {
+      uint8_t b[kMSALanesByte];
+      uint16_t h[kMSALanesHalf];
+      uint32_t w[kMSALanesWord];
+      uint64_t d[kMSALanesDword];
+      float f[kMSALanesWord];
+      double df[kMSALanesDword];
+    } v;
+    memcpy(v.b, value, kMSALanesByte);
+
+    if (std::is_same<T, int32_t>::value) {
+      SNPrintF(trace_buf_,
+               "LO: %016" PRIx64 "  HI: %016" PRIx64 "    (%" PRIu64
+               ")    int32[0..3]:%" PRId32 "  %" PRId32 "  %" PRId32
+               "  %" PRId32,
+               v.d[0], v.d[1], icount_, v.w[0], v.w[1], v.w[2], v.w[3]);
+    } else if (std::is_same<T, float>::value) {
+      SNPrintF(trace_buf_,
+               "LO: %016" PRIx64 "  HI: %016" PRIx64 "    (%" PRIu64
+               ")    flt[0..3]:%e  %e  %e  %e",
+               v.d[0], v.d[1], icount_, v.f[0], v.f[1], v.f[2], v.f[3]);
+    } else if (std::is_same<T, double>::value) {
+      SNPrintF(trace_buf_,
+               "LO: %016" PRIx64 "  HI: %016" PRIx64 "    (%" PRIu64
+               ")    dbl[0..1]:%e  %e",
+               v.d[0], v.d[1], icount_, v.df[0], v.df[1]);
+    } else {
+      SNPrintF(trace_buf_,
+               "LO: %016" PRIx64 "  HI: %016" PRIx64 "    (%" PRIu64 ")",
+               v.d[0], v.d[1], icount_);
+    }
+  }
+}
+
+// TODO(plind): consider making icount_ printing a flag option.
+void Simulator::TraceMemRd(int64_t addr, int64_t value, TraceType t) {
+  if (::v8::internal::FLAG_trace_sim) {
+    union {
+      int64_t fmt_int64;
+      int32_t fmt_int32[2];
+      float fmt_float[2];
+      double fmt_double;
+    } v;
+    v.fmt_int64 = value;
+
+    switch (t) {
+      case WORD:
+        SNPrintF(trace_buf_, "%016" PRIx64 "  <-- [%016" PRIx64 "]    (%" PRId64
+                             ")    int32:%" PRId32 " uint32:%" PRIu32,
+                 v.fmt_int64, addr, icount_, v.fmt_int32[0], v.fmt_int32[0]);
+        break;
+      case DWORD:
+        SNPrintF(trace_buf_, "%016" PRIx64 "  <-- [%016" PRIx64 "]    (%" PRId64
+                             ")    int64:%" PRId64 " uint64:%" PRIu64,
+                 value, addr, icount_, value, value);
+        break;
+      case FLOAT:
+        SNPrintF(trace_buf_, "%016" PRIx64 "  <-- [%016" PRIx64 "]    (%" PRId64
+                             ")    flt:%e",
+                 v.fmt_int64, addr, icount_, v.fmt_float[0]);
+        break;
+      case DOUBLE:
+        SNPrintF(trace_buf_, "%016" PRIx64 "  <-- [%016" PRIx64 "]    (%" PRId64
+                             ")    dbl:%e",
+                 v.fmt_int64, addr, icount_, v.fmt_double);
+        break;
+      case FLOAT_DOUBLE:
+        SNPrintF(trace_buf_, "%016" PRIx64 "  <-- [%016" PRIx64 "]    (%" PRId64
+                             ")    flt:%e dbl:%e",
+                 v.fmt_int64, addr, icount_, v.fmt_float[0], v.fmt_double);
+        break;
+      default:
+        UNREACHABLE();
+    }
+  }
+}
+
+
+void Simulator::TraceMemWr(int64_t addr, int64_t value, TraceType t) {
+  if (::v8::internal::FLAG_trace_sim) {
+    switch (t) {
+      case BYTE:
+        SNPrintF(trace_buf_, "               %02" PRIx8 " --> [%016" PRIx64
+                             "]    (%" PRId64 ")",
+                 static_cast<uint8_t>(value), addr, icount_);
+        break;
+      case HALF:
+        SNPrintF(trace_buf_, "            %04" PRIx16 " --> [%016" PRIx64
+                             "]    (%" PRId64 ")",
+                 static_cast<uint16_t>(value), addr, icount_);
+        break;
+      case WORD:
+        SNPrintF(trace_buf_,
+                 "        %08" PRIx32 " --> [%016" PRIx64 "]    (%" PRId64 ")",
+                 static_cast<uint32_t>(value), addr, icount_);
+        break;
+      case DWORD:
+        SNPrintF(trace_buf_,
+                 "%016" PRIx64 "  --> [%016" PRIx64 "]    (%" PRId64 " )",
+                 value, addr, icount_);
+        break;
+      default:
+        UNREACHABLE();
+    }
+  }
+}
+
+template <typename T>
+void Simulator::TraceMemRd(int64_t addr, T value) {
+  if (::v8::internal::FLAG_trace_sim) {
+    switch (sizeof(T)) {
+      case 1:
+        SNPrintF(trace_buf_,
+                 "%08" PRIx8 " <-- [%08" PRIx64 "]    (%" PRIu64
+                 ")    int8:%" PRId8 " uint8:%" PRIu8,
+                 static_cast<uint8_t>(value), addr, icount_,
+                 static_cast<int8_t>(value), static_cast<uint8_t>(value));
+        break;
+      case 2:
+        SNPrintF(trace_buf_,
+                 "%08" PRIx16 " <-- [%08" PRIx64 "]    (%" PRIu64
+                 ")    int16:%" PRId16 " uint16:%" PRIu16,
+                 static_cast<uint16_t>(value), addr, icount_,
+                 static_cast<int16_t>(value), static_cast<uint16_t>(value));
+        break;
+      case 4:
+        SNPrintF(trace_buf_,
+                 "%08" PRIx32 " <-- [%08" PRIx64 "]    (%" PRIu64
+                 ")    int32:%" PRId32 " uint32:%" PRIu32,
+                 static_cast<uint32_t>(value), addr, icount_,
+                 static_cast<int32_t>(value), static_cast<uint32_t>(value));
+        break;
+      case 8:
+        SNPrintF(trace_buf_,
+                 "%08" PRIx64 " <-- [%08" PRIx64 "]    (%" PRIu64
+                 ")    int64:%" PRId64 " uint64:%" PRIu64,
+                 static_cast<uint64_t>(value), addr, icount_,
+                 static_cast<int64_t>(value), static_cast<uint64_t>(value));
+        break;
+      default:
+        UNREACHABLE();
+    }
+  }
+}
+
+template <typename T>
+void Simulator::TraceMemWr(int64_t addr, T value) {
+  if (::v8::internal::FLAG_trace_sim) {
+    switch (sizeof(T)) {
+      case 1:
+        SNPrintF(trace_buf_,
+                 "      %02" PRIx8 " --> [%08" PRIx64 "]    (%" PRIu64 ")",
+                 static_cast<uint8_t>(value), addr, icount_);
+        break;
+      case 2:
+        SNPrintF(trace_buf_,
+                 "    %04" PRIx16 " --> [%08" PRIx64 "]    (%" PRIu64 ")",
+                 static_cast<uint16_t>(value), addr, icount_);
+        break;
+      case 4:
+        SNPrintF(trace_buf_,
+                 "%08" PRIx32 " --> [%08" PRIx64 "]    (%" PRIu64 ")",
+                 static_cast<uint32_t>(value), addr, icount_);
+        break;
+      case 8:
+        SNPrintF(trace_buf_,
+                 "%16" PRIx64 " --> [%08" PRIx64 "]    (%" PRIu64 ")",
+                 static_cast<uint64_t>(value), addr, icount_);
+        break;
+      default:
+        UNREACHABLE();
+    }
+  }
+}
+
+// TODO(plind): sign-extend and zero-extend not implmented properly
+// on all the ReadXX functions, I don't think re-interpret cast does it.
+int32_t Simulator::ReadW(int64_t addr, Instruction* instr, TraceType t) {
+  if (addr >=0 && addr < 0x400) {
+    // This has to be a nullptr-dereference, drop into debugger.
+    PrintF("Memory read from bad address: 0x%08" PRIx64 " , pc=0x%08" PRIxPTR
+           " \n",
+           addr, reinterpret_cast<intptr_t>(instr));
+    DieOrDebug();
+  }
+  if ((addr & 0x3) == 0 || kArchVariant == kSw64r3) {
+    int32_t* ptr = reinterpret_cast<int32_t*>(addr);
+    TraceMemRd(addr, static_cast<int64_t>(*ptr), t);
+    return *ptr;
+  }
+  PrintF("Unaligned read at 0x%08" PRIx64 " , pc=0x%08" V8PRIxPTR "\n", addr,
+         reinterpret_cast<intptr_t>(instr));
+  DieOrDebug();
+  return 0;
+}
+
+
+uint32_t Simulator::ReadWU(int64_t addr, Instruction* instr) {
+  if (addr >=0 && addr < 0x400) {
+    // This has to be a nullptr-dereference, drop into debugger.
+    PrintF("Memory read from bad address: 0x%08" PRIx64 " , pc=0x%08" PRIxPTR
+           " \n",
+           addr, reinterpret_cast<intptr_t>(instr));
+    DieOrDebug();
+  }
+  if ((addr & 0x3) == 0 || kArchVariant == kSw64r3) {
+    uint32_t* ptr = reinterpret_cast<uint32_t*>(addr);
+    TraceMemRd(addr, static_cast<int64_t>(*ptr), WORD);
+    return *ptr;
+  }
+  PrintF("Unaligned read at 0x%08" PRIx64 " , pc=0x%08" V8PRIxPTR "\n", addr,
+         reinterpret_cast<intptr_t>(instr));
+  DieOrDebug();
+  return 0;
+}
+
+
+void Simulator::WriteW(int64_t addr, int32_t value, Instruction* instr) {
+  if (addr >= 0 && addr < 0x400) {
+    // This has to be a nullptr-dereference, drop into debugger.
+    PrintF("Memory write to bad address: 0x%08" PRIx64 " , pc=0x%08" PRIxPTR
+           " \n",
+           addr, reinterpret_cast<intptr_t>(instr));
+    DieOrDebug();
+  }
+  if ((addr & 0x3) == 0 || kArchVariant == kSw64r3) {
+    TraceMemWr(addr, value, WORD);
+    int* ptr = reinterpret_cast<int*>(addr);
+    *ptr = value;
+    return;
+  }
+  PrintF("Unaligned write at 0x%08" PRIx64 " , pc=0x%08" V8PRIxPTR "\n", addr,
+         reinterpret_cast<intptr_t>(instr));
+  DieOrDebug();
+}
+
+
+int64_t Simulator::Read2W(int64_t addr, Instruction* instr) {
+  if (addr >=0 && addr < 0x400) {
+    // This has to be a nullptr-dereference, drop into debugger.
+    PrintF("Memory read from bad address: 0x%08" PRIx64 " , pc=0x%08" PRIxPTR
+           " \n",
+           addr, reinterpret_cast<intptr_t>(instr));
+    DieOrDebug();
+  }
+  if ((addr & kPointerAlignmentMask) == 0 || kArchVariant == kSw64r3) {
+    int64_t* ptr = reinterpret_cast<int64_t*>(addr);
+    TraceMemRd(addr, *ptr);
+    return *ptr;
+  }
+  PrintF("Unaligned read at 0x%08" PRIx64 " , pc=0x%08" V8PRIxPTR "\n", addr,
+         reinterpret_cast<intptr_t>(instr));
+  DieOrDebug();
+  return 0;
+}
+
+
+void Simulator::Write2W(int64_t addr, int64_t value, Instruction* instr) {
+  if (addr >= 0 && addr < 0x400) {
+    // This has to be a nullptr-dereference, drop into debugger.
+    PrintF("Memory write to bad address: 0x%08" PRIx64 " , pc=0x%08" PRIxPTR
+           "\n",
+           addr, reinterpret_cast<intptr_t>(instr));
+    DieOrDebug();
+  }
+  if ((addr & kPointerAlignmentMask) == 0 || kArchVariant == kSw64r3) {
+    TraceMemWr(addr, value, DWORD);
+    int64_t* ptr = reinterpret_cast<int64_t*>(addr);
+    *ptr = value;
+    return;
+  }
+  PrintF("Unaligned write at 0x%08" PRIx64 " , pc=0x%08" V8PRIxPTR "\n", addr,
+         reinterpret_cast<intptr_t>(instr));
+  DieOrDebug();
+}
+
+
+double Simulator::ReadD(int64_t addr, Instruction* instr) {
+  if ((addr & kDoubleAlignmentMask) == 0 || kArchVariant == kSw64r3) {
+    double* ptr = reinterpret_cast<double*>(addr);
+    return *ptr;
+  }
+  PrintF("Unaligned (double) read at 0x%08" PRIx64 " , pc=0x%08" V8PRIxPTR "\n",
+         addr, reinterpret_cast<intptr_t>(instr));
+  base::OS::Abort();
+  return 0;
+}
+
+
+void Simulator::WriteD(int64_t addr, double value, Instruction* instr) {
+  if ((addr & kDoubleAlignmentMask) == 0 || kArchVariant == kSw64r3) {
+    double* ptr = reinterpret_cast<double*>(addr);
+    *ptr = value;
+    return;
+  }
+  PrintF("Unaligned (double) write at 0x%08" PRIx64 " , pc=0x%08" V8PRIxPTR
+         "\n",
+         addr, reinterpret_cast<intptr_t>(instr));
+  DieOrDebug();
+}
+
+
+uint16_t Simulator::ReadHU(int64_t addr, Instruction* instr) {
+  if ((addr & 1) == 0 || kArchVariant == kSw64r3) {
+    uint16_t* ptr = reinterpret_cast<uint16_t*>(addr);
+    TraceMemRd(addr, static_cast<int64_t>(*ptr));
+    return *ptr;
+  }
+  PrintF("Unaligned unsigned halfword read at 0x%08" PRIx64
+         " , pc=0x%08" V8PRIxPTR "\n",
+         addr, reinterpret_cast<intptr_t>(instr));
+  DieOrDebug();
+  return 0;
+}
+
+
+int16_t Simulator::ReadH(int64_t addr, Instruction* instr) {
+  if ((addr & 1) == 0 || kArchVariant == kSw64r3) {
+    int16_t* ptr = reinterpret_cast<int16_t*>(addr);
+    TraceMemRd(addr, static_cast<int64_t>(*ptr));
+    return *ptr;
+  }
+  PrintF("Unaligned signed halfword read at 0x%08" PRIx64
+         " , pc=0x%08" V8PRIxPTR "\n",
+         addr, reinterpret_cast<intptr_t>(instr));
+  DieOrDebug();
+  return 0;
+}
+
+
+void Simulator::WriteH(int64_t addr, uint16_t value, Instruction* instr) {
+  if ((addr & 1) == 0 || kArchVariant == kSw64r3) {
+    TraceMemWr(addr, value, HALF);
+    uint16_t* ptr = reinterpret_cast<uint16_t*>(addr);
+    *ptr = value;
+    return;
+  }
+  PrintF("Unaligned unsigned halfword write at 0x%08" PRIx64
+         " , pc=0x%08" V8PRIxPTR "\n",
+         addr, reinterpret_cast<intptr_t>(instr));
+  DieOrDebug();
+}
+
+
+void Simulator::WriteH(int64_t addr, int16_t value, Instruction* instr) {
+  if ((addr & 1) == 0 || kArchVariant == kSw64r3) {
+    TraceMemWr(addr, value, HALF);
+    int16_t* ptr = reinterpret_cast<int16_t*>(addr);
+    *ptr = value;
+    return;
+  }
+  PrintF("Unaligned halfword write at 0x%08" PRIx64 " , pc=0x%08" V8PRIxPTR
+         "\n",
+         addr, reinterpret_cast<intptr_t>(instr));
+  DieOrDebug();
+}
+
+
+uint32_t Simulator::ReadBU(int64_t addr) {
+  uint8_t* ptr = reinterpret_cast<uint8_t*>(addr);
+  TraceMemRd(addr, static_cast<int64_t>(*ptr));
+  return *ptr & 0xFF;
+}
+
+
+int32_t Simulator::ReadB(int64_t addr) {
+  int8_t* ptr = reinterpret_cast<int8_t*>(addr);
+  TraceMemRd(addr, static_cast<int64_t>(*ptr));
+  return *ptr;
+}
+
+
+void Simulator::WriteB(int64_t addr, uint8_t value) {
+  TraceMemWr(addr, value, BYTE);
+  uint8_t* ptr = reinterpret_cast<uint8_t*>(addr);
+  *ptr = value;
+}
+
+
+void Simulator::WriteB(int64_t addr, int8_t value) {
+  TraceMemWr(addr, value, BYTE);
+  int8_t* ptr = reinterpret_cast<int8_t*>(addr);
+  *ptr = value;
+}
+
+template <typename T>
+T Simulator::ReadMem(int64_t addr, Instruction* instr) {
+  int alignment_mask = (1 << sizeof(T)) - 1;
+  if ((addr & alignment_mask) == 0 || kArchVariant == kSw64r3) {
+    T* ptr = reinterpret_cast<T*>(addr);
+    TraceMemRd(addr, *ptr);
+    return *ptr;
+  }
+  PrintF("Unaligned read of type sizeof(%ld) at 0x%08lx, pc=0x%08" V8PRIxPTR
+         "\n",
+         sizeof(T), addr, reinterpret_cast<intptr_t>(instr));
+  base::OS::Abort();
+  return 0;
+}
+
+template <typename T>
+void Simulator::WriteMem(int64_t addr, T value, Instruction* instr) {
+  int alignment_mask = (1 << sizeof(T)) - 1;
+  if ((addr & alignment_mask) == 0 || kArchVariant == kSw64r3) {
+    T* ptr = reinterpret_cast<T*>(addr);
+    *ptr = value;
+    TraceMemWr(addr, value);
+    return;
+  }
+  PrintF("Unaligned write of type sizeof(%ld) at 0x%08lx, pc=0x%08" V8PRIxPTR
+         "\n",
+         sizeof(T), addr, reinterpret_cast<intptr_t>(instr));
+  base::OS::Abort();
+}
+
+// Returns the limit of the stack area to enable checking for stack overflows.
+uintptr_t Simulator::StackLimit(uintptr_t c_limit) const {
+  // The simulator uses a separate JS stack. If we have exhausted the C stack,
+  // we also drop down the JS limit to reflect the exhaustion on the JS stack.
+  if (GetCurrentStackPosition() < c_limit) {
+    return reinterpret_cast<uintptr_t>(get_sp());
+  }
+
+  // Otherwise the limit is the JS stack. Leave a safety margin of 1024 bytes
+  // to prevent overrunning the stack when pushing values.
+  return reinterpret_cast<uintptr_t>(stack_) + 1024;
+}
+
+
+// Unsupported instructions use Format to print an error and stop execution.
+void Simulator::Format(Instruction* instr, const char* format) {
+  PrintF("Simulator found unsupported instruction: 0x%08" PRIxPTR " : %s\n",
+         reinterpret_cast<intptr_t>(instr), format);
+  UNIMPLEMENTED_SW64();
+}
+
+
+// Calls into the V8 runtime are based on this very simple interface.
+// Note: To be able to return two values from some calls the code in runtime.cc
+// uses the ObjectPair which is essentially two 32-bit values stuffed into a
+// 64-bit value. With the code below we assume that all runtime calls return
+// 64 bits of result. If they don't, the v1 result register contains a bogus
+// value, which is fine because it is caller-saved.
+
+using SimulatorRuntimeCall = ObjectPair (*)(int64_t arg0, int64_t arg1,
+                                           int64_t arg2, int64_t arg3,
+                                           int64_t arg4, int64_t arg5,
+                                           int64_t arg6, int64_t arg7,
+                                            int64_t arg8, int64_t arg9);
+
+// These prototypes handle the four types of FP calls.
+using SimulatorRuntimeCompareCall = int64_t (*)(double darg0, double darg1);
+using SimulatorRuntimeFPFPCall = double (*)(double darg0, double darg1);
+using SimulatorRuntimeFPCall = double (*)(double darg0);
+using SimulatorRuntimeFPIntCall = double (*)(double darg0, int32_t arg0);
+
+// This signature supports direct call in to API function native callback
+// (refer to InvocationCallback in v8.h).
+using SimulatorRuntimeDirectApiCall = void (*)(int64_t arg0);
+using SimulatorRuntimeProfilingApiCall = void (*)(int64_t arg0, void* arg1);
+
+// This signature supports direct call to accessor getter callback.
+using SimulatorRuntimeDirectGetterCall = void (*)(int64_t arg0, int64_t arg1);
+using SimulatorRuntimeProfilingGetterCall = void (*)(int64_t arg0, int64_t arg1,
+                                                     void* arg2);
+
+// Software interrupt instructions are used by the simulator to call into the
+// C-based V8 runtime. They are also used for debugging with simulator.
+void Simulator::SoftwareInterrupt() {
+  //int32_t func = instr_.FunctionFieldRaw();
+  //uint32_t code = (func == BREAK) ? instr_.Bits(25, 6) : -1;
+  // We first check if we met a REDIRECT.
+  uint32_t code = instr_.SwFunctionFieldValue(0, 0);
+  if (code == REDIRECT) {
+    Redirection* redirection = Redirection::FromInstruction(instr_.instr());
+
+    int64_t* stack_pointer = reinterpret_cast<int64_t*>(get_register(sp));
+
+    int64_t arg0 = get_register(a0);
+    int64_t arg1 = get_register(a1);
+    int64_t arg2 = get_register(a2);
+    int64_t arg3 = get_register(a3);
+    int64_t arg4 = get_register(a4);
+    int64_t arg5 = get_register(a5);
+    int64_t arg6 = stack_pointer[0];
+    int64_t arg7 = stack_pointer[1];
+    int64_t arg8 = stack_pointer[2];
+    int64_t arg9 = stack_pointer[1];
+    STATIC_ASSERT(kMaxCParameters == 10);
+
+    bool fp_call =
+         (redirection->type() == ExternalReference::BUILTIN_FP_FP_CALL) ||
+         (redirection->type() == ExternalReference::BUILTIN_COMPARE_CALL) ||
+         (redirection->type() == ExternalReference::BUILTIN_FP_CALL) ||
+         (redirection->type() == ExternalReference::BUILTIN_FP_INT_CALL);
+
+    if (!IsSw64SoftFloatABI) {
+      // With the hard floating point calling convention, double
+      // arguments are passed in FPU registers. Fetch the arguments
+      // from there and call the builtin using soft floating point
+      // convention.
+      switch (redirection->type()) {
+      case ExternalReference::BUILTIN_FP_FP_CALL:
+      case ExternalReference::BUILTIN_COMPARE_CALL:
+        arg0 = get_fpu_register(f12);
+        arg1 = get_fpu_register(f13);
+        arg2 = get_fpu_register(f14);
+        arg3 = get_fpu_register(f15);
+        break;
+      case ExternalReference::BUILTIN_FP_CALL:
+        arg0 = get_fpu_register(f12);
+        arg1 = get_fpu_register(f13);
+        break;
+      case ExternalReference::BUILTIN_FP_INT_CALL:
+        arg0 = get_fpu_register(f12);
+        arg1 = get_fpu_register(f13);
+        arg2 = get_register(a2);
+        break;
+      default:
+        break;
+      }
+    }
+
+    // This is dodgy but it works because the C entry stubs are never moved.
+    // See comment in codegen-arm.cc and bug 1242173.
+    int64_t saved_ra = get_register(ra);
+
+    intptr_t external =
+          reinterpret_cast<intptr_t>(redirection->external_function());
+
+    // Based on CpuFeatures::IsSupported(FPU), Sw64 will use either hardware
+    // FPU, or gcc soft-float routines. Hardware FPU is simulated in this
+    // simulator. Soft-float has additional abstraction of ExternalReference,
+    // to support serialization.
+    if (fp_call) {
+      double dval0, dval1;  // one or two double parameters
+      int32_t ival;         // zero or one integer parameters
+      int64_t iresult = 0;  // integer return value
+      double dresult = 0;   // double return value
+      GetFpArgs(&dval0, &dval1, &ival);
+      SimulatorRuntimeCall generic_target =
+          reinterpret_cast<SimulatorRuntimeCall>(external);
+      if (::v8::internal::FLAG_trace_sim) {
+        switch (redirection->type()) {
+          case ExternalReference::BUILTIN_FP_FP_CALL:
+          case ExternalReference::BUILTIN_COMPARE_CALL:
+            PrintF("Call to host function at %p with args %f, %f",
+                   reinterpret_cast<void*>(FUNCTION_ADDR(generic_target)),
+                   dval0, dval1);
+            break;
+          case ExternalReference::BUILTIN_FP_CALL:
+            PrintF("Call to host function at %p with arg %f",
+                   reinterpret_cast<void*>(FUNCTION_ADDR(generic_target)),
+                   dval0);
+            break;
+          case ExternalReference::BUILTIN_FP_INT_CALL:
+            PrintF("Call to host function at %p with args %f, %d",
+                   reinterpret_cast<void*>(FUNCTION_ADDR(generic_target)),
+                   dval0, ival);
+            break;
+          default:
+            UNREACHABLE();
+            break;
+        }
+      }
+      switch (redirection->type()) {
+      case ExternalReference::BUILTIN_COMPARE_CALL: {
+        SimulatorRuntimeCompareCall target =
+          reinterpret_cast<SimulatorRuntimeCompareCall>(external);
+        iresult = target(dval0, dval1);
+        set_register(v0, static_cast<int64_t>(iresult));
+      //  set_register(v1, static_cast<int64_t>(iresult >> 32));
+        break;
+      }
+      case ExternalReference::BUILTIN_FP_FP_CALL: {
+        SimulatorRuntimeFPFPCall target =
+          reinterpret_cast<SimulatorRuntimeFPFPCall>(external);
+        dresult = target(dval0, dval1);
+        SetFpResult(dresult);
+        break;
+      }
+      case ExternalReference::BUILTIN_FP_CALL: {
+        SimulatorRuntimeFPCall target =
+          reinterpret_cast<SimulatorRuntimeFPCall>(external);
+        dresult = target(dval0);
+        SetFpResult(dresult);
+        break;
+      }
+      case ExternalReference::BUILTIN_FP_INT_CALL: {
+        SimulatorRuntimeFPIntCall target =
+          reinterpret_cast<SimulatorRuntimeFPIntCall>(external);
+        dresult = target(dval0, ival);
+        SetFpResult(dresult);
+        break;
+      }
+      default:
+        UNREACHABLE();
+        break;
+      }
+      if (::v8::internal::FLAG_trace_sim) {
+        switch (redirection->type()) {
+        case ExternalReference::BUILTIN_COMPARE_CALL:
+          PrintF("Returned %08x\n", static_cast<int32_t>(iresult));
+          break;
+        case ExternalReference::BUILTIN_FP_FP_CALL:
+        case ExternalReference::BUILTIN_FP_CALL:
+        case ExternalReference::BUILTIN_FP_INT_CALL:
+          PrintF("Returned %f\n", dresult);
+          break;
+        default:
+          UNREACHABLE();
+          break;
+        }
+      }
+    } else if (redirection->type() == ExternalReference::DIRECT_API_CALL) {
+      if (::v8::internal::FLAG_trace_sim) {
+        PrintF("Call to host function at %p args %08" PRIx64 " \n",
+               reinterpret_cast<void*>(external), arg0);
+      }
+      SimulatorRuntimeDirectApiCall target =
+          reinterpret_cast<SimulatorRuntimeDirectApiCall>(external);
+      target(arg0);
+    } else if (
+        redirection->type() == ExternalReference::PROFILING_API_CALL) {
+      if (::v8::internal::FLAG_trace_sim) {
+        PrintF("Call to host function at %p args %08" PRIx64 "  %08" PRIx64
+               " \n",
+               reinterpret_cast<void*>(external), arg0, arg1);
+      }
+      SimulatorRuntimeProfilingApiCall target =
+          reinterpret_cast<SimulatorRuntimeProfilingApiCall>(external);
+      target(arg0, Redirection::ReverseRedirection(arg1));
+    } else if (
+        redirection->type() == ExternalReference::DIRECT_GETTER_CALL) {
+      if (::v8::internal::FLAG_trace_sim) {
+        PrintF("Call to host function at %p args %08" PRIx64 "  %08" PRIx64
+               " \n",
+               reinterpret_cast<void*>(external), arg0, arg1);
+      }
+      SimulatorRuntimeDirectGetterCall target =
+          reinterpret_cast<SimulatorRuntimeDirectGetterCall>(external);
+      target(arg0, arg1);
+    } else if (
+        redirection->type() == ExternalReference::PROFILING_GETTER_CALL) {
+      if (::v8::internal::FLAG_trace_sim) {
+        PrintF("Call to host function at %p args %08" PRIx64 "  %08" PRIx64
+               "  %08" PRIx64 " \n",
+               reinterpret_cast<void*>(external), arg0, arg1, arg2);
+      }
+      SimulatorRuntimeProfilingGetterCall target =
+          reinterpret_cast<SimulatorRuntimeProfilingGetterCall>(external);
+      target(arg0, arg1, Redirection::ReverseRedirection(arg2));
+    } else {
+      DCHECK(redirection->type() == ExternalReference::BUILTIN_CALL ||
+             redirection->type() == ExternalReference::BUILTIN_CALL_PAIR);
+      SimulatorRuntimeCall target =
+                  reinterpret_cast<SimulatorRuntimeCall>(external);
+      if (::v8::internal::FLAG_trace_sim) {
+        PrintF(
+            "Call to host function at %p "
+            "args %08" PRIx64 " , %08" PRIx64 " , %08" PRIx64 " , %08" PRIx64
+            " , %08" PRIx64 " , %08" PRIx64 " , %08" PRIx64 " , %08" PRIx64
+            " , %08" PRIx64 " , %08" PRIx64 " \n",
+            reinterpret_cast<void*>(FUNCTION_ADDR(target)), arg0, arg1, arg2,
+            arg3, arg4, arg5, arg6, arg7, arg8, arg9);
+      }
+      ObjectPair result =
+          target(arg0, arg1, arg2, arg3, arg4, arg5, arg6, arg7, arg8, arg9);
+      set_register(v0, (int64_t)(result.x));
+      set_register(a5, (int64_t)(result.y));
+    }
+     if (::v8::internal::FLAG_trace_sim) {
+       PrintF("Returned %08" PRIx64 "  : %08" PRIx64 " \n", get_register(t4),
+              get_register(v0));
+    }
+    set_register(ra, saved_ra);
+    set_pc(get_register(ra));
+
+  //} else if (func == BREAK && code <= kMaxStopCode) {
+  //  if (IsWatchpoint(code)) {
+  //    PrintWatchpoint(code);
+  //  } else {
+  //    IncreaseStopCounter(code);
+  //    HandleStop(code, instr_.instr());
+  //  }
+  } else if (code == BREAK) {
+    // All remaining break_ codes, and all traps are handled here.
+    Sw64Debugger dbg(this);
+    dbg.Debug();
+  }
+}
+
+
+// Stop helper functions.
+bool Simulator::IsWatchpoint(uint64_t code) {
+  return (code <= kMaxWatchpointCode);
+}
+
+
+void Simulator::PrintWatchpoint(uint64_t code) {
+  Sw64Debugger dbg(this);
+  ++break_count_;
+  PrintF("\n---- break %" PRId64 "  marker: %3d  (instr count: %8" PRId64
+         " ) ----------"
+         "----------------------------------",
+         code, break_count_, icount_);
+  dbg.PrintAllRegs();  // Print registers and continue running.
+}
+
+
+void Simulator::HandleStop(uint64_t code, Instruction* instr) {
+  // Stop if it is enabled, otherwise go on jumping over the stop
+  // and the message address.
+  if (IsEnabledStop(code)) {
+    Sw64Debugger dbg(this);
+    dbg.Stop(instr);
+  }
+}
+
+
+bool Simulator::IsStopInstruction(Instruction* instr) {
+  int32_t func = instr->FunctionFieldRaw();
+  uint32_t code = static_cast<uint32_t>(instr->Bits(25, 6));
+  return (func == BREAK) && code > kMaxWatchpointCode && code <= kMaxStopCode;
+}
+
+
+bool Simulator::IsEnabledStop(uint64_t code) {
+  DCHECK_LE(code, kMaxStopCode);
+  DCHECK_GT(code, kMaxWatchpointCode);
+  return !(watched_stops_[code].count & kStopDisabledBit);
+}
+
+
+void Simulator::EnableStop(uint64_t code) {
+  if (!IsEnabledStop(code)) {
+    watched_stops_[code].count &= ~kStopDisabledBit;
+  }
+}
+
+
+void Simulator::DisableStop(uint64_t code) {
+  if (IsEnabledStop(code)) {
+    watched_stops_[code].count |= kStopDisabledBit;
+  }
+}
+
+
+void Simulator::IncreaseStopCounter(uint64_t code) {
+  DCHECK_LE(code, kMaxStopCode);
+  if ((watched_stops_[code].count & ~(1 << 31)) == 0x7FFFFFFF) {
+    PrintF("Stop counter for code %" PRId64
+           "  has overflowed.\n"
+           "Enabling this code and reseting the counter to 0.\n",
+           code);
+    watched_stops_[code].count = 0;
+    EnableStop(code);
+  } else {
+    watched_stops_[code].count++;
+  }
+}
+
+
+// Print a stop status.
+void Simulator::PrintStopInfo(uint64_t code) {
+  if (code <= kMaxWatchpointCode) {
+    PrintF("That is a watchpoint, not a stop.\n");
+    return;
+  } else if (code > kMaxStopCode) {
+    PrintF("Code too large, only %u stops can be used\n", kMaxStopCode + 1);
+    return;
+  }
+  const char* state = IsEnabledStop(code) ? "Enabled" : "Disabled";
+  int32_t count = watched_stops_[code].count & ~kStopDisabledBit;
+  // Don't print the state of unused breakpoints.
+  if (count != 0) {
+    if (watched_stops_[code].desc) {
+      PrintF("stop %" PRId64 "  - 0x%" PRIx64 " : \t%s, \tcounter = %i, \t%s\n",
+             code, code, state, count, watched_stops_[code].desc);
+    } else {
+      PrintF("stop %" PRId64 "  - 0x%" PRIx64 " : \t%s, \tcounter = %i\n", code,
+             code, state, count);
+    }
+  }
+}
+
+
+void Simulator::SignalException(Exception e) {
+  FATAL("Error: Exception %i raised.", static_cast<int>(e));
+}
+
+// Min/Max template functions for Double and Single arguments.
+
+template <typename T>
+static T FPAbs(T a);
+
+template <>
+double FPAbs<double>(double a) {
+  return fabs(a);
+}
+
+template <>
+float FPAbs<float>(float a) {
+  return fabsf(a);
+}
+
+template <typename T>
+static bool FPUProcessNaNsAndZeros(T a, T b, MaxMinKind kind, T* result) {
+  if (std::isnan(a) && std::isnan(b)) {
+    *result = a;
+  } else if (std::isnan(a)) {
+    *result = b;
+  } else if (std::isnan(b)) {
+    *result = a;
+  } else if (b == a) {
+    // Handle -0.0 == 0.0 case.
+    // std::signbit() returns int 0 or 1 so subtracting MaxMinKind::kMax
+    // negates the result.
+    *result = std::signbit(b) - static_cast<int>(kind) ? b : a;
+  } else {
+    return false;
+  }
+  return true;
+}
+
+template <typename T>
+static T FPUMin(T a, T b) {
+  T result;
+  if (FPUProcessNaNsAndZeros(a, b, MaxMinKind::kMin, &result)) {
+    return result;
+  } else {
+    return b < a ? b : a;
+  }
+}
+
+template <typename T>
+static T FPUMax(T a, T b) {
+  T result;
+  if (FPUProcessNaNsAndZeros(a, b, MaxMinKind::kMax, &result)) {
+    return result;
+  } else {
+    return b > a ? b : a;
+  }
+}
+
+template <typename T>
+static T FPUMinA(T a, T b) {
+  T result;
+  if (!FPUProcessNaNsAndZeros(a, b, MaxMinKind::kMin, &result)) {
+    if (FPAbs(a) < FPAbs(b)) {
+      result = a;
+    } else if (FPAbs(b) < FPAbs(a)) {
+      result = b;
+    } else {
+      result = a < b ? a : b;
+    }
+  }
+  return result;
+}
+
+template <typename T>
+static T FPUMaxA(T a, T b) {
+  T result;
+  if (!FPUProcessNaNsAndZeros(a, b, MaxMinKind::kMin, &result)) {
+    if (FPAbs(a) > FPAbs(b)) {
+      result = a;
+    } else if (FPAbs(b) > FPAbs(a)) {
+      result = b;
+    } else {
+      result = a > b ? a : b;
+    }
+  }
+  return result;
+}
+
+enum class KeepSign : bool { no = false, yes };
+
+template <typename T, typename std::enable_if<std::is_floating_point<T>::value,
+                                              int>::type = 0>
+T FPUCanonalizeNaNArg(T result, T arg, KeepSign keepSign = KeepSign::no) {
+  DCHECK(std::isnan(arg));
+  T qNaN = std::numeric_limits<T>::quiet_NaN();
+  if (keepSign == KeepSign::yes) {
+    return std::copysign(qNaN, result);
+  }
+  return qNaN;
+}
+
+template <typename T>
+T FPUCanonalizeNaNArgs(T result, KeepSign keepSign, T first) {
+  if (std::isnan(first)) {
+    return FPUCanonalizeNaNArg(result, first, keepSign);
+  }
+  return result;
+}
+
+template <typename T, typename... Args>
+T FPUCanonalizeNaNArgs(T result, KeepSign keepSign, T first, Args... args) {
+  if (std::isnan(first)) {
+    return FPUCanonalizeNaNArg(result, first, keepSign);
+  }
+  return FPUCanonalizeNaNArgs(result, keepSign, args...);
+}
+
+template <typename Func, typename T, typename... Args>
+T FPUCanonalizeOperation(Func f, T first, Args... args) {
+  return FPUCanonalizeOperation(f, KeepSign::no, first, args...);
+}
+
+template <typename Func, typename T, typename... Args>
+T FPUCanonalizeOperation(Func f, KeepSign keepSign, T first, Args... args) {
+  T result = f(first, args...);
+  if (std::isnan(result)) {
+    result = FPUCanonalizeNaNArgs(result, keepSign, first, args...);
+  }
+  return result;
+}
+
+// Handle execution based on instruction types.
+
+void Simulator::DecodeTypeRegisterSRsType() {
+  float fs, ft, fd;
+  fs = get_fpu_register_float(fs_reg());
+  ft = get_fpu_register_float(ft_reg());
+  fd = get_fpu_register_float(fd_reg());
+  int32_t ft_int = bit_cast<int32_t>(ft);
+  int32_t fd_int = bit_cast<int32_t>(fd);
+  uint32_t cc, fcsr_cc;
+  cc = instr_.FCccValue();
+  fcsr_cc = get_fcsr_condition_bit(cc);
+  switch (instr_.FunctionFieldRaw()) {
+    case RINT: {
+      DCHECK_EQ(kArchVariant, kSw64r3);
+      float result, temp_result;
+      double temp;
+      float upper = std::ceil(fs);
+      float lower = std::floor(fs);
+      switch (get_fcsr_rounding_mode()) {
+        case kRoundToNearest:
+          if (upper - fs < fs - lower) {
+            result = upper;
+          } else if (upper - fs > fs - lower) {
+            result = lower;
+          } else {
+            temp_result = upper / 2;
+            float reminder = modf(temp_result, &temp);
+            if (reminder == 0) {
+              result = upper;
+            } else {
+              result = lower;
+            }
+          }
+          break;
+        case kRoundToZero:
+          result = (fs > 0 ? lower : upper);
+          break;
+        case kRoundToPlusInf:
+          result = upper;
+          break;
+        case kRoundToMinusInf:
+          result = lower;
+          break;
+      }
+      SetFPUFloatResult(fd_reg(), result);
+      if (result != fs) {
+        set_fcsr_bit(kFCSRInexactFlagBit, true);
+      }
+      break;
+    }
+    case ADD_S:
+      SetFPUFloatResult(
+          fd_reg(),
+          FPUCanonalizeOperation([](float lhs, float rhs) { return lhs + rhs; },
+                                 fs, ft));
+      break;
+    case SUB_S:
+      SetFPUFloatResult(
+          fd_reg(),
+          FPUCanonalizeOperation([](float lhs, float rhs) { return lhs - rhs; },
+                                 fs, ft));
+      break;
+    case MADDF_S:
+      DCHECK_EQ(kArchVariant, kSw64r3);
+      SetFPUFloatResult(fd_reg(), std::fma(fs, ft, fd));
+      break;
+    case MSUBF_S:
+      DCHECK_EQ(kArchVariant, kSw64r3);
+      SetFPUFloatResult(fd_reg(), std::fma(-fs, ft, fd));
+      break;
+    case MUL_S:
+      SetFPUFloatResult(
+          fd_reg(),
+          FPUCanonalizeOperation([](float lhs, float rhs) { return lhs * rhs; },
+                                 fs, ft));
+      break;
+    case DIV_S:
+      SetFPUFloatResult(
+          fd_reg(),
+          FPUCanonalizeOperation([](float lhs, float rhs) { return lhs / rhs; },
+                                 fs, ft));
+      break;
+    case ABS_S:
+      SetFPUFloatResult(fd_reg(), FPUCanonalizeOperation(
+                                      [](float fs) { return FPAbs(fs); }, fs));
+      break;
+    case MOV_S:
+      SetFPUFloatResult(fd_reg(), fs);
+      break;
+    case NEG_S:
+      SetFPUFloatResult(fd_reg(),
+                        FPUCanonalizeOperation([](float src) { return -src; },
+                                               KeepSign::yes, fs));
+      break;
+    case SQRT_S:
+      SetFPUFloatResult(
+          fd_reg(),
+          FPUCanonalizeOperation([](float src) { return std::sqrt(src); }, fs));
+      break;
+    case RSQRT_S:
+      SetFPUFloatResult(
+          fd_reg(), FPUCanonalizeOperation(
+                        [](float src) { return 1.0 / std::sqrt(src); }, fs));
+      break;
+    case RECIP_S:
+      SetFPUFloatResult(fd_reg(), FPUCanonalizeOperation(
+                                      [](float src) { return 1.0 / src; }, fs));
+      break;
+    case C_F_D:
+      set_fcsr_bit(fcsr_cc, false);
+      TraceRegWr(test_fcsr_bit(fcsr_cc));
+      break;
+    case C_UN_D:
+      set_fcsr_bit(fcsr_cc, std::isnan(fs) || std::isnan(ft));
+      TraceRegWr(test_fcsr_bit(fcsr_cc));
+      break;
+    case C_EQ_D:
+      set_fcsr_bit(fcsr_cc, (fs == ft));
+      TraceRegWr(test_fcsr_bit(fcsr_cc));
+      break;
+    case C_UEQ_D:
+      set_fcsr_bit(fcsr_cc, (fs == ft) || (std::isnan(fs) || std::isnan(ft)));
+      TraceRegWr(test_fcsr_bit(fcsr_cc));
+      break;
+    case C_OLT_D:
+      set_fcsr_bit(fcsr_cc, (fs < ft));
+      TraceRegWr(test_fcsr_bit(fcsr_cc));
+      break;
+    case C_ULT_D:
+      set_fcsr_bit(fcsr_cc, (fs < ft) || (std::isnan(fs) || std::isnan(ft)));
+      TraceRegWr(test_fcsr_bit(fcsr_cc));
+      break;
+    case C_OLE_D:
+      set_fcsr_bit(fcsr_cc, (fs <= ft));
+      TraceRegWr(test_fcsr_bit(fcsr_cc));
+      break;
+    case C_ULE_D:
+      set_fcsr_bit(fcsr_cc, (fs <= ft) || (std::isnan(fs) || std::isnan(ft)));
+      TraceRegWr(test_fcsr_bit(fcsr_cc));
+      break;
+    case CVT_D_S:
+      SetFPUDoubleResult(fd_reg(), static_cast<double>(fs));
+      break;
+    case CLASS_S: {
+      // Convert float input to uint32_t for easier bit manipulation
+      uint32_t classed = bit_cast<uint32_t>(fs);
+
+      // Extracting sign, exponent and mantissa from the input float
+      uint32_t sign = (classed >> 31) & 1;
+      uint32_t exponent = (classed >> 23) & 0x000000FF;
+      uint32_t mantissa = classed & 0x007FFFFF;
+      uint32_t result;
+      float fResult;
+
+      // Setting flags if input float is negative infinity,
+      // positive infinity, negative zero or positive zero
+      bool negInf = (classed == 0xFF800000);
+      bool posInf = (classed == 0x7F800000);
+      bool negZero = (classed == 0x80000000);
+      bool posZero = (classed == 0x00000000);
+
+      bool signalingNan;
+      bool quietNan;
+      bool negSubnorm;
+      bool posSubnorm;
+      bool negNorm;
+      bool posNorm;
+
+      // Setting flags if float is NaN
+      signalingNan = false;
+      quietNan = false;
+      if (!negInf && !posInf && (exponent == 0xFF)) {
+        quietNan = ((mantissa & 0x00200000) == 0) &&
+                   ((mantissa & (0x00200000 - 1)) == 0);
+        signalingNan = !quietNan;
+      }
+
+      // Setting flags if float is subnormal number
+      posSubnorm = false;
+      negSubnorm = false;
+      if ((exponent == 0) && (mantissa != 0)) {
+        DCHECK(sign == 0 || sign == 1);
+        posSubnorm = (sign == 0);
+        negSubnorm = (sign == 1);
+      }
+
+      // Setting flags if float is normal number
+      posNorm = false;
+      negNorm = false;
+      if (!posSubnorm && !negSubnorm && !posInf && !negInf && !signalingNan &&
+          !quietNan && !negZero && !posZero) {
+        DCHECK(sign == 0 || sign == 1);
+        posNorm = (sign == 0);
+        negNorm = (sign == 1);
+      }
+
+      // Calculating result according to description of CLASS.S instruction
+      result = (posZero << 9) | (posSubnorm << 8) | (posNorm << 7) |
+               (posInf << 6) | (negZero << 5) | (negSubnorm << 4) |
+               (negNorm << 3) | (negInf << 2) | (quietNan << 1) | signalingNan;
+
+      DCHECK_NE(result, 0);
+
+      fResult = bit_cast<float>(result);
+      SetFPUFloatResult(fd_reg(), fResult);
+      break;
+    }
+    case CVT_L_S: {
+      float rounded;
+      int64_t result;
+      round64_according_to_fcsr(fs, &rounded, &result, fs);
+      SetFPUResult(fd_reg(), result);
+      if (set_fcsr_round64_error(fs, rounded)) {
+        set_fpu_register_invalid_result64(fs, rounded);
+      }
+      break;
+    }
+    case CVT_W_S: {
+      float rounded;
+      int32_t result;
+      round_according_to_fcsr(fs, &rounded, &result, fs);
+      SetFPUWordResult(fd_reg(), result);
+      if (set_fcsr_round_error(fs, rounded)) {
+        set_fpu_register_word_invalid_result(fs, rounded);
+      }
+      break;
+    }
+    case TRUNC_W_S: {  // Truncate single to word (round towards 0).
+      float rounded = trunc(fs);
+      int32_t result = static_cast<int32_t>(rounded);
+      SetFPUWordResult(fd_reg(), result);
+      if (set_fcsr_round_error(fs, rounded)) {
+        set_fpu_register_word_invalid_result(fs, rounded);
+      }
+    } break;
+    case TRUNC_L_S: {
+      float rounded = trunc(fs);
+      int64_t result = static_cast<int64_t>(rounded);
+      SetFPUResult(fd_reg(), result);
+      if (set_fcsr_round64_error(fs, rounded)) {
+        set_fpu_register_invalid_result64(fs, rounded);
+      }
+      break;
+    }
+    case ROUND_W_S: {
+      float rounded = std::floor(fs + 0.5);
+      int32_t result = static_cast<int32_t>(rounded);
+      if ((result & 1) != 0 && result - fs == 0.5) {
+        // If the number is halfway between two integers,
+        // round to the even one.
+        result--;
+      }
+      SetFPUWordResult(fd_reg(), result);
+      if (set_fcsr_round_error(fs, rounded)) {
+        set_fpu_register_word_invalid_result(fs, rounded);
+      }
+      break;
+    }
+    case ROUND_L_S: {
+      float rounded = std::floor(fs + 0.5);
+      int64_t result = static_cast<int64_t>(rounded);
+      if ((result & 1) != 0 && result - fs == 0.5) {
+        // If the number is halfway between two integers,
+        // round to the even one.
+        result--;
+      }
+      int64_t i64 = static_cast<int64_t>(result);
+      SetFPUResult(fd_reg(), i64);
+      if (set_fcsr_round64_error(fs, rounded)) {
+        set_fpu_register_invalid_result64(fs, rounded);
+      }
+      break;
+    }
+    case FLOOR_L_S: {
+      float rounded = floor(fs);
+      int64_t result = static_cast<int64_t>(rounded);
+      SetFPUResult(fd_reg(), result);
+      if (set_fcsr_round64_error(fs, rounded)) {
+        set_fpu_register_invalid_result64(fs, rounded);
+      }
+      break;
+    }
+    case FLOOR_W_S:  // Round double to word towards negative infinity.
+    {
+      float rounded = std::floor(fs);
+      int32_t result = static_cast<int32_t>(rounded);
+      SetFPUWordResult(fd_reg(), result);
+      if (set_fcsr_round_error(fs, rounded)) {
+        set_fpu_register_word_invalid_result(fs, rounded);
+      }
+    } break;
+    case CEIL_W_S:  // Round double to word towards positive infinity.
+    {
+      float rounded = std::ceil(fs);
+      int32_t result = static_cast<int32_t>(rounded);
+      SetFPUWordResult(fd_reg(), result);
+      if (set_fcsr_round_error(fs, rounded)) {
+        set_fpu_register_invalid_result(fs, rounded);
+      }
+    } break;
+    case CEIL_L_S: {
+      float rounded = ceil(fs);
+      int64_t result = static_cast<int64_t>(rounded);
+      SetFPUResult(fd_reg(), result);
+      if (set_fcsr_round64_error(fs, rounded)) {
+        set_fpu_register_invalid_result64(fs, rounded);
+      }
+      break;
+    }
+    case MINA:
+      DCHECK_EQ(kArchVariant, kSw64r3);
+      SetFPUFloatResult(fd_reg(), FPUMinA(ft, fs));
+      break;
+    case MAXA:
+      DCHECK_EQ(kArchVariant, kSw64r3);
+      SetFPUFloatResult(fd_reg(), FPUMaxA(ft, fs));
+      break;
+    case MIN:
+      DCHECK_EQ(kArchVariant, kSw64r3);
+      SetFPUFloatResult(fd_reg(), FPUMin(ft, fs));
+      break;
+    case MAX:
+      DCHECK_EQ(kArchVariant, kSw64r3);
+      SetFPUFloatResult(fd_reg(), FPUMax(ft, fs));
+      break;
+    case SEL:
+      DCHECK_EQ(kArchVariant, kSw64r3);
+      SetFPUFloatResult(fd_reg(), (fd_int & 0x1) == 0 ? fs : ft);
+      break;
+    case SELEQZ_C:
+      DCHECK_EQ(kArchVariant, kSw64r3);
+      SetFPUFloatResult(
+          fd_reg(),
+          (ft_int & 0x1) == 0 ? get_fpu_register_float(fs_reg()) : 0.0);
+      break;
+    case SELNEZ_C:
+      DCHECK_EQ(kArchVariant, kSw64r3);
+      SetFPUFloatResult(
+          fd_reg(),
+          (ft_int & 0x1) != 0 ? get_fpu_register_float(fs_reg()) : 0.0);
+      break;
+    case MOVZ_C: {
+      DCHECK_EQ(kArchVariant, kSw64r3);
+      if (rt() == 0) {
+        SetFPUFloatResult(fd_reg(), fs);
+      }
+      break;
+    }
+    case MOVN_C: {
+      DCHECK_EQ(kArchVariant, kSw64r3);
+      if (rt() != 0) {
+        SetFPUFloatResult(fd_reg(), fs);
+      }
+      break;
+    }
+    case MOVF: {
+      // Same function field for MOVT.D and MOVF.D
+      uint32_t ft_cc = (ft_reg() >> 2) & 0x7;
+      ft_cc = get_fcsr_condition_bit(ft_cc);
+
+      if (instr_.Bit(16)) {  // Read Tf bit.
+        // MOVT.D
+        if (test_fcsr_bit(ft_cc)) SetFPUFloatResult(fd_reg(), fs);
+      } else {
+        // MOVF.D
+        if (!test_fcsr_bit(ft_cc)) SetFPUFloatResult(fd_reg(), fs);
+      }
+      break;
+    }
+    default:
+      // TRUNC_W_S ROUND_W_S ROUND_L_S FLOOR_W_S FLOOR_L_S
+      // CEIL_W_S CEIL_L_S CVT_PS_S are unimplemented.
+      UNREACHABLE();
+  }
+}
+
+
+void Simulator::DecodeTypeRegisterDRsType() {
+  double ft, fs, fd;
+  uint32_t cc, fcsr_cc;
+  fs = get_fpu_register_double(fs_reg());
+  ft = (instr_.FunctionFieldRaw() != MOVF) ? get_fpu_register_double(ft_reg())
+                                           : 0.0;
+  fd = get_fpu_register_double(fd_reg());
+  cc = instr_.FCccValue();
+  fcsr_cc = get_fcsr_condition_bit(cc);
+  int64_t ft_int = bit_cast<int64_t>(ft);
+  int64_t fd_int = bit_cast<int64_t>(fd);
+  switch (instr_.FunctionFieldRaw()) {
+    case RINT: {
+      DCHECK_EQ(kArchVariant, kSw64r3);
+      double result, temp, temp_result;
+      double upper = std::ceil(fs);
+      double lower = std::floor(fs);
+      switch (get_fcsr_rounding_mode()) {
+        case kRoundToNearest:
+          if (upper - fs < fs - lower) {
+            result = upper;
+          } else if (upper - fs > fs - lower) {
+            result = lower;
+          } else {
+            temp_result = upper / 2;
+            double reminder = modf(temp_result, &temp);
+            if (reminder == 0) {
+              result = upper;
+            } else {
+              result = lower;
+            }
+          }
+          break;
+        case kRoundToZero:
+          result = (fs > 0 ? lower : upper);
+          break;
+        case kRoundToPlusInf:
+          result = upper;
+          break;
+        case kRoundToMinusInf:
+          result = lower;
+          break;
+      }
+      SetFPUDoubleResult(fd_reg(), result);
+      if (result != fs) {
+        set_fcsr_bit(kFCSRInexactFlagBit, true);
+      }
+      break;
+    }
+    case SEL:
+      DCHECK_EQ(kArchVariant, kSw64r3);
+      SetFPUDoubleResult(fd_reg(), (fd_int & 0x1) == 0 ? fs : ft);
+      break;
+    case SELEQZ_C:
+      DCHECK_EQ(kArchVariant, kSw64r3);
+      SetFPUDoubleResult(fd_reg(), (ft_int & 0x1) == 0 ? fs : 0.0);
+      break;
+    case SELNEZ_C:
+      DCHECK_EQ(kArchVariant, kSw64r3);
+      SetFPUDoubleResult(fd_reg(), (ft_int & 0x1) != 0 ? fs : 0.0);
+      break;
+    case MOVZ_C: {
+      DCHECK_EQ(kArchVariant, kSw64r2);
+      if (rt() == 0) {
+        SetFPUDoubleResult(fd_reg(), fs);
+      }
+      break;
+    }
+    case MOVN_C: {
+      DCHECK_EQ(kArchVariant, kSw64r2);
+      if (rt() != 0) {
+        SetFPUDoubleResult(fd_reg(), fs);
+      }
+      break;
+    }
+    case MOVF: {
+      // Same function field for MOVT.D and MOVF.D
+      uint32_t ft_cc = (ft_reg() >> 2) & 0x7;
+      ft_cc = get_fcsr_condition_bit(ft_cc);
+      if (instr_.Bit(16)) {  // Read Tf bit.
+        // MOVT.D
+        if (test_fcsr_bit(ft_cc)) SetFPUDoubleResult(fd_reg(), fs);
+      } else {
+        // MOVF.D
+        if (!test_fcsr_bit(ft_cc)) SetFPUDoubleResult(fd_reg(), fs);
+      }
+      break;
+    }
+    case MINA:
+      DCHECK_EQ(kArchVariant, kSw64r3);
+      SetFPUDoubleResult(fd_reg(), FPUMinA(ft, fs));
+      break;
+    case MAXA:
+      DCHECK_EQ(kArchVariant, kSw64r3);
+      SetFPUDoubleResult(fd_reg(), FPUMaxA(ft, fs));
+      break;
+    case MIN:
+      DCHECK_EQ(kArchVariant, kSw64r3);
+      SetFPUDoubleResult(fd_reg(), FPUMin(ft, fs));
+      break;
+    case MAX:
+      DCHECK_EQ(kArchVariant, kSw64r3);
+      SetFPUDoubleResult(fd_reg(), FPUMax(ft, fs));
+      break;
+    case ADD_D:
+      SetFPUDoubleResult(
+          fd_reg(),
+          FPUCanonalizeOperation(
+              [](double lhs, double rhs) { return lhs + rhs; }, fs, ft));
+      break;
+    case SUB_D:
+      SetFPUDoubleResult(
+          fd_reg(),
+          FPUCanonalizeOperation(
+              [](double lhs, double rhs) { return lhs - rhs; }, fs, ft));
+      break;
+    case MADDF_D:
+      DCHECK_EQ(kArchVariant, kSw64r3);
+      SetFPUDoubleResult(fd_reg(), std::fma(fs, ft, fd));
+      break;
+    case MSUBF_D:
+      DCHECK_EQ(kArchVariant, kSw64r3);
+      SetFPUDoubleResult(fd_reg(), std::fma(-fs, ft, fd));
+      break;
+    case MUL_D:
+      SetFPUDoubleResult(
+          fd_reg(),
+          FPUCanonalizeOperation(
+              [](double lhs, double rhs) { return lhs * rhs; }, fs, ft));
+      break;
+    case DIV_D:
+      SetFPUDoubleResult(
+          fd_reg(),
+          FPUCanonalizeOperation(
+              [](double lhs, double rhs) { return lhs / rhs; }, fs, ft));
+      break;
+    case ABS_D:
+      SetFPUDoubleResult(
+          fd_reg(),
+          FPUCanonalizeOperation([](double fs) { return FPAbs(fs); }, fs));
+      break;
+    case MOV_D:
+      SetFPUDoubleResult(fd_reg(), fs);
+      break;
+    case NEG_D:
+      SetFPUDoubleResult(fd_reg(),
+                         FPUCanonalizeOperation([](double src) { return -src; },
+                                                KeepSign::yes, fs));
+      break;
+    case SQRT_D:
+      SetFPUDoubleResult(
+          fd_reg(),
+          FPUCanonalizeOperation([](double fs) { return std::sqrt(fs); }, fs));
+      break;
+    case RSQRT_D:
+      SetFPUDoubleResult(
+          fd_reg(), FPUCanonalizeOperation(
+                        [](double fs) { return 1.0 / std::sqrt(fs); }, fs));
+      break;
+    case RECIP_D:
+      SetFPUDoubleResult(fd_reg(), FPUCanonalizeOperation(
+                                       [](double fs) { return 1.0 / fs; }, fs));
+      break;
+    case C_UN_D:
+      set_fcsr_bit(fcsr_cc, std::isnan(fs) || std::isnan(ft));
+      TraceRegWr(test_fcsr_bit(fcsr_cc));
+      break;
+    case C_EQ_D:
+      set_fcsr_bit(fcsr_cc, (fs == ft));
+      TraceRegWr(test_fcsr_bit(fcsr_cc));
+      break;
+    case C_UEQ_D:
+      set_fcsr_bit(fcsr_cc, (fs == ft) || (std::isnan(fs) || std::isnan(ft)));
+      TraceRegWr(test_fcsr_bit(fcsr_cc));
+      break;
+    case C_OLT_D:
+      set_fcsr_bit(fcsr_cc, (fs < ft));
+      TraceRegWr(test_fcsr_bit(fcsr_cc));
+      break;
+    case C_ULT_D:
+      set_fcsr_bit(fcsr_cc, (fs < ft) || (std::isnan(fs) || std::isnan(ft)));
+      TraceRegWr(test_fcsr_bit(fcsr_cc));
+      break;
+    case C_OLE_D:
+      set_fcsr_bit(fcsr_cc, (fs <= ft));
+      TraceRegWr(test_fcsr_bit(fcsr_cc));
+      break;
+    case C_ULE_D:
+      set_fcsr_bit(fcsr_cc, (fs <= ft) || (std::isnan(fs) || std::isnan(ft)));
+      TraceRegWr(test_fcsr_bit(fcsr_cc));
+      break;
+    case CVT_W_D: {  // Convert double to word.
+      double rounded;
+      int32_t result;
+      round_according_to_fcsr(fs, &rounded, &result, fs);
+      SetFPUWordResult(fd_reg(), result);
+      if (set_fcsr_round_error(fs, rounded)) {
+        set_fpu_register_word_invalid_result(fs, rounded);
+      }
+      break;
+    }
+    case ROUND_W_D:  // Round double to word (round half to even).
+    {
+      double rounded = std::floor(fs + 0.5);
+      int32_t result = static_cast<int32_t>(rounded);
+      if ((result & 1) != 0 && result - fs == 0.5) {
+        // If the number is halfway between two integers,
+        // round to the even one.
+        result--;
+      }
+      SetFPUWordResult(fd_reg(), result);
+      if (set_fcsr_round_error(fs, rounded)) {
+        set_fpu_register_invalid_result(fs, rounded);
+      }
+    } break;
+    case TRUNC_W_D:  // Truncate double to word (round towards 0).
+    {
+      double rounded = trunc(fs);
+      int32_t result = static_cast<int32_t>(rounded);
+      SetFPUWordResult(fd_reg(), result);
+      if (set_fcsr_round_error(fs, rounded)) {
+        set_fpu_register_invalid_result(fs, rounded);
+      }
+    } break;
+    case FLOOR_W_D:  // Round double to word towards negative infinity.
+    {
+      double rounded = std::floor(fs);
+      int32_t result = static_cast<int32_t>(rounded);
+      SetFPUWordResult(fd_reg(), result);
+      if (set_fcsr_round_error(fs, rounded)) {
+        set_fpu_register_invalid_result(fs, rounded);
+      }
+    } break;
+    case CEIL_W_D:  // Round double to word towards positive infinity.
+    {
+      double rounded = std::ceil(fs);
+      int32_t result = static_cast<int32_t>(rounded);
+      SetFPUWordResult2(fd_reg(), result);
+      if (set_fcsr_round_error(fs, rounded)) {
+        set_fpu_register_invalid_result(fs, rounded);
+      }
+    } break;
+    case CVT_S_D:  // Convert double to float (single).
+      SetFPUFloatResult(fd_reg(), static_cast<float>(fs));
+      break;
+    case CVT_L_D: {
+      double rounded;
+      int64_t result;
+      round64_according_to_fcsr(fs, &rounded, &result, fs);
+      SetFPUResult(fd_reg(), result);
+      if (set_fcsr_round64_error(fs, rounded)) {
+        set_fpu_register_invalid_result64(fs, rounded);
+      }
+      break;
+    }
+    case ROUND_L_D: {
+      double rounded = std::floor(fs + 0.5);
+      int64_t result = static_cast<int64_t>(rounded);
+      if ((result & 1) != 0 && result - fs == 0.5) {
+        // If the number is halfway between two integers,
+        // round to the even one.
+        result--;
+      }
+      int64_t i64 = static_cast<int64_t>(result);
+      SetFPUResult(fd_reg(), i64);
+      if (set_fcsr_round64_error(fs, rounded)) {
+        set_fpu_register_invalid_result64(fs, rounded);
+      }
+      break;
+    }
+    case TRUNC_L_D: {
+      double rounded = trunc(fs);
+      int64_t result = static_cast<int64_t>(rounded);
+      SetFPUResult(fd_reg(), result);
+      if (set_fcsr_round64_error(fs, rounded)) {
+        set_fpu_register_invalid_result64(fs, rounded);
+      }
+      break;
+    }
+    case FLOOR_L_D: {
+      double rounded = floor(fs);
+      int64_t result = static_cast<int64_t>(rounded);
+      SetFPUResult(fd_reg(), result);
+      if (set_fcsr_round64_error(fs, rounded)) {
+        set_fpu_register_invalid_result64(fs, rounded);
+      }
+      break;
+    }
+    case CEIL_L_D: {
+      double rounded = ceil(fs);
+      int64_t result = static_cast<int64_t>(rounded);
+      SetFPUResult(fd_reg(), result);
+      if (set_fcsr_round64_error(fs, rounded)) {
+        set_fpu_register_invalid_result64(fs, rounded);
+      }
+      break;
+    }
+    case CLASS_D: {
+      // Convert double input to uint64_t for easier bit manipulation
+      uint64_t classed = bit_cast<uint64_t>(fs);
+
+      // Extracting sign, exponent and mantissa from the input double
+      uint32_t sign = (classed >> 63) & 1;
+      uint32_t exponent = (classed >> 52) & 0x00000000000007FF;
+      uint64_t mantissa = classed & 0x000FFFFFFFFFFFFF;
+      uint64_t result;
+      double dResult;
+
+      // Setting flags if input double is negative infinity,
+      // positive infinity, negative zero or positive zero
+      bool negInf = (classed == 0xFFF0000000000000);
+      bool posInf = (classed == 0x7FF0000000000000);
+      bool negZero = (classed == 0x8000000000000000);
+      bool posZero = (classed == 0x0000000000000000);
+
+      bool signalingNan;
+      bool quietNan;
+      bool negSubnorm;
+      bool posSubnorm;
+      bool negNorm;
+      bool posNorm;
+
+      // Setting flags if double is NaN
+      signalingNan = false;
+      quietNan = false;
+      if (!negInf && !posInf && exponent == 0x7FF) {
+        quietNan = ((mantissa & 0x0008000000000000) != 0) &&
+                   ((mantissa & (0x0008000000000000 - 1)) == 0);
+        signalingNan = !quietNan;
+      }
+
+      // Setting flags if double is subnormal number
+      posSubnorm = false;
+      negSubnorm = false;
+      if ((exponent == 0) && (mantissa != 0)) {
+        DCHECK(sign == 0 || sign == 1);
+        posSubnorm = (sign == 0);
+        negSubnorm = (sign == 1);
+      }
+
+      // Setting flags if double is normal number
+      posNorm = false;
+      negNorm = false;
+      if (!posSubnorm && !negSubnorm && !posInf && !negInf && !signalingNan &&
+          !quietNan && !negZero && !posZero) {
+        DCHECK(sign == 0 || sign == 1);
+        posNorm = (sign == 0);
+        negNorm = (sign == 1);
+      }
+
+      // Calculating result according to description of CLASS.D instruction
+      result = (posZero << 9) | (posSubnorm << 8) | (posNorm << 7) |
+               (posInf << 6) | (negZero << 5) | (negSubnorm << 4) |
+               (negNorm << 3) | (negInf << 2) | (quietNan << 1) | signalingNan;
+
+      DCHECK_NE(result, 0);
+
+      dResult = bit_cast<double>(result);
+      SetFPUDoubleResult(fd_reg(), dResult);
+      break;
+    }
+    case C_F_D: {
+      set_fcsr_bit(fcsr_cc, false);
+      TraceRegWr(test_fcsr_bit(fcsr_cc));
+      break;
+    }
+    default:
+      UNREACHABLE();
+  }
+}
+
+
+void Simulator::DecodeTypeRegisterWRsType() {
+  float fs = get_fpu_register_float(fs_reg());
+  float ft = get_fpu_register_float(ft_reg());
+  int64_t alu_out = 0x12345678;
+  switch (instr_.FunctionFieldRaw()) {
+    case CVT_S_W:  // Convert word to float (single).
+      alu_out = get_fpu_register_signed_word(fs_reg());
+      SetFPUFloatResult(fd_reg(), static_cast<float>(alu_out));
+      break;
+    case CVT_D_W:  // Convert word to double.
+      alu_out = get_fpu_register_signed_word(fs_reg());
+      SetFPUDoubleResult(fd_reg(), static_cast<double>(alu_out));
+      break;
+    case CMP_AF:
+      SetFPUWordResult2(fd_reg(), 0);
+      break;
+    case CMP_UN:
+      if (std::isnan(fs) || std::isnan(ft)) {
+        SetFPUWordResult2(fd_reg(), -1);
+      } else {
+        SetFPUWordResult2(fd_reg(), 0);
+      }
+      break;
+    case CMP_EQ:
+      if (fs == ft) {
+        SetFPUWordResult2(fd_reg(), -1);
+      } else {
+        SetFPUWordResult2(fd_reg(), 0);
+      }
+      break;
+    case CMP_UEQ:
+      if ((fs == ft) || (std::isnan(fs) || std::isnan(ft))) {
+        SetFPUWordResult2(fd_reg(), -1);
+      } else {
+        SetFPUWordResult2(fd_reg(), 0);
+      }
+      break;
+    case CMP_LT:
+      if (fs < ft) {
+        SetFPUWordResult2(fd_reg(), -1);
+      } else {
+        SetFPUWordResult2(fd_reg(), 0);
+      }
+      break;
+    case CMP_ULT:
+      if ((fs < ft) || (std::isnan(fs) || std::isnan(ft))) {
+        SetFPUWordResult2(fd_reg(), -1);
+      } else {
+        SetFPUWordResult2(fd_reg(), 0);
+      }
+      break;
+    case CMP_LE:
+      if (fs <= ft) {
+        SetFPUWordResult2(fd_reg(), -1);
+      } else {
+        SetFPUWordResult2(fd_reg(), 0);
+      }
+      break;
+    case CMP_ULE:
+      if ((fs <= ft) || (std::isnan(fs) || std::isnan(ft))) {
+        SetFPUWordResult2(fd_reg(), -1);
+      } else {
+        SetFPUWordResult2(fd_reg(), 0);
+      }
+      break;
+    case CMP_OR:
+      if (!std::isnan(fs) && !std::isnan(ft)) {
+        SetFPUWordResult2(fd_reg(), -1);
+      } else {
+        SetFPUWordResult2(fd_reg(), 0);
+      }
+      break;
+    case CMP_UNE:
+      if ((fs != ft) || (std::isnan(fs) || std::isnan(ft))) {
+        SetFPUWordResult2(fd_reg(), -1);
+      } else {
+        SetFPUWordResult2(fd_reg(), 0);
+      }
+      break;
+    case CMP_NE:
+      if (fs != ft) {
+        SetFPUWordResult2(fd_reg(), -1);
+      } else {
+        SetFPUWordResult2(fd_reg(), 0);
+      }
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+
+void Simulator::DecodeTypeRegisterLRsType() {
+  double fs = get_fpu_register_double(fs_reg());
+  double ft = get_fpu_register_double(ft_reg());
+  int64_t i64;
+  switch (instr_.FunctionFieldRaw()) {
+    case CVT_D_L:
+      i64 = get_fpu_register(fs_reg());
+      SetFPUDoubleResult(fd_reg(), static_cast<double>(i64));
+      break;
+    case CVT_S_L:
+      i64 = get_fpu_register(fs_reg());
+      SetFPUFloatResult(fd_reg(), static_cast<float>(i64));
+      break;
+    case CMP_AF:
+      SetFPUResult(fd_reg(), 0);
+      break;
+    case CMP_UN:
+      if (std::isnan(fs) || std::isnan(ft)) {
+        SetFPUResult(fd_reg(), -1);
+      } else {
+        SetFPUResult(fd_reg(), 0);
+      }
+      break;
+    case CMP_EQ:
+      if (fs == ft) {
+        SetFPUResult(fd_reg(), -1);
+      } else {
+        SetFPUResult(fd_reg(), 0);
+      }
+      break;
+    case CMP_UEQ:
+      if ((fs == ft) || (std::isnan(fs) || std::isnan(ft))) {
+        SetFPUResult(fd_reg(), -1);
+      } else {
+        SetFPUResult(fd_reg(), 0);
+      }
+      break;
+    case CMP_LT:
+      if (fs < ft) {
+        SetFPUResult(fd_reg(), -1);
+      } else {
+        SetFPUResult(fd_reg(), 0);
+      }
+      break;
+    case CMP_ULT:
+      if ((fs < ft) || (std::isnan(fs) || std::isnan(ft))) {
+        SetFPUResult(fd_reg(), -1);
+      } else {
+        SetFPUResult(fd_reg(), 0);
+      }
+      break;
+    case CMP_LE:
+      if (fs <= ft) {
+        SetFPUResult(fd_reg(), -1);
+      } else {
+        SetFPUResult(fd_reg(), 0);
+      }
+      break;
+    case CMP_ULE:
+      if ((fs <= ft) || (std::isnan(fs) || std::isnan(ft))) {
+        SetFPUResult(fd_reg(), -1);
+      } else {
+        SetFPUResult(fd_reg(), 0);
+      }
+      break;
+    case CMP_OR:
+      if (!std::isnan(fs) && !std::isnan(ft)) {
+        SetFPUResult(fd_reg(), -1);
+      } else {
+        SetFPUResult(fd_reg(), 0);
+      }
+      break;
+    case CMP_UNE:
+      if ((fs != ft) || (std::isnan(fs) || std::isnan(ft))) {
+        SetFPUResult(fd_reg(), -1);
+      } else {
+        SetFPUResult(fd_reg(), 0);
+      }
+      break;
+    case CMP_NE:
+      if (fs != ft && (!std::isnan(fs) && !std::isnan(ft))) {
+        SetFPUResult(fd_reg(), -1);
+      } else {
+        SetFPUResult(fd_reg(), 0);
+      }
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+
+void Simulator::DecodeTypeRegisterCOP1() {
+  switch (instr_.RsFieldRaw()) {
+    case BC1:  // Branch on coprocessor condition.
+    case BC1EQZ:
+    case BC1NEZ:
+      UNREACHABLE();
+      break;
+    case CFC1:
+      // At the moment only FCSR is supported.
+      DCHECK_EQ(fs_reg(), kFCSRRegister);
+      SetResult(rt_reg(), FCSR_);
+      break;
+    case MFC1:
+      set_register(rt_reg(),
+                   static_cast<int64_t>(get_fpu_register_word(fs_reg())));
+      TraceRegWr(get_register(rt_reg()), WORD_DWORD);
+      break;
+    case DMFC1:
+      SetResult(rt_reg(), get_fpu_register(fs_reg()));
+      break;
+    case MFHC1:
+      SetResult(rt_reg(), get_fpu_register_hi_word(fs_reg()));
+      break;
+    case CTC1: {
+      // At the moment only FCSR is supported.
+      DCHECK_EQ(fs_reg(), kFCSRRegister);
+      uint32_t reg = static_cast<uint32_t>(rt());
+      if (kArchVariant == kSw64r3) {
+        FCSR_ = reg | kFCSRNaN2008FlagMask;
+      } else {
+        DCHECK_EQ(kArchVariant, kSw64r2);
+        FCSR_ = reg & ~kFCSRNaN2008FlagMask;
+      }
+      TraceRegWr(FCSR_);
+      break;
+    }
+    case MTC1:
+      // Hardware writes upper 32-bits to zero on ifmovs.
+      set_fpu_register_hi_word(fs_reg(), 0);
+      set_fpu_register_word(fs_reg(), static_cast<int32_t>(rt()));
+      TraceRegWr(get_fpu_register(fs_reg()), FLOAT_DOUBLE);
+      break;
+    case DMTC1:
+      SetFPUResult2(fs_reg(), rt());
+      break;
+    case MTHC1:
+      set_fpu_register_hi_word(fs_reg(), static_cast<int32_t>(rt()));
+      TraceRegWr(get_fpu_register(fs_reg()), DOUBLE);
+      break;
+    case S:
+      DecodeTypeRegisterSRsType();
+      break;
+    case D:
+      DecodeTypeRegisterDRsType();
+      break;
+    case W:
+      DecodeTypeRegisterWRsType();
+      break;
+    case L:
+      DecodeTypeRegisterLRsType();
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+
+void Simulator::DecodeTypeRegisterCOP1X() {
+  switch (instr_.FunctionFieldRaw()) {
+    case MADD_S: {
+      DCHECK_EQ(kArchVariant, kSw64r2);
+      float fr, ft, fs;
+      fr = get_fpu_register_float(fr_reg());
+      fs = get_fpu_register_float(fs_reg());
+      ft = get_fpu_register_float(ft_reg());
+      SetFPUFloatResult(fd_reg(), fs * ft + fr);
+      break;
+    }
+    case MSUB_S: {
+      DCHECK_EQ(kArchVariant, kSw64r2);
+      float fr, ft, fs;
+      fr = get_fpu_register_float(fr_reg());
+      fs = get_fpu_register_float(fs_reg());
+      ft = get_fpu_register_float(ft_reg());
+      SetFPUFloatResult(fd_reg(), fs * ft - fr);
+      break;
+    }
+    case MADD_D: {
+      DCHECK_EQ(kArchVariant, kSw64r2);
+      double fr, ft, fs;
+      fr = get_fpu_register_double(fr_reg());
+      fs = get_fpu_register_double(fs_reg());
+      ft = get_fpu_register_double(ft_reg());
+      SetFPUDoubleResult(fd_reg(), fs * ft + fr);
+      break;
+    }
+    case MSUB_D: {
+      DCHECK_EQ(kArchVariant, kSw64r2);
+      double fr, ft, fs;
+      fr = get_fpu_register_double(fr_reg());
+      fs = get_fpu_register_double(fs_reg());
+      ft = get_fpu_register_double(ft_reg());
+      SetFPUDoubleResult(fd_reg(), fs * ft - fr);
+      break;
+    }
+    default:
+      UNREACHABLE();
+  }
+}
+
+
+void Simulator::DecodeTypeRegisterSPECIAL() {
+  //int64_t i64hilo;
+  //uint64_t u64hilo;
+  //int64_t alu_out;
+  //bool do_interrupt = false;
+
+  //switch (instr_.FunctionFieldRaw()) {
+  //  case SELEQZ_S:
+  //    DCHECK_EQ(kArchVariant, kSw64r3);
+  //    SetResult(rd_reg(), rt() == 0 ? rs() : 0);
+  //    break;
+  //  case SELNEZ_S:
+  //    DCHECK_EQ(kArchVariant, kSw64r3);
+  //    SetResult(rd_reg(), rt() != 0 ? rs() : 0);
+  //    break;
+  //  case JR: {
+  //    int64_t next_pc = rs();
+  //    int64_t current_pc = get_pc();
+  //    Instruction* branch_delay_instr =
+  //        reinterpret_cast<Instruction*>(current_pc + Instruction::kInstrSize);
+  //    BranchDelayInstructionDecode(branch_delay_instr);
+  //    set_pc(next_pc);
+  //    pc_modified_ = true;
+  //    break;
+  //  }
+  //  case JALR: {
+  //    int64_t next_pc = rs();
+  //    int64_t current_pc = get_pc();
+  //    int32_t return_addr_reg = rd_reg();
+  //    Instruction* branch_delay_instr =
+  //        reinterpret_cast<Instruction*>(current_pc + Instruction::kInstrSize);
+  //    BranchDelayInstructionDecode(branch_delay_instr);
+  //    set_register(return_addr_reg, current_pc + 2 * Instruction::kInstrSize);
+  //    set_pc(next_pc);
+  //    pc_modified_ = true;
+  //    break;
+  //  }
+  //  case SLL:
+  //    SetResult(rd_reg(), static_cast<int32_t>(rt()) << sa());
+  //    break;
+  //  case DSLL:
+  //    SetResult(rd_reg(), rt() << sa());
+  //    break;
+  //  case DSLL32:
+  //    SetResult(rd_reg(), rt() << sa() << 32);
+  //    break;
+  //  case SRL:
+  //    if (rs_reg() == 0) {
+  //      // Regular logical right shift of a word by a fixed number of
+  //      // bits instruction. RS field is always equal to 0.
+  //      // Sign-extend the 32-bit result.
+  //      alu_out = static_cast<int32_t>(static_cast<uint32_t>(rt_u()) >> sa());
+  //    } else if (rs_reg() == 1) {
+  //      // Logical right-rotate of a word by a fixed number of bits. This
+  //      // is special case of SRL instruction.
+  //      // RS field is equal to 00001.
+  //      alu_out = static_cast<int32_t>(
+  //          base::bits::RotateRight32(static_cast<const uint32_t>(rt_u()),
+  //                                    static_cast<const uint32_t>(sa())));
+  //    } else {
+  //      UNREACHABLE();
+  //    }
+  //    SetResult(rd_reg(), alu_out);
+  //    break;
+  //  case DSRL:
+  //    if (rs_reg() == 0) {
+  //      // Regular logical right shift of a word by a fixed number of
+  //      // bits instruction. RS field is always equal to 0.
+  //      // Sign-extend the 64-bit result.
+  //      alu_out = static_cast<int64_t>(rt_u() >> sa());
+  //    } else if (rs_reg() == 1) {
+  //      // Logical right-rotate of a word by a fixed number of bits. This
+  //      // is special case of SRL instruction.
+  //      // RS field is equal to 00001.
+  //      alu_out = static_cast<int64_t>(base::bits::RotateRight64(rt_u(), sa()));
+  //    } else {
+  //      UNREACHABLE();
+  //    }
+  //    SetResult(rd_reg(), alu_out);
+  //    break;
+  //  case DSRL32:
+  //    if (rs_reg() == 0) {
+  //      // Regular logical right shift of a word by a fixed number of
+  //      // bits instruction. RS field is always equal to 0.
+  //      // Sign-extend the 64-bit result.
+  //      alu_out = static_cast<int64_t>(rt_u() >> sa() >> 32);
+  //    } else if (rs_reg() == 1) {
+  //      // Logical right-rotate of a word by a fixed number of bits. This
+  //      // is special case of SRL instruction.
+  //      // RS field is equal to 00001.
+  //      alu_out =
+  //          static_cast<int64_t>(base::bits::RotateRight64(rt_u(), sa() + 32));
+  //    } else {
+  //      UNREACHABLE();
+  //    }
+  //    SetResult(rd_reg(), alu_out);
+  //    break;
+  //  case SRA:
+  //    SetResult(rd_reg(), (int32_t)rt() >> sa());
+  //    break;
+  //  case DSRA:
+  //    SetResult(rd_reg(), rt() >> sa());
+  //    break;
+  //  case DSRA32:
+  //    SetResult(rd_reg(), rt() >> sa() >> 32);
+  //    break;
+  //  case SLLV:
+  //    SetResult(rd_reg(), (int32_t)rt() << rs());
+  //    break;
+  //  case DSLLV:
+  //    SetResult(rd_reg(), rt() << rs());
+  //    break;
+  //  case SRLV:
+  //    if (sa() == 0) {
+  //      // Regular logical right-shift of a word by a variable number of
+  //      // bits instruction. SA field is always equal to 0.
+  //      alu_out = static_cast<int32_t>((uint32_t)rt_u() >> rs());
+  //    } else {
+  //      // Logical right-rotate of a word by a variable number of bits.
+  //      // This is special case od SRLV instruction.
+  //      // SA field is equal to 00001.
+  //      alu_out = static_cast<int32_t>(
+  //          base::bits::RotateRight32(static_cast<const uint32_t>(rt_u()),
+  //                                    static_cast<const uint32_t>(rs_u())));
+  //    }
+  //    SetResult(rd_reg(), alu_out);
+  //    break;
+  //  case DSRLV:
+  //    if (sa() == 0) {
+  //      // Regular logical right-shift of a word by a variable number of
+  //      // bits instruction. SA field is always equal to 0.
+  //      alu_out = static_cast<int64_t>(rt_u() >> rs());
+  //    } else {
+  //      // Logical right-rotate of a word by a variable number of bits.
+  //      // This is special case od SRLV instruction.
+  //      // SA field is equal to 00001.
+  //      alu_out =
+  //          static_cast<int64_t>(base::bits::RotateRight64(rt_u(), rs_u()));
+  //    }
+  //    SetResult(rd_reg(), alu_out);
+  //    break;
+  //  case SRAV:
+  //    SetResult(rd_reg(), (int32_t)rt() >> rs());
+  //    break;
+  //  case DSRAV:
+  //    SetResult(rd_reg(), rt() >> rs());
+  //    break;
+  //  case LSA: {
+  //    DCHECK_EQ(kArchVariant, kSw64r3);
+  //    int8_t sa = lsa_sa() + 1;
+  //    int32_t _rt = static_cast<int32_t>(rt());
+  //    int32_t _rs = static_cast<int32_t>(rs());
+  //    int32_t res = _rs << sa;
+  //    res += _rt;
+  //    SetResult(rd_reg(), static_cast<int64_t>(res));
+  //    break;
+  //  }
+  //  case DLSA:
+  //    DCHECK_EQ(kArchVariant, kSw64r3);
+  //    SetResult(rd_reg(), (rs() << (lsa_sa() + 1)) + rt());
+  //    break;
+  //  case MFHI:  // MFHI == CLZ on R6.
+  //    if (kArchVariant != kSw64r3) {
+  //      DCHECK_EQ(sa(), 0);
+  //      alu_out = get_register(HI);
+  //    } else {
+  //      // SW64 spec: If no bits were set in GPR rs(), the result written to
+  //      // GPR rd() is 32.
+  //      DCHECK_EQ(sa(), 1);
+  //      alu_out = base::bits::CountLeadingZeros32(static_cast<int32_t>(rs_u()));
+  //    }
+  //    SetResult(rd_reg(), alu_out);
+  //    break;
+  //  case MFLO:  // MFLO == DCLZ on R6.
+  //    if (kArchVariant != kSw64r3) {
+  //      DCHECK_EQ(sa(), 0);
+  //      alu_out = get_register(LO);
+  //    } else {
+  //      // SW64 spec: If no bits were set in GPR rs(), the result written to
+  //      // GPR rd() is 64.
+  //      DCHECK_EQ(sa(), 1);
+  //      alu_out = base::bits::CountLeadingZeros64(static_cast<int64_t>(rs_u()));
+  //    }
+  //    SetResult(rd_reg(), alu_out);
+  //    break;
+  //  // Instructions using HI and LO registers.
+  //  case MULT: {  // MULT == D_MUL_MUH.
+  //    int32_t rs_lo = static_cast<int32_t>(rs());
+  //    int32_t rt_lo = static_cast<int32_t>(rt());
+  //    i64hilo = static_cast<int64_t>(rs_lo) * static_cast<int64_t>(rt_lo);
+  //    if (kArchVariant != kSw64r3) {
+  //      set_register(LO, static_cast<int32_t>(i64hilo & 0xFFFFFFFF));
+  //      set_register(HI, static_cast<int32_t>(i64hilo >> 32));
+  //    } else {
+  //      switch (sa()) {
+  //        case MUL_OP:
+  //          SetResult(rd_reg(), static_cast<int32_t>(i64hilo & 0xFFFFFFFF));
+  //          break;
+  //        case MUH_OP:
+  //          SetResult(rd_reg(), static_cast<int32_t>(i64hilo >> 32));
+  //          break;
+  //        default:
+  //          UNIMPLEMENTED_SW64();
+  //          break;
+  //      }
+  //    }
+  //    break;
+  //  }
+  //  case MULTU:
+  //    u64hilo = static_cast<uint64_t>(rs_u() & 0xFFFFFFFF) *
+  //              static_cast<uint64_t>(rt_u() & 0xFFFFFFFF);
+  //    if (kArchVariant != kSw64r3) {
+  //      set_register(LO, static_cast<int32_t>(u64hilo & 0xFFFFFFFF));
+  //      set_register(HI, static_cast<int32_t>(u64hilo >> 32));
+  //    } else {
+  //      switch (sa()) {
+  //        case MUL_OP:
+  //          SetResult(rd_reg(), static_cast<int32_t>(u64hilo & 0xFFFFFFFF));
+  //          break;
+  //        case MUH_OP:
+  //          SetResult(rd_reg(), static_cast<int32_t>(u64hilo >> 32));
+  //          break;
+  //        default:
+  //          UNIMPLEMENTED_SW64();
+  //          break;
+  //      }
+  //    }
+  //    break;
+  //  case DMULT:  // DMULT == D_MUL_MUH.
+  //    if (kArchVariant != kSw64r3) {
+  //      set_register(LO, rs() * rt());
+  //      set_register(HI, MultiplyHighSigned(rs(), rt()));
+  //    } else {
+  //      switch (sa()) {
+  //        case MUL_OP:
+  //          SetResult(rd_reg(), rs() * rt());
+  //          break;
+  //        case MUH_OP:
+  //          SetResult(rd_reg(), MultiplyHighSigned(rs(), rt()));
+  //          break;
+  //        default:
+  //          UNIMPLEMENTED_SW64();
+  //          break;
+  //      }
+  //    }
+  //    break;
+  //  case DMULTU:
+  //    UNIMPLEMENTED_SW64();
+  //    break;
+  //  case DIV:
+  //  case DDIV: {
+  //    const int64_t int_min_value =
+  //        instr_.FunctionFieldRaw() == DIV ? INT_MIN : LONG_MIN;
+  //    switch (kArchVariant) {
+  //      case kSw64r2:
+  //        // Divide by zero and overflow was not checked in the
+  //        // configuration step - div and divu do not raise exceptions. On
+  //        // division by 0 the result will be UNPREDICTABLE. On overflow
+  //        // (INT_MIN/-1), return INT_MIN which is what the hardware does.
+  //        if (rs() == int_min_value && rt() == -1) {
+  //          set_register(LO, int_min_value);
+  //          set_register(HI, 0);
+  //        } else if (rt() != 0) {
+  //          set_register(LO, rs() / rt());
+  //          set_register(HI, rs() % rt());
+  //        }
+  //        break;
+  //      case kSw64r3:
+  //        switch (sa()) {
+  //          case DIV_OP:
+  //            if (rs() == int_min_value && rt() == -1) {
+  //              SetResult(rd_reg(), int_min_value);
+  //            } else if (rt() != 0) {
+  //              SetResult(rd_reg(), rs() / rt());
+  //            }
+  //            break;
+  //          case MOD_OP:
+  //            if (rs() == int_min_value && rt() == -1) {
+  //              SetResult(rd_reg(), 0);
+  //            } else if (rt() != 0) {
+  //              SetResult(rd_reg(), rs() % rt());
+  //            }
+  //            break;
+  //          default:
+  //            UNIMPLEMENTED_SW64();
+  //            break;
+  //        }
+  //        break;
+  //      default:
+  //        break;
+  //    }
+  //    break;
+  //  }
+  //  case DIVU:
+  //    switch (kArchVariant) {
+  //      case kSw64r3: {
+  //        uint32_t rt_u_32 = static_cast<uint32_t>(rt_u());
+  //        uint32_t rs_u_32 = static_cast<uint32_t>(rs_u());
+  //        switch (sa()) {
+  //          case DIV_OP:
+  //            if (rt_u_32 != 0) {
+  //              SetResult(rd_reg(), rs_u_32 / rt_u_32);
+  //            }
+  //            break;
+  //          case MOD_OP:
+  //            if (rt_u() != 0) {
+  //              SetResult(rd_reg(), rs_u_32 % rt_u_32);
+  //            }
+  //            break;
+  //          default:
+  //            UNIMPLEMENTED_SW64();
+  //            break;
+  //        }
+  //      } break;
+  //      default: {
+  //        if (rt_u() != 0) {
+  //          uint32_t rt_u_32 = static_cast<uint32_t>(rt_u());
+  //          uint32_t rs_u_32 = static_cast<uint32_t>(rs_u());
+  //          set_register(LO, rs_u_32 / rt_u_32);
+  //          set_register(HI, rs_u_32 % rt_u_32);
+  //        }
+  //      }
+  //    }
+  //    break;
+  //  case DDIVU:
+  //    switch (kArchVariant) {
+  //      case kSw64r3: {
+  //        switch (instr_.SaValue()) {
+  //          case DIV_OP:
+  //            if (rt_u() != 0) {
+  //              SetResult(rd_reg(), rs_u() / rt_u());
+  //            }
+  //            break;
+  //          case MOD_OP:
+  //            if (rt_u() != 0) {
+  //              SetResult(rd_reg(), rs_u() % rt_u());
+  //            }
+  //            break;
+  //          default:
+  //            UNIMPLEMENTED_SW64();
+  //            break;
+  //        }
+  //      } break;
+  //      default: {
+  //        if (rt_u() != 0) {
+  //          set_register(LO, rs_u() / rt_u());
+  //          set_register(HI, rs_u() % rt_u());
+  //        }
+  //      }
+  //    }
+  //    break;
+  //  case ADD:
+  //  case DADD:
+  //    if (HaveSameSign(rs(), rt())) {
+  //      if (rs() > 0) {
+  //        if (rs() > (Registers::kMaxValue - rt())) {
+  //          SignalException(kIntegerOverflow);
+  //        }
+  //      } else if (rs() < 0) {
+  //        if (rs() < (Registers::kMinValue - rt())) {
+  //          SignalException(kIntegerUnderflow);
+  //        }
+  //      }
+  //    }
+  //    SetResult(rd_reg(), rs() + rt());
+  //    break;
+  //  case ADDU: {
+  //    int32_t alu32_out = static_cast<int32_t>(rs() + rt());
+  //    // Sign-extend result of 32bit operation into 64bit register.
+  //    SetResult(rd_reg(), static_cast<int64_t>(alu32_out));
+  //    break;
+  //  }
+  //  case DADDU:
+  //    SetResult(rd_reg(), rs() + rt());
+  //    break;
+  //  case SUB:
+  //  case DSUB:
+  //    if (!HaveSameSign(rs(), rt())) {
+  //      if (rs() > 0) {
+  //        if (rs() > (Registers::kMaxValue + rt())) {
+  //          SignalException(kIntegerOverflow);
+  //        }
+  //      } else if (rs() < 0) {
+  //        if (rs() < (Registers::kMinValue + rt())) {
+  //          SignalException(kIntegerUnderflow);
+  //        }
+  //      }
+  //    }
+  //    SetResult(rd_reg(), rs() - rt());
+  //    break;
+  //  case SUBU: {
+  //    int32_t alu32_out = static_cast<int32_t>(rs() - rt());
+  //    // Sign-extend result of 32bit operation into 64bit register.
+  //    SetResult(rd_reg(), static_cast<int64_t>(alu32_out));
+  //    break;
+  //  }
+  //  case DSUBU:
+  //    SetResult(rd_reg(), rs() - rt());
+  //    break;
+  //  case AND:
+  //    SetResult(rd_reg(), rs() & rt());
+  //    break;
+  //  case OR:
+  //    SetResult(rd_reg(), rs() | rt());
+  //    break;
+  //  case XOR:
+  //    SetResult(rd_reg(), rs() ^ rt());
+  //    break;
+  //  case NOR:
+  //    SetResult(rd_reg(), ~(rs() | rt()));
+  //    break;
+  //  case SLT:
+  //    SetResult(rd_reg(), rs() < rt() ? 1 : 0);
+  //    break;
+  //  case SLTU:
+  //    SetResult(rd_reg(), rs_u() < rt_u() ? 1 : 0);
+  //    break;
+  //  // Break and trap instructions.
+  //  case BREAK:
+  //    do_interrupt = true;
+  //    break;
+  //  case TGE:
+  //    do_interrupt = rs() >= rt();
+  //    break;
+  //  case TGEU:
+  //    do_interrupt = rs_u() >= rt_u();
+  //    break;
+  //  case TLT:
+  //    do_interrupt = rs() < rt();
+  //    break;
+  //  case TLTU:
+  //    do_interrupt = rs_u() < rt_u();
+  //    break;
+  //  case TEQ:
+  //    do_interrupt = rs() == rt();
+  //    break;
+  //  case TNE:
+  //    do_interrupt = rs() != rt();
+  //    break;
+  //  case SYNC:
+  //    // TODO(palfia): Ignore memb instruction for now.
+  //    break;
+  //  // Conditional moves.
+  //  case MOVN:
+  //    if (rt()) {
+  //      SetResult(rd_reg(), rs());
+  //    }
+  //    break;
+  //  case MOVCI: {
+  //    uint32_t cc = instr_.FBccValue();
+  //    uint32_t fcsr_cc = get_fcsr_condition_bit(cc);
+  //    if (instr_.Bit(16)) {  // Read Tf bit.
+  //      if (test_fcsr_bit(fcsr_cc)) SetResult(rd_reg(), rs());
+  //    } else {
+  //      if (!test_fcsr_bit(fcsr_cc)) SetResult(rd_reg(), rs());
+  //    }
+  //    break;
+  //  }
+  //  case MOVZ:
+  //    if (!rt()) {
+  //      SetResult(rd_reg(), rs());
+  //    }
+  //    break;
+  //  default:
+  //    UNREACHABLE();
+  //}
+  //if (do_interrupt) {
+  //  SoftwareInterrupt();
+  //}
+}
+
+
+void Simulator::DecodeTypeRegisterSPECIAL2() {
+  int64_t alu_out;
+  switch (instr_.FunctionFieldRaw()) {
+    case MUL:
+      alu_out = static_cast<int32_t>(rs_u()) * static_cast<int32_t>(rt_u());
+      SetResult(rd_reg(), alu_out);
+      // HI and LO are UNPREDICTABLE after the operation.
+      set_register(LO, Unpredictable);
+      set_register(HI, Unpredictable);
+      break;
+    case CLZ:
+      // If no bits were set in GPR rs(), the result written to
+      // GPR rd is 32.
+      alu_out = base::bits::CountLeadingZeros32(static_cast<uint32_t>(rs_u()));
+      SetResult(rd_reg(), alu_out);
+      break;
+    case DCLZ:
+      // If no bits were set in GPR rs(), the result written to
+      // GPR rd is 64.
+      alu_out = base::bits::CountLeadingZeros64(static_cast<uint64_t>(rs_u()));
+      SetResult(rd_reg(), alu_out);
+      break;
+    default:
+      alu_out = 0x12345678;
+      UNREACHABLE();
+  }
+}
+
+
+void Simulator::DecodeTypeRegisterSPECIAL3() {
+  int64_t alu_out;
+  switch (instr_.FunctionFieldRaw()) {
+    case EXT: {
+      // Interpret rd field as 5-bit msbd of extract.
+      uint16_t msbd = rd_reg();
+      // Interpret sa field as 5-bit lsb of extract.
+      uint16_t lsb = sa();
+      uint16_t size = msbd + 1;
+      uint64_t mask = (1ULL << size) - 1;
+      alu_out = static_cast<int32_t>((rs_u() & (mask << lsb)) >> lsb);
+      SetResult(rt_reg(), alu_out);
+      break;
+    }
+    case DEXT: {
+      // Interpret rd field as 5-bit msbd of extract.
+      uint16_t msbd = rd_reg();
+      // Interpret sa field as 5-bit lsb of extract.
+      uint16_t lsb = sa();
+      uint16_t size = msbd + 1;
+      uint64_t mask = (size == 64) ? UINT64_MAX : (1ULL << size) - 1;
+      alu_out = static_cast<int64_t>((rs_u() & (mask << lsb)) >> lsb);
+      SetResult(rt_reg(), alu_out);
+      break;
+    }
+    case DEXTM: {
+      // Interpret rd field as 5-bit msbdminus32 of extract.
+      uint16_t msbdminus32 = rd_reg();
+      // Interpret sa field as 5-bit lsb of extract.
+      uint16_t lsb = sa();
+      uint16_t size = msbdminus32 + 1 + 32;
+      uint64_t mask = (size == 64) ? UINT64_MAX : (1ULL << size) - 1;
+      alu_out = static_cast<int64_t>((rs_u() & (mask << lsb)) >> lsb);
+      SetResult(rt_reg(), alu_out);
+      break;
+    }
+    case DEXTU: {
+      // Interpret rd field as 5-bit msbd of extract.
+      uint16_t msbd = rd_reg();
+      // Interpret sa field as 5-bit lsbminus32 of extract and add 32 to get
+      // lsb.
+      uint16_t lsb = sa() + 32;
+      uint16_t size = msbd + 1;
+      uint64_t mask = (size == 64) ? UINT64_MAX : (1ULL << size) - 1;
+      alu_out = static_cast<int64_t>((rs_u() & (mask << lsb)) >> lsb);
+      SetResult(rt_reg(), alu_out);
+      break;
+    }
+    case INS: {
+      // Interpret rd field as 5-bit msb of insert.
+      uint16_t msb = rd_reg();
+      // Interpret sa field as 5-bit lsb of insert.
+      uint16_t lsb = sa();
+      uint16_t size = msb - lsb + 1;
+      uint64_t mask = (1ULL << size) - 1;
+      alu_out = static_cast<int32_t>((rt_u() & ~(mask << lsb)) |
+                                     ((rs_u() & mask) << lsb));
+      SetResult(rt_reg(), alu_out);
+      break;
+    }
+    case DINS: {
+      // Interpret rd field as 5-bit msb of insert.
+      uint16_t msb = rd_reg();
+      // Interpret sa field as 5-bit lsb of insert.
+      uint16_t lsb = sa();
+      uint16_t size = msb - lsb + 1;
+      uint64_t mask = (1ULL << size) - 1;
+      alu_out = (rt_u() & ~(mask << lsb)) | ((rs_u() & mask) << lsb);
+      SetResult(rt_reg(), alu_out);
+      break;
+    }
+    case DINSM: {
+      // Interpret rd field as 5-bit msbminus32 of insert.
+      uint16_t msbminus32 = rd_reg();
+      // Interpret sa field as 5-bit lsb of insert.
+      uint16_t lsb = sa();
+      uint16_t size = msbminus32 + 32 - lsb + 1;
+      uint64_t mask;
+      if (size < 64)
+        mask = (1ULL << size) - 1;
+      else
+        mask = std::numeric_limits<uint64_t>::max();
+      alu_out = (rt_u() & ~(mask << lsb)) | ((rs_u() & mask) << lsb);
+      SetResult(rt_reg(), alu_out);
+      break;
+    }
+    case DINSU: {
+      // Interpret rd field as 5-bit msbminus32 of insert.
+      uint16_t msbminus32 = rd_reg();
+      // Interpret rd field as 5-bit lsbminus32 of insert.
+      uint16_t lsbminus32 = sa();
+      uint16_t lsb = lsbminus32 + 32;
+      uint16_t size = msbminus32 + 32 - lsb + 1;
+      uint64_t mask = (1ULL << size) - 1;
+      alu_out = (rt_u() & ~(mask << lsb)) | ((rs_u() & mask) << lsb);
+      SetResult(rt_reg(), alu_out);
+      break;
+    }
+    case BSHFL: {
+      int32_t sa = instr_.SaFieldRaw() >> kSaShift;
+      switch (sa) {
+        case BITSWAP: {
+          uint32_t input = static_cast<uint32_t>(rt());
+          uint32_t output = 0;
+          uint8_t i_byte, o_byte;
+
+          // Reverse the bit in byte for each individual byte
+          for (int i = 0; i < 4; i++) {
+            output = output >> 8;
+            i_byte = input & 0xFF;
+
+            // Fast way to reverse bits in byte
+            // Devised by Sean Anderson, July 13, 2001
+            o_byte = static_cast<uint8_t>(((i_byte * 0x0802LU & 0x22110LU) |
+                                           (i_byte * 0x8020LU & 0x88440LU)) *
+                                              0x10101LU >>
+                                          16);
+
+            output = output | (static_cast<uint32_t>(o_byte << 24));
+            input = input >> 8;
+          }
+
+          alu_out = static_cast<int64_t>(static_cast<int32_t>(output));
+          break;
+        }
+        case SEB: {
+          uint8_t input = static_cast<uint8_t>(rt());
+          uint32_t output = input;
+          uint32_t mask = 0x00000080;
+
+          // Extending sign
+          if (mask & input) {
+            output |= 0xFFFFFF00;
+          }
+
+          alu_out = static_cast<int32_t>(output);
+          break;
+        }
+        case SEH: {
+          uint16_t input = static_cast<uint16_t>(rt());
+          uint32_t output = input;
+          uint32_t mask = 0x00008000;
+
+          // Extending sign
+          if (mask & input) {
+            output |= 0xFFFF0000;
+          }
+
+          alu_out = static_cast<int32_t>(output);
+          break;
+        }
+        case WSBH: {
+          uint32_t input = static_cast<uint32_t>(rt());
+          uint64_t output = 0;
+
+          uint32_t mask = 0xFF000000;
+          for (int i = 0; i < 4; i++) {
+            uint32_t tmp = mask & input;
+            if (i % 2 == 0) {
+              tmp = tmp >> 8;
+            } else {
+              tmp = tmp << 8;
+            }
+            output = output | tmp;
+            mask = mask >> 8;
+          }
+          mask = 0x80000000;
+
+          // Extending sign
+          if (mask & output) {
+            output |= 0xFFFFFFFF00000000;
+          }
+
+          alu_out = static_cast<int64_t>(output);
+          break;
+        }
+        default: {
+          const uint8_t bp2 = instr_.Bp2Value();
+          sa >>= kBp2Bits;
+          switch (sa) {
+            case ALIGN: {
+              if (bp2 == 0) {
+                alu_out = static_cast<int32_t>(rt());
+              } else {
+                uint64_t rt_hi = rt() << (8 * bp2);
+                uint64_t rs_lo = rs() >> (8 * (4 - bp2));
+                alu_out = static_cast<int32_t>(rt_hi | rs_lo);
+              }
+              break;
+            }
+            default:
+              alu_out = 0x12345678;
+              UNREACHABLE();
+              break;
+          }
+          break;
+        }
+      }
+      SetResult(rd_reg(), alu_out);
+      break;
+    }
+    case DBSHFL: {
+      int32_t sa = instr_.SaFieldRaw() >> kSaShift;
+      switch (sa) {
+        case DBITSWAP: {
+          switch (sa) {
+            case DBITSWAP_SA: {
+              uint64_t input = static_cast<uint64_t>(rt());
+              uint64_t output = 0;
+              uint8_t i_byte, o_byte;
+
+              // Reverse the bit in byte for each individual byte
+              for (int i = 0; i < 8; i++) {
+                output = output >> 8;
+                i_byte = input & 0xFF;
+
+                // Fast way to reverse bits in byte
+                // Devised by Sean Anderson, July 13, 2001
+                o_byte =
+                    static_cast<uint8_t>(((i_byte * 0x0802LU & 0x22110LU) |
+                                          (i_byte * 0x8020LU & 0x88440LU)) *
+                                             0x10101LU >>
+                                         16);
+
+                output = output | ((static_cast<uint64_t>(o_byte) << 56));
+                input = input >> 8;
+              }
+
+              alu_out = static_cast<int64_t>(output);
+              break;
+            }
+          }
+          break;
+        }
+        case DSBH: {
+          uint64_t input = static_cast<uint64_t>(rt());
+          uint64_t output = 0;
+
+          uint64_t mask = 0xFF00000000000000;
+          for (int i = 0; i < 8; i++) {
+            uint64_t tmp = mask & input;
+            if (i % 2 == 0)
+              tmp = tmp >> 8;
+            else
+              tmp = tmp << 8;
+
+            output = output | tmp;
+            mask = mask >> 8;
+          }
+
+          alu_out = static_cast<int64_t>(output);
+          break;
+        }
+        case DSHD: {
+          uint64_t input = static_cast<uint64_t>(rt());
+          uint64_t output = 0;
+
+          uint64_t mask = 0xFFFF000000000000;
+          for (int i = 0; i < 4; i++) {
+            uint64_t tmp = mask & input;
+            if (i == 0)
+              tmp = tmp >> 48;
+            else if (i == 1)
+              tmp = tmp >> 16;
+            else if (i == 2)
+              tmp = tmp << 16;
+            else
+              tmp = tmp << 48;
+            output = output | tmp;
+            mask = mask >> 16;
+          }
+
+          alu_out = static_cast<int64_t>(output);
+          break;
+        }
+        default: {
+          const uint8_t bp3 = instr_.Bp3Value();
+          sa >>= kBp3Bits;
+          switch (sa) {
+            case DALIGN: {
+              if (bp3 == 0) {
+                alu_out = static_cast<int64_t>(rt());
+              } else {
+                uint64_t rt_hi = rt() << (8 * bp3);
+                uint64_t rs_lo = rs() >> (8 * (8 - bp3));
+                alu_out = static_cast<int64_t>(rt_hi | rs_lo);
+              }
+              break;
+            }
+            default:
+              alu_out = 0x12345678;
+              UNREACHABLE();
+              break;
+          }
+          break;
+        }
+      }
+      SetResult(rd_reg(), alu_out);
+      break;
+    }
+    default:
+      UNREACHABLE();
+  }
+}
+
+int Simulator::DecodeMsaDataFormat() {
+  int df = -1;
+  if (instr_.IsMSABranchInstr()) {
+    switch (instr_.RsFieldRaw()) {
+      case BZ_V:
+      case BNZ_V:
+        df = MSA_VECT;
+        break;
+      case BZ_B:
+      case BNZ_B:
+        df = MSA_BYTE;
+        break;
+      case BZ_H:
+      case BNZ_H:
+        df = MSA_HALF;
+        break;
+      case BZ_W:
+      case BNZ_W:
+        df = MSA_WORD;
+        break;
+      case BZ_D:
+      case BNZ_D:
+        df = MSA_DWORD;
+        break;
+      default:
+        UNREACHABLE();
+        break;
+    }
+  } else {
+    int DF[] = {MSA_BYTE, MSA_HALF, MSA_WORD, MSA_DWORD};
+    switch (instr_.MSAMinorOpcodeField()) {
+      case kMsaMinorI5:
+      case kMsaMinorI10:
+      case kMsaMinor3R:
+        df = DF[instr_.Bits(22, 21)];
+        break;
+      case kMsaMinorMI10:
+        df = DF[instr_.Bits(1, 0)];
+        break;
+      case kMsaMinorBIT:
+        df = DF[instr_.MsaBitDf()];
+        break;
+      case kMsaMinorELM:
+        df = DF[instr_.MsaElmDf()];
+        break;
+      case kMsaMinor3RF: {
+        uint32_t opcode = instr_.InstructionBits() & kMsa3RFMask;
+        switch (opcode) {
+          case FEXDO:
+          case FTQ:
+          case MUL_Q:
+          case MADD_Q:
+          case MSUB_Q:
+          case MULR_Q:
+          case MADDR_Q:
+          case MSUBR_Q:
+            df = DF[1 + instr_.Bit(21)];
+            break;
+          default:
+            df = DF[2 + instr_.Bit(21)];
+            break;
+        }
+      } break;
+      case kMsaMinor2R:
+        df = DF[instr_.Bits(17, 16)];
+        break;
+      case kMsaMinor2RF:
+        df = DF[2 + instr_.Bit(16)];
+        break;
+      default:
+        UNREACHABLE();
+        break;
+    }
+  }
+  return df;
+}
+
+void Simulator::DecodeTypeMsaI8() {
+  DCHECK_EQ(kArchVariant, kSw64r3);
+  DCHECK(CpuFeatures::IsSupported(SW64_SIMD));
+  uint32_t opcode = instr_.InstructionBits() & kMsaI8Mask;
+  int8_t i8 = instr_.MsaImm8Value();
+  msa_reg_t ws, wd;
+
+  switch (opcode) {
+    case ANDI_B:
+      get_msa_register(instr_.WsValue(), ws.b);
+      for (int i = 0; i < kMSALanesByte; i++) {
+        wd.b[i] = ws.b[i] & i8;
+      }
+      set_msa_register(instr_.WdValue(), wd.b);
+      TraceMSARegWr(wd.b);
+      break;
+    case ORI_B:
+      get_msa_register(instr_.WsValue(), ws.b);
+      for (int i = 0; i < kMSALanesByte; i++) {
+        wd.b[i] = ws.b[i] | i8;
+      }
+      set_msa_register(instr_.WdValue(), wd.b);
+      TraceMSARegWr(wd.b);
+      break;
+    case NORI_B:
+      get_msa_register(instr_.WsValue(), ws.b);
+      for (int i = 0; i < kMSALanesByte; i++) {
+        wd.b[i] = ~(ws.b[i] | i8);
+      }
+      set_msa_register(instr_.WdValue(), wd.b);
+      TraceMSARegWr(wd.b);
+      break;
+    case XORI_B:
+      get_msa_register(instr_.WsValue(), ws.b);
+      for (int i = 0; i < kMSALanesByte; i++) {
+        wd.b[i] = ws.b[i] ^ i8;
+      }
+      set_msa_register(instr_.WdValue(), wd.b);
+      TraceMSARegWr(wd.b);
+      break;
+    case BMNZI_B:
+      get_msa_register(instr_.WsValue(), ws.b);
+      get_msa_register(instr_.WdValue(), wd.b);
+      for (int i = 0; i < kMSALanesByte; i++) {
+        wd.b[i] = (ws.b[i] & i8) | (wd.b[i] & ~i8);
+      }
+      set_msa_register(instr_.WdValue(), wd.b);
+      TraceMSARegWr(wd.b);
+      break;
+    case BMZI_B:
+      get_msa_register(instr_.WsValue(), ws.b);
+      get_msa_register(instr_.WdValue(), wd.b);
+      for (int i = 0; i < kMSALanesByte; i++) {
+        wd.b[i] = (ws.b[i] & ~i8) | (wd.b[i] & i8);
+      }
+      set_msa_register(instr_.WdValue(), wd.b);
+      TraceMSARegWr(wd.b);
+      break;
+    case BSELI_B:
+      get_msa_register(instr_.WsValue(), ws.b);
+      get_msa_register(instr_.WdValue(), wd.b);
+      for (int i = 0; i < kMSALanesByte; i++) {
+        wd.b[i] = (ws.b[i] & ~wd.b[i]) | (wd.b[i] & i8);
+      }
+      set_msa_register(instr_.WdValue(), wd.b);
+      TraceMSARegWr(wd.b);
+      break;
+    case SHF_B:
+      get_msa_register(instr_.WsValue(), ws.b);
+      for (int i = 0; i < kMSALanesByte; i++) {
+        int j = i % 4;
+        int k = (i8 >> (2 * j)) & 0x3;
+        wd.b[i] = ws.b[i - j + k];
+      }
+      set_msa_register(instr_.WdValue(), wd.b);
+      TraceMSARegWr(wd.b);
+      break;
+    case SHF_H:
+      get_msa_register(instr_.WsValue(), ws.h);
+      for (int i = 0; i < kMSALanesHalf; i++) {
+        int j = i % 4;
+        int k = (i8 >> (2 * j)) & 0x3;
+        wd.h[i] = ws.h[i - j + k];
+      }
+      set_msa_register(instr_.WdValue(), wd.h);
+      TraceMSARegWr(wd.h);
+      break;
+    case SHF_W:
+      get_msa_register(instr_.WsValue(), ws.w);
+      for (int i = 0; i < kMSALanesWord; i++) {
+        int j = (i8 >> (2 * i)) & 0x3;
+        wd.w[i] = ws.w[j];
+      }
+      set_msa_register(instr_.WdValue(), wd.w);
+      TraceMSARegWr(wd.w);
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+template <typename T>
+T Simulator::MsaI5InstrHelper(uint32_t opcode, T ws, int32_t i5) {
+  T res;
+  uint32_t ui5 = i5 & 0x1Fu;
+  uint64_t ws_u64 = static_cast<uint64_t>(ws);
+  uint64_t ui5_u64 = static_cast<uint64_t>(ui5);
+
+  switch (opcode) {
+    case ADDVI:
+      res = static_cast<T>(ws + ui5);
+      break;
+    case SUBVI:
+      res = static_cast<T>(ws - ui5);
+      break;
+    case MAXI_S:
+      res = static_cast<T>(Max(ws, static_cast<T>(i5)));
+      break;
+    case MINI_S:
+      res = static_cast<T>(Min(ws, static_cast<T>(i5)));
+      break;
+    case MAXI_U:
+      res = static_cast<T>(Max(ws_u64, ui5_u64));
+      break;
+    case MINI_U:
+      res = static_cast<T>(Min(ws_u64, ui5_u64));
+      break;
+    case CEQI:
+      res = static_cast<T>(!Compare(ws, static_cast<T>(i5)) ? -1ull : 0ull);
+      break;
+    case CLTI_S:
+      res = static_cast<T>((Compare(ws, static_cast<T>(i5)) == -1) ? -1ull
+                                                                   : 0ull);
+      break;
+    case CLTI_U:
+      res = static_cast<T>((Compare(ws_u64, ui5_u64) == -1) ? -1ull : 0ull);
+      break;
+    case CLEI_S:
+      res =
+          static_cast<T>((Compare(ws, static_cast<T>(i5)) != 1) ? -1ull : 0ull);
+      break;
+    case CLEI_U:
+      res = static_cast<T>((Compare(ws_u64, ui5_u64) != 1) ? -1ull : 0ull);
+      break;
+    default:
+      UNREACHABLE();
+  }
+  return res;
+}
+
+void Simulator::DecodeTypeMsaI5() {
+  DCHECK_EQ(kArchVariant, kSw64r3);
+  DCHECK(CpuFeatures::IsSupported(SW64_SIMD));
+  uint32_t opcode = instr_.InstructionBits() & kMsaI5Mask;
+  msa_reg_t ws, wd;
+
+  // sign extend 5bit value to int32_t
+  int32_t i5 = static_cast<int32_t>(instr_.MsaImm5Value() << 27) >> 27;
+
+#define MSA_I5_DF(elem, num_of_lanes)                      \
+  get_msa_register(instr_.WsValue(), ws.elem);             \
+  for (int i = 0; i < num_of_lanes; i++) {                 \
+    wd.elem[i] = MsaI5InstrHelper(opcode, ws.elem[i], i5); \
+  }                                                        \
+  set_msa_register(instr_.WdValue(), wd.elem);             \
+  TraceMSARegWr(wd.elem)
+
+  switch (DecodeMsaDataFormat()) {
+    case MSA_BYTE:
+      MSA_I5_DF(b, kMSALanesByte);
+      break;
+    case MSA_HALF:
+      MSA_I5_DF(h, kMSALanesHalf);
+      break;
+    case MSA_WORD:
+      MSA_I5_DF(w, kMSALanesWord);
+      break;
+    case MSA_DWORD:
+      MSA_I5_DF(d, kMSALanesDword);
+      break;
+    default:
+      UNREACHABLE();
+  }
+#undef MSA_I5_DF
+}
+
+void Simulator::DecodeTypeMsaI10() {
+  DCHECK_EQ(kArchVariant, kSw64r3);
+  DCHECK(CpuFeatures::IsSupported(SW64_SIMD));
+  uint32_t opcode = instr_.InstructionBits() & kMsaI5Mask;
+  int64_t s10 = (static_cast<int64_t>(instr_.MsaImm10Value()) << 54) >> 54;
+  msa_reg_t wd;
+
+#define MSA_I10_DF(elem, num_of_lanes, T)      \
+  for (int i = 0; i < num_of_lanes; ++i) {     \
+    wd.elem[i] = static_cast<T>(s10);          \
+  }                                            \
+  set_msa_register(instr_.WdValue(), wd.elem); \
+  TraceMSARegWr(wd.elem)
+
+  if (opcode == LDI) {
+    switch (DecodeMsaDataFormat()) {
+      case MSA_BYTE:
+        MSA_I10_DF(b, kMSALanesByte, int8_t);
+        break;
+      case MSA_HALF:
+        MSA_I10_DF(h, kMSALanesHalf, int16_t);
+        break;
+      case MSA_WORD:
+        MSA_I10_DF(w, kMSALanesWord, int32_t);
+        break;
+      case MSA_DWORD:
+        MSA_I10_DF(d, kMSALanesDword, int64_t);
+        break;
+      default:
+        UNREACHABLE();
+    }
+  } else {
+    UNREACHABLE();
+  }
+#undef MSA_I10_DF
+}
+
+void Simulator::DecodeTypeMsaELM() {
+  DCHECK_EQ(kArchVariant, kSw64r3);
+  DCHECK(CpuFeatures::IsSupported(SW64_SIMD));
+  uint32_t opcode = instr_.InstructionBits() & kMsaLongerELMMask;
+  int32_t n = instr_.MsaElmNValue();
+  int64_t alu_out;
+  switch (opcode) {
+    case CTCMSA:
+      DCHECK_EQ(sa(), kMSACSRRegister);
+      MSACSR_ = bit_cast<uint32_t>(
+          static_cast<int32_t>(registers_[rd_reg()] & kMaxUInt32));
+      TraceRegWr(static_cast<int32_t>(MSACSR_));
+      break;
+    case CFCMSA:
+      DCHECK_EQ(rd_reg(), kMSACSRRegister);
+      // FIXME: SetResult(sa(), static_cast<int64_t>(bit_cast<int32_t>(MSACSR_)));
+      break;
+    case MOVE_V: {
+      msa_reg_t ws;
+      get_msa_register(ws_reg(), &ws);
+      set_msa_register(wd_reg(), &ws);
+      TraceMSARegWr(&ws);
+    } break;
+    default:
+      opcode &= kMsaELMMask;
+      switch (opcode) {
+        case COPY_S:
+        case COPY_U: {
+          msa_reg_t ws;
+          switch (DecodeMsaDataFormat()) {
+            case MSA_BYTE:
+              DCHECK_LT(n, kMSALanesByte);
+              get_msa_register(instr_.WsValue(), ws.b);
+              alu_out = static_cast<int32_t>(ws.b[n]);
+              SetResult(wd_reg(),
+                        (opcode == COPY_U) ? alu_out & 0xFFu : alu_out);
+              break;
+            case MSA_HALF:
+              DCHECK_LT(n, kMSALanesHalf);
+              get_msa_register(instr_.WsValue(), ws.h);
+              alu_out = static_cast<int32_t>(ws.h[n]);
+              SetResult(wd_reg(),
+                        (opcode == COPY_U) ? alu_out & 0xFFFFu : alu_out);
+              break;
+            case MSA_WORD:
+              DCHECK_LT(n, kMSALanesWord);
+              get_msa_register(instr_.WsValue(), ws.w);
+              alu_out = static_cast<int32_t>(ws.w[n]);
+              SetResult(wd_reg(),
+                        (opcode == COPY_U) ? alu_out & 0xFFFFFFFFu : alu_out);
+              break;
+            case MSA_DWORD:
+              DCHECK_LT(n, kMSALanesDword);
+              get_msa_register(instr_.WsValue(), ws.d);
+              alu_out = static_cast<int64_t>(ws.d[n]);
+              SetResult(wd_reg(), alu_out);
+              break;
+            default:
+              UNREACHABLE();
+          }
+        } break;
+        case INSERT: {
+          msa_reg_t wd;
+          switch (DecodeMsaDataFormat()) {
+            case MSA_BYTE: {
+              DCHECK_LT(n, kMSALanesByte);
+              int64_t rs = get_register(instr_.WsValue());
+              get_msa_register(instr_.WdValue(), wd.b);
+              wd.b[n] = rs & 0xFFu;
+              set_msa_register(instr_.WdValue(), wd.b);
+              TraceMSARegWr(wd.b);
+              break;
+            }
+            case MSA_HALF: {
+              DCHECK_LT(n, kMSALanesHalf);
+              int64_t rs = get_register(instr_.WsValue());
+              get_msa_register(instr_.WdValue(), wd.h);
+              wd.h[n] = rs & 0xFFFFu;
+              set_msa_register(instr_.WdValue(), wd.h);
+              TraceMSARegWr(wd.h);
+              break;
+            }
+            case MSA_WORD: {
+              DCHECK_LT(n, kMSALanesWord);
+              int64_t rs = get_register(instr_.WsValue());
+              get_msa_register(instr_.WdValue(), wd.w);
+              wd.w[n] = rs & 0xFFFFFFFFu;
+              set_msa_register(instr_.WdValue(), wd.w);
+              TraceMSARegWr(wd.w);
+              break;
+            }
+            case MSA_DWORD: {
+              DCHECK_LT(n, kMSALanesDword);
+              int64_t rs = get_register(instr_.WsValue());
+              get_msa_register(instr_.WdValue(), wd.d);
+              wd.d[n] = rs;
+              set_msa_register(instr_.WdValue(), wd.d);
+              TraceMSARegWr(wd.d);
+              break;
+            }
+            default:
+              UNREACHABLE();
+          }
+        } break;
+        case SLDI: {
+          uint8_t v[32];
+          msa_reg_t ws;
+          msa_reg_t wd;
+          get_msa_register(ws_reg(), &ws);
+          get_msa_register(wd_reg(), &wd);
+#define SLDI_DF(s, k)                \
+  for (unsigned i = 0; i < s; i++) { \
+    v[i] = ws.b[s * k + i];          \
+    v[i + s] = wd.b[s * k + i];      \
+  }                                  \
+  for (unsigned i = 0; i < s; i++) { \
+    wd.b[s * k + i] = v[i + n];      \
+  }
+          switch (DecodeMsaDataFormat()) {
+            case MSA_BYTE:
+              DCHECK(n < kMSALanesByte);
+              SLDI_DF(kMSARegSize / sizeof(int8_t) / kBitsPerByte, 0)
+              break;
+            case MSA_HALF:
+              DCHECK(n < kMSALanesHalf);
+              for (int k = 0; k < 2; ++k) {
+                SLDI_DF(kMSARegSize / sizeof(int16_t) / kBitsPerByte, k)
+              }
+              break;
+            case MSA_WORD:
+              DCHECK(n < kMSALanesWord);
+              for (int k = 0; k < 4; ++k) {
+                SLDI_DF(kMSARegSize / sizeof(int32_t) / kBitsPerByte, k)
+              }
+              break;
+            case MSA_DWORD:
+              DCHECK(n < kMSALanesDword);
+              for (int k = 0; k < 8; ++k) {
+                SLDI_DF(kMSARegSize / sizeof(int64_t) / kBitsPerByte, k)
+              }
+              break;
+            default:
+              UNREACHABLE();
+          }
+          set_msa_register(wd_reg(), &wd);
+          TraceMSARegWr(&wd);
+        } break;
+#undef SLDI_DF
+        case SPLATI:
+        case INSVE:
+          UNIMPLEMENTED();
+          break;
+        default:
+          UNREACHABLE();
+      }
+      break;
+  }
+}
+
+template <typename T>
+T Simulator::MsaBitInstrHelper(uint32_t opcode, T wd, T ws, int32_t m) {
+  using uT = typename std::make_unsigned<T>::type;
+  T res;
+  switch (opcode) {
+    case SLLI:
+      res = static_cast<T>(ws << m);
+      break;
+    case SRAI:
+      res = static_cast<T>(ArithmeticShiftRight(ws, m));
+      break;
+    case SRLI:
+      res = static_cast<T>(static_cast<uT>(ws) >> m);
+      break;
+    case BCLRI:
+      res = static_cast<T>(static_cast<T>(~(1ull << m)) & ws);
+      break;
+    case BSETI:
+      res = static_cast<T>(static_cast<T>(1ull << m) | ws);
+      break;
+    case BNEGI:
+      res = static_cast<T>(static_cast<T>(1ull << m) ^ ws);
+      break;
+    case BINSLI: {
+      int elem_size = 8 * sizeof(T);
+      int bits = m + 1;
+      if (bits == elem_size) {
+        res = static_cast<T>(ws);
+      } else {
+        uint64_t mask = ((1ull << bits) - 1) << (elem_size - bits);
+        res = static_cast<T>((static_cast<T>(mask) & ws) |
+                             (static_cast<T>(~mask) & wd));
+      }
+    } break;
+    case BINSRI: {
+      int elem_size = 8 * sizeof(T);
+      int bits = m + 1;
+      if (bits == elem_size) {
+        res = static_cast<T>(ws);
+      } else {
+        uint64_t mask = (1ull << bits) - 1;
+        res = static_cast<T>((static_cast<T>(mask) & ws) |
+                             (static_cast<T>(~mask) & wd));
+      }
+    } break;
+    case SAT_S: {
+#define M_MAX_INT(x) static_cast<int64_t>((1LL << ((x)-1)) - 1)
+#define M_MIN_INT(x) static_cast<int64_t>(-(1LL << ((x)-1)))
+      int shift = 64 - 8 * sizeof(T);
+      int64_t ws_i64 = (static_cast<int64_t>(ws) << shift) >> shift;
+      res = static_cast<T>(ws_i64 < M_MIN_INT(m + 1)
+                               ? M_MIN_INT(m + 1)
+                               : ws_i64 > M_MAX_INT(m + 1) ? M_MAX_INT(m + 1)
+                                                           : ws_i64);
+#undef M_MAX_INT
+#undef M_MIN_INT
+    } break;
+    case SAT_U: {
+#define M_MAX_UINT(x) static_cast<uint64_t>(-1ULL >> (64 - (x)))
+      uint64_t mask = static_cast<uint64_t>(-1ULL >> (64 - 8 * sizeof(T)));
+      uint64_t ws_u64 = static_cast<uint64_t>(ws) & mask;
+      res = static_cast<T>(ws_u64 < M_MAX_UINT(m + 1) ? ws_u64
+                                                      : M_MAX_UINT(m + 1));
+#undef M_MAX_UINT
+    } break;
+    case SRARI:
+      if (!m) {
+        res = static_cast<T>(ws);
+      } else {
+        res = static_cast<T>(ArithmeticShiftRight(ws, m)) +
+              static_cast<T>((ws >> (m - 1)) & 0x1);
+      }
+      break;
+    case SRLRI:
+      if (!m) {
+        res = static_cast<T>(ws);
+      } else {
+        res = static_cast<T>(static_cast<uT>(ws) >> m) +
+              static_cast<T>((ws >> (m - 1)) & 0x1);
+      }
+      break;
+    default:
+      UNREACHABLE();
+  }
+  return res;
+}
+
+void Simulator::DecodeTypeMsaBIT() {
+  DCHECK_EQ(kArchVariant, kSw64r3);
+  DCHECK(CpuFeatures::IsSupported(SW64_SIMD));
+  uint32_t opcode = instr_.InstructionBits() & kMsaBITMask;
+  int32_t m = instr_.MsaBitMValue();
+  msa_reg_t wd, ws;
+
+#define MSA_BIT_DF(elem, num_of_lanes)                                 \
+  get_msa_register(instr_.WsValue(), ws.elem);                         \
+  if (opcode == BINSLI || opcode == BINSRI) {                          \
+    get_msa_register(instr_.WdValue(), wd.elem);                       \
+  }                                                                    \
+  for (int i = 0; i < num_of_lanes; i++) {                             \
+    wd.elem[i] = MsaBitInstrHelper(opcode, wd.elem[i], ws.elem[i], m); \
+  }                                                                    \
+  set_msa_register(instr_.WdValue(), wd.elem);                         \
+  TraceMSARegWr(wd.elem)
+
+  switch (DecodeMsaDataFormat()) {
+    case MSA_BYTE:
+      DCHECK(m < kMSARegSize / kMSALanesByte);
+      MSA_BIT_DF(b, kMSALanesByte);
+      break;
+    case MSA_HALF:
+      DCHECK(m < kMSARegSize / kMSALanesHalf);
+      MSA_BIT_DF(h, kMSALanesHalf);
+      break;
+    case MSA_WORD:
+      DCHECK(m < kMSARegSize / kMSALanesWord);
+      MSA_BIT_DF(w, kMSALanesWord);
+      break;
+    case MSA_DWORD:
+      DCHECK(m < kMSARegSize / kMSALanesDword);
+      MSA_BIT_DF(d, kMSALanesDword);
+      break;
+    default:
+      UNREACHABLE();
+  }
+#undef MSA_BIT_DF
+}
+
+void Simulator::DecodeTypeMsaMI10() {
+  DCHECK_EQ(kArchVariant, kSw64r3);
+  DCHECK(CpuFeatures::IsSupported(SW64_SIMD));
+  uint32_t opcode = instr_.InstructionBits() & kMsaMI10Mask;
+  int64_t s10 = (static_cast<int64_t>(instr_.MsaImmMI10Value()) << 54) >> 54;
+  int64_t rs = get_register(instr_.WsValue());
+  int64_t addr;
+  msa_reg_t wd;
+
+#define MSA_MI10_LOAD(elem, num_of_lanes, T)       \
+  for (int i = 0; i < num_of_lanes; ++i) {         \
+    addr = rs + (s10 + i) * sizeof(T);             \
+    wd.elem[i] = ReadMem<T>(addr, instr_.instr()); \
+  }                                                \
+  set_msa_register(instr_.WdValue(), wd.elem);
+
+#define MSA_MI10_STORE(elem, num_of_lanes, T)      \
+  get_msa_register(instr_.WdValue(), wd.elem);     \
+  for (int i = 0; i < num_of_lanes; ++i) {         \
+    addr = rs + (s10 + i) * sizeof(T);             \
+    WriteMem<T>(addr, wd.elem[i], instr_.instr()); \
+  }
+
+  if (opcode == MSA_LD) {
+    switch (DecodeMsaDataFormat()) {
+      case MSA_BYTE:
+        MSA_MI10_LOAD(b, kMSALanesByte, int8_t);
+        break;
+      case MSA_HALF:
+        MSA_MI10_LOAD(h, kMSALanesHalf, int16_t);
+        break;
+      case MSA_WORD:
+        MSA_MI10_LOAD(w, kMSALanesWord, int32_t);
+        break;
+      case MSA_DWORD:
+        MSA_MI10_LOAD(d, kMSALanesDword, int64_t);
+        break;
+      default:
+        UNREACHABLE();
+    }
+  } else if (opcode == MSA_ST) {
+    switch (DecodeMsaDataFormat()) {
+      case MSA_BYTE:
+        MSA_MI10_STORE(b, kMSALanesByte, int8_t);
+        break;
+      case MSA_HALF:
+        MSA_MI10_STORE(h, kMSALanesHalf, int16_t);
+        break;
+      case MSA_WORD:
+        MSA_MI10_STORE(w, kMSALanesWord, int32_t);
+        break;
+      case MSA_DWORD:
+        MSA_MI10_STORE(d, kMSALanesDword, int64_t);
+        break;
+      default:
+        UNREACHABLE();
+    }
+  } else {
+    UNREACHABLE();
+  }
+
+#undef MSA_MI10_LOAD
+#undef MSA_MI10_STORE
+}
+
+template <typename T>
+T Simulator::Msa3RInstrHelper(uint32_t opcode, T wd, T ws, T wt) {
+  using uT = typename std::make_unsigned<T>::type;
+  T res;
+  int wt_modulo = wt % (sizeof(T) * 8);
+  switch (opcode) {
+    case SLL_MSA:
+      res = static_cast<T>(ws << wt_modulo);
+      break;
+    case SRA_MSA:
+      res = static_cast<T>(ArithmeticShiftRight(ws, wt_modulo));
+      break;
+    case SRL_MSA:
+      res = static_cast<T>(static_cast<uT>(ws) >> wt_modulo);
+      break;
+    case BCLR:
+      res = static_cast<T>(static_cast<T>(~(1ull << wt_modulo)) & ws);
+      break;
+    case BSET:
+      res = static_cast<T>(static_cast<T>(1ull << wt_modulo) | ws);
+      break;
+    case BNEG:
+      res = static_cast<T>(static_cast<T>(1ull << wt_modulo) ^ ws);
+      break;
+    case BINSL: {
+      int elem_size = 8 * sizeof(T);
+      int bits = wt_modulo + 1;
+      if (bits == elem_size) {
+        res = static_cast<T>(ws);
+      } else {
+        uint64_t mask = ((1ull << bits) - 1) << (elem_size - bits);
+        res = static_cast<T>((static_cast<T>(mask) & ws) |
+                             (static_cast<T>(~mask) & wd));
+      }
+    } break;
+    case BINSR: {
+      int elem_size = 8 * sizeof(T);
+      int bits = wt_modulo + 1;
+      if (bits == elem_size) {
+        res = static_cast<T>(ws);
+      } else {
+        uint64_t mask = (1ull << bits) - 1;
+        res = static_cast<T>((static_cast<T>(mask) & ws) |
+                             (static_cast<T>(~mask) & wd));
+      }
+    } break;
+    case ADDV:
+      res = ws + wt;
+      break;
+    case SUBV:
+      res = ws - wt;
+      break;
+    case MAX_S:
+      res = Max(ws, wt);
+      break;
+    case MAX_U:
+      res = static_cast<T>(Max(static_cast<uT>(ws), static_cast<uT>(wt)));
+      break;
+    case MIN_S:
+      res = Min(ws, wt);
+      break;
+    case MIN_U:
+      res = static_cast<T>(Min(static_cast<uT>(ws), static_cast<uT>(wt)));
+      break;
+    case MAX_A:
+      // We use negative abs in order to avoid problems
+      // with corner case for MIN_INT
+      res = Nabs(ws) < Nabs(wt) ? ws : wt;
+      break;
+    case MIN_A:
+      // We use negative abs in order to avoid problems
+      // with corner case for MIN_INT
+      res = Nabs(ws) > Nabs(wt) ? ws : wt;
+      break;
+    case CEQ:
+      res = static_cast<T>(!Compare(ws, wt) ? -1ull : 0ull);
+      break;
+    case CLT_S:
+      res = static_cast<T>((Compare(ws, wt) == -1) ? -1ull : 0ull);
+      break;
+    case CLT_U:
+      res = static_cast<T>(
+          (Compare(static_cast<uT>(ws), static_cast<uT>(wt)) == -1) ? -1ull
+                                                                    : 0ull);
+      break;
+    case CLE_S:
+      res = static_cast<T>((Compare(ws, wt) != 1) ? -1ull : 0ull);
+      break;
+    case CLE_U:
+      res = static_cast<T>(
+          (Compare(static_cast<uT>(ws), static_cast<uT>(wt)) != 1) ? -1ull
+                                                                   : 0ull);
+      break;
+    case ADD_A:
+      res = static_cast<T>(Abs(ws) + Abs(wt));
+      break;
+    case ADDS_A: {
+      T ws_nabs = Nabs(ws);
+      T wt_nabs = Nabs(wt);
+      if (ws_nabs < -std::numeric_limits<T>::max() - wt_nabs) {
+        res = std::numeric_limits<T>::max();
+      } else {
+        res = -(ws_nabs + wt_nabs);
+      }
+    } break;
+    case ADDS_S:
+      res = SaturateAdd(ws, wt);
+      break;
+    case ADDS_U: {
+      uT ws_u = static_cast<uT>(ws);
+      uT wt_u = static_cast<uT>(wt);
+      res = static_cast<T>(SaturateAdd(ws_u, wt_u));
+    } break;
+    case AVE_S:
+      res = static_cast<T>((wt & ws) + ((wt ^ ws) >> 1));
+      break;
+    case AVE_U: {
+      uT ws_u = static_cast<uT>(ws);
+      uT wt_u = static_cast<uT>(wt);
+      res = static_cast<T>((wt_u & ws_u) + ((wt_u ^ ws_u) >> 1));
+    } break;
+    case AVER_S:
+      res = static_cast<T>((wt | ws) - ((wt ^ ws) >> 1));
+      break;
+    case AVER_U: {
+      uT ws_u = static_cast<uT>(ws);
+      uT wt_u = static_cast<uT>(wt);
+      res = static_cast<T>((wt_u | ws_u) - ((wt_u ^ ws_u) >> 1));
+    } break;
+    case SUBS_S:
+      res = SaturateSub(ws, wt);
+      break;
+    case SUBS_U: {
+      uT ws_u = static_cast<uT>(ws);
+      uT wt_u = static_cast<uT>(wt);
+      res = static_cast<T>(SaturateSub(ws_u, wt_u));
+    } break;
+    case SUBSUS_U: {
+      uT wsu = static_cast<uT>(ws);
+      if (wt > 0) {
+        uT wtu = static_cast<uT>(wt);
+        if (wtu > wsu) {
+          res = 0;
+        } else {
+          res = static_cast<T>(wsu - wtu);
+        }
+      } else {
+        if (wsu > std::numeric_limits<uT>::max() + wt) {
+          res = static_cast<T>(std::numeric_limits<uT>::max());
+        } else {
+          res = static_cast<T>(wsu - wt);
+        }
+      }
+    } break;
+    case SUBSUU_S: {
+      uT wsu = static_cast<uT>(ws);
+      uT wtu = static_cast<uT>(wt);
+      uT wdu;
+      if (wsu > wtu) {
+        wdu = wsu - wtu;
+        if (wdu > std::numeric_limits<T>::max()) {
+          res = std::numeric_limits<T>::max();
+        } else {
+          res = static_cast<T>(wdu);
+        }
+      } else {
+        wdu = wtu - wsu;
+        CHECK(-std::numeric_limits<T>::max() ==
+              std::numeric_limits<T>::min() + 1);
+        if (wdu <= std::numeric_limits<T>::max()) {
+          res = -static_cast<T>(wdu);
+        } else {
+          res = std::numeric_limits<T>::min();
+        }
+      }
+    } break;
+    case ASUB_S:
+      res = static_cast<T>(Abs(ws - wt));
+      break;
+    case ASUB_U: {
+      uT wsu = static_cast<uT>(ws);
+      uT wtu = static_cast<uT>(wt);
+      res = static_cast<T>(wsu > wtu ? wsu - wtu : wtu - wsu);
+    } break;
+    case MULV:
+      res = ws * wt;
+      break;
+    case MADDV:
+      res = wd + ws * wt;
+      break;
+    case MSUBV:
+      res = wd - ws * wt;
+      break;
+    case DIV_S_MSA:
+      res = wt != 0 ? ws / wt : static_cast<T>(Unpredictable);
+      break;
+    case DIV_U:
+      res = wt != 0 ? static_cast<T>(static_cast<uT>(ws) / static_cast<uT>(wt))
+                    : static_cast<T>(Unpredictable);
+      break;
+    case MOD_S:
+      res = wt != 0 ? ws % wt : static_cast<T>(Unpredictable);
+      break;
+    case MOD_U:
+      res = wt != 0 ? static_cast<T>(static_cast<uT>(ws) % static_cast<uT>(wt))
+                    : static_cast<T>(Unpredictable);
+      break;
+    case DOTP_S:
+    case DOTP_U:
+    case DPADD_S:
+    case DPADD_U:
+    case DPSUB_S:
+    case DPSUB_U:
+    case SLD:
+    case SPLAT:
+      UNIMPLEMENTED();
+      break;
+    case SRAR: {
+      int bit = wt_modulo == 0 ? 0 : (ws >> (wt_modulo - 1)) & 1;
+      res = static_cast<T>(ArithmeticShiftRight(ws, wt_modulo) + bit);
+    } break;
+    case SRLR: {
+      uT wsu = static_cast<uT>(ws);
+      int bit = wt_modulo == 0 ? 0 : (wsu >> (wt_modulo - 1)) & 1;
+      res = static_cast<T>((wsu >> wt_modulo) + bit);
+    } break;
+    default:
+      UNREACHABLE();
+  }
+  return res;
+}
+template <typename T_int, typename T_reg>
+void Msa3RInstrHelper_shuffle(const uint32_t opcode, T_reg ws, T_reg wt,
+                              T_reg wd, const int i, const int num_of_lanes) {
+  T_int *ws_p, *wt_p, *wd_p;
+  ws_p = reinterpret_cast<T_int*>(ws);
+  wt_p = reinterpret_cast<T_int*>(wt);
+  wd_p = reinterpret_cast<T_int*>(wd);
+  switch (opcode) {
+    case PCKEV:
+      wd_p[i] = wt_p[2 * i];
+      wd_p[i + num_of_lanes / 2] = ws_p[2 * i];
+      break;
+    case PCKOD:
+      wd_p[i] = wt_p[2 * i + 1];
+      wd_p[i + num_of_lanes / 2] = ws_p[2 * i + 1];
+      break;
+    case ILVL:
+      wd_p[2 * i] = wt_p[i + num_of_lanes / 2];
+      wd_p[2 * i + 1] = ws_p[i + num_of_lanes / 2];
+      break;
+    case ILVR:
+      wd_p[2 * i] = wt_p[i];
+      wd_p[2 * i + 1] = ws_p[i];
+      break;
+    case ILVEV:
+      wd_p[2 * i] = wt_p[2 * i];
+      wd_p[2 * i + 1] = ws_p[2 * i];
+      break;
+    case ILVOD:
+      wd_p[2 * i] = wt_p[2 * i + 1];
+      wd_p[2 * i + 1] = ws_p[2 * i + 1];
+      break;
+    case VSHF: {
+      const int mask_not_valid = 0xC0;
+      const int mask_6_bits = 0x3F;
+      if ((wd_p[i] & mask_not_valid)) {
+        wd_p[i] = 0;
+      } else {
+        int k = (wd_p[i] & mask_6_bits) % (num_of_lanes * 2);
+        wd_p[i] = k >= num_of_lanes ? ws_p[k - num_of_lanes] : wt_p[k];
+      }
+    } break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+template <typename T_int, typename T_smaller_int, typename T_reg>
+void Msa3RInstrHelper_horizontal(const uint32_t opcode, T_reg ws, T_reg wt,
+                                 T_reg wd, const int i,
+                                 const int num_of_lanes) {
+  using T_uint = typename std::make_unsigned<T_int>::type;
+  using T_smaller_uint = typename std::make_unsigned<T_smaller_int>::type;
+  T_int* wd_p;
+  T_smaller_int *ws_p, *wt_p;
+  ws_p = reinterpret_cast<T_smaller_int*>(ws);
+  wt_p = reinterpret_cast<T_smaller_int*>(wt);
+  wd_p = reinterpret_cast<T_int*>(wd);
+  T_uint* wd_pu;
+  T_smaller_uint *ws_pu, *wt_pu;
+  ws_pu = reinterpret_cast<T_smaller_uint*>(ws);
+  wt_pu = reinterpret_cast<T_smaller_uint*>(wt);
+  wd_pu = reinterpret_cast<T_uint*>(wd);
+  switch (opcode) {
+    case HADD_S:
+      wd_p[i] =
+          static_cast<T_int>(ws_p[2 * i + 1]) + static_cast<T_int>(wt_p[2 * i]);
+      break;
+    case HADD_U:
+      wd_pu[i] = static_cast<T_uint>(ws_pu[2 * i + 1]) +
+                 static_cast<T_uint>(wt_pu[2 * i]);
+      break;
+    case HSUB_S:
+      wd_p[i] =
+          static_cast<T_int>(ws_p[2 * i + 1]) - static_cast<T_int>(wt_p[2 * i]);
+      break;
+    case HSUB_U:
+      wd_pu[i] = static_cast<T_uint>(ws_pu[2 * i + 1]) -
+                 static_cast<T_uint>(wt_pu[2 * i]);
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+void Simulator::DecodeTypeMsa3R() {
+  DCHECK_EQ(kArchVariant, kSw64r3);
+  DCHECK(CpuFeatures::IsSupported(SW64_SIMD));
+  uint32_t opcode = instr_.InstructionBits() & kMsa3RMask;
+  msa_reg_t ws, wd, wt;
+  get_msa_register(ws_reg(), &ws);
+  get_msa_register(wt_reg(), &wt);
+  get_msa_register(wd_reg(), &wd);
+  switch (opcode) {
+    case HADD_S:
+    case HADD_U:
+    case HSUB_S:
+    case HSUB_U:
+#define HORIZONTAL_ARITHMETIC_DF(num_of_lanes, int_type, lesser_int_type) \
+  for (int i = 0; i < num_of_lanes; ++i) {                                \
+    Msa3RInstrHelper_horizontal<int_type, lesser_int_type>(               \
+        opcode, &ws, &wt, &wd, i, num_of_lanes);                          \
+  }
+      switch (DecodeMsaDataFormat()) {
+        case MSA_HALF:
+          HORIZONTAL_ARITHMETIC_DF(kMSALanesHalf, int16_t, int8_t);
+          break;
+        case MSA_WORD:
+          HORIZONTAL_ARITHMETIC_DF(kMSALanesWord, int32_t, int16_t);
+          break;
+        case MSA_DWORD:
+          HORIZONTAL_ARITHMETIC_DF(kMSALanesDword, int64_t, int32_t);
+          break;
+        default:
+          UNREACHABLE();
+      }
+      break;
+#undef HORIZONTAL_ARITHMETIC_DF
+    case VSHF:
+#define VSHF_DF(num_of_lanes, int_type)                          \
+  for (int i = 0; i < num_of_lanes; ++i) {                       \
+    Msa3RInstrHelper_shuffle<int_type>(opcode, &ws, &wt, &wd, i, \
+                                       num_of_lanes);            \
+  }
+      switch (DecodeMsaDataFormat()) {
+        case MSA_BYTE:
+          VSHF_DF(kMSALanesByte, int8_t);
+          break;
+        case MSA_HALF:
+          VSHF_DF(kMSALanesHalf, int16_t);
+          break;
+        case MSA_WORD:
+          VSHF_DF(kMSALanesWord, int32_t);
+          break;
+        case MSA_DWORD:
+          VSHF_DF(kMSALanesDword, int64_t);
+          break;
+        default:
+          UNREACHABLE();
+      }
+#undef VSHF_DF
+      break;
+    case PCKEV:
+    case PCKOD:
+    case ILVL:
+    case ILVR:
+    case ILVEV:
+    case ILVOD:
+#define INTERLEAVE_PACK_DF(num_of_lanes, int_type)               \
+  for (int i = 0; i < num_of_lanes / 2; ++i) {                   \
+    Msa3RInstrHelper_shuffle<int_type>(opcode, &ws, &wt, &wd, i, \
+                                       num_of_lanes);            \
+  }
+      switch (DecodeMsaDataFormat()) {
+        case MSA_BYTE:
+          INTERLEAVE_PACK_DF(kMSALanesByte, int8_t);
+          break;
+        case MSA_HALF:
+          INTERLEAVE_PACK_DF(kMSALanesHalf, int16_t);
+          break;
+        case MSA_WORD:
+          INTERLEAVE_PACK_DF(kMSALanesWord, int32_t);
+          break;
+        case MSA_DWORD:
+          INTERLEAVE_PACK_DF(kMSALanesDword, int64_t);
+          break;
+        default:
+          UNREACHABLE();
+      }
+      break;
+#undef INTERLEAVE_PACK_DF
+    default:
+#define MSA_3R_DF(elem, num_of_lanes)                                          \
+  for (int i = 0; i < num_of_lanes; i++) {                                     \
+    wd.elem[i] = Msa3RInstrHelper(opcode, wd.elem[i], ws.elem[i], wt.elem[i]); \
+  }
+
+      switch (DecodeMsaDataFormat()) {
+        case MSA_BYTE:
+          MSA_3R_DF(b, kMSALanesByte);
+          break;
+        case MSA_HALF:
+          MSA_3R_DF(h, kMSALanesHalf);
+          break;
+        case MSA_WORD:
+          MSA_3R_DF(w, kMSALanesWord);
+          break;
+        case MSA_DWORD:
+          MSA_3R_DF(d, kMSALanesDword);
+          break;
+        default:
+          UNREACHABLE();
+      }
+#undef MSA_3R_DF
+      break;
+  }
+  set_msa_register(wd_reg(), &wd);
+  TraceMSARegWr(&wd);
+}
+
+template <typename T_int, typename T_fp, typename T_reg>
+void Msa3RFInstrHelper(uint32_t opcode, T_reg ws, T_reg wt, T_reg* wd) {
+  const T_int all_ones = static_cast<T_int>(-1);
+  const T_fp s_element = *reinterpret_cast<T_fp*>(&ws);
+  const T_fp t_element = *reinterpret_cast<T_fp*>(&wt);
+  switch (opcode) {
+    case FCUN: {
+      if (std::isnan(s_element) || std::isnan(t_element)) {
+        *wd = all_ones;
+      } else {
+        *wd = 0;
+      }
+    } break;
+    case FCEQ: {
+      if (s_element != t_element || std::isnan(s_element) ||
+          std::isnan(t_element)) {
+        *wd = 0;
+      } else {
+        *wd = all_ones;
+      }
+    } break;
+    case FCUEQ: {
+      if (s_element == t_element || std::isnan(s_element) ||
+          std::isnan(t_element)) {
+        *wd = all_ones;
+      } else {
+        *wd = 0;
+      }
+    } break;
+    case FCLT: {
+      if (s_element >= t_element || std::isnan(s_element) ||
+          std::isnan(t_element)) {
+        *wd = 0;
+      } else {
+        *wd = all_ones;
+      }
+    } break;
+    case FCULT: {
+      if (s_element < t_element || std::isnan(s_element) ||
+          std::isnan(t_element)) {
+        *wd = all_ones;
+      } else {
+        *wd = 0;
+      }
+    } break;
+    case FCLE: {
+      if (s_element > t_element || std::isnan(s_element) ||
+          std::isnan(t_element)) {
+        *wd = 0;
+      } else {
+        *wd = all_ones;
+      }
+    } break;
+    case FCULE: {
+      if (s_element <= t_element || std::isnan(s_element) ||
+          std::isnan(t_element)) {
+        *wd = all_ones;
+      } else {
+        *wd = 0;
+      }
+    } break;
+    case FCOR: {
+      if (std::isnan(s_element) || std::isnan(t_element)) {
+        *wd = 0;
+      } else {
+        *wd = all_ones;
+      }
+    } break;
+    case FCUNE: {
+      if (s_element != t_element || std::isnan(s_element) ||
+          std::isnan(t_element)) {
+        *wd = all_ones;
+      } else {
+        *wd = 0;
+      }
+    } break;
+    case FCNE: {
+      if (s_element == t_element || std::isnan(s_element) ||
+          std::isnan(t_element)) {
+        *wd = 0;
+      } else {
+        *wd = all_ones;
+      }
+    } break;
+    case FADD:
+      *wd = bit_cast<T_int>(s_element + t_element);
+      break;
+    case FSUB:
+      *wd = bit_cast<T_int>(s_element - t_element);
+      break;
+    case FMUL:
+      *wd = bit_cast<T_int>(s_element * t_element);
+      break;
+    case FDIV: {
+      if (t_element == 0) {
+        *wd = bit_cast<T_int>(std::numeric_limits<T_fp>::quiet_NaN());
+      } else {
+        *wd = bit_cast<T_int>(s_element / t_element);
+      }
+    } break;
+    case FMADD:
+      *wd = bit_cast<T_int>(
+          std::fma(s_element, t_element, *reinterpret_cast<T_fp*>(wd)));
+      break;
+    case FMSUB:
+      *wd = bit_cast<T_int>(
+          std::fma(-s_element, t_element, *reinterpret_cast<T_fp*>(wd)));
+      break;
+    case FEXP2:
+      *wd = bit_cast<T_int>(std::ldexp(s_element, static_cast<int>(wt)));
+      break;
+    case FMIN:
+      *wd = bit_cast<T_int>(std::min(s_element, t_element));
+      break;
+    case FMAX:
+      *wd = bit_cast<T_int>(std::max(s_element, t_element));
+      break;
+    case FMIN_A: {
+      *wd = bit_cast<T_int>(
+          std::fabs(s_element) < std::fabs(t_element) ? s_element : t_element);
+    } break;
+    case FMAX_A: {
+      *wd = bit_cast<T_int>(
+          std::fabs(s_element) > std::fabs(t_element) ? s_element : t_element);
+    } break;
+    case FSOR:
+    case FSUNE:
+    case FSNE:
+    case FSAF:
+    case FSUN:
+    case FSEQ:
+    case FSUEQ:
+    case FSLT:
+    case FSULT:
+    case FSLE:
+    case FSULE:
+      UNIMPLEMENTED();
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+template <typename T_int, typename T_int_dbl, typename T_reg>
+void Msa3RFInstrHelper2(uint32_t opcode, T_reg ws, T_reg wt, T_reg* wd) {
+  //  using T_uint = typename std::make_unsigned<T_int>::type;
+  using T_uint_dbl = typename std::make_unsigned<T_int_dbl>::type;
+  const T_int max_int = std::numeric_limits<T_int>::max();
+  const T_int min_int = std::numeric_limits<T_int>::min();
+  const int shift = kBitsPerByte * sizeof(T_int) - 1;
+  const T_int_dbl reg_s = ws;
+  const T_int_dbl reg_t = wt;
+  T_int_dbl product, result;
+  product = reg_s * reg_t;
+  switch (opcode) {
+    case MUL_Q: {
+      const T_int_dbl min_fix_dbl =
+          bit_cast<T_uint_dbl>(std::numeric_limits<T_int_dbl>::min()) >> 1U;
+      const T_int_dbl max_fix_dbl = std::numeric_limits<T_int_dbl>::max() >> 1U;
+      if (product == min_fix_dbl) {
+        product = max_fix_dbl;
+      }
+      *wd = static_cast<T_int>(product >> shift);
+    } break;
+    case MADD_Q: {
+      result = (product + (static_cast<T_int_dbl>(*wd) << shift)) >> shift;
+      *wd = static_cast<T_int>(
+          result > max_int ? max_int : result < min_int ? min_int : result);
+    } break;
+    case MSUB_Q: {
+      result = (-product + (static_cast<T_int_dbl>(*wd) << shift)) >> shift;
+      *wd = static_cast<T_int>(
+          result > max_int ? max_int : result < min_int ? min_int : result);
+    } break;
+    case MULR_Q: {
+      const T_int_dbl min_fix_dbl =
+          bit_cast<T_uint_dbl>(std::numeric_limits<T_int_dbl>::min()) >> 1U;
+      const T_int_dbl max_fix_dbl = std::numeric_limits<T_int_dbl>::max() >> 1U;
+      if (product == min_fix_dbl) {
+        *wd = static_cast<T_int>(max_fix_dbl >> shift);
+        break;
+      }
+      *wd = static_cast<T_int>((product + (1 << (shift - 1))) >> shift);
+    } break;
+    case MADDR_Q: {
+      result = (product + (static_cast<T_int_dbl>(*wd) << shift) +
+                (1 << (shift - 1))) >>
+               shift;
+      *wd = static_cast<T_int>(
+          result > max_int ? max_int : result < min_int ? min_int : result);
+    } break;
+    case MSUBR_Q: {
+      result = (-product + (static_cast<T_int_dbl>(*wd) << shift) +
+                (1 << (shift - 1))) >>
+               shift;
+      *wd = static_cast<T_int>(
+          result > max_int ? max_int : result < min_int ? min_int : result);
+    } break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+void Simulator::DecodeTypeMsa3RF() {
+  DCHECK_EQ(kArchVariant, kSw64r3);
+  DCHECK(CpuFeatures::IsSupported(SW64_SIMD));
+  uint32_t opcode = instr_.InstructionBits() & kMsa3RFMask;
+  msa_reg_t wd, ws, wt;
+  if (opcode != FCAF) {
+    get_msa_register(ws_reg(), &ws);
+    get_msa_register(wt_reg(), &wt);
+  }
+  switch (opcode) {
+    case FCAF:
+      wd.d[0] = 0;
+      wd.d[1] = 0;
+      break;
+    case FEXDO:
+#define PACK_FLOAT16(sign, exp, frac) \
+  static_cast<uint16_t>(((sign) << 15) + ((exp) << 10) + (frac))
+#define FEXDO_DF(source, dst)                                        \
+  do {                                                               \
+    element = source;                                                \
+    aSign = element >> 31;                                           \
+    aExp = element >> 23 & 0xFF;                                     \
+    aFrac = element & 0x007FFFFF;                                    \
+    if (aExp == 0xFF) {                                              \
+      if (aFrac) {                                                   \
+        /* Input is a NaN */                                         \
+        dst = 0x7DFFU;                                               \
+        break;                                                       \
+      }                                                              \
+      /* Infinity */                                                 \
+      dst = PACK_FLOAT16(aSign, 0x1F, 0);                            \
+      break;                                                         \
+    } else if (aExp == 0 && aFrac == 0) {                            \
+      dst = PACK_FLOAT16(aSign, 0, 0);                               \
+      break;                                                         \
+    } else {                                                         \
+      int maxexp = 29;                                               \
+      uint32_t mask;                                                 \
+      uint32_t increment;                                            \
+      bool rounding_bumps_exp;                                       \
+      aFrac |= 0x00800000;                                           \
+      aExp -= 0x71;                                                  \
+      if (aExp < 1) {                                                \
+        /* Will be denormal in halfprec */                           \
+        mask = 0x00FFFFFF;                                           \
+        if (aExp >= -11) {                                           \
+          mask >>= 11 + aExp;                                        \
+        }                                                            \
+      } else {                                                       \
+        /* Normal number in halfprec */                              \
+        mask = 0x00001FFF;                                           \
+      }                                                              \
+      switch (MSACSR_ & 3) {                                         \
+        case kRoundToNearest:                                        \
+          increment = (mask + 1) >> 1;                               \
+          if ((aFrac & mask) == increment) {                         \
+            increment = aFrac & (increment << 1);                    \
+          }                                                          \
+          break;                                                     \
+        case kRoundToPlusInf:                                        \
+          increment = aSign ? 0 : mask;                              \
+          break;                                                     \
+        case kRoundToMinusInf:                                       \
+          increment = aSign ? mask : 0;                              \
+          break;                                                     \
+        case kRoundToZero:                                           \
+          increment = 0;                                             \
+          break;                                                     \
+      }                                                              \
+      rounding_bumps_exp = (aFrac + increment >= 0x01000000);        \
+      if (aExp > maxexp || (aExp == maxexp && rounding_bumps_exp)) { \
+        dst = PACK_FLOAT16(aSign, 0x1F, 0);                          \
+        break;                                                       \
+      }                                                              \
+      aFrac += increment;                                            \
+      if (rounding_bumps_exp) {                                      \
+        aFrac >>= 1;                                                 \
+        aExp++;                                                      \
+      }                                                              \
+      if (aExp < -10) {                                              \
+        dst = PACK_FLOAT16(aSign, 0, 0);                             \
+        break;                                                       \
+      }                                                              \
+      if (aExp < 0) {                                                \
+        aFrac >>= -aExp;                                             \
+        aExp = 0;                                                    \
+      }                                                              \
+      dst = PACK_FLOAT16(aSign, aExp, aFrac >> 13);                  \
+    }                                                                \
+  } while (0);
+      switch (DecodeMsaDataFormat()) {
+        case MSA_HALF:
+          for (int i = 0; i < kMSALanesWord; i++) {
+            uint_fast32_t element;
+            uint_fast32_t aSign, aFrac;
+            int_fast32_t aExp;
+            FEXDO_DF(ws.uw[i], wd.uh[i + kMSALanesHalf / 2])
+            FEXDO_DF(wt.uw[i], wd.uh[i])
+          }
+          break;
+        case MSA_WORD:
+          for (int i = 0; i < kMSALanesDword; i++) {
+            wd.w[i + kMSALanesWord / 2] = bit_cast<int32_t>(
+                static_cast<float>(bit_cast<double>(ws.d[i])));
+            wd.w[i] = bit_cast<int32_t>(
+                static_cast<float>(bit_cast<double>(wt.d[i])));
+          }
+          break;
+        default:
+          UNREACHABLE();
+      }
+      break;
+#undef PACK_FLOAT16
+#undef FEXDO_DF
+    case FTQ:
+#define FTQ_DF(source, dst, fp_type, int_type)                 \
+  element = bit_cast<fp_type>(source) *                        \
+            (1U << (sizeof(int_type) * kBitsPerByte - 1));     \
+  if (element > std::numeric_limits<int_type>::max()) {        \
+    dst = std::numeric_limits<int_type>::max();                \
+  } else if (element < std::numeric_limits<int_type>::min()) { \
+    dst = std::numeric_limits<int_type>::min();                \
+  } else if (std::isnan(element)) {                            \
+    dst = 0;                                                   \
+  } else {                                                     \
+    int_type fixed_point;                                      \
+    round_according_to_msacsr(element, &element, &fixed_point); \
+    dst = fixed_point;                                         \
+  }
+
+      switch (DecodeMsaDataFormat()) {
+        case MSA_HALF:
+          for (int i = 0; i < kMSALanesWord; i++) {
+            float element;
+            FTQ_DF(ws.w[i], wd.h[i + kMSALanesHalf / 2], float, int16_t)
+            FTQ_DF(wt.w[i], wd.h[i], float, int16_t)
+          }
+          break;
+        case MSA_WORD:
+          double element;
+          for (int i = 0; i < kMSALanesDword; i++) {
+            FTQ_DF(ws.d[i], wd.w[i + kMSALanesWord / 2], double, int32_t)
+            FTQ_DF(wt.d[i], wd.w[i], double, int32_t)
+          }
+          break;
+        default:
+          UNREACHABLE();
+      }
+      break;
+#undef FTQ_DF
+#define MSA_3RF_DF(T1, T2, Lanes, ws, wt, wd)      \
+  for (int i = 0; i < Lanes; i++) {                \
+    Msa3RFInstrHelper<T1, T2>(opcode, ws, wt, &(wd)); \
+  }
+#define MSA_3RF_DF2(T1, T2, Lanes, ws, wt, wd)      \
+  for (int i = 0; i < Lanes; i++) {                 \
+    Msa3RFInstrHelper2<T1, T2>(opcode, ws, wt, &(wd)); \
+  }
+    case MADD_Q:
+    case MSUB_Q:
+    case MADDR_Q:
+    case MSUBR_Q:
+      get_msa_register(wd_reg(), &wd);
+      V8_FALLTHROUGH;
+    case MUL_Q:
+    case MULR_Q:
+      switch (DecodeMsaDataFormat()) {
+        case MSA_HALF:
+          MSA_3RF_DF2(int16_t, int32_t, kMSALanesHalf, ws.h[i], wt.h[i],
+                      wd.h[i])
+          break;
+        case MSA_WORD:
+          MSA_3RF_DF2(int32_t, int64_t, kMSALanesWord, ws.w[i], wt.w[i],
+                      wd.w[i])
+          break;
+        default:
+          UNREACHABLE();
+      }
+      break;
+    default:
+      if (opcode == FMADD || opcode == FMSUB) {
+        get_msa_register(wd_reg(), &wd);
+      }
+      switch (DecodeMsaDataFormat()) {
+        case MSA_WORD:
+          MSA_3RF_DF(int32_t, float, kMSALanesWord, ws.w[i], wt.w[i], wd.w[i])
+          break;
+        case MSA_DWORD:
+          MSA_3RF_DF(int64_t, double, kMSALanesDword, ws.d[i], wt.d[i], wd.d[i])
+          break;
+        default:
+          UNREACHABLE();
+      }
+      break;
+#undef MSA_3RF_DF
+#undef MSA_3RF_DF2
+  }
+  set_msa_register(wd_reg(), &wd);
+  TraceMSARegWr(&wd);
+}
+
+void Simulator::DecodeTypeMsaVec() {
+  DCHECK_EQ(kArchVariant, kSw64r3);
+  DCHECK(CpuFeatures::IsSupported(SW64_SIMD));
+  uint32_t opcode = instr_.InstructionBits() & kMsaVECMask;
+  msa_reg_t wd, ws, wt;
+
+  get_msa_register(instr_.WsValue(), ws.d);
+  get_msa_register(instr_.WtValue(), wt.d);
+  if (opcode == BMNZ_V || opcode == BMZ_V || opcode == BSEL_V) {
+    get_msa_register(instr_.WdValue(), wd.d);
+  }
+
+  for (int i = 0; i < kMSALanesDword; i++) {
+    switch (opcode) {
+      case AND_V:
+        wd.d[i] = ws.d[i] & wt.d[i];
+        break;
+      case OR_V:
+        wd.d[i] = ws.d[i] | wt.d[i];
+        break;
+      case NOR_V:
+        wd.d[i] = ~(ws.d[i] | wt.d[i]);
+        break;
+      case XOR_V:
+        wd.d[i] = ws.d[i] ^ wt.d[i];
+        break;
+      case BMNZ_V:
+        wd.d[i] = (wt.d[i] & ws.d[i]) | (~wt.d[i] & wd.d[i]);
+        break;
+      case BMZ_V:
+        wd.d[i] = (~wt.d[i] & ws.d[i]) | (wt.d[i] & wd.d[i]);
+        break;
+      case BSEL_V:
+        wd.d[i] = (~wd.d[i] & ws.d[i]) | (wd.d[i] & wt.d[i]);
+        break;
+      default:
+        UNREACHABLE();
+    }
+  }
+  set_msa_register(instr_.WdValue(), wd.d);
+  TraceMSARegWr(wd.d);
+}
+
+void Simulator::DecodeTypeMsa2R() {
+  DCHECK_EQ(kArchVariant, kSw64r3);
+  DCHECK(CpuFeatures::IsSupported(SW64_SIMD));
+  uint32_t opcode = instr_.InstructionBits() & kMsa2RMask;
+  msa_reg_t wd, ws;
+  switch (opcode) {
+    case FILL:
+      switch (DecodeMsaDataFormat()) {
+        case MSA_BYTE: {
+          int64_t rs = get_register(instr_.WsValue());
+          for (int i = 0; i < kMSALanesByte; i++) {
+            wd.b[i] = rs & 0xFFu;
+          }
+          set_msa_register(instr_.WdValue(), wd.b);
+          TraceMSARegWr(wd.b);
+          break;
+        }
+        case MSA_HALF: {
+          int64_t rs = get_register(instr_.WsValue());
+          for (int i = 0; i < kMSALanesHalf; i++) {
+            wd.h[i] = rs & 0xFFFFu;
+          }
+          set_msa_register(instr_.WdValue(), wd.h);
+          TraceMSARegWr(wd.h);
+          break;
+        }
+        case MSA_WORD: {
+          int64_t rs = get_register(instr_.WsValue());
+          for (int i = 0; i < kMSALanesWord; i++) {
+            wd.w[i] = rs & 0xFFFFFFFFu;
+          }
+          set_msa_register(instr_.WdValue(), wd.w);
+          TraceMSARegWr(wd.w);
+          break;
+        }
+        case MSA_DWORD: {
+          int64_t rs = get_register(instr_.WsValue());
+          wd.d[0] = wd.d[1] = rs;
+          set_msa_register(instr_.WdValue(), wd.d);
+          TraceMSARegWr(wd.d);
+          break;
+        }
+        default:
+          UNREACHABLE();
+      }
+      break;
+    case PCNT:
+#define PCNT_DF(elem, num_of_lanes)                       \
+  get_msa_register(instr_.WsValue(), ws.elem);            \
+  for (int i = 0; i < num_of_lanes; i++) {                \
+    uint64_t u64elem = static_cast<uint64_t>(ws.elem[i]); \
+    wd.elem[i] = base::bits::CountPopulation(u64elem);    \
+  }                                                       \
+  set_msa_register(instr_.WdValue(), wd.elem);            \
+  TraceMSARegWr(wd.elem)
+
+      switch (DecodeMsaDataFormat()) {
+        case MSA_BYTE:
+          PCNT_DF(ub, kMSALanesByte);
+          break;
+        case MSA_HALF:
+          PCNT_DF(uh, kMSALanesHalf);
+          break;
+        case MSA_WORD:
+          PCNT_DF(uw, kMSALanesWord);
+          break;
+        case MSA_DWORD:
+          PCNT_DF(ud, kMSALanesDword);
+          break;
+        default:
+          UNREACHABLE();
+      }
+#undef PCNT_DF
+      break;
+    case NLOC:
+#define NLOC_DF(elem, num_of_lanes)                                         \
+  get_msa_register(instr_.WsValue(), ws.elem);                              \
+  for (int i = 0; i < num_of_lanes; i++) {                                  \
+    const uint64_t mask = (num_of_lanes == kMSALanesDword)                  \
+                              ? UINT64_MAX                                  \
+                              : (1ULL << (kMSARegSize / num_of_lanes)) - 1; \
+    uint64_t u64elem = static_cast<uint64_t>(~ws.elem[i]) & mask;           \
+    wd.elem[i] = base::bits::CountLeadingZeros64(u64elem) -                 \
+                 (64 - kMSARegSize / num_of_lanes);                         \
+  }                                                                         \
+  set_msa_register(instr_.WdValue(), wd.elem);                              \
+  TraceMSARegWr(wd.elem)
+
+      switch (DecodeMsaDataFormat()) {
+        case MSA_BYTE:
+          NLOC_DF(ub, kMSALanesByte);
+          break;
+        case MSA_HALF:
+          NLOC_DF(uh, kMSALanesHalf);
+          break;
+        case MSA_WORD:
+          NLOC_DF(uw, kMSALanesWord);
+          break;
+        case MSA_DWORD:
+          NLOC_DF(ud, kMSALanesDword);
+          break;
+        default:
+          UNREACHABLE();
+      }
+#undef NLOC_DF
+      break;
+    case NLZC:
+#define NLZC_DF(elem, num_of_lanes)                         \
+  get_msa_register(instr_.WsValue(), ws.elem);              \
+  for (int i = 0; i < num_of_lanes; i++) {                  \
+    uint64_t u64elem = static_cast<uint64_t>(ws.elem[i]);   \
+    wd.elem[i] = base::bits::CountLeadingZeros64(u64elem) - \
+                 (64 - kMSARegSize / num_of_lanes);         \
+  }                                                         \
+  set_msa_register(instr_.WdValue(), wd.elem);              \
+  TraceMSARegWr(wd.elem)
+
+      switch (DecodeMsaDataFormat()) {
+        case MSA_BYTE:
+          NLZC_DF(ub, kMSALanesByte);
+          break;
+        case MSA_HALF:
+          NLZC_DF(uh, kMSALanesHalf);
+          break;
+        case MSA_WORD:
+          NLZC_DF(uw, kMSALanesWord);
+          break;
+        case MSA_DWORD:
+          NLZC_DF(ud, kMSALanesDword);
+          break;
+        default:
+          UNREACHABLE();
+      }
+#undef NLZC_DF
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+#define BIT(n) (0x1LL << n)
+#define QUIET_BIT_S(nan) (bit_cast<int32_t>(nan) & BIT(22))
+#define QUIET_BIT_D(nan) (bit_cast<int64_t>(nan) & BIT(51))
+static inline bool isSnan(float fp) { return !QUIET_BIT_S(fp); }
+static inline bool isSnan(double fp) { return !QUIET_BIT_D(fp); }
+#undef QUIET_BIT_S
+#undef QUIET_BIT_D
+
+template <typename T_int, typename T_fp, typename T_src, typename T_dst>
+T_int Msa2RFInstrHelper(uint32_t opcode, T_src src, T_dst* dst,
+                        Simulator* sim) {
+  using T_uint = typename std::make_unsigned<T_int>::type;
+  switch (opcode) {
+    case FCLASS: {
+#define SNAN_BIT BIT(0)
+#define QNAN_BIT BIT(1)
+#define NEG_INFINITY_BIT BIT(2)
+#define NEG_NORMAL_BIT BIT(3)
+#define NEG_SUBNORMAL_BIT BIT(4)
+#define NEG_ZERO_BIT BIT(5)
+#define POS_INFINITY_BIT BIT(6)
+#define POS_NORMAL_BIT BIT(7)
+#define POS_SUBNORMAL_BIT BIT(8)
+#define POS_ZERO_BIT BIT(9)
+      T_fp element = *reinterpret_cast<T_fp*>(&src);
+      switch (std::fpclassify(element)) {
+        case FP_INFINITE:
+          if (std::signbit(element)) {
+            *dst = NEG_INFINITY_BIT;
+          } else {
+            *dst = POS_INFINITY_BIT;
+          }
+          break;
+        case FP_NAN:
+          if (isSnan(element)) {
+            *dst = SNAN_BIT;
+          } else {
+            *dst = QNAN_BIT;
+          }
+          break;
+        case FP_NORMAL:
+          if (std::signbit(element)) {
+            *dst = NEG_NORMAL_BIT;
+          } else {
+            *dst = POS_NORMAL_BIT;
+          }
+          break;
+        case FP_SUBNORMAL:
+          if (std::signbit(element)) {
+            *dst = NEG_SUBNORMAL_BIT;
+          } else {
+            *dst = POS_SUBNORMAL_BIT;
+          }
+          break;
+        case FP_ZERO:
+          if (std::signbit(element)) {
+            *dst = NEG_ZERO_BIT;
+          } else {
+            *dst = POS_ZERO_BIT;
+          }
+          break;
+        default:
+          UNREACHABLE();
+      }
+      break;
+    }
+#undef BIT
+#undef SNAN_BIT
+#undef QNAN_BIT
+#undef NEG_INFINITY_BIT
+#undef NEG_NORMAL_BIT
+#undef NEG_SUBNORMAL_BIT
+#undef NEG_ZERO_BIT
+#undef POS_INFINITY_BIT
+#undef POS_NORMAL_BIT
+#undef POS_SUBNORMAL_BIT
+#undef POS_ZERO_BIT
+    case FTRUNC_S: {
+      T_fp element = bit_cast<T_fp>(src);
+      const T_int max_int = std::numeric_limits<T_int>::max();
+      const T_int min_int = std::numeric_limits<T_int>::min();
+      if (std::isnan(element)) {
+        *dst = 0;
+      } else if (element >= max_int || element <= min_int) {
+        *dst = element >= max_int ? max_int : min_int;
+      } else {
+        *dst = static_cast<T_int>(std::trunc(element));
+      }
+      break;
+    }
+    case FTRUNC_U: {
+      T_fp element = bit_cast<T_fp>(src);
+      const T_uint max_int = std::numeric_limits<T_uint>::max();
+      if (std::isnan(element)) {
+        *dst = 0;
+      } else if (element >= max_int || element <= 0) {
+        *dst = element >= max_int ? max_int : 0;
+      } else {
+        *dst = static_cast<T_uint>(std::trunc(element));
+      }
+      break;
+    }
+    case FSQRT: {
+      T_fp element = bit_cast<T_fp>(src);
+      if (element < 0 || std::isnan(element)) {
+        *dst = bit_cast<T_int>(std::numeric_limits<T_fp>::quiet_NaN());
+      } else {
+        *dst = bit_cast<T_int>(std::sqrt(element));
+      }
+      break;
+    }
+    case FRSQRT: {
+      T_fp element = bit_cast<T_fp>(src);
+      if (element < 0 || std::isnan(element)) {
+        *dst = bit_cast<T_int>(std::numeric_limits<T_fp>::quiet_NaN());
+      } else {
+        *dst = bit_cast<T_int>(1 / std::sqrt(element));
+      }
+      break;
+    }
+    case FRCP: {
+      T_fp element = bit_cast<T_fp>(src);
+      if (std::isnan(element)) {
+        *dst = bit_cast<T_int>(std::numeric_limits<T_fp>::quiet_NaN());
+      } else {
+        *dst = bit_cast<T_int>(1 / element);
+      }
+      break;
+    }
+    case FRINT: {
+      T_fp element = bit_cast<T_fp>(src);
+      if (std::isnan(element)) {
+        *dst = bit_cast<T_int>(std::numeric_limits<T_fp>::quiet_NaN());
+      } else {
+        T_int dummy;
+        sim->round_according_to_msacsr<T_fp, T_int>(element, &element, &dummy);
+        *dst = bit_cast<T_int>(element);
+      }
+      break;
+    }
+    case FLOG2: {
+      T_fp element = bit_cast<T_fp>(src);
+      switch (std::fpclassify(element)) {
+        case FP_NORMAL:
+        case FP_SUBNORMAL:
+          *dst = bit_cast<T_int>(std::logb(element));
+          break;
+        case FP_ZERO:
+          *dst = bit_cast<T_int>(-std::numeric_limits<T_fp>::infinity());
+          break;
+        case FP_NAN:
+          *dst = bit_cast<T_int>(std::numeric_limits<T_fp>::quiet_NaN());
+          break;
+        case FP_INFINITE:
+          if (element < 0) {
+            *dst = bit_cast<T_int>(std::numeric_limits<T_fp>::quiet_NaN());
+          } else {
+            *dst = bit_cast<T_int>(std::numeric_limits<T_fp>::infinity());
+          }
+          break;
+        default:
+          UNREACHABLE();
+      }
+      break;
+    }
+    case FTINT_S: {
+      T_fp element = bit_cast<T_fp>(src);
+      const T_int max_int = std::numeric_limits<T_int>::max();
+      const T_int min_int = std::numeric_limits<T_int>::min();
+      if (std::isnan(element)) {
+        *dst = 0;
+      } else if (element < min_int || element > max_int) {
+        *dst = element > max_int ? max_int : min_int;
+      } else {
+        sim->round_according_to_msacsr<T_fp, T_int>(element, &element, dst);
+      }
+      break;
+    }
+    case FTINT_U: {
+      T_fp element = bit_cast<T_fp>(src);
+      const T_uint max_uint = std::numeric_limits<T_uint>::max();
+      if (std::isnan(element)) {
+        *dst = 0;
+      } else if (element < 0 || element > max_uint) {
+        *dst = element > max_uint ? max_uint : 0;
+      } else {
+        T_uint res;
+        sim->round_according_to_msacsr<T_fp, T_uint>(element, &element, &res);
+        *dst = *reinterpret_cast<T_int*>(&res);
+      }
+      break;
+    }
+    case FFINT_S:
+      *dst = bit_cast<T_int>(static_cast<T_fp>(src));
+      break;
+    case FFINT_U:
+      using uT_src = typename std::make_unsigned<T_src>::type;
+      *dst = bit_cast<T_int>(static_cast<T_fp>(bit_cast<uT_src>(src)));
+      break;
+    default:
+      UNREACHABLE();
+  }
+  return 0;
+}
+
+template <typename T_int, typename T_fp, typename T_reg>
+T_int Msa2RFInstrHelper2(uint32_t opcode, T_reg ws, int i) {
+  switch (opcode) {
+#define EXTRACT_FLOAT16_SIGN(fp16) (fp16 >> 15)
+#define EXTRACT_FLOAT16_EXP(fp16) (fp16 >> 10 & 0x1F)
+#define EXTRACT_FLOAT16_FRAC(fp16) (fp16 & 0x3FF)
+#define PACK_FLOAT32(sign, exp, frac) \
+  static_cast<uint32_t>(((sign) << 31) + ((exp) << 23) + (frac))
+#define FEXUP_DF(src_index)                                                   \
+  uint_fast16_t element = ws.uh[src_index];                                   \
+  uint_fast32_t aSign, aFrac;                                                 \
+  int_fast32_t aExp;                                                          \
+  aSign = EXTRACT_FLOAT16_SIGN(element);                                      \
+  aExp = EXTRACT_FLOAT16_EXP(element);                                        \
+  aFrac = EXTRACT_FLOAT16_FRAC(element);                                      \
+  if (V8_LIKELY(aExp && aExp != 0x1F)) {                                      \
+    return PACK_FLOAT32(aSign, aExp + 0x70, aFrac << 13);                     \
+  } else if (aExp == 0x1F) {                                                  \
+    if (aFrac) {                                                              \
+      return bit_cast<int32_t>(std::numeric_limits<float>::quiet_NaN());      \
+    } else {                                                                  \
+      return bit_cast<uint32_t>(std::numeric_limits<float>::infinity()) |     \
+             static_cast<uint32_t>(aSign) << 31;                              \
+    }                                                                         \
+  } else {                                                                    \
+    if (aFrac == 0) {                                                         \
+      return PACK_FLOAT32(aSign, 0, 0);                                       \
+    } else {                                                                  \
+      int_fast16_t shiftCount =                                               \
+          base::bits::CountLeadingZeros32(static_cast<uint32_t>(aFrac)) - 21; \
+      aFrac <<= shiftCount;                                                   \
+      aExp = -shiftCount;                                                     \
+      return PACK_FLOAT32(aSign, aExp + 0x70, aFrac << 13);                   \
+    }                                                                         \
+  }
+    case FEXUPL:
+      if (std::is_same<int32_t, T_int>::value) {
+        FEXUP_DF(i + kMSALanesWord)
+      } else {
+        return bit_cast<int64_t>(
+            static_cast<double>(bit_cast<float>(ws.w[i + kMSALanesDword])));
+      }
+    case FEXUPR:
+      if (std::is_same<int32_t, T_int>::value) {
+        FEXUP_DF(i)
+      } else {
+        return bit_cast<int64_t>(static_cast<double>(bit_cast<float>(ws.w[i])));
+      }
+    case FFQL: {
+      if (std::is_same<int32_t, T_int>::value) {
+        return bit_cast<int32_t>(static_cast<float>(ws.h[i + kMSALanesWord]) /
+                                 (1U << 15));
+      } else {
+        return bit_cast<int64_t>(static_cast<double>(ws.w[i + kMSALanesDword]) /
+                                 (1U << 31));
+      }
+      break;
+    }
+    case FFQR: {
+      if (std::is_same<int32_t, T_int>::value) {
+        return bit_cast<int32_t>(static_cast<float>(ws.h[i]) / (1U << 15));
+      } else {
+        return bit_cast<int64_t>(static_cast<double>(ws.w[i]) / (1U << 31));
+      }
+      break;
+      default:
+        UNREACHABLE();
+    }
+  }
+#undef EXTRACT_FLOAT16_SIGN
+#undef EXTRACT_FLOAT16_EXP
+#undef EXTRACT_FLOAT16_FRAC
+#undef PACK_FLOAT32
+#undef FEXUP_DF
+}
+
+void Simulator::DecodeTypeMsa2RF() {
+  DCHECK_EQ(kArchVariant, kSw64r3);
+  DCHECK(CpuFeatures::IsSupported(SW64_SIMD));
+  uint32_t opcode = instr_.InstructionBits() & kMsa2RFMask;
+  msa_reg_t wd, ws;
+  get_msa_register(ws_reg(), &ws);
+  if (opcode == FEXUPL || opcode == FEXUPR || opcode == FFQL ||
+      opcode == FFQR) {
+    switch (DecodeMsaDataFormat()) {
+      case MSA_WORD:
+        for (int i = 0; i < kMSALanesWord; i++) {
+          wd.w[i] = Msa2RFInstrHelper2<int32_t, float>(opcode, ws, i);
+        }
+        break;
+      case MSA_DWORD:
+        for (int i = 0; i < kMSALanesDword; i++) {
+          wd.d[i] = Msa2RFInstrHelper2<int64_t, double>(opcode, ws, i);
+        }
+        break;
+      default:
+        UNREACHABLE();
+    }
+  } else {
+    switch (DecodeMsaDataFormat()) {
+      case MSA_WORD:
+        for (int i = 0; i < kMSALanesWord; i++) {
+          Msa2RFInstrHelper<int32_t, float>(opcode, ws.w[i], &wd.w[i], this);
+        }
+        break;
+      case MSA_DWORD:
+        for (int i = 0; i < kMSALanesDword; i++) {
+          Msa2RFInstrHelper<int64_t, double>(opcode, ws.d[i], &wd.d[i], this);
+        }
+        break;
+      default:
+        UNREACHABLE();
+    }
+  }
+  set_msa_register(wd_reg(), &wd);
+  TraceMSARegWr(&wd);
+}
+
+void Simulator::DecodeTypeRegister() {
+  // ---------- Execution.
+  switch (instr_.OpcodeFieldRaw()) {
+    case COP1:
+      DecodeTypeRegisterCOP1();
+      break;
+    case COP1X:
+      DecodeTypeRegisterCOP1X();
+      break;
+    case SPECIAL:
+      DecodeTypeRegisterSPECIAL();
+      break;
+    case SPECIAL2:
+      DecodeTypeRegisterSPECIAL2();
+      break;
+    case SPECIAL3:
+      DecodeTypeRegisterSPECIAL3();
+      break;
+    case MSA:
+      switch (instr_.MSAMinorOpcodeField()) {
+        case kMsaMinor3R:
+          DecodeTypeMsa3R();
+          break;
+        case kMsaMinor3RF:
+          DecodeTypeMsa3RF();
+          break;
+        case kMsaMinorVEC:
+          DecodeTypeMsaVec();
+          break;
+        case kMsaMinor2R:
+          DecodeTypeMsa2R();
+          break;
+        case kMsaMinor2RF:
+          DecodeTypeMsa2RF();
+          break;
+        case kMsaMinorELM:
+          DecodeTypeMsaELM();
+          break;
+        default:
+          UNREACHABLE();
+      }
+      break;
+    // Unimplemented opcodes raised an error in the configuration step before,
+    // so we can use the default here to set the destination register in common
+    // cases.
+    default:
+      UNREACHABLE();
+  }
+}
+
+
+// Type 2: instructions using a 16, 21 or 26 bits immediate. (e.g. beq, beqc).
+void Simulator::DecodeTypeImmediate() {
+  // Instruction fields.
+  Opcode op = instr_.OpcodeFieldRaw();
+  int32_t rs_reg = instr_.RsValue();
+  int64_t rs = get_register(instr_.RsValue());
+  uint64_t rs_u = static_cast<uint64_t>(rs);
+  int32_t rt_reg = instr_.RtValue();  // Destination register.
+  int64_t rt = get_register(rt_reg);
+  int16_t imm16 = instr_.Imm16Value();
+  int32_t imm18 = instr_.Imm18Value();
+
+  int32_t ft_reg = instr_.FtValue();  // Destination register.
+
+  // Zero extended immediate.
+  uint64_t oe_imm16 = 0xFFFF & imm16;
+  // Sign extended immediate.
+  int64_t se_imm16 = imm16;
+  int64_t se_imm18 = imm18 | ((imm18 & 0x20000) ? 0xFFFFFFFFFFFC0000 : 0);
+
+  // Next pc.
+  int64_t next_pc = bad_ra;
+
+  // Used for conditional branch instructions.
+  bool execute_branch_delay_instruction = false;
+
+  // Used for arithmetic instructions.
+  int64_t alu_out = 0;
+
+  // Used for memory instructions.
+  int64_t addr = 0x0;
+  // Alignment for 32-bit integers used in LWL, LWR, etc.
+  const int kInt32AlignmentMask = sizeof(uint32_t) - 1;
+  // Alignment for 64-bit integers used in LDL, LDR, etc.
+  const int kInt64AlignmentMask = sizeof(uint64_t) - 1;
+
+  // Branch instructions common part.
+  auto BranchAndLinkHelper =
+      [this, &next_pc, &execute_branch_delay_instruction](bool do_branch) {
+        execute_branch_delay_instruction = true;
+        int64_t current_pc = get_pc();
+        if (do_branch) {
+          int16_t imm16 = instr_.Imm16Value();
+          next_pc = current_pc + (imm16 << 2) + kInstrSize;
+          set_register(31, current_pc + 2 * kInstrSize);
+        } else {
+          next_pc = current_pc + 2 * kInstrSize;
+        }
+      };
+
+  auto BranchHelper = [this, &next_pc,
+                       &execute_branch_delay_instruction](bool do_branch) {
+    execute_branch_delay_instruction = true;
+    int64_t current_pc = get_pc();
+    if (do_branch) {
+      int16_t imm16 = instr_.Imm16Value();
+      next_pc = current_pc + (imm16 << 2) + kInstrSize;
+    } else {
+      next_pc = current_pc + 2 * kInstrSize;
+    }
+  };
+
+  auto BranchHelper_MSA = [this, &next_pc, imm16,
+                           &execute_branch_delay_instruction](bool do_branch) {
+    execute_branch_delay_instruction = true;
+    int64_t current_pc = get_pc();
+    const int32_t bitsIn16Int = sizeof(int16_t) * kBitsPerByte;
+    if (do_branch) {
+      if (FLAG_debug_code) {
+        int16_t bits = imm16 & 0xFC;
+        if (imm16 >= 0) {
+          CHECK_EQ(bits, 0);
+        } else {
+          CHECK_EQ(bits ^ 0xFC, 0);
+        }
+      }
+      // jump range :[pc + kInstrSize - 512 * kInstrSize,
+      //              pc + kInstrSize + 511 * kInstrSize]
+      int16_t offset = static_cast<int16_t>(imm16 << (bitsIn16Int - 10)) >>
+                       (bitsIn16Int - 12);
+      next_pc = current_pc + offset + kInstrSize;
+    } else {
+      next_pc = current_pc + 2 * kInstrSize;
+    }
+  };
+
+  auto BranchAndLinkCompactHelper = [this, &next_pc](bool do_branch, int bits) {
+    int64_t current_pc = get_pc();
+    CheckForbiddenSlot(current_pc);
+    if (do_branch) {
+      int32_t imm = instr_.ImmValue(bits);
+      imm <<= 32 - bits;
+      imm >>= 32 - bits;
+      next_pc = current_pc + (imm << 2) + kInstrSize;
+      set_register(31, current_pc + kInstrSize);
+    }
+  };
+
+  auto BranchCompactHelper = [this, &next_pc](bool do_branch, int bits) {
+    int64_t current_pc = get_pc();
+    CheckForbiddenSlot(current_pc);
+    if (do_branch) {
+      int32_t imm = instr_.ImmValue(bits);
+      imm <<= 32 - bits;
+      imm >>= 32 - bits;
+      next_pc = get_pc() + (imm << 2) + kInstrSize;
+    }
+  };
+
+  switch (op) {
+    // ------------- COP1. Coprocessor instructions.
+    case COP1:
+      switch (instr_.RsFieldRaw()) {
+        case BC1: {  // Branch on coprocessor condition.
+          uint32_t cc = instr_.FBccValue();
+          uint32_t fcsr_cc = get_fcsr_condition_bit(cc);
+          uint32_t cc_value = test_fcsr_bit(fcsr_cc);
+          bool do_branch = (instr_.FBtrueValue()) ? cc_value : !cc_value;
+          BranchHelper(do_branch);
+          break;
+        }
+        case BC1EQZ:
+          BranchHelper(!(get_fpu_register(ft_reg) & 0x1));
+          break;
+        case BC1NEZ:
+          BranchHelper(get_fpu_register(ft_reg) & 0x1);
+          break;
+        case BZ_V: {
+          msa_reg_t wt;
+          get_msa_register(wt_reg(), &wt);
+          BranchHelper_MSA(wt.d[0] == 0 && wt.d[1] == 0);
+        } break;
+#define BZ_DF(witdh, lanes)          \
+  {                                  \
+    msa_reg_t wt;                    \
+    get_msa_register(wt_reg(), &wt); \
+    int i;                           \
+    for (i = 0; i < lanes; ++i) {    \
+      if (wt.witdh[i] == 0) {        \
+        break;                       \
+      }                              \
+    }                                \
+    BranchHelper_MSA(i != lanes);    \
+  }
+        case BZ_B:
+          BZ_DF(b, kMSALanesByte)
+          break;
+        case BZ_H:
+          BZ_DF(h, kMSALanesHalf)
+          break;
+        case BZ_W:
+          BZ_DF(w, kMSALanesWord)
+          break;
+        case BZ_D:
+          BZ_DF(d, kMSALanesDword)
+          break;
+#undef BZ_DF
+        case BNZ_V: {
+          msa_reg_t wt;
+          get_msa_register(wt_reg(), &wt);
+          BranchHelper_MSA(wt.d[0] != 0 || wt.d[1] != 0);
+        } break;
+#define BNZ_DF(witdh, lanes)         \
+  {                                  \
+    msa_reg_t wt;                    \
+    get_msa_register(wt_reg(), &wt); \
+    int i;                           \
+    for (i = 0; i < lanes; ++i) {    \
+      if (wt.witdh[i] == 0) {        \
+        break;                       \
+      }                              \
+    }                                \
+    BranchHelper_MSA(i == lanes);    \
+  }
+        case BNZ_B:
+          BNZ_DF(b, kMSALanesByte)
+          break;
+        case BNZ_H:
+          BNZ_DF(h, kMSALanesHalf)
+          break;
+        case BNZ_W:
+          BNZ_DF(w, kMSALanesWord)
+          break;
+        case BNZ_D:
+          BNZ_DF(d, kMSALanesDword)
+          break;
+#undef BNZ_DF
+        default:
+          UNREACHABLE();
+      }
+      break;
+    // ------------- REGIMM class.
+    case REGIMM:
+      switch (instr_.RtFieldRaw()) {
+        case BLTZ:
+          BranchHelper(rs < 0);
+          break;
+        case BGEZ:
+          BranchHelper(rs >= 0);
+          break;
+        case BLTZAL:
+          BranchAndLinkHelper(rs < 0);
+          break;
+        case BGEZAL:
+          BranchAndLinkHelper(rs >= 0);
+          break;
+        case DAHI:
+          SetResult(rs_reg, rs + (se_imm16 << 32));
+          break;
+        case DATI:
+          SetResult(rs_reg, rs + (se_imm16 << 48));
+          break;
+        default:
+          UNREACHABLE();
+      }
+      break;  // case REGIMM.
+    // ------------- Branch instructions.
+    // When comparing to zero, the encoding of rt field is always 0, so we don't
+    // need to replace rt with zero.
+    case BEQ:
+      BranchHelper(rs == rt);
+      break;
+    case BNE:
+      BranchHelper(rs != rt);
+      break;
+    case POP06:  // BLEZALC, BGEZALC, BGEUC, BLEZ (pre-r6)
+      if (kArchVariant == kSw64r3) {
+        if (rt_reg != 0) {
+          if (rs_reg == 0) {  // BLEZALC
+            BranchAndLinkCompactHelper(rt <= 0, 16);
+          } else {
+            if (rs_reg == rt_reg) {  // BGEZALC
+              BranchAndLinkCompactHelper(rt >= 0, 16);
+            } else {  // BGEUC
+              BranchCompactHelper(
+                  static_cast<uint64_t>(rs) >= static_cast<uint64_t>(rt), 16);
+            }
+          }
+        } else {  // BLEZ
+          BranchHelper(rs <= 0);
+        }
+      } else {  // BLEZ
+        BranchHelper(rs <= 0);
+      }
+      break;
+    case POP07:  // BGTZALC, BLTZALC, BLTUC, BGTZ (pre-r6)
+      if (kArchVariant == kSw64r3) {
+        if (rt_reg != 0) {
+          if (rs_reg == 0) {  // BGTZALC
+            BranchAndLinkCompactHelper(rt > 0, 16);
+          } else {
+            if (rt_reg == rs_reg) {  // BLTZALC
+              BranchAndLinkCompactHelper(rt < 0, 16);
+            } else {  // BLTUC
+              BranchCompactHelper(
+                  static_cast<uint64_t>(rs) < static_cast<uint64_t>(rt), 16);
+            }
+          }
+        } else {  // BGTZ
+          BranchHelper(rs > 0);
+        }
+      } else {  // BGTZ
+        BranchHelper(rs > 0);
+      }
+      break;
+    case POP26:  // BLEZC, BGEZC, BGEC/BLEC / BLEZL (pre-r6)
+      if (kArchVariant == kSw64r3) {
+        if (rt_reg != 0) {
+          if (rs_reg == 0) {  // BLEZC
+            BranchCompactHelper(rt <= 0, 16);
+          } else {
+            if (rs_reg == rt_reg) {  // BGEZC
+              BranchCompactHelper(rt >= 0, 16);
+            } else {  // BGEC/BLEC
+              BranchCompactHelper(rs >= rt, 16);
+            }
+          }
+        }
+      } else {  // BLEZL
+        BranchAndLinkHelper(rs <= 0);
+      }
+      break;
+    case POP27:  // BGTZC, BLTZC, BLTC/BGTC / BGTZL (pre-r6)
+      if (kArchVariant == kSw64r3) {
+        if (rt_reg != 0) {
+          if (rs_reg == 0) {  // BGTZC
+            BranchCompactHelper(rt > 0, 16);
+          } else {
+            if (rs_reg == rt_reg) {  // BLTZC
+              BranchCompactHelper(rt < 0, 16);
+            } else {  // BLTC/BGTC
+              BranchCompactHelper(rs < rt, 16);
+            }
+          }
+        }
+      } else {  // BGTZL
+        BranchAndLinkHelper(rs > 0);
+      }
+      break;
+    case POP66:           // BEQZC, JIC
+      if (rs_reg != 0) {  // BEQZC
+        BranchCompactHelper(rs == 0, 21);
+      } else {  // JIC
+        next_pc = rt + imm16;
+      }
+      break;
+    case POP76:           // BNEZC, JIALC
+      if (rs_reg != 0) {  // BNEZC
+        BranchCompactHelper(rs != 0, 21);
+      } else {  // JIALC
+        int64_t current_pc = get_pc();
+        set_register(31, current_pc + kInstrSize);
+        next_pc = rt + imm16;
+      }
+      break;
+    case BC:
+      BranchCompactHelper(true, 26);
+      break;
+    case BALC:
+      BranchAndLinkCompactHelper(true, 26);
+      break;
+    case POP10:  // BOVC, BEQZALC, BEQC / ADDI (pre-r6)
+      if (kArchVariant == kSw64r3) {
+        if (rs_reg >= rt_reg) {  // BOVC
+          bool condition = !is_int32(rs) || !is_int32(rt) || !is_int32(rs + rt);
+          BranchCompactHelper(condition, 16);
+        } else {
+          if (rs_reg == 0) {  // BEQZALC
+            BranchAndLinkCompactHelper(rt == 0, 16);
+          } else {  // BEQC
+            BranchCompactHelper(rt == rs, 16);
+          }
+        }
+      } else {  // ADDI
+        if (HaveSameSign(rs, se_imm16)) {
+          if (rs > 0) {
+            if (rs <= Registers::kMaxValue - se_imm16) {
+              SignalException(kIntegerOverflow);
+            }
+          } else if (rs < 0) {
+            if (rs >= Registers::kMinValue - se_imm16) {
+              SignalException(kIntegerUnderflow);
+            }
+          }
+        }
+        SetResult(rt_reg, rs + se_imm16);
+      }
+      break;
+    case POP30:  // BNVC, BNEZALC, BNEC / DADDI (pre-r6)
+      if (kArchVariant == kSw64r3) {
+        if (rs_reg >= rt_reg) {  // BNVC
+          bool condition = is_int32(rs) && is_int32(rt) && is_int32(rs + rt);
+          BranchCompactHelper(condition, 16);
+        } else {
+          if (rs_reg == 0) {  // BNEZALC
+            BranchAndLinkCompactHelper(rt != 0, 16);
+          } else {  // BNEC
+            BranchCompactHelper(rt != rs, 16);
+          }
+        }
+      }
+      break;
+    // ------------- Arithmetic instructions.
+    case ADDIU: {
+      int32_t alu32_out = static_cast<int32_t>(rs + se_imm16);
+      // Sign-extend result of 32bit operation into 64bit register.
+      SetResult(rt_reg, static_cast<int64_t>(alu32_out));
+      break;
+    }
+//    case DADDIU:
+//      SetResult(rt_reg, rs + se_imm16);
+//      break;
+    case SLTI:
+      SetResult(rt_reg, rs < se_imm16 ? 1 : 0);
+      break;
+    case SLTIU:
+      SetResult(rt_reg, rs_u < static_cast<uint64_t>(se_imm16) ? 1 : 0);
+      break;
+    case ANDI:
+      SetResult(rt_reg, rs & oe_imm16);
+      break;
+    case ORI:
+      SetResult(rt_reg, rs | oe_imm16);
+      break;
+    case XORI:
+      SetResult(rt_reg, rs ^ oe_imm16);
+      break;
+    case LUI:
+      if (rs_reg != 0) {
+        // AUI instruction.
+        DCHECK_EQ(kArchVariant, kSw64r3);
+        int32_t alu32_out = static_cast<int32_t>(rs + (se_imm16 << 16));
+        SetResult(rt_reg, static_cast<int64_t>(alu32_out));
+      } else {
+        // LUI instruction.
+        int32_t alu32_out = static_cast<int32_t>(oe_imm16 << 16);
+        // Sign-extend result of 32bit operation into 64bit register.
+        SetResult(rt_reg, static_cast<int64_t>(alu32_out));
+      }
+      break;
+    case DAUI:
+      DCHECK_EQ(kArchVariant, kSw64r3);
+      DCHECK_NE(rs_reg, 0);
+      SetResult(rt_reg, rs + (se_imm16 << 16));
+      break;
+    // ------------- Memory instructions.
+    case LB:
+      set_register(rt_reg, ReadB(rs + se_imm16));
+      break;
+    case LH:
+      set_register(rt_reg, ReadH(rs + se_imm16, instr_.instr()));
+      break;
+    case LWL: {
+      // al_offset is offset of the effective address within an aligned word.
+      uint8_t al_offset = (rs + se_imm16) & kInt32AlignmentMask;
+      uint8_t byte_shift = kInt32AlignmentMask - al_offset;
+      uint32_t mask = (1 << byte_shift * 8) - 1;
+      addr = rs + se_imm16 - al_offset;
+      int32_t val = ReadW(addr, instr_.instr());
+      val <<= byte_shift * 8;
+      val |= rt & mask;
+      set_register(rt_reg, static_cast<int64_t>(val));
+      break;
+    }
+    case LW:
+      set_register(rt_reg, ReadW(rs + se_imm16, instr_.instr()));
+      break;
+    case LWU:
+      set_register(rt_reg, ReadWU(rs + se_imm16, instr_.instr()));
+      break;
+    case LD:
+      set_register(rt_reg, Read2W(rs + se_imm16, instr_.instr()));
+      break;
+    case LBU:
+      set_register(rt_reg, ReadBU(rs + se_imm16));
+      break;
+    case LHU:
+      set_register(rt_reg, ReadHU(rs + se_imm16, instr_.instr()));
+      break;
+    case LWR: {
+      // al_offset is offset of the effective address within an aligned word.
+      uint8_t al_offset = (rs + se_imm16) & kInt32AlignmentMask;
+      uint8_t byte_shift = kInt32AlignmentMask - al_offset;
+      uint32_t mask = al_offset ? (~0 << (byte_shift + 1) * 8) : 0;
+      addr = rs + se_imm16 - al_offset;
+      alu_out = ReadW(addr, instr_.instr());
+      alu_out = static_cast<uint32_t> (alu_out) >> al_offset * 8;
+      alu_out |= rt & mask;
+      set_register(rt_reg, alu_out);
+      break;
+    }
+    case LDL: {
+      // al_offset is offset of the effective address within an aligned word.
+      uint8_t al_offset = (rs + se_imm16) & kInt64AlignmentMask;
+      uint8_t byte_shift = kInt64AlignmentMask - al_offset;
+      uint64_t mask = (1UL << byte_shift * 8) - 1;
+      addr = rs + se_imm16 - al_offset;
+      alu_out = Read2W(addr, instr_.instr());
+      alu_out <<= byte_shift * 8;
+      alu_out |= rt & mask;
+      set_register(rt_reg, alu_out);
+      break;
+    }
+    case LDR: {
+      // al_offset is offset of the effective address within an aligned word.
+      uint8_t al_offset = (rs + se_imm16) & kInt64AlignmentMask;
+      uint8_t byte_shift = kInt64AlignmentMask - al_offset;
+      uint64_t mask = al_offset ? (~0UL << (byte_shift + 1) * 8) : 0UL;
+      addr = rs + se_imm16 - al_offset;
+      alu_out = Read2W(addr, instr_.instr());
+      alu_out = alu_out >> al_offset * 8;
+      alu_out |= rt & mask;
+      set_register(rt_reg, alu_out);
+      break;
+    }
+    case SB:
+      WriteB(rs + se_imm16, static_cast<int8_t>(rt));
+      break;
+    case SH:
+      WriteH(rs + se_imm16, static_cast<uint16_t>(rt), instr_.instr());
+      break;
+    case SWL: {
+      uint8_t al_offset = (rs + se_imm16) & kInt32AlignmentMask;
+      uint8_t byte_shift = kInt32AlignmentMask - al_offset;
+      uint32_t mask = byte_shift ? (~0 << (al_offset + 1) * 8) : 0;
+      addr = rs + se_imm16 - al_offset;
+      uint64_t mem_value = ReadW(addr, instr_.instr()) & mask;
+      mem_value |= static_cast<uint32_t>(rt) >> byte_shift * 8;
+      WriteW(addr, static_cast<int32_t>(mem_value), instr_.instr());
+      break;
+    }
+    case SW:
+      WriteW(rs + se_imm16, static_cast<int32_t>(rt), instr_.instr());
+      break;
+    case SD:
+      Write2W(rs + se_imm16, rt, instr_.instr());
+      break;
+    case SWR: {
+      uint8_t al_offset = (rs + se_imm16) & kInt32AlignmentMask;
+      uint32_t mask = (1 << al_offset * 8) - 1;
+      addr = rs + se_imm16 - al_offset;
+      uint64_t mem_value = ReadW(addr, instr_.instr());
+      mem_value = (rt << al_offset * 8) | (mem_value & mask);
+      WriteW(addr, static_cast<int32_t>(mem_value), instr_.instr());
+      break;
+    }
+    case SDL: {
+      uint8_t al_offset = (rs + se_imm16) & kInt64AlignmentMask;
+      uint8_t byte_shift = kInt64AlignmentMask - al_offset;
+      uint64_t mask = byte_shift ? (~0UL << (al_offset + 1) * 8) : 0;
+      addr = rs + se_imm16 - al_offset;
+      uint64_t mem_value = Read2W(addr, instr_.instr()) & mask;
+      mem_value |= static_cast<uint64_t>(rt) >> byte_shift * 8;
+      Write2W(addr, mem_value, instr_.instr());
+      break;
+    }
+    case SDR: {
+      uint8_t al_offset = (rs + se_imm16) & kInt64AlignmentMask;
+      uint64_t mask = (1UL << al_offset * 8) - 1;
+      addr = rs + se_imm16 - al_offset;
+      uint64_t mem_value = Read2W(addr, instr_.instr());
+      mem_value = (rt << al_offset * 8) | (mem_value & mask);
+      Write2W(addr, mem_value, instr_.instr());
+      break;
+    }
+    case LL: {
+      // LL/SC sequence cannot be simulated properly
+      DCHECK_EQ(kArchVariant, kSw64r2);
+      set_register(rt_reg, ReadW(rs + se_imm16, instr_.instr()));
+      break;
+    }
+    case SC: {
+      // LL/SC sequence cannot be simulated properly
+      DCHECK_EQ(kArchVariant, kSw64r2);
+      WriteW(rs + se_imm16, static_cast<int32_t>(rt), instr_.instr());
+      set_register(rt_reg, 1);
+      break;
+    }
+    case LLD: {
+      // LL/SC sequence cannot be simulated properly
+      DCHECK_EQ(kArchVariant, kSw64r2);
+      set_register(rt_reg, ReadD(rs + se_imm16, instr_.instr()));
+      break;
+    }
+    case SCD: {
+      // LL/SC sequence cannot be simulated properly
+      DCHECK_EQ(kArchVariant, kSw64r2);
+      WriteD(rs + se_imm16, rt, instr_.instr());
+      set_register(rt_reg, 1);
+      break;
+    }
+    case LWC1:
+      set_fpu_register(ft_reg, kFPUInvalidResult);  // Trash upper 32 bits.
+      set_fpu_register_word(ft_reg,
+                            ReadW(rs + se_imm16, instr_.instr(), FLOAT_DOUBLE));
+      break;
+    case LDC1:
+      set_fpu_register_double(ft_reg, ReadD(rs + se_imm16, instr_.instr()));
+      TraceMemRd(addr, get_fpu_register(ft_reg), DOUBLE);
+      break;
+    case SWC1: {
+      int32_t alu_out_32 = static_cast<int32_t>(get_fpu_register(ft_reg));
+      WriteW(rs + se_imm16, alu_out_32, instr_.instr());
+      break;
+    }
+    case SDC1:
+      WriteD(rs + se_imm16, get_fpu_register_double(ft_reg), instr_.instr());
+      TraceMemWr(rs + se_imm16, get_fpu_register(ft_reg), DWORD);
+      break;
+    // ------------- PC-Relative instructions.
+    case PCREL: {
+      // rt field: checking 5-bits.
+      int32_t imm21 = instr_.Imm21Value();
+      int64_t current_pc = get_pc();
+      uint8_t rt = (imm21 >> kImm16Bits);
+      switch (rt) {
+        case ALUIPC:
+          addr = current_pc + (se_imm16 << 16);
+          alu_out = static_cast<int64_t>(~0x0FFFF) & addr;
+          break;
+        case AUIPC:
+          alu_out = current_pc + (se_imm16 << 16);
+          break;
+        default: {
+          int32_t imm19 = instr_.Imm19Value();
+          // rt field: checking the most significant 3-bits.
+          rt = (imm21 >> kImm18Bits);
+          switch (rt) {
+            case LDPC:
+              addr =
+                  (current_pc & static_cast<int64_t>(~0x7)) + (se_imm18 << 3);
+              alu_out = Read2W(addr, instr_.instr());
+              break;
+            default: {
+              // rt field: checking the most significant 2-bits.
+              rt = (imm21 >> kImm19Bits);
+              switch (rt) {
+                case LWUPC: {
+                  // Set sign.
+                  imm19 <<= (kOpcodeBits + kRsBits + 2);
+                  imm19 >>= (kOpcodeBits + kRsBits + 2);
+                  addr = current_pc + (imm19 << 2);
+                  uint32_t* ptr = reinterpret_cast<uint32_t*>(addr);
+                  alu_out = *ptr;
+                  break;
+                }
+                case LWPC: {
+                  // Set sign.
+                  imm19 <<= (kOpcodeBits + kRsBits + 2);
+                  imm19 >>= (kOpcodeBits + kRsBits + 2);
+                  addr = current_pc + (imm19 << 2);
+                  int32_t* ptr = reinterpret_cast<int32_t*>(addr);
+                  alu_out = *ptr;
+                  break;
+                }
+                case ADDIUPC: {
+                  int64_t se_imm19 =
+                      imm19 | ((imm19 & 0x40000) ? 0xFFFFFFFFFFF80000 : 0);
+                  alu_out = current_pc + (se_imm19 << 2);
+                  break;
+                }
+                default:
+                  UNREACHABLE();
+                  break;
+              }
+              break;
+            }
+          }
+          break;
+        }
+      }
+      SetResult(rs_reg, alu_out);
+      break;
+    }
+    case SPECIAL3: {
+      switch (instr_.FunctionFieldRaw()) {
+        case LL_R6: {
+          // LL/SC sequence cannot be simulated properly
+          DCHECK_EQ(kArchVariant, kSw64r3);
+          int64_t base = get_register(instr_.BaseValue());
+          int32_t offset9 = instr_.Imm9Value();
+          set_register(rt_reg, ReadW(base + offset9, instr_.instr()));
+          break;
+        }
+        case LLD_R6: {
+          // LL/SC sequence cannot be simulated properly
+          DCHECK_EQ(kArchVariant, kSw64r3);
+          int64_t base = get_register(instr_.BaseValue());
+          int32_t offset9 = instr_.Imm9Value();
+          set_register(rt_reg, ReadD(base + offset9, instr_.instr()));
+          break;
+        }
+        case SC_R6: {
+          // LL/SC sequence cannot be simulated properly
+          DCHECK_EQ(kArchVariant, kSw64r3);
+          int64_t base = get_register(instr_.BaseValue());
+          int32_t offset9 = instr_.Imm9Value();
+          WriteW(base + offset9, static_cast<int32_t>(rt), instr_.instr());
+          set_register(rt_reg, 1);
+          break;
+        }
+        case SCD_R6: {
+          // LL/SC sequence cannot be simulated properly
+          DCHECK_EQ(kArchVariant, kSw64r3);
+          int64_t base = get_register(instr_.BaseValue());
+          int32_t offset9 = instr_.Imm9Value();
+          WriteD(base + offset9, rt, instr_.instr());
+          set_register(rt_reg, 1);
+          break;
+        }
+        default:
+          UNREACHABLE();
+      }
+      break;
+    }
+
+    case MSA:
+      switch (instr_.MSAMinorOpcodeField()) {
+        case kMsaMinorI8:
+          DecodeTypeMsaI8();
+          break;
+        case kMsaMinorI5:
+          DecodeTypeMsaI5();
+          break;
+        case kMsaMinorI10:
+          DecodeTypeMsaI10();
+          break;
+        case kMsaMinorELM:
+          DecodeTypeMsaELM();
+          break;
+        case kMsaMinorBIT:
+          DecodeTypeMsaBIT();
+          break;
+        case kMsaMinorMI10:
+          DecodeTypeMsaMI10();
+          break;
+        default:
+          UNREACHABLE();
+          break;
+      }
+      break;
+    default:
+      UNREACHABLE();
+  }
+
+  if (execute_branch_delay_instruction) {
+    // Execute branch delay slot
+    // We don't check for end_sim_pc. First it should not be met as the current
+    // pc is valid. Secondly a jump should always execute its branch delay slot.
+    Instruction* branch_delay_instr =
+        reinterpret_cast<Instruction*>(get_pc() + kInstrSize);
+    BranchDelayInstructionDecode(branch_delay_instr);
+  }
+
+  // If needed update pc after the branch delay execution.
+  if (next_pc != bad_ra) {
+    set_pc(next_pc);
+  }
+}
+
+
+// Type 3: instructions using a 26 bytes immediate. (e.g. j, jal).
+void Simulator::DecodeTypeJump() {
+  SimInstruction simInstr = instr_;
+  // Get current pc.
+  int64_t current_pc = get_pc();
+  // Get unchanged bits of pc.
+  int64_t pc_high_bits = current_pc & 0xFFFFFFFFF0000000;
+  // Next pc.
+  int64_t next_pc = pc_high_bits | (simInstr.Imm26Value() << 2);
+
+  // Execute branch delay slot.
+  // We don't check for end_sim_pc. First it should not be met as the current pc
+  // is valid. Secondly a jump should always execute its branch delay slot.
+  Instruction* branch_delay_instr =
+      reinterpret_cast<Instruction*>(current_pc + kInstrSize);
+  BranchDelayInstructionDecode(branch_delay_instr);
+
+  // Update pc and ra if necessary.
+  // Do this after the branch delay execution.
+  if (simInstr.IsLinkingInstruction()) {
+    set_register(31, current_pc + 2 * kInstrSize);
+  }
+  set_pc(next_pc);
+  pc_modified_ = true;
+}
+
+#define SEXT(v) (v)
+
+static float DoubleRToFloat(int64_t fa_d)
+{
+    float r = 0.0f;
+    *reinterpret_cast<uint32_t*>(&r) = ((fa_d & 0xc000000000000000) >> 32) |
+        ((fa_d & 0x07ffffffe0000000) >> 29);
+    return r;
+}
+
+static int64_t FloatToDoubleR(float fv)
+{
+    int64_t r;
+    uint32_t fv_m  = *reinterpret_cast<uint32_t*>(&fv);
+    uint64_t s = ((uint64_t)(fv_m >> 31)) << 63;
+    uint64_t e = fv_m << 1 >> 24;
+    if (e != 0) {
+        if (e == 255)
+            e = 0x7ff;
+        else if (e >= 128)
+            e = (e & 0x7f) | 0x400;
+        else
+            e = (e & 0x7f) | 0x380;
+    }
+    e <<= 52;
+    uint64_t f = ((uint64_t)(fv_m & 0x7fffff)) << 29;
+    s = s | e | f;
+    r = *reinterpret_cast<int64_t*>(&s);
+    return r;
+}
+
+static void BYTE_ZAP(void* x, uint8_t y)
+{
+    char* p = (char*)x;
+    uint8_t mask = 1;
+    for (int i = 0; i<8; ++i) {
+        if (y & mask) {
+            p[i] = 0;
+        }
+        mask <<= 1;
+    }
+}
+
+static int64_t InsLowXxx(uint32_t mask, int64_t ra, int64_t rb)
+{
+    uint32_t index = (rb & 0x7);
+    uint32_t byte_mask = mask << index;
+    uint32_t byte_loc = index * 8;
+    int64_t tmp = ra << (byte_loc & 0x3f);
+    BYTE_ZAP(&tmp, ~((uint8_t)byte_mask));
+
+    return tmp;
+}
+
+static int64_t InsHighXxx(uint32_t mask, int64_t ra, int64_t rb)
+{
+    uint32_t index = (rb & 0x7);
+    uint32_t byte_mask = mask << index;
+    uint32_t byte_loc = 64 - index * 8;
+    int64_t tmp = (uint64_t)ra >> (byte_loc & 0x3f);
+    BYTE_ZAP(&tmp, ~((uint8_t)(byte_mask>>8)));
+
+    return tmp;
+}
+
+static int64_t ExtLowXxx(uint32_t byte_mask, int64_t ra, int64_t rb)
+{
+    uint32_t index = (rb & 0x7);
+    uint32_t byte_loc = index * 8;
+    int64_t tmp = (uint64_t)ra >> (byte_loc & 0x3f);
+    BYTE_ZAP(&tmp, ~((uint8_t)byte_mask));
+
+    return tmp;
+}
+
+static int64_t ExtHighXxx(uint32_t byte_mask, int64_t ra, int64_t rb)
+{
+    uint32_t index = (rb & 0x7);
+    uint32_t byte_loc = 64 - index * 8;
+    int64_t tmp = ra << (byte_loc & 0x3f);
+    BYTE_ZAP(&tmp, ~((uint8_t)byte_mask));
+
+    return tmp;
+}
+
+static int64_t MaskLowXxx(uint32_t mask, int64_t ra, int64_t rb)
+{
+    int index = (rb & 0x7);
+    int byte_mask = mask << index;
+    int64_t tmp = ra;
+    BYTE_ZAP(&tmp, (uint8_t)byte_mask);
+
+    return tmp;
+}
+
+static int64_t MaskHighXxx(uint32_t mask, int64_t ra, int64_t rb)
+{
+    uint32_t index = (rb & 0x7);
+    uint32_t byte_mask = mask << index;
+    int64_t tmp = ra;
+    BYTE_ZAP(&tmp, (uint8_t)(byte_mask >> 8));
+
+    return tmp;
+}
+
+void Simulator::DecodeTypeSyscall()
+{
+    Instruction* instr = instr_.instr();
+    if (instr->OpcodeFieldValue() == op_sys_call){
+        Format(instr, "sys_call '0x(25-0)");
+    }
+}
+
+void Simulator::DecodeTypeTransfer()
+{
+    Instruction* instr = instr_.instr();
+    int Ra = instr->SwRaValue();
+    int imm21 = instr->SwImmOrDispFieldValue(20, 0);
+    int64_t current_pc = get_pc() + kInstrSize;
+    int64_t next_pc = current_pc + kInstrSize * imm21;
+    int64_t fa;
+    switch(instr->OpcodeFieldValue()){
+        case op_br:
+            //Format(instr,"br  'ra, 'tr_disp(20-0)");//ld 20150320
+            set_register(Ra, current_pc);
+            set_pc(next_pc);
+            break;
+        case op_bsr:
+            //Format(instr,"bsr 'ra, 'tr_disp(20-0)");
+            set_register(Ra, current_pc);
+            set_pc(next_pc);
+            break;
+        case op_beq:
+            //Format(instr,"beq 'ra, 'tr_disp(20-0)");
+            if (get_register(Ra) == 0)
+                set_pc(next_pc);
+            break;
+        case op_bne:
+            //Format(instr,"bne 'ra, 'tr_disp(20-0)");
+            if (get_register(Ra) != 0)
+                set_pc(next_pc);
+            break;
+        case op_blt:
+            //Format(instr,"blt 'ra, 'tr_disp(20-0)");
+            if (get_register(Ra) < 0)
+                set_pc(next_pc);
+            break;
+        case op_ble:
+            //Format(instr,"ble 'ra, 'tr_disp(20-0)");
+            if (get_register(Ra) <= 0)
+                set_pc(next_pc);
+            break;
+        case op_bgt:
+            //Format(instr,"bgt 'ra, 'tr_disp(20-0)");
+            if (get_register(Ra) > 0)
+                set_pc(next_pc);
+            break;
+        case op_bge:
+            //Format(instr,"bge 'ra, 'tr_disp(20-0)");
+            if (get_register(Ra) >= 0)
+                set_pc(next_pc);
+            break;
+        case op_blbc:
+            //Format(instr,"blbc 'ra, 'tr_disp(20-0)");
+            if (!(get_register(Ra) & 0x1))
+                set_pc(next_pc);
+            break;
+        case op_blbs:
+            //Format(instr,"blbs 'ra, 'tr_disp(20-0)");
+            if (get_register(Ra) & 0x1)
+                set_pc(next_pc);
+            break;
+        case op_fbeq:
+            //Format(instr,"fbeq 'fa, 'tr_disp(20-0)");
+            if ((get_fpu_register(Ra) & 0x7fffffffffffffff) == 0)
+                set_pc(next_pc);
+            break;
+        case op_fbne:
+            //Format(instr,"fbne 'fa, 'tr_disp(20-0)");
+            if ((get_fpu_register(Ra) & 0x7fffffffffffffff) != 0)
+                set_pc(next_pc);
+            break;
+        case op_fblt:
+            //Format(instr,"fblt 'fa, 'tr_disp(20-0)");
+            fa = get_fpu_register(Ra);
+            if (*((double*)&fa) < 0)
+                set_pc(next_pc);
+            break;
+        case op_fble:
+            //Format(instr,"fble 'fa, 'tr_disp(20-0)");
+            fa = get_fpu_register(Ra);
+            if (*((double*)&fa) <= 0)
+                set_pc(next_pc);
+            break;
+        case op_fbgt:
+            //Format(instr,"fbgt 'fa, 'tr_disp(20-0)");
+            fa = get_fpu_register(Ra);
+            if (*((double*)&fa) > 0)
+                set_pc(next_pc);
+            break;
+        case op_fbge:
+            //Format(instr,"fbge 'fa, 'tr_disp(20-0)");
+            fa = get_fpu_register(Ra);
+            if (*((double*)&fa) >= 0)
+                set_pc(next_pc);
+            break;
+        default:
+            printf("a 0x%x \n", instr->OpcodeFieldRaw());
+            UNREACHABLE();
+    }
+}
+
+#define OP(x)           (((x) & 0x3F) << 26)
+void Simulator::DecodeTypeStorage()
+{
+    int64_t vaddr;
+    int opcode_func_raw = 0;
+    Instruction* instr = instr_.instr();
+    int Ra = instr->SwRaValue();
+    int Rb = instr->SwRbValue();
+    int64_t imm16 = instr->SwImmOrDispFieldValue(15, 0);
+    int64_t imm12 = instr->SwImmOrDispFieldValue(11, 0);
+
+    //
+    if ( instr->OpcodeFieldValue() == OP(0x06) ||
+         instr->OpcodeFieldValue() == OP(0x08)  ) {
+        vaddr = get_register(Rb) + imm12;
+    } else {
+        vaddr = get_register(Rb) + imm16;
+    }
+    switch (instr->OpcodeFieldValue()) {
+        case op_call:
+            //Format(instr, "call 'ra, ('rb)");
+            set_register(Ra, get_pc() + kInstrSize);
+            set_pc(get_register(Rb));
+            break;
+        case op_ret:
+            //Format(instr, "ret 'ra, ('rb)");
+            set_register(Ra, get_pc() + kInstrSize);
+            set_pc(get_register(Rb));
+            break;
+        case op_jmp:
+            //Format(instr, "jmp 'ra, ('rb)");
+            set_register(Ra, get_pc() + kInstrSize);
+            set_pc(get_register(Rb));
+            break;
+
+        case op_ldbu:
+            //Format(instr, "ldbu 'ra, 'disp(15-0)('rb)");
+            set_register(Ra, *(uint8_t*)(vaddr));
+            break;
+        case op_ldhu:
+            //Format(instr, "ldhu 'ra, 'disp(15-0)('rb)");
+            if (vaddr & 0x1)
+                PrintF("unaligned ldhu");
+            set_register(Ra, *(uint16_t*)(vaddr));
+            break;
+        case op_ldw:
+            //Format(instr, "ldw 'ra, 'disp(15-0)('rb)");
+            if (vaddr & 0x3)
+                PrintF("unaligned ldw");
+            set_register(Ra, *(int32_t*)(vaddr));
+            break;
+        case op_ldl:
+            //Format(instr, "ldl 'ra, 'disp(15-0)('rb)");
+            if (vaddr & 0x7)
+                PrintF("unaligned ldl");
+            set_register(Ra, *(int64_t*)vaddr);
+            break;
+        case op_ldl_u:
+            //Format(instr, "ldl_u 'ra, 'disp(15-0)('rb)");
+            set_register(Ra, *(int64_t*)(vaddr & ~0x07));
+            break;
+        case op_stb:
+            //Format(instr, "stb 'ra, 'disp(15-0)('rb)");
+            *(uint8_t*)(get_register(Rb) + imm16) = get_register(Ra) & 0xff;
+            break;
+        case op_sth:
+            //Format(instr, "sth 'ra, 'disp(15-0)('rb)");
+            if (vaddr & 0x01)
+                PrintF("unaligned sth");
+            *(uint16_t*)vaddr = get_register(Ra) & 0xffff;
+            break;
+        case op_stw:
+            //Format(instr, "stw 'ra, 'disp(15-0)('rb)");
+            if (vaddr & 0x03)
+                PrintF("unaligned stw");
+            *(uint32_t*)vaddr = get_register(Ra) & 0xffffffff;
+            break;
+        case op_stl:
+            //Format(instr, "stl 'ra, 'disp(15-0)('rb)");
+            if (vaddr & 0x07)
+                PrintF("unaligned stl");
+            *(int64_t*)vaddr = get_register(Ra);
+            break;
+        case op_stl_u:
+            //Format(instr, "stl_u 'ra, 'disp(15-0)('rb)");
+            *(int64_t*)(vaddr & ~0x07) = get_register(Ra);
+            break;
+        case op_ldi:
+            //Format(instr, "ldi 'ra, 'disp(15-0)('rb)");
+            set_register(Ra, get_register(Rb) + imm16);
+            break;
+        case op_ldih:
+            //Format(instr, "ldih 'ra, 'disp(15-0)('rb)");
+            set_register(Ra, get_register(Rb) + (imm16 << 16));
+            break;
+
+        case op_flds:
+            //Format(instr, "flds 'fa, 'disp(15-0)('rb)");
+            if (vaddr & 0x03)
+                PrintF("unaligned flds");
+            set_fpu_register(Ra, FloatToDoubleR(*(float*)vaddr));
+            break;
+        case op_fldd:
+            //Format(instr, "fldd 'fa, 'disp(15-0)('rb)");
+            if (vaddr & 0x07)
+                PrintF("unaligned fldd");
+            set_fpu_register(Ra, *(int64_t*)vaddr);
+            break;
+        case op_fsts:
+            //Format(instr, "fsts 'fa, 'disp(15-0)('rb)");
+            if (vaddr & 0x03)
+                PrintF("unaligned fsts");
+            *(float*)vaddr = DoubleRToFloat(get_fpu_register(Ra));
+            break;
+        case op_fstd:
+            //Format(instr, "fstd 'fa, 'disp(15-0)('rb)");
+            if (vaddr & 0x07)
+                PrintF("unaligned fstd");
+            *(int64_t*)vaddr = get_fpu_register(Ra);
+            break;
+
+        case op_ldwe:
+        case op_ldse:
+        case op_ldde:
+        case op_vlds:
+        case op_vldd:
+        case op_vsts:
+        case op_vstd:
+            UNIMPLEMENTED_SW64();
+            break;
+
+        case OP(0x08):
+        case OP(0x06): //
+            opcode_func_raw = instr->SwFunctionFieldRaw(15, 0) | instr->OpcodeFieldRaw();
+            switch (opcode_func_raw) {
+            case op_lldw:
+                //Format(instr, "lldw  'ra, 'disp(11-0)('rb)");    
+                if (vaddr & 0x3)
+                    PrintF("unaligned ldw");
+                set_register(Ra, *(int32_t*)(vaddr));
+                lock_valid = 1;
+                lock_register_padd = vaddr;
+                lock_register_flag = 1;
+                break;
+            case op_lldl:
+                //Format(instr, "lldl  'ra, 'disp(11-0)('rb)");
+                if (vaddr & 0x7)
+                    PrintF("unaligned ldl");
+                set_register(Ra, *(int64_t*)vaddr);
+                lock_valid = 1;
+                lock_register_padd = vaddr;
+                lock_register_flag = 1;
+                break;
+            case op_ldw_inc:
+                Format(instr, "ldw_inc  'ra, 'disp(11-0)('rb)");
+                break;
+            case op_ldl_inc:
+                Format(instr, "ldl_inc  'ra, 'disp(11-0)('rb)");
+                break;
+            case op_ldw_dec:
+                Format(instr, "ldw_dec  'ra, 'disp(11-0)('rb)");
+                break;
+            case op_ldl_dec:
+                Format(instr, "ldl_dec  'ra, 'disp(11-0)('rb)");
+                break;
+            case op_ldw_set:
+                Format(instr, "ldw_set  'ra, 'disp(11-0)('rb)");
+                break;
+            case op_ldl_set:
+                Format(instr, "ldl_set  'ra, 'disp(11-0)('rb)");
+                break;
+            case op_lstw:
+                //Format(instr, "lstw  'ra, 'disp(11-0)('rb)");
+                if(lock_flag == 1 && lock_valid == 1  &&
+                    vaddr==lock_register_padd && lock_register_flag == 1) {
+                    if (vaddr & 0x03)
+                        PrintF("unaligned stw");
+                    *(uint32_t*)vaddr = get_register(Ra) & 0xffffffff;
+                    lock_success = 1;
+                    lock_valid = 0;
+                    lock_flag = 0;
+                } else {
+                    lock_success = 0;
+                    lock_valid = 0;
+                    lock_flag = 0;
+                }
+                break;
+            case op_lstl:
+                //Format(instr, "lstl  'ra, 'disp(11-0)('rb)");
+                if(lock_flag == 1 && lock_valid == 1  &&
+                    vaddr==lock_register_padd && lock_register_flag == 1) {
+                    PrintF("do lstl\n");
+                    if (vaddr & 0x07)
+                        PrintF("unaligned stl");
+                    *(int64_t*)vaddr = get_register(Ra);
+                    lock_success = 1;
+                    lock_valid = 0;
+                    lock_flag = 0;
+                } else {
+                    lock_success = 0;
+                    lock_valid = 0;
+                    lock_flag = 0;
+                }
+                break;
+            case op_memb:
+                break;
+            case op_rtc:
+                Format(instr, "rtc 'ra, 'rb");
+                break;
+            case op_rcid:
+                Format(instr, "rcid 'ra");
+                break;
+            case op_halt:
+                Format(instr, "halt");
+                break;
+            case op_rd_f:
+                set_register(Ra,lock_success);
+                lock_success = 0;
+                break;
+            case op_wr_f:
+                lock_flag = get_register(Ra) & 0x0000000000000001;
+                break;
+            default:
+                UNIMPLEMENTED_SW64();
+            }
+            break;
+
+        default:
+            printf("a 0x%x \n", instr->OpcodeFieldRaw());
+            UNREACHABLE();
+    }
+}
+
+void Simulator::DecodeTypeSimpleCalc()
+{
+    // TODO: how to process compute overflow exception!
+    Instruction* instr = instr_.instr();
+    int simple_calculation_op = instr->SwFunctionFieldRaw(12, 5) | instr->OpcodeFieldValue();
+    int op = simple_calculation_op >> 26;
+    if (op == 0x10) {
+        int ra = instr->SwRaValue();
+        int rb = instr->SwRbValue();
+        int rc = instr->SwRcValue(4, 0);
+        switch (simple_calculation_op) {
+        case op_addw:
+            // Format(instr, "addw 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = (int64_t)(int)(registers_[ra] + registers_[rb]);
+            break;
+        case op_subw:
+            // Format(instr, "subw 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = (int64_t)(int)(registers_[ra] - registers_[rb]);
+            break;
+        case op_s4addw:
+            // Format(instr, "s4addw 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = (int64_t)(int)(registers_[ra]*4 + registers_[rb]);
+            break;
+        case op_s4subw:
+            // Format(instr, "s4subw 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = (int64_t)(int)(registers_[ra]*4 - registers_[rb]);
+            break;
+        case op_s8addw:
+            // Format(instr, "s8addw 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = (int64_t)(int)(registers_[ra]*8 + registers_[rb]);
+            break;
+        case op_s8subw:
+            // Format(instr, "s8subw 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = (int64_t)(int)(registers_[ra]*8 - registers_[rb]);
+            break;
+        case op_addl:
+            // Format(instr, "addl 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = registers_[ra] + registers_[rb];
+            break;
+        case op_subl:
+            // Format(instr, "subl 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = registers_[ra] - registers_[rb];
+            break;
+        case op_s4addl:
+            // Format(instr, "s4addl 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = registers_[ra]*4 + registers_[rb];
+            break;
+        case op_s4subl:
+            // Format(instr, "s4subl 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = registers_[ra]*4 - registers_[rb];
+            break;
+        case op_s8addl:
+            // Format(instr, "s8addl 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = registers_[ra]*8 + registers_[rb];
+            break;
+        case op_s8subl:
+            // Format(instr, "s8subl 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = registers_[ra]*8 - registers_[rb];
+            break;
+        case op_mulw:
+            // Format(instr, "mulw 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = (int64_t)(int)(registers_[ra] * registers_[rb]);
+            break;
+        case op_mull:
+            // Format(instr, "mull 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = registers_[ra] * registers_[rb];
+            break;
+        case op_umulh:
+            Format(instr, "umulh 'ra, 'rb, 'rc(4-0)");
+            // TODO:how to store high 64 bit of 128 bit result!
+            // registers_[rc] = (uint64_t)registers_[ra] * (uint64_t)registers_[rb];
+            break;
+        case op_cmpeq:
+            // Format(instr, "cmpeq 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = registers_[ra] == registers_[rb] ? 1 : 0;
+            break;
+        case op_cmplt:
+            // Format(instr, "cmplt 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = registers_[ra] < registers_[rb] ? 1 : 0;
+            break;
+        case op_cmple:
+            // Format(instr, "cmple 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = registers_[ra] <= registers_[rb] ? 1 : 0;
+            break;
+        case op_cmpult:
+            // Format(instr, "cmpult 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = (uint64_t)registers_[ra] < (uint64_t)registers_[rb] ? 1 : 0;
+            break;
+        case op_cmpule:
+            // Format(instr, "cmpule 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = (uint64_t)registers_[ra] <= (uint64_t)registers_[rb] ? 1 : 0;
+            break;
+        case op_and:
+            // Format(instr, "and 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = registers_[ra] & registers_[rb];
+            break;
+        case op_bic:
+            // Format(instr, "bic 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = registers_[ra] & (~registers_[rb]);
+            break;
+        case op_bis: //case op_or:
+            // Format(instr, "or 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = registers_[ra] | registers_[rb];
+            break;
+        case op_ornot:
+            // Format(instr, "ornot 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = registers_[ra] | (~registers_[rb]);
+            break;
+        case op_xor:
+            // Format(instr, "xor 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = registers_[ra] ^ registers_[rb];
+            break;
+        case op_eqv:
+            // Format(instr, "eqv 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = registers_[ra] ^ (~registers_[rb]);
+            break;
+        case op_inslb:
+            // Format(instr, "inslb 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = InsLowXxx(0x1, registers_[ra], registers_[rb]);
+            break;
+        case op_inslh:
+            // Format(instr, "inslh 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = InsLowXxx(0x3, registers_[ra], registers_[rb]);
+            break;
+        case op_inslw:
+            // Format(instr, "inslw 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = InsLowXxx(0xf, registers_[ra], registers_[rb]);
+            break;
+        case op_insll:
+            // Format(instr, "insll 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = InsLowXxx(0xff, registers_[ra], registers_[rb]);
+            break;
+        case op_inshb:
+            // Format(instr, "inshb 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = InsHighXxx(0x1, registers_[ra], registers_[rb]);
+            break;
+        case op_inshh:
+            // Format(instr, "inshh 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = InsHighXxx(0x3, registers_[ra], registers_[rb]);
+            break;
+        case op_inshw:
+            // Format(instr, "inshw 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = InsHighXxx(0xf, registers_[ra], registers_[rb]);
+            break;
+        case op_inshl:
+            // Format(instr, "inshl 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = InsHighXxx(0xff, registers_[ra], registers_[rb]);
+            break;
+        case op_slll:
+            // Format(instr, "slll 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = registers_[ra] << (registers_[rb] & 0x3f);
+            break;
+        case op_srll:
+            // Format(instr, "srll 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = (uint64_t)registers_[ra] >> (registers_[rb] & 0x3f);
+            break;
+        case op_sral:
+            // Format(instr, "sral 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = registers_[ra] >> (registers_[rb] & 0x3f);
+            break;
+        case op_extlb:
+            // Format(instr, "extlb 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = ExtLowXxx(0x1, registers_[ra], registers_[rb]);
+            break;
+        case op_extlh:
+            // Format(instr, "extlh 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = ExtLowXxx(0x3, registers_[ra], registers_[rb]);
+            break;
+        case op_extlw:
+            // Format(instr, "extlw 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = ExtLowXxx(0xf, registers_[ra], registers_[rb]);
+            break;
+        case op_extll:
+            // Format(instr, "extll 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = ExtLowXxx(0xff, registers_[ra], registers_[rb]);
+            break;
+        case op_exthb:
+            // Format(instr, "exthb 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = ExtHighXxx(0x1, registers_[ra], registers_[rb]);
+            break;
+        case op_exthh:
+            // Format(instr, "exthh 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = ExtHighXxx(0x3, registers_[ra], registers_[rb]);
+            break;
+        case op_exthw:
+            // Format(instr, "exthw 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = ExtHighXxx(0xf, registers_[ra], registers_[rb]);
+            break;
+        case op_exthl:
+            // Format(instr, "exthl 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = ExtHighXxx(0xff, registers_[ra], registers_[rb]);
+            break;
+        case op_ctpop:
+            // Format(instr, "ctpop 'rb, 'rc(4-0)");
+            {
+                int c = 0;
+                uint64_t v = registers_[rb];
+                for (int i = 0; i<64; ++i)
+                {
+                    if (v&1) ++c;
+                    v >>= 1;
+                }
+                registers_[rc] = c;
+            }
+            break;
+        case op_ctlz:
+            // Format(instr, "ctlz 'rb, 'rc(4-0)");
+            {
+                int c = 0;
+                uint64_t v = registers_[rb];
+                for (int i = 0; i<64; ++i)
+                {
+                    if (!(v & 0x8000000000000000)) ++c;
+                    else break;
+                    v <<= 1;
+                }
+                registers_[rc] = c;
+            }
+            break;
+        case op_cttz:
+            // Format(instr, "cttz 'rb, 'rc(4-0)");
+            {
+                int c = 0;
+                uint64_t v = registers_[rb];
+                for (int i = 0; i<64; ++i)
+                {
+                    if (!(v&1)) ++c;
+                    else break;
+                    v >>= 1;
+                }
+                registers_[rc] = c;
+            }
+            break;
+        case op_masklb:
+            // Format(instr, "masklb 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = MaskLowXxx(0x1, registers_[ra], registers_[rb]);
+            break;
+        case op_masklh:
+            // Format(instr, "masklh 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = MaskLowXxx(0x3, registers_[ra], registers_[rb]);
+            break;
+        case op_masklw:
+            // Format(instr, "masklw 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = MaskLowXxx(0xf, registers_[ra], registers_[rb]);
+            break;
+        case op_maskll:
+            // Format(instr, "maskll 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = MaskLowXxx(0xff, registers_[ra], registers_[rb]);
+            break;
+        case op_maskhb:
+            // Format(instr, "maskhb 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = MaskHighXxx(0x1, registers_[ra], registers_[rb]);
+            break;
+        case op_maskhh:
+            // Format(instr, "maskhh 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = MaskHighXxx(0x3, registers_[ra], registers_[rb]);
+            break;
+        case op_maskhw:
+            // Format(instr, "maskhw 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = MaskHighXxx(0xf, registers_[ra], registers_[rb]);
+            break;
+        case op_maskhl:
+            // Format(instr, "maskhl 'ra, 'rb, 'rc(4-0)");
+            registers_[rc] = MaskHighXxx(0xff, registers_[ra], registers_[rb]);
+            break;
+        case op_zap:
+            // Format(instr, "zap 'ra, 'rb, 'rc(4-0)");
+            {
+                long tmp = registers_[ra];
+                BYTE_ZAP(&tmp, (uint8_t)registers_[rb]);
+                registers_[rc] = tmp;
+            }
+            break;
+        case op_zapnot:
+            // Format(instr, "zapnot 'ra, 'rb, 'rc(4-0)");
+            {
+                long tmp = registers_[ra];
+                BYTE_ZAP(&tmp, ~(uint8_t)registers_[rb]);
+                registers_[rc] = tmp;
+            }
+            break;
+        case op_sextb:
+            // Format(instr, "sextb 'rb, 'rc(4-0)");
+            registers_[rc] = SEXT((int8_t)registers_[rb]);
+            break;
+        case op_sexth:
+            // Format(instr, "sexth 'rb, 'rc(4-0)");
+            registers_[rc] = SEXT((int16_t)registers_[rb]);
+            break;
+        case op_cmpgeb:
+            // Format(instr, "cmpgeb 'ra, 'rb, 'rc(4-0)");
+            {
+                uint8_t* p = (uint8_t*)&registers_[ra];
+                uint8_t* q = (uint8_t*)&registers_[rb];
+                uint8_t v = 0;
+                for (int i = 0; i<=7; ++i)
+                {
+                    if (*p >= *q) {
+                        v |= (1<<i);
+                    }
+                    ++p;
+                    ++q;
+                }
+                registers_[rc] = v;
+            }
+            break;
+        case op_fimovs:
+            {
+                // Format(instr, "fimovs 'fa, 'rc(4-0)");
+                int64_t v = 0;
+                int fa = instr->SwFaValue();
+                *(float*)&v = DoubleRToFloat(get_fpu_register(fa));
+                v = v << 32 >> 32;
+                set_register(rc, v);
+            }
+            break;
+        case op_fimovd:
+            {
+                // Format(instr, "fimovd 'fa, 'rc(4-0)");
+                int fa = instr->SwFaValue();
+                set_register(rc, get_fpu_register(fa));
+            }
+            break;
+        default:
+            break;
+        }
+    }
+    else if (op == 0x12) { // has imm8
+        int ra = instr->SwRaValue();
+        int rc = instr->SwRcValue(4, 0);
+        int imm8 = instr->SwImmOrDispFieldRaw(20, 13) >> 13;
+        switch (simple_calculation_op) {
+        case op_addw_l:
+            // Format(instr, "addw 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = SEXT((int)registers_[ra] + imm8);
+            break;
+        case op_subw_l:
+            // Format(instr, "subw 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = SEXT((int)registers_[ra] - imm8);
+            break;
+        case op_s4addw_l:
+            // Format(instr, "s4addw 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = SEXT((int)(registers_[ra]*4 + imm8));
+            break;
+        case op_s4subw_l:
+            // Format(instr, "s4subw 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = SEXT((int)(registers_[ra]*4 - imm8));
+            break;
+        case op_s8addw_l:
+            // Format(instr, "s8addw 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = SEXT((int)(registers_[ra]*8 + imm8));
+            break;
+        case op_s8subw_l:
+            // Format(instr, "s8subw 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = SEXT((int)(registers_[ra]*8 - imm8));
+            break;
+        case op_addl_l:
+            // Format(instr, "addl 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = registers_[ra] + imm8;
+            break;
+        case op_subl_l:
+            // Format(instr, "subl 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = registers_[ra] - imm8;
+            break;
+        case op_s4addl_l:
+            // Format(instr, "s4addl 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = registers_[ra]*4 + imm8;
+            break;
+        case op_s4subl_l:
+            // Format(instr, "s4subl 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = registers_[ra]*4 - imm8;
+            break;
+        case op_s8addl_l:
+            // Format(instr, "s8addl 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = registers_[ra]*8 + imm8;
+            break;
+        case op_s8subl_l:
+            // Format(instr, "s8subl 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = registers_[ra]*8 - imm8;
+            break;
+        case op_mulw_l:
+            // Format(instr, "mulw 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = SEXT((int)registers_[ra] * imm8);
+            break;
+        case op_mull_l:
+            // Format(instr, "mull 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = registers_[ra] * imm8;
+            break;
+        case op_umulh_l:
+            Format(instr, "umulh 'ra, 'imm(20-13), 'rc(4-0)");
+            break;
+        case op_cmpeq_l:
+            // Format(instr, "cmpeq 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = registers_[ra] == imm8 ? 1 : 0;
+            break;
+        case op_cmplt_l:
+            // Format(instr, "cmplt 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = registers_[ra] < imm8 ? 1 : 0;
+            break;
+        case op_cmple_l:
+            // Format(instr, "cmple 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = registers_[ra] <= imm8 ? 1 : 0;
+            break;
+        case op_cmpult_l:
+            // Format(instr, "cmpult 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = (uint64_t)registers_[ra] < (uint8_t)imm8 ? 1 : 0;
+            break;
+        case op_cmpule_l:
+            // Format(instr, "cmpule 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = (uint64_t)registers_[ra] <= (uint8_t)imm8 ? 1 : 0;
+            break;
+        case op_and_l:
+            // Format(instr, "and 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = registers_[ra] & (uint8_t)imm8;
+            break;
+        case op_bic_l:
+            // Format(instr, "bic 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = registers_[ra] & (uint8_t)(~imm8);
+            break;
+        case op_bis_l: //case op_or_l:
+            // Format(instr, "or 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = registers_[ra] | (uint8_t)imm8;
+            break;
+        case op_ornot_l:
+            // Format(instr, "ornot 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = registers_[ra] | (uint8_t)(~imm8);
+            break;
+        case op_xor_l:
+            // Format(instr, "xor 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = registers_[ra] ^ (uint8_t)imm8;
+            break;
+        case op_eqv_l:
+            // Format(instr, "eqv 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = registers_[ra] ^ (uint8_t)(~imm8);
+            break;
+        case op_inslb_l:
+            // Format(instr, "inslb 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = InsLowXxx(0x1, registers_[ra], imm8);
+            break;
+        case op_inslh_l:
+            // Format(instr, "inslh 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = InsLowXxx(0x3, registers_[ra], imm8);
+            break;
+        case op_inslw_l:
+            // Format(instr, "inslw 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = InsLowXxx(0xf, registers_[ra], imm8);
+            break;
+        case op_insll_l:
+            // Format(instr, "insll 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = InsLowXxx(0xff, registers_[ra], imm8);
+            break;
+        case op_inshb_l:
+            // Format(instr, "inshb 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = InsHighXxx(0x1, registers_[ra], imm8);
+            break;
+        case op_inshh_l:
+            // Format(instr, "inshh 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = InsHighXxx(0x3, registers_[ra], imm8);
+            break;
+        case op_inshw_l:
+            // Format(instr, "inshw 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = InsHighXxx(0xf, registers_[ra], imm8);
+            break;
+        case op_inshl_l:
+            // Format(instr, "inshl 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = InsHighXxx(0xff, registers_[ra], imm8);
+            break;
+        case op_slll_l:
+            // Format(instr, "slll 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = registers_[ra] << (imm8 & 0x3f);
+            break;
+        case op_srll_l:
+            // Format(instr, "srll 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = (uint64_t)registers_[ra] >> (imm8 & 0x3f);
+            break;
+        case op_sral_l:
+            // Format(instr, "sral 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = registers_[ra] >> (imm8 & 0x3f);
+            break;
+        case op_extlb_l:
+            // Format(instr, "extlb 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = ExtLowXxx(0x1, registers_[ra], imm8);
+            break;
+        case op_extlh_l:
+            // Format(instr, "extlh 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = ExtLowXxx(0x3, registers_[ra], imm8);
+            break;
+        case op_extlw_l:
+            // Format(instr, "extlw 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = ExtLowXxx(0xf, registers_[ra], imm8);
+            break;
+        case op_extll_l:
+            // Format(instr, "extll 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = ExtLowXxx(0xff, registers_[ra], imm8);
+            break;
+        case op_exthb_l:
+            // Format(instr, "exthb 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = ExtHighXxx(0x1, registers_[ra], imm8);
+            break;
+        case op_exthh_l:
+            // Format(instr, "exthh 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = ExtHighXxx(0x3, registers_[ra], imm8);
+            break;
+        case op_exthw_l:
+            // Format(instr, "exthw 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = ExtHighXxx(0xf, registers_[ra], imm8);
+            break;
+        case op_exthl_l:
+            // Format(instr, "exthl 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = ExtHighXxx(0xff, registers_[ra], imm8);
+            break;
+        case op_masklb_l:
+            // Format(instr, "masklb 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = MaskLowXxx(0x1, registers_[ra], imm8);
+            break;
+        case op_masklh_l:
+            // Format(instr, "masklh 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = MaskLowXxx(0x3, registers_[ra], imm8);
+            break;
+        case op_masklw_l:
+            // Format(instr, "masklw 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = MaskLowXxx(0xf, registers_[ra], imm8);
+            break;
+        case op_maskll_l:
+            // Format(instr, "maskll 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = MaskLowXxx(0xff, registers_[ra], imm8);
+            break;
+        case op_maskhb_l:
+            // Format(instr, "maskhb 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = MaskHighXxx(0x1, registers_[ra], imm8);
+            break;
+        case op_maskhh_l:
+            // Format(instr, "maskhh 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = MaskHighXxx(0x3, registers_[ra], imm8);
+            break;
+        case op_maskhw_l:
+            // Format(instr, "maskhw 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = MaskHighXxx(0xf, registers_[ra], imm8);
+            break;
+        case op_maskhl_l:
+            // Format(instr, "maskhl 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = MaskHighXxx(0xff, registers_[ra], imm8);
+            break;
+        case op_zap_l:
+            // Format(instr, "zap 'ra, 'imm(20-13), 'rc(4-0)");
+            {
+                long tmp = registers_[ra];
+                BYTE_ZAP(&tmp, (uint8_t)imm8);
+                registers_[rc] = tmp;
+            }
+            break;
+        case op_zapnot_l:
+            // Format(instr, "zapnot 'ra, 'imm(20-13), 'rc(4-0)");
+            {
+                long tmp = registers_[ra];
+                BYTE_ZAP(&tmp, ~(uint8_t)imm8);
+                registers_[rc] = tmp;
+            }
+            break;
+        case op_sextb_l:
+            // Format(instr, "sextb 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = SEXT((int8_t)imm8);
+            break;
+        case op_sexth_l:
+            // Format(instr, "sexth 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = SEXT((int16_t)imm8);
+            break;
+        case op_cmpgeb_l:
+            // Format(instr, "cmpgeb 'ra, 'imm(20-13), 'rc(4-0)");
+            registers_[rc] = registers_[ra] == (uint8_t)imm8;
+            break;
+        default:
+            break;
+        }
+    }
+    else if (op == 0x18) { //?        
+        int fa = instr->SwFaValue();
+        int fb = instr->SwFbValue();
+        int fc = instr->SwFcValue(4, 0);
+        //TODO: need support round mode set in fpcr register
+        switch (simple_calculation_op) {
+        case op_fadds:
+            // Format(instr, "fadds 'fa, 'fb, 'fc(4-0)");
+            // f10  3.3000001907348633	(raw 0x400a666680000000)
+            // f11  1.1000000238418579	(raw 0x3ff19999a0000000)
+            //  (Vaddr)<31> || MAP_S((Vaddr)<30:23>) || (Vaddr)<22:0> || 0<28:0>
+            //   1                8->11                 23               29
+            {
+                float a = DoubleRToFloat(get_fpu_register(fa));
+                float b = DoubleRToFloat(get_fpu_register(fb));
+                set_fpu_register(fc, FloatToDoubleR(a + b));
+            }
+            break;
+        case op_faddd:
+            // Format(instr, "faddd 'fa, 'fb, 'fc(4-0)");
+            // f10 2.2000000000000002	(raw 0x400199999999999a)
+            // f11 1.1000000000000001	(raw 0x3ff199999999999a)
+            set_fpu_register_double(fc, get_fpu_register_double(fa) + get_fpu_register_double(fb));
+            break;
+        case op_fsubs:
+            // Format(instr, "fsubs 'fa, 'fb, 'fc(4-0)");
+            {
+                float a = DoubleRToFloat(get_fpu_register(fa));
+                float b = DoubleRToFloat(get_fpu_register(fb));
+                set_fpu_register(fc, FloatToDoubleR(a - b));
+            }
+            break;
+        case op_fsubd:
+            // Format(instr, "fsubd 'fa, 'fb, 'fc(4-0)");
+            set_fpu_register_double(fc, get_fpu_register_double(fa) - get_fpu_register_double(fb));
+            break;
+        case op_fmuls:
+            // Format(instr, "fmuls 'fa, 'fb, 'fc(4-0)");
+            {
+                float a = DoubleRToFloat(get_fpu_register(fa));
+                float b = DoubleRToFloat(get_fpu_register(fb));
+                set_fpu_register(fc, FloatToDoubleR(a * b));
+            }
+            break;
+        case op_fmuld:
+            // Format(instr, "fmuld 'fa, 'fb, 'fc(4-0)");
+            set_fpu_register_double(fc, get_fpu_register_double(fa) * get_fpu_register_double(fb));
+            break;
+            break;
+        case op_fdivs:
+            // Format(instr, "fdivs 'fa, 'fb, 'fc(4-0)");
+            {
+                float a = DoubleRToFloat(get_fpu_register(fa));
+                float b = DoubleRToFloat(get_fpu_register(fb));
+                set_fpu_register(fc, FloatToDoubleR(a / b));
+            }
+            break;
+        case op_fdivd:
+            // Format(instr, "fdivd 'fa, 'fb, 'fc(4-0)");
+            set_fpu_register_double(fc, get_fpu_register_double(fa) / get_fpu_register_double(fb));
+            break;
+        case op_fsqrts:
+            // Format(instr, "fsqrts 'fb, 'fc(4-0)");
+            {
+                float b = DoubleRToFloat(get_fpu_register(fb));
+                set_fpu_register(fc, FloatToDoubleR(sqrt(b)));
+            }
+            break;
+        case op_fsqrtd:
+            // Format(instr, "fsqrtd 'fb, 'fc(4-0)");
+            set_fpu_register_double(fc, sqrt(get_fpu_register_double(fb)));
+            break;
+        case op_fcmpeq:
+            // Format(instr, "fcmpeq 'fa, 'fb, 'fc(4-0)");
+            set_fpu_register(fc, get_fpu_register_double(fa) == get_fpu_register_double(fb) ? 0x4000000000000000LL : 0LL);
+            break;
+        case op_fcmple:
+            // Format(instr, "fcmple 'fa, 'fb, 'fc(4-0)");
+            set_fpu_register(fc, get_fpu_register_double(fa) <= get_fpu_register_double(fb) ? 0x4000000000000000LL : 0LL);
+            break;
+        case op_fcmplt:
+            // Format(instr, "fcmplt 'fa, 'fb, 'fc(4-0)");
+            set_fpu_register(fc, get_fpu_register_double(fa) < get_fpu_register_double(fb) ? 0x4000000000000000LL : 0LL);
+            break;
+        case op_fcmpun:
+            //Format(instr, "fcmpun 'fa, 'fb, 'fc(4-0)");
+            set_fpu_register(fc, (std::isnan(get_fpu_register_double(fa)) || std::isnan(get_fpu_register_double(fb))) ? 0x4000000000000000LL : 0LL);
+            break;
+        case op_fcvtsd:
+            // Format(instr, "fcvtsd 'fb, 'fc(4-0)");
+            {
+                double b = (double)DoubleRToFloat(get_fpu_register(fb));
+                set_fpu_register_double(fc, b);
+            }
+            break;
+        case op_fcvtds:
+            // Format(instr, "fcvtds 'fb, 'fc(4-0)");
+            {
+                float b = get_fpu_register_double(fb);
+                set_fpu_register(fc, FloatToDoubleR(b));
+            }
+            break;
+        case op_fcvtdl_g:
+            // Format(instr, "fcvtdl_g 'fb, 'fc(4-0)");
+            {
+                double d = get_fpu_register_double(fb);
+                int64_t v = (int64_t)rint(d);
+                set_fpu_register(fc, v);
+                set_fcsr_bit(SW64_INE0_BIT, v!=d);
+            }
+            break;
+        case op_fcvtdl_p:
+            // Format(instr, "fcvtdl_p 'fb, 'fc(4-0)");
+            {
+                double d = get_fpu_register_double(fb);
+                int64_t v = (int64_t)ceil(d);
+                set_fpu_register(fc, v);
+                set_fcsr_bit(SW64_INE0_BIT, v>d);
+            }
+            break;
+        case op_fcvtdl_z:
+            // Format(instr, "fcvtdl_z 'fb, 'fc(4-0)");
+            {
+                double d = get_fpu_register_double(fb);
+                int64_t v = (int64_t)trunc(d);
+                set_fpu_register(fc, v);
+                set_fcsr_bit(SW64_INE0_BIT, v!=d);
+            }
+            break;
+        case op_fcvtdl_n:
+            // Format(instr, "fcvtdl_n 'fb, 'fc(4-0)");
+            {
+                double d = get_fpu_register_double(fb);
+                int64_t v = (int64_t)floor(d);
+                set_fpu_register(fc, v);
+                set_fcsr_bit(SW64_INE0_BIT, v<d);
+            }
+            break;
+        case op_fcvtdl:
+            // Format(instr, "fcvtdl 'fb, 'fc(4-0)");
+            // FIXME: how to dynamic ?
+            {
+                double d = get_fpu_register_double(fb);
+                int64_t v = (int64_t)round(d);
+                set_fpu_register(fc, v);
+                set_fcsr_bit(SW64_INE0_BIT, v!=d);
+            }
+            break;
+        case op_fcvtwl:
+            // Format(instr, "fcvtwl 'fb, 'fc(4-0)");
+            {
+                int64_t fcv = 0;
+                int64_t fbv = get_fpu_register(fb);
+                int v = ((uint64_t)fbv >> 29) & 0x3fffffff;
+                v |= ((uint64_t)fbv >> 62) << 30;
+                fcv = v;
+                set_fpu_register(fc, fcv);
+            }
+            break;
+        case op_fcvtlw:
+            // Format(instr, "fcvtlw 'fb, 'fc(4-0)");
+            {
+                int64_t fbv = get_fpu_register(fb);
+                int64_t v = ((uint64_t)fbv & 0x3fffffff) << 29;
+                v |= (((uint64_t)fbv >> 30) & 0x3) << 62;
+                set_fpu_register(fc, v);
+            }
+            break;
+        case op_fcvtls:
+            // Format(instr, "fcvtls 'fb, 'fc(4-0)");
+            set_fpu_register(fc, FloatToDoubleR((float)get_fpu_register(fb)));
+            break;
+        case op_fcvtld:
+            // Format(instr, "fcvtld 'fb, 'fc(4-0)");
+            set_fpu_register_double(fc, (double)get_fpu_register(fb));
+            break;
+        case op_fcpys:
+            {
+                // Format(instr, "fcpys 'fa, 'fb, 'fc(4-0)");
+                int64_t fcv = (get_fpu_register(fb) & 0x7fffffffffffffffLL);
+                if (get_fpu_register(fa) < 0) {
+                    fcv |= 0x8000000000000000LL;
+                }
+                set_fpu_register(fc, fcv);
+            }
+            break;
+        case op_fcpyse:
+            {
+                // Format(instr, "fcpyse 'fa, 'fb, 'fc(4-0)");
+                int64_t fcv = (get_fpu_register(fb) & 0x000fffffffffffffLL);
+                fcv |= (get_fpu_register(fa) & 0xfff0000000000000LL);
+                set_fpu_register(fc, fcv);
+            }
+            break;
+        case op_fcpysn:
+            {
+                // Format(instr, "fcpysn 'fa, 'fb, 'fc(4-0)");
+                int64_t fcv = (get_fpu_register(fb) & 0x7fffffffffffffffLL);
+                if (get_fpu_register(fa) >= 0) {
+                    fcv |= 0x8000000000000000LL;
+                }
+                set_fpu_register(fc, fcv);
+            }
+            break;
+        case op_ifmovs:
+            {
+                // Format(instr, "ifmovs 'ra, 'fc(4-0)");
+                int ra = instr->SwRaValue();
+                int64_t v = get_register(ra);
+                set_fpu_register(fc, FloatToDoubleR(*bit_cast<float*>(&v)));
+            }
+            break;
+        case op_ifmovd:
+            {
+                // Format(instr, "ifmovd 'ra, 'fc(4-0)");
+                int ra = instr->SwRaValue();
+                int64_t v = get_register(ra);
+                set_fpu_register(fc, v);
+            }
+            break;
+        case op_rfpcr:
+            // Format(instr, "rfpcr 'fa, FPCR");
+            set_fpu_register(fa, FCSR_);
+            break;
+        case op_wfpcr:
+            // Format(instr, "wfpcr 'fa, FPCR");
+            FCSR_ = get_fpu_register(fa);
+            break;
+        case op_setfpec0:
+            // Format(instr, "setfpec0");
+            FCSR_ &= 0xfffffffffffffffc;
+            break;
+        case op_setfpec1:
+            //Format(instr, "setfpec1");
+            FCSR_ &= 0xfffffffffffffffc;
+            FCSR_ |= 0x1;
+            break;
+        case op_setfpec2:
+            // Format(instr, "setfpec2");
+            FCSR_ &= 0xfffffffffffffffc;
+            FCSR_ |= 0x2;
+            break;
+        case op_setfpec3:
+            // Format(instr, "setfpec3");
+            FCSR_ |= 0x3;
+            break;
+        default:
+            break;
+        }
+    }
+    else
+    {
+        printf("a 0x%x \n", instr->OpcodeFieldRaw());
+        UNSUPPORTED();
+    }
+}
+
+void Simulator::SwDecodeTypeCompositeCalculationInteger(Instruction* instr) {
+    int ra = instr->SwRaValue();
+    int rb = instr->SwRbValue();
+    int rc = instr->SwRcValue(9, 5);
+    int rd = instr->SwRdValue();
+    int composite_calculation_op = instr->SwFunctionFieldRaw(12, 10) | instr->OpcodeFieldValue();
+
+    switch (composite_calculation_op) {
+    case op_seleq:
+        // Format(instr, "seleq 'ra, 'rb, 'rc(9-5), 'rd");
+        registers_[rd] = (registers_[ra] == 0) ?  registers_[rb] : registers_[rc];
+        break;
+    case op_selge:
+        // Format(instr, "selge 'ra, 'rb, 'rc(9-5), 'rd");
+        registers_[rd] = (registers_[ra] >= 0) ?  registers_[rb] : registers_[rc];
+        break;
+    case op_selgt:
+        // Format(instr, "selgt 'ra, 'rb, 'rc(9-5), 'rd");
+        registers_[rd] = (registers_[ra] > 0) ?  registers_[rb] : registers_[rc];
+        break;
+    case op_selle:
+        // Format(instr, "selle 'ra, 'rb, 'rc(9-5), 'rd");
+        registers_[rd] = (registers_[ra] <= 0) ?  registers_[rb] : registers_[rc];
+        break;
+    case op_sellt:
+        // Format(instr, "sellt 'ra, 'rb, 'rc(9-5), 'rd");
+        registers_[rd] = (registers_[ra] < 0) ?  registers_[rb] : registers_[rc];
+        break;
+    case op_selne:
+        // Format(instr, "selne 'ra, 'rb, 'rc(9-5), 'rd");
+        registers_[rd] = (registers_[ra] != 0) ?  registers_[rb] : registers_[rc];
+        break;
+    case op_sellbc:
+        // Format(instr, "sellbc 'ra, 'rb, 'rc(9-5), 'rd");
+        registers_[rd] = (0 == (registers_[ra] & 1)) ?  registers_[rb] : registers_[rc];
+        break;
+    case op_sellbs:
+        // Format(instr, "sellbs 'ra, 'rb, 'rc(9-5), 'rd");
+        registers_[rd] = (registers_[ra] & 1) ?  registers_[rb] : registers_[rc];
+        break;
+    default:
+        printf("a 0x%x \n", instr->OpcodeFieldRaw());
+        UNREACHABLE();
+        break;
+    }
+}
+
+void Simulator::SwDecodeTypeCompositeCalculationIntegerImm(Instruction* instr)
+{
+    int ra = instr->SwRaValue();
+    int imm8 = instr->SwImmOrDispFieldRaw(20, 13) >> 13;
+    int rc = instr->SwRcValue(9, 5);
+    int rd = instr->SwRdValue();
+    int composite_calculation_op = instr->SwFunctionFieldRaw(12, 10) | instr->OpcodeFieldValue();
+    switch (composite_calculation_op) {
+    case op_seleq_l:
+        // Format(instr, "seleq 'ra, 'imm(20-13), 'rc(9-5), 'rd");
+        registers_[rd] = (registers_[ra] == 0) ?  imm8 : registers_[rc];
+        break;
+    case op_selge_l:
+        // Format(instr, "selge 'ra, 'imm(20-13), 'rc(9-5), 'rd");
+        registers_[rd] = (registers_[ra] >= 0) ?  imm8 : registers_[rc];
+        break;
+    case op_selgt_l:
+        // Format(instr, "selgt 'ra, 'imm(20-13), 'rc(9-5), 'rd");
+        registers_[rd] = (registers_[ra] > 0) ?  imm8 : registers_[rc];
+        break;
+    case op_selle_l:
+        // Format(instr, "selle 'ra, 'imm(20-13), 'rc(9-5), 'rd");
+        registers_[rd] = (registers_[ra] <= 0) ?  imm8 : registers_[rc];
+        break;
+    case op_sellt_l:
+        // Format(instr, "sellt 'ra, 'imm(20-13), 'rc(9-5), 'rd");
+        registers_[rd] = (registers_[ra] < 0) ?  imm8 : registers_[rc];
+        break;
+    case op_selne_l:
+        // Format(instr, "selne 'ra, 'imm(20-13), 'rc(9-5), 'rd");
+        registers_[rd] = (registers_[ra] != 0) ?  imm8 : registers_[rc];
+        break;
+    case op_sellbc_l:
+        // Format(instr, "sellbc 'ra, 'imm(20-13), 'rc(9-5), 'rd");
+        registers_[rd] = (0 == (registers_[ra] & 1)) ?  imm8 : registers_[rc];
+        break;
+    case op_sellbs_l:
+        // Format(instr, "sellbs 'ra, 'imm(20-13), 'rc(9-5), 'rd");
+        registers_[rd] = (registers_[ra] & 1) ?  imm8 : registers_[rc];
+        break;
+    default:
+        printf("a 0x%x \n", instr->OpcodeFieldRaw());
+        UNREACHABLE();
+        break;
+    }
+}
+
+void Simulator::SwDecodeTypeCompositeCalculationFloatintPoint(Instruction* instr) {
+    int fa = instr->SwFaValue();
+    int fb = instr->SwFbValue();
+    int fc = instr->SwFcValue(9, 5);
+    int fd = instr->SwFdValue();
+    int composite_fp_calculation_op = instr->SwFunctionFieldRaw(15, 10) | instr->OpcodeFieldValue();
+
+    switch (composite_fp_calculation_op) {
+        case op_fmas:
+            // Format(instr, "fmas 'fa, 'fb, 'fc(9-5), 'fd");
+            {
+                float a = DoubleRToFloat(get_fpu_register(fa));
+                float b = DoubleRToFloat(get_fpu_register(fb));
+                float c = DoubleRToFloat(get_fpu_register(fc));
+                set_fpu_register(fd, FloatToDoubleR(a*b+c));
+            }
+            break;
+        case op_fmad:
+            // Format(instr, "fmad 'fa, 'fb, 'fc(9-5), 'fd");
+            set_fpu_register_double(fd, get_fpu_register_double(fa) *
+                    get_fpu_register_double(fb) + get_fpu_register_double(fc));
+            break;
+        case op_fmss:
+            // Format(instr, "fmss 'fa, 'fb, 'fc(9-5), 'fd");
+            {
+                float a = DoubleRToFloat(get_fpu_register(fa));
+                float b = DoubleRToFloat(get_fpu_register(fb));
+                float c = DoubleRToFloat(get_fpu_register(fc));
+                set_fpu_register(fd, FloatToDoubleR(a*b-c));
+            }
+            break;
+        case op_fmsd:
+            // Format(instr, "fmsd 'fa, 'fb, 'fc(9-5), 'fd");
+            set_fpu_register_double(fd, get_fpu_register_double(fa) *
+                    get_fpu_register_double(fb) - get_fpu_register_double(fc));
+            break;
+        case op_fnmas:
+            // Format(instr, "fnmas 'fa, 'fb, 'fc(9-5), 'fd");
+            {
+                float a = DoubleRToFloat(get_fpu_register(fa));
+                float b = DoubleRToFloat(get_fpu_register(fb));
+                float c = DoubleRToFloat(get_fpu_register(fc));
+                set_fpu_register(fd, FloatToDoubleR(-a*b+c));
+            }
+            break;
+        case op_fnmad:
+            // Format(instr, "fnmad 'fa, 'fb, 'fc(9-5), 'fd");
+            set_fpu_register_double(fd, -get_fpu_register_double(fa) *
+                    get_fpu_register_double(fb) + get_fpu_register_double(fc));
+            break;
+        case op_fnmss:
+            // Format(instr, "fnmss 'fa, 'fb, 'fc(9-5), 'fd");
+            {
+                float a = DoubleRToFloat(get_fpu_register(fa));
+                float b = DoubleRToFloat(get_fpu_register(fb));
+                float c = DoubleRToFloat(get_fpu_register(fc));
+                set_fpu_register(fd, FloatToDoubleR(-a*b-c));
+            }
+            break;
+        case op_fnmsd:
+            Format(instr, "fnmsd 'fa, 'fb, 'fc(9-5), 'fd");
+            set_fpu_register_double(fd, -get_fpu_register_double(fa) *
+                    get_fpu_register_double(fb) - get_fpu_register_double(fc));
+            break;
+        case op_fseleq:
+            // Format(instr, "fseleq 'fa, 'fb, 'fc(9-5), 'fd");
+            set_fpu_register(fd, (get_fpu_register_double(fa) == 0) ?  get_fpu_register(fb) : get_fpu_register(fc));
+            break;
+        case op_fselne:
+            // Format(instr, "fselne 'fa, 'fb, 'fc(9-5), 'fd");
+            set_fpu_register(fd, (get_fpu_register_double(fa) != 0) ?  get_fpu_register(fb) : get_fpu_register(fc));
+            break;
+        case op_fsellt:
+            // Format(instr, "fsellt 'fa, 'fb, 'fc(9-5), 'fd");
+            set_fpu_register(fd, (get_fpu_register_double(fa) < 0) ?  get_fpu_register(fb) : get_fpu_register(fc));
+            break;
+        case op_fselle:
+            // Format(instr, "fselle 'fa, 'fb, 'fc(9-5), 'fd");
+            set_fpu_register(fd, (get_fpu_register_double(fa) <= 0) ?  get_fpu_register(fb) : get_fpu_register(fc));
+            break;
+        case op_fselgt:
+            // Format(instr, "fselgt 'fa, 'fb, 'fc(9-5), 'fd");
+            set_fpu_register(fd, (get_fpu_register_double(fa) > 0) ?  get_fpu_register(fb) : get_fpu_register(fc));
+            break;
+        case op_fselge:
+            // Format(instr, "fselge 'fa, 'fb, 'fc(9-5), 'fd");
+            set_fpu_register(fd, (get_fpu_register_double(fa) >= 0) ?  get_fpu_register(fb) : get_fpu_register(fc));
+            break;
+        default:
+            printf("a 0x%x \n", instr->OpcodeFieldRaw());
+            UNREACHABLE();
+    }
+}
+
+void Simulator::DecodeTypeCompoCalc()
+{
+    Instruction* instr = instr_.instr();
+    switch (instr->OpcodeFieldValue()) {
+        case OP(0x11)://
+            SwDecodeTypeCompositeCalculationInteger(instr);
+            break;
+        case OP(0x13)://
+            SwDecodeTypeCompositeCalculationIntegerImm(instr);
+            break;
+        case OP(0x19)://
+            SwDecodeTypeCompositeCalculationFloatintPoint(instr);
+            break;
+    }
+}
+
+void Simulator::DecodeTypeExten()
+{
+      UNSUPPORTED();
+}
+
+void Simulator::DecodeTypeSimulatorTrap()
+{
+    SoftwareInterrupt();
+}
+
+// Executes the current instruction.
+void Simulator::InstructionDecode(Instruction* instr) {
+  if (v8::internal::FLAG_check_icache) {
+    CheckICache(i_cache(), instr);
+  }
+  pc_modified_ = false;
+
+  v8::internal::EmbeddedVector<char, 256> buffer;
+
+  if (::v8::internal::FLAG_trace_sim) {
+    SNPrintF(trace_buf_, " ");
+    disasm::NameConverter converter;
+    disasm::Disassembler dasm(converter);
+    // Use a reasonably large buffer.
+    dasm.InstructionDecode(buffer, reinterpret_cast<byte*>(instr));
+  }
+
+  instr_ = instr;
+  switch (instr_.InstructionType()) {
+    case Instruction::kImmediateType:
+      DecodeTypeImmediate();
+      break;
+    case Instruction::kJumpType:
+      DecodeTypeJump();
+      break;
+    case Instruction::kSwSyscallType:
+      DecodeTypeSyscall();
+      break;
+    case Instruction::kSwTransferanceType:
+      DecodeTypeTransfer();
+      break;
+    case Instruction::kSwStorageType:
+      DecodeTypeStorage();
+      break;
+    case Instruction::kSwSimpleCalculationType:
+      DecodeTypeSimpleCalc();
+      break;
+    case Instruction::kSwCompositeCalculationType:
+      DecodeTypeCompoCalc();
+      break;
+    case Instruction::kSwExtendType:
+      DecodeTypeExten();
+      break;
+    case Instruction::kSwSimulatorTrap:
+      DecodeTypeSimulatorTrap();
+      break;
+    default:
+      UNSUPPORTED();
+  }
+
+  if (::v8::internal::FLAG_trace_sim) {
+    PrintF("  0x%08" PRIxPTR "   %-44s   %s\n",
+           reinterpret_cast<intptr_t>(instr), buffer.begin(),
+           trace_buf_.begin());
+  }
+
+  if (!pc_modified_) {
+    set_register(pc, reinterpret_cast<int64_t>(instr) +
+                 kInstrSize);
+  }
+}
+
+
+
+void Simulator::Execute() {
+  // Get the PC to simulate. Cannot use the accessor here as we need the
+  // raw PC value and not the one used as input to arithmetic instructions.
+  int64_t program_counter = get_pc();
+  if (::v8::internal::FLAG_stop_sim_at == 0) {
+    // Fast version of the dispatch loop without checking whether the simulator
+    // should be stopping at a particular executed instruction.
+    while (program_counter != end_sim_pc) {
+      Instruction* instr = reinterpret_cast<Instruction*>(program_counter);
+      icount_++;
+      InstructionDecode(instr);
+      program_counter = get_pc();
+    }
+  } else {
+    // FLAG_stop_sim_at is at the non-default value. Stop in the debugger when
+    // we reach the particular instruction count.
+    while (program_counter != end_sim_pc) {
+      Instruction* instr = reinterpret_cast<Instruction*>(program_counter);
+      icount_++;
+      PrintF("%ld: \n", icount_);
+      if (icount_ == static_cast<int64_t>(::v8::internal::FLAG_stop_sim_at)) {
+        Sw64Debugger dbg(this);
+        dbg.Debug();
+      } else {
+        InstructionDecode(instr);
+      }
+      program_counter = get_pc();
+    }
+  }
+}
+
+void Simulator::CallInternal(Address entry) {
+  // Adjust JS-based stack limit to C-based stack limit.
+  isolate_->stack_guard()->AdjustStackLimitForSimulator();
+
+  // Prepare to execute the code at entry.
+  set_register(pc, static_cast<int64_t>(entry));
+  // Put down marker for end of simulation. The simulator will stop simulation
+  // when the PC reaches this value. By saving the "end simulation" value into
+  // the LR the simulation stops when returning to this call point.
+  set_register(ra, end_sim_pc);
+
+  // Remember the values of callee-saved registers.
+  // The code below assumes that r9 is not used as sb (static base) in
+  // simulator code and therefore is regarded as a callee-saved register.
+  int64_t s0_val = get_register(s0);
+  int64_t s1_val = get_register(s1);
+  int64_t s2_val = get_register(s2);
+  int64_t s3_val = get_register(s3);
+  int64_t s4_val = get_register(s4);
+  int64_t s5_val = get_register(s5);
+  int64_t fp_val = get_register(fp);
+
+  // Set up the callee-saved registers with a known value. To be able to check
+  // that they are preserved properly across JS execution.
+  int64_t callee_saved_value = icount_;
+  set_register(s0, callee_saved_value);
+  set_register(s1, callee_saved_value);
+  set_register(s2, callee_saved_value);
+  set_register(s3, callee_saved_value);
+  set_register(s4, callee_saved_value);
+  set_register(s5, callee_saved_value);
+  set_register(fp, callee_saved_value);
+
+  // Start the simulation.
+  Execute();
+
+  // Check that the callee-saved registers have been preserved.
+  CHECK_EQ(callee_saved_value, get_register(s0));
+  CHECK_EQ(callee_saved_value, get_register(s1));
+  CHECK_EQ(callee_saved_value, get_register(s2));
+  CHECK_EQ(callee_saved_value, get_register(s3));
+  CHECK_EQ(callee_saved_value, get_register(s4));
+  CHECK_EQ(callee_saved_value, get_register(s5));
+  CHECK_EQ(callee_saved_value, get_register(fp));
+
+  // Restore callee-saved registers with the original value.
+  set_register(s0, s0_val);
+  set_register(s1, s1_val);
+  set_register(s2, s2_val);
+  set_register(s3, s3_val);
+  set_register(s4, s4_val);
+  set_register(s5, s5_val);
+  set_register(fp, fp_val);
+}
+
+intptr_t Simulator::CallImpl(Address entry, int argument_count,
+                             const intptr_t* arguments) {
+  constexpr int kRegisterPassedArguments = 6;
+  // Set up arguments.
+
+  // First six arguments passed in registers.
+  int reg_arg_count = std::min(kRegisterPassedArguments, argument_count);
+  if (reg_arg_count > 0) set_register(a0, arguments[0]);
+  if (reg_arg_count > 1) set_register(a1, arguments[1]);
+  if (reg_arg_count > 2) set_register(a2, arguments[2]);
+  if (reg_arg_count > 3) set_register(a3, arguments[3]);
+  if (reg_arg_count > 4) set_register(a4, arguments[4]);
+  if (reg_arg_count > 5) set_register(a5, arguments[5]);
+
+  // Remaining arguments passed on stack.
+  int64_t original_stack = get_register(sp);
+  // Compute position of stack on entry to generated code.
+  int stack_args_count = argument_count - reg_arg_count;
+  int stack_args_size = stack_args_count * sizeof(*arguments) + kCArgsSlotsSize;
+  int64_t entry_stack = original_stack - stack_args_size;
+
+  if (base::OS::ActivationFrameAlignment() != 0) {
+    entry_stack &= -base::OS::ActivationFrameAlignment();
+  }
+  // Store remaining arguments on stack, from low to high memory.
+  intptr_t* stack_argument = reinterpret_cast<intptr_t*>(entry_stack);
+  memcpy(stack_argument + kCArgSlotCount, arguments + reg_arg_count,
+         stack_args_count * sizeof(*arguments));
+  set_register(sp, entry_stack);
+
+  CallInternal(entry);
+
+  // Pop stack passed arguments.
+  CHECK_EQ(entry_stack, get_register(sp));
+  set_register(sp, original_stack);
+
+  return get_register(v0);
+}
+
+double Simulator::CallFP(Address entry, double d0, double d1) {
+  if (!IsSw64SoftFloatABI) {
+    const FPURegister fparg2 = f13;
+    set_fpu_register_double(f12, d0);
+    set_fpu_register_double(fparg2, d1);
+  } else {
+    int buffer[2];
+    DCHECK(sizeof(buffer[0]) * 2 == sizeof(d0));
+    memcpy(buffer, &d0, sizeof(d0));
+    set_dw_register(a0, buffer);
+    memcpy(buffer, &d1, sizeof(d1));
+    set_dw_register(a2, buffer);
+  }
+  CallInternal(entry);
+  if (!IsSw64SoftFloatABI) {
+    return get_fpu_register_double(f0);
+  } else {
+    return get_double_from_register_pair(v0);
+  }
+}
+
+
+uintptr_t Simulator::PushAddress(uintptr_t address) {
+  int64_t new_sp = get_register(sp) - sizeof(uintptr_t);
+  uintptr_t* stack_slot = reinterpret_cast<uintptr_t*>(new_sp);
+  *stack_slot = address;
+  set_register(sp, new_sp);
+  return new_sp;
+}
+
+
+uintptr_t Simulator::PopAddress() {
+  int64_t current_sp = get_register(sp);
+  uintptr_t* stack_slot = reinterpret_cast<uintptr_t*>(current_sp);
+  uintptr_t address = *stack_slot;
+  set_register(sp, current_sp + sizeof(uintptr_t));
+  return address;
+}
+
+
+#undef UNSUPPORTED
+}  // namespace internal
+}  // namespace v8
+
+#endif  // USE_SIMULATOR
diff --git a/src/3rdparty/chromium/v8/src/execution/sw64/simulator-sw64.h b/src/3rdparty/chromium/v8/src/execution/sw64/simulator-sw64.h
new file mode 100755
index 0000000000..cc850b2551
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/execution/sw64/simulator-sw64.h
@@ -0,0 +1,620 @@
+// Copyright 2011 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+// Declares a Simulator for SW64 instructions if we are not generating a native
+// SW64 binary. This Simulator allows us to run and debug SW64 code generation
+// on regular desktop machines.
+// V8 calls into generated code via the GeneratedCode wrapper,
+// which will start execution in the Simulator or forwards to the real entry
+// on a SW64 HW platform.
+
+#ifndef V8_EXECUTION_SW64_SIMULATOR_SW64_H_
+#define V8_EXECUTION_SW64_SIMULATOR_SW64_H_
+
+// globals.h defines USE_SIMULATOR.
+#include "src/common/globals.h"
+
+template <typename T>
+int Compare(const T& a, const T& b) {
+  if (a == b)
+    return 0;
+  else if (a < b)
+    return -1;
+  else
+    return 1;
+}
+
+// Returns the negative absolute value of its argument.
+template <typename T,
+          typename = typename std::enable_if<std::is_signed<T>::value>::type>
+T Nabs(T a) {
+  return a < 0 ? a : -a;
+}
+
+#if defined(USE_SIMULATOR)
+// Running with a simulator.
+
+#include "src/base/hashmap.h"
+#include "src/codegen/assembler.h"
+#include "src/codegen/sw64/constants-sw64.h"
+#include "src/execution/simulator-base.h"
+#include "src/utils/allocation.h"
+
+namespace v8 {
+namespace internal {
+
+// -----------------------------------------------------------------------------
+// Utility functions
+
+class CachePage {
+ public:
+  static const int LINE_VALID = 0;
+  static const int LINE_INVALID = 1;
+
+  static const int kPageShift = 12;
+  static const int kPageSize = 1 << kPageShift;
+  static const int kPageMask = kPageSize - 1;
+  static const int kLineShift = 2;  // The cache line is only 4 bytes right now.
+  static const int kLineLength = 1 << kLineShift;
+  static const int kLineMask = kLineLength - 1;
+
+  CachePage() {
+    memset(&validity_map_, LINE_INVALID, sizeof(validity_map_));
+  }
+
+  char* ValidityByte(int offset) {
+    return &validity_map_[offset >> kLineShift];
+  }
+
+  char* CachedData(int offset) {
+    return &data_[offset];
+  }
+
+ private:
+  char data_[kPageSize];   // The cached data.
+  static const int kValidityMapSize = kPageSize >> kLineShift;
+  char validity_map_[kValidityMapSize];  // One byte per line.
+};
+
+class SimInstructionBase : public InstructionBase {
+ public:
+  Type InstructionType() const { return type_; }
+  inline Instruction* instr() const { return instr_; }
+  inline int32_t operand() const { return operand_; }
+
+ protected:
+  SimInstructionBase() : operand_(-1), instr_(nullptr), type_(kUnsupported) {}
+  explicit SimInstructionBase(Instruction* instr) {}
+
+  int32_t operand_;
+  Instruction* instr_;
+  Type type_;
+
+ private:
+  DISALLOW_ASSIGN(SimInstructionBase);
+};
+
+class SimInstruction : public InstructionGetters<SimInstructionBase> {
+ public:
+  SimInstruction() {}
+
+  explicit SimInstruction(Instruction* instr) { *this = instr; }
+
+  SimInstruction& operator=(Instruction* instr) {
+    operand_ = *reinterpret_cast<const int32_t*>(instr);
+    instr_ = instr;
+    type_ = InstructionBase::InstructionType();
+    DCHECK(reinterpret_cast<void*>(&operand_) == this);
+    return *this;
+  }
+};
+
+class Simulator : public SimulatorBase {
+ public:
+  friend class Sw64Debugger;
+
+  // Registers are declared in order. See SMRL chapter 2.
+  enum Register {
+    no_reg = -1,
+    v0,
+    t0, t1, t2, t3, t4, t5, t6, t7,
+    s0, s1, s2, s3, s4, s5, s6,
+    a0, a1, a2, a3, a4, a5,
+    t8, t9, t10, t11,
+    ra,
+    t12,
+    at,
+    gp,
+    sp,
+    zero_reg,
+    // LO, HI, and pc.
+    LO,
+    HI,
+    pc,   // pc must be the last register.
+    kNumSimuRegisters,
+    // alias
+    fp = s6,
+    pv = t12
+  };
+
+  // Coprocessor registers.
+  // Generated code will always use doubles. So we will only use even registers.
+  enum FPURegister {
+    f0, f1, f2, f3, f4, f5, f6, f7, f8, f9, f10, f11,
+    f12, f13, f14, f15,   // f12 and f14 are arguments FPURegisters.
+    f16, f17, f18, f19, f20, f21, f22, f23, f24, f25,
+    f26, f27, f28, f29, f30, f31,
+    kNumFPURegisters
+  };
+
+  // MSA registers
+  enum MSARegister {
+    w0,
+    w1,
+    w2,
+    w3,
+    w4,
+    w5,
+    w6,
+    w7,
+    w8,
+    w9,
+    w10,
+    w11,
+    w12,
+    w13,
+    w14,
+    w15,
+    w16,
+    w17,
+    w18,
+    w19,
+    w20,
+    w21,
+    w22,
+    w23,
+    w24,
+    w25,
+    w26,
+    w27,
+    w28,
+    w29,
+    w30,
+    w31,
+    kNumMSARegisters
+  };
+
+  explicit Simulator(Isolate* isolate);
+  ~Simulator();
+
+  // The currently executing Simulator instance. Potentially there can be one
+  // for each native thread.
+  V8_EXPORT_PRIVATE static Simulator* current(v8::internal::Isolate* isolate);
+
+  // Accessors for register state. Reading the pc value adheres to the SW64
+  // architecture specification and is off by a 8 from the currently executing
+  // instruction.
+  void set_register(int reg, int64_t value);
+  void set_register_word(int reg, int32_t value);
+  void set_dw_register(int dreg, const int* dbl);
+  int64_t get_register(int64_t reg) const;
+  double get_double_from_register_pair(int reg);
+  // Same for FPURegisters.
+  void set_fpu_register(int fpureg, int64_t value);
+  void set_fpu_register_word(int fpureg, int32_t value);
+  void set_fpu_register_hi_word(int fpureg, int32_t value);
+  void set_fpu_register_float(int fpureg, float value);
+  void set_fpu_register_double(int fpureg, double value);
+  void set_fpu_register_invalid_result64(float original, float rounded);
+  void set_fpu_register_invalid_result(float original, float rounded);
+  void set_fpu_register_word_invalid_result(float original, float rounded);
+  void set_fpu_register_invalid_result64(double original, double rounded);
+  void set_fpu_register_invalid_result(double original, double rounded);
+  void set_fpu_register_word_invalid_result(double original, double rounded);
+  int64_t get_fpu_register(int fpureg) const;
+  int32_t get_fpu_register_word(int fpureg) const;
+  int32_t get_fpu_register_signed_word(int fpureg) const;
+  int32_t get_fpu_register_hi_word(int fpureg) const;
+  float get_fpu_register_float(int fpureg) const;
+  double get_fpu_register_double(int fpureg) const;
+  template <typename T>
+  void get_msa_register(int wreg, T* value);
+  template <typename T>
+  void set_msa_register(int wreg, const T* value);
+  void set_fcsr_bit(uint32_t cc, bool value);
+  bool test_fcsr_bit(uint32_t cc);
+  bool set_fcsr_round_error(double original, double rounded);
+  bool set_fcsr_round64_error(double original, double rounded);
+  bool set_fcsr_round_error(float original, float rounded);
+  bool set_fcsr_round64_error(float original, float rounded);
+  void round_according_to_fcsr(double toRound, double* rounded,
+                               int32_t* rounded_int, double fs);
+  void round64_according_to_fcsr(double toRound, double* rounded,
+                                 int64_t* rounded_int, double fs);
+  void round_according_to_fcsr(float toRound, float* rounded,
+                               int32_t* rounded_int, float fs);
+  void round64_according_to_fcsr(float toRound, float* rounded,
+                                 int64_t* rounded_int, float fs);
+  template <typename T_fp, typename T_int>
+  void round_according_to_msacsr(T_fp toRound, T_fp* rounded,
+                                 T_int* rounded_int);
+  void set_fcsr_rounding_mode(FPURoundingMode mode);
+  void set_msacsr_rounding_mode(FPURoundingMode mode);
+  unsigned int get_fcsr_rounding_mode();
+  unsigned int get_msacsr_rounding_mode();
+  // Special case of set_register and get_register to access the raw PC value.
+  void set_pc(int64_t value);
+  int64_t get_pc() const;
+
+  Address get_sp() const { return static_cast<Address>(get_register(sp)); }
+
+  // Accessor to the internal simulator stack area.
+  uintptr_t StackLimit(uintptr_t c_limit) const;
+
+  // Executes SW64 instructions until the PC reaches end_sim_pc.
+  void Execute();
+
+  template <typename Return, typename... Args>
+  Return Call(Address entry, Args... args) {
+    return VariadicCall<Return>(this, &Simulator::CallImpl, entry, args...);
+  }
+
+  // Alternative: call a 2-argument double function.
+  double CallFP(Address entry, double d0, double d1);
+
+  // Push an address onto the JS stack.
+  uintptr_t PushAddress(uintptr_t address);
+
+  // Pop an address from the JS stack.
+  uintptr_t PopAddress();
+
+  // Debugger input.
+  void set_last_debugger_input(char* input);
+  char* last_debugger_input() { return last_debugger_input_; }
+
+  // Redirection support.
+  static void SetRedirectInstruction(Instruction* instruction);
+
+  // ICache checking.
+  static bool ICacheMatch(void* one, void* two);
+  static void FlushICache(base::CustomMatcherHashMap* i_cache, void* start,
+                          size_t size);
+
+  // Returns true if pc register contains one of the 'special_values' defined
+  // below (bad_ra, end_sim_pc).
+  bool has_bad_pc() const;
+
+ private:
+  enum special_values {
+    // Known bad pc value to ensure that the simulator does not execute
+    // without being properly setup.
+    bad_ra = -1,
+    // A pc value used to signal the simulator to stop execution.  Generally
+    // the ra is set to this value on transition from native C code to
+    // simulated execution, so that the simulator can "return" to the native
+    // C code.
+    end_sim_pc = -2,
+    // Unpredictable value.
+    Unpredictable = 0xbadbeaf
+  };
+
+  V8_EXPORT_PRIVATE intptr_t CallImpl(Address entry, int argument_count,
+                                      const intptr_t* arguments);
+
+  // Unsupported instructions use Format to print an error and stop execution.
+  void Format(Instruction* instr, const char* format);
+
+  // Helpers for data value tracing.
+  enum TraceType {
+    BYTE,
+    HALF,
+    WORD,
+    DWORD,
+    FLOAT,
+    DOUBLE,
+    FLOAT_DOUBLE,
+    WORD_DWORD
+  };
+
+  // MSA Data Format
+  enum MSADataFormat { MSA_VECT = 0, MSA_BYTE, MSA_HALF, MSA_WORD, MSA_DWORD };
+  union msa_reg_t {
+    int8_t b[kMSALanesByte];
+    uint8_t ub[kMSALanesByte];
+    int16_t h[kMSALanesHalf];
+    uint16_t uh[kMSALanesHalf];
+    int32_t w[kMSALanesWord];
+    uint32_t uw[kMSALanesWord];
+    int64_t d[kMSALanesDword];
+    uint64_t ud[kMSALanesDword];
+  };
+
+  // Read and write memory.
+  inline uint32_t ReadBU(int64_t addr);
+  inline int32_t ReadB(int64_t addr);
+  inline void WriteB(int64_t addr, uint8_t value);
+  inline void WriteB(int64_t addr, int8_t value);
+
+  inline uint16_t ReadHU(int64_t addr, Instruction* instr);
+  inline int16_t ReadH(int64_t addr, Instruction* instr);
+  // Note: Overloaded on the sign of the value.
+  inline void WriteH(int64_t addr, uint16_t value, Instruction* instr);
+  inline void WriteH(int64_t addr, int16_t value, Instruction* instr);
+
+  inline uint32_t ReadWU(int64_t addr, Instruction* instr);
+  inline int32_t ReadW(int64_t addr, Instruction* instr, TraceType t = WORD);
+  inline void WriteW(int64_t addr, int32_t value, Instruction* instr);
+  inline int64_t Read2W(int64_t addr, Instruction* instr);
+  inline void Write2W(int64_t addr, int64_t value, Instruction* instr);
+
+  inline double ReadD(int64_t addr, Instruction* instr);
+  inline void WriteD(int64_t addr, double value, Instruction* instr);
+
+  template <typename T>
+  T ReadMem(int64_t addr, Instruction* instr);
+  template <typename T>
+  void WriteMem(int64_t addr, T value, Instruction* instr);
+
+  // Helper for debugging memory access.
+  inline void DieOrDebug();
+
+  void TraceRegWr(int64_t value, TraceType t = DWORD);
+  template <typename T>
+  void TraceMSARegWr(T* value, TraceType t);
+  template <typename T>
+  void TraceMSARegWr(T* value);
+  void TraceMemWr(int64_t addr, int64_t value, TraceType t);
+  void TraceMemRd(int64_t addr, int64_t value, TraceType t = DWORD);
+  template <typename T>
+  void TraceMemRd(int64_t addr, T value);
+  template <typename T>
+  void TraceMemWr(int64_t addr, T value);
+
+  // Operations depending on endianness.
+  // Get Double Higher / Lower word.
+  inline int32_t GetDoubleHIW(double* addr);
+  inline int32_t GetDoubleLOW(double* addr);
+  // Set Double Higher / Lower word.
+  inline int32_t SetDoubleHIW(double* addr);
+  inline int32_t SetDoubleLOW(double* addr);
+
+  SimInstruction instr_;
+
+  // functions called from DecodeTypeRegister.
+  void DecodeTypeRegisterCOP1();
+
+  void DecodeTypeRegisterCOP1X();
+
+  void DecodeTypeRegisterSPECIAL();
+
+
+  void DecodeTypeRegisterSPECIAL2();
+
+  void DecodeTypeRegisterSPECIAL3();
+
+  void DecodeTypeRegisterSRsType();
+
+  void DecodeTypeRegisterDRsType();
+
+  void DecodeTypeRegisterWRsType();
+
+  void DecodeTypeRegisterLRsType();
+
+  int DecodeMsaDataFormat();
+  void DecodeTypeMsaI8();
+  void DecodeTypeMsaI5();
+  void DecodeTypeMsaI10();
+  void DecodeTypeMsaELM();
+  void DecodeTypeMsaBIT();
+  void DecodeTypeMsaMI10();
+  void DecodeTypeMsa3R();
+  void DecodeTypeMsa3RF();
+  void DecodeTypeMsaVec();
+  void DecodeTypeMsa2R();
+  void DecodeTypeMsa2RF();
+  template <typename T>
+  T MsaI5InstrHelper(uint32_t opcode, T ws, int32_t i5);
+  template <typename T>
+  T MsaBitInstrHelper(uint32_t opcode, T wd, T ws, int32_t m);
+  template <typename T>
+  T Msa3RInstrHelper(uint32_t opcode, T wd, T ws, T wt);
+
+  // Executing is handled based on the instruction type.
+  void DecodeTypeRegister();
+
+  inline int32_t rs_reg() const { return instr_.RsValue(); }
+  inline int64_t rs() const { return get_register(rs_reg()); }
+  inline uint64_t rs_u() const {
+    return static_cast<uint64_t>(get_register(rs_reg()));
+  }
+  inline int32_t rt_reg() const { return instr_.RtValue(); }
+  inline int64_t rt() const { return get_register(rt_reg()); }
+  inline uint64_t rt_u() const {
+    return static_cast<uint64_t>(get_register(rt_reg()));
+  }
+  inline int32_t rd_reg() const { return instr_.RdValue(); }
+  inline int32_t fr_reg() const { return instr_.FrValue(); }
+  inline int32_t fs_reg() const { return instr_.FsValue(); }
+  inline int32_t ft_reg() const { return instr_.FtValue(); }
+  inline int32_t fd_reg() const { return instr_.FdValue(); }
+  inline int32_t sa() const { return instr_.SaValue(); }
+  inline int32_t lsa_sa() const { return instr_.LsaSaValue(); }
+  inline int32_t ws_reg() const { return instr_.WsValue(); }
+  inline int32_t wt_reg() const { return instr_.WtValue(); }
+  inline int32_t wd_reg() const { return instr_.WdValue(); }
+
+  inline void SetResult(const int32_t rd_reg, const int64_t alu_out) {
+    set_register(rd_reg, alu_out);
+    TraceRegWr(alu_out);
+  }
+
+  inline void SetFPUWordResult(int32_t fd_reg, int32_t alu_out) {
+    set_fpu_register_word(fd_reg, alu_out);
+    TraceRegWr(get_fpu_register(fd_reg), WORD);
+  }
+
+  inline void SetFPUWordResult2(int32_t fd_reg, int32_t alu_out) {
+    set_fpu_register_word(fd_reg, alu_out);
+    TraceRegWr(get_fpu_register(fd_reg));
+  }
+
+  inline void SetFPUResult(int32_t fd_reg, int64_t alu_out) {
+    set_fpu_register(fd_reg, alu_out);
+    TraceRegWr(get_fpu_register(fd_reg));
+  }
+
+  inline void SetFPUResult2(int32_t fd_reg, int64_t alu_out) {
+    set_fpu_register(fd_reg, alu_out);
+    TraceRegWr(get_fpu_register(fd_reg), DOUBLE);
+  }
+
+  inline void SetFPUFloatResult(int32_t fd_reg, float alu_out) {
+    set_fpu_register_float(fd_reg, alu_out);
+    TraceRegWr(get_fpu_register(fd_reg), FLOAT);
+  }
+
+  inline void SetFPUDoubleResult(int32_t fd_reg, double alu_out) {
+    set_fpu_register_double(fd_reg, alu_out);
+    TraceRegWr(get_fpu_register(fd_reg), DOUBLE);
+  }
+
+  void DecodeTypeImmediate();
+  void DecodeTypeJump();
+  void DecodeTypeSyscall();
+  void DecodeTypeTransfer();
+  void DecodeTypeStorage();
+  void DecodeTypeSimpleCalc();
+  void DecodeTypeCompoCalc();
+  void DecodeTypeExten();
+  void DecodeTypeSimulatorTrap();
+  void SwDecodeTypeCompositeCalculationInteger(Instruction* instr);
+  void SwDecodeTypeCompositeCalculationIntegerImm(Instruction* instr);
+  void SwDecodeTypeCompositeCalculationFloatintPoint(Instruction* instr);
+
+  // Used for breakpoints and traps.
+  void SoftwareInterrupt();
+
+  // Compact branch guard.
+  void CheckForbiddenSlot(int64_t current_pc) {
+    Instruction* instr_after_compact_branch =
+        reinterpret_cast<Instruction*>(current_pc + kInstrSize);
+    if (instr_after_compact_branch->IsForbiddenAfterBranch()) {
+      FATAL(
+          "Error: Unexpected instruction 0x%08x immediately after a "
+          "compact branch instruction.",
+          *reinterpret_cast<uint32_t*>(instr_after_compact_branch));
+    }
+  }
+
+  // Stop helper functions.
+  bool IsWatchpoint(uint64_t code);
+  void PrintWatchpoint(uint64_t code);
+  void HandleStop(uint64_t code, Instruction* instr);
+  bool IsStopInstruction(Instruction* instr);
+  bool IsEnabledStop(uint64_t code);
+  void EnableStop(uint64_t code);
+  void DisableStop(uint64_t code);
+  void IncreaseStopCounter(uint64_t code);
+  void PrintStopInfo(uint64_t code);
+
+
+  // Executes one instruction.
+  void InstructionDecode(Instruction* instr);
+  // Execute one instruction placed in a branch delay slot.
+  void BranchDelayInstructionDecode(Instruction* instr) {
+    if (instr->InstructionBits() == nopInstr) {
+      // Short-cut generic nop instructions. They are always valid and they
+      // never change the simulator state.
+      return;
+    }
+
+    if (instr->IsForbiddenAfterBranch()) {
+      FATAL("Eror:Unexpected %i opcode in a branch delay slot.",
+            instr->OpcodeValue());
+    }
+    InstructionDecode(instr);
+    SNPrintF(trace_buf_, " ");
+  }
+
+  // ICache.
+  static void CheckICache(base::CustomMatcherHashMap* i_cache,
+                          Instruction* instr);
+  static void FlushOnePage(base::CustomMatcherHashMap* i_cache, intptr_t start,
+                           size_t size);
+  static CachePage* GetCachePage(base::CustomMatcherHashMap* i_cache,
+                                 void* page);
+
+  enum Exception {
+    none,
+    kIntegerOverflow,
+    kIntegerUnderflow,
+    kDivideByZero,
+    kNumExceptions
+  };
+
+  // Exceptions.
+  void SignalException(Exception e);
+
+  // Handle arguments and return value for runtime FP functions.
+  void GetFpArgs(double* x, double* y, int32_t* z);
+  void SetFpResult(const double& result);
+
+  void CallInternal(Address entry);
+
+  // Architecture state.
+  // Registers.
+  int64_t registers_[kNumSimuRegisters];
+  // Coprocessor Registers.
+  // Note: FPUregisters_[] array is increased to 64 * 8B = 32 * 16B in
+  // order to support MSA registers
+  int64_t FPUregisters_[kNumFPURegisters * 2];
+  // FPU control register.
+  uint64_t FCSR_;
+  // MSA control register.
+  uint64_t MSACSR_;
+
+  //for lock instruction
+  int64_t lock_valid;
+  int64_t lock_success;
+  int64_t lock_flag;
+  int64_t lock_register_padd;
+  int64_t lock_register_flag;
+
+  // Simulator support.
+  // Allocate 1MB for stack.
+  size_t stack_size_;
+  char* stack_;
+  bool pc_modified_;
+  int64_t icount_;
+  int break_count_;
+  EmbeddedVector<char, 128> trace_buf_;
+
+  // Debugger input.
+  char* last_debugger_input_;
+
+  v8::internal::Isolate* isolate_;
+
+  // Registered breakpoints.
+  Instruction* break_pc_;
+  Instr break_instr_;
+
+  // Stop is disabled if bit 31 is set.
+  static const uint32_t kStopDisabledBit = 1 << 31;
+
+  // A stop is enabled, meaning the simulator will stop when meeting the
+  // instruction, if bit 31 of watched_stops_[code].count is unset.
+  // The value watched_stops_[code].count & ~(1 << 31) indicates how many times
+  // the breakpoint was hit or gone through.
+  struct StopCountAndDesc {
+    uint32_t count;
+    char* desc;
+  };
+  StopCountAndDesc watched_stops_[kMaxStopCode + 1];
+};
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // defined(USE_SIMULATOR)
+#endif  // V8_EXECUTION_SW64_SIMULATOR_SW64_H_
diff --git a/src/3rdparty/chromium/v8/src/flags/flag-definitions.h b/src/3rdparty/chromium/v8/src/flags/flag-definitions.h
index ab689283e9..2ed7aec46e 100644
--- a/src/3rdparty/chromium/v8/src/flags/flag-definitions.h
+++ b/src/3rdparty/chromium/v8/src/flags/flag-definitions.h
@@ -1744,7 +1744,7 @@ DEFINE_NEG_IMPLICATION(perf_prof, wasm_write_protect_code_memory)
 
 // --perf-prof-unwinding-info is available only on selected architectures.
 #if !V8_TARGET_ARCH_ARM && !V8_TARGET_ARCH_ARM64 && !V8_TARGET_ARCH_X64 && \
-    !V8_TARGET_ARCH_S390X && !V8_TARGET_ARCH_PPC64
+    !V8_TARGET_ARCH_S390X && !V8_TARGET_ARCH_PPC64 && !V8_TARGET_ARCH_SW64
 #undef DEFINE_PERF_PROF_BOOL
 #define DEFINE_PERF_PROF_BOOL(nam, cmt) DEFINE_BOOL_READONLY(nam, false, cmt)
 #undef DEFINE_PERF_PROF_IMPLICATION
diff --git a/src/3rdparty/chromium/v8/src/heap/base/asm/sw64/push_registers_asm.cc b/src/3rdparty/chromium/v8/src/heap/base/asm/sw64/push_registers_asm.cc
new file mode 100755
index 0000000000..79b0597b2c
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/heap/base/asm/sw64/push_registers_asm.cc
@@ -0,0 +1,46 @@
+// Copyright 2020 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+// Push all callee-saved registers to get them on the stack for conservative
+// stack scanning.
+//
+// See asm/x64/push_registers_clang.cc for why the function is not generated
+// using clang.
+//
+// Do not depend on V8_TARGET_OS_* defines as some embedders may override the
+// GN toolchain (e.g. ChromeOS) and not provide them.
+asm(".set noreorder                                      \n"
+    ".global PushAllRegistersAndIterateStack             \n"
+    ".type PushAllRegistersAndIterateStack, %function    \n"
+    ".hidden PushAllRegistersAndIterateStack             \n"
+    "PushAllRegistersAndIterateStack:                    \n"
+    // Push all callee-saved registers and save return address.
+    "  subl $30, 80, $30                                 \n"
+    "  stl $26, 72($30)                                  \n"
+    "  stl $15, 64($30)                                  \n"
+    "  stl $30, 56($30)                                  \n"
+    "  stl $29, 48($30)                                  \n"
+    "  stl $14, 40($30)                                  \n"
+    "  stl $13, 32($30)                                  \n"
+    "  stl $12, 24($30)                                  \n"
+    "  stl $11, 16($30)                                  \n"
+    "  stl $10, 8($30)                                   \n"
+    "  stl $9,  0($30)                                   \n"
+    // Maintain frame pointer.
+    "  bis $31, $30, $15                                 \n"
+    // Pass 1st parameter (a0) unchanged (Stack*).
+    // Pass 2nd parameter (a1) unchanged (StackVisitor*).
+    // Save 3rd parameter (a2; IterateStackCallback).
+    "  bis $31, $18, $19                                 \n"
+    // Pass 3rd parameter as sp (stack pointer).
+    "  bis $31, $30, $18                                 \n"
+    // Call the callback.
+    "  call $26, ($19), 0                                \n"
+    // Load return address.
+    "  ldl $26, 72($30)                                  \n"
+    // Restore frame pointer.
+    "  ldl $15, 64($30)                                  \n"
+    // Discard all callee-saved registers.
+    "  addl $30, 80, $30                                 \n"
+    "  ret $31, ($26), 0                                 \n");
diff --git a/src/3rdparty/chromium/v8/src/interpreter/interpreter-assembler.cc b/src/3rdparty/chromium/v8/src/interpreter/interpreter-assembler.cc
index e6fd97ddf2..e46e4adbf7 100644
--- a/src/3rdparty/chromium/v8/src/interpreter/interpreter-assembler.cc
+++ b/src/3rdparty/chromium/v8/src/interpreter/interpreter-assembler.cc
@@ -1352,7 +1352,7 @@ bool InterpreterAssembler::TargetSupportsUnalignedAccess() {
   return false;
 #elif V8_TARGET_ARCH_IA32 || V8_TARGET_ARCH_X64 || V8_TARGET_ARCH_S390 || \
     V8_TARGET_ARCH_ARM || V8_TARGET_ARCH_ARM64 || V8_TARGET_ARCH_PPC ||   \
-    V8_TARGET_ARCH_PPC64
+    V8_TARGET_ARCH_PPC64 || V8_TARGET_ARCH_SW64
   return true;
 #else
 #error "Unknown Architecture"
diff --git a/src/3rdparty/chromium/v8/src/libsampler/sampler.cc b/src/3rdparty/chromium/v8/src/libsampler/sampler.cc
index 9631d2f478..8e30ed67b7 100644
--- a/src/3rdparty/chromium/v8/src/libsampler/sampler.cc
+++ b/src/3rdparty/chromium/v8/src/libsampler/sampler.cc
@@ -415,6 +415,10 @@ void SignalHandler::FillRegisterState(void* context, RegisterState* state) {
   state->pc = reinterpret_cast<void*>(mcontext.pc);
   state->sp = reinterpret_cast<void*>(mcontext.gregs[29]);
   state->fp = reinterpret_cast<void*>(mcontext.gregs[30]);
+#elif V8_HOST_ARCH_SW64
+  state->pc = reinterpret_cast<void*>(mcontext.sc_pc);
+  state->sp = reinterpret_cast<void*>(mcontext.sc_regs[30]);
+  state->fp = reinterpret_cast<void*>(mcontext.sc_regs[15]);
 #elif V8_HOST_ARCH_PPC || V8_HOST_ARCH_PPC64
 #if V8_LIBC_GLIBC
   state->pc = reinterpret_cast<void*>(ucontext->uc_mcontext.regs->nip);
diff --git a/src/3rdparty/chromium/v8/src/logging/log.cc b/src/3rdparty/chromium/v8/src/logging/log.cc
index efd7c2b5f3..c8333dc007 100644
--- a/src/3rdparty/chromium/v8/src/logging/log.cc
+++ b/src/3rdparty/chromium/v8/src/logging/log.cc
@@ -594,6 +594,8 @@ void LowLevelLogger::LogCodeInfo() {
   const char arch[] = "arm64";
 #elif V8_TARGET_ARCH_S390
   const char arch[] = "s390";
+#elif V8_TARGET_ARCH_SW64
+  const char arch[] = "sw_64";
 #else
   const char arch[] = "unknown";
 #endif
diff --git a/src/3rdparty/chromium/v8/src/objects/code.cc b/src/3rdparty/chromium/v8/src/objects/code.cc
index c796904718..3ad42ecd11 100644
--- a/src/3rdparty/chromium/v8/src/objects/code.cc
+++ b/src/3rdparty/chromium/v8/src/objects/code.cc
@@ -234,7 +234,7 @@ bool Code::IsIsolateIndependent(Isolate* isolate) {
                  RelocInfo::ModeMask(RelocInfo::WASM_STUB_CALL)));
 
 #if defined(V8_TARGET_ARCH_PPC) || defined(V8_TARGET_ARCH_PPC64) || \
-    defined(V8_TARGET_ARCH_MIPS64)
+    defined(V8_TARGET_ARCH_MIPS64) || defined(V8_TARGET_ARCH_SW64)
   return RelocIterator(*this, kModeMask).done();
 #elif defined(V8_TARGET_ARCH_X64) || defined(V8_TARGET_ARCH_ARM64) || \
     defined(V8_TARGET_ARCH_ARM) || defined(V8_TARGET_ARCH_MIPS) ||    \
diff --git a/src/3rdparty/chromium/v8/src/objects/code.h b/src/3rdparty/chromium/v8/src/objects/code.h
index d71a0b1132..83618c306d 100644
--- a/src/3rdparty/chromium/v8/src/objects/code.h
+++ b/src/3rdparty/chromium/v8/src/objects/code.h
@@ -461,6 +461,8 @@ class Code : public HeapObject {
                                          : (COMPRESS_POINTERS_BOOL ? 16 : 28);
 #elif V8_TARGET_ARCH_S390X
   static constexpr int kHeaderPaddingSize = COMPRESS_POINTERS_BOOL ? 16 : 28;
+#elif V8_TARGET_ARCH_SW64
+  static constexpr int kHeaderPaddingSize = 28;
 #else
 #error Unknown architecture.
 #endif
diff --git a/src/3rdparty/chromium/v8/src/profiler/tick-sample.cc b/src/3rdparty/chromium/v8/src/profiler/tick-sample.cc
index 45fc911856..8f18dfa773 100644
--- a/src/3rdparty/chromium/v8/src/profiler/tick-sample.cc
+++ b/src/3rdparty/chromium/v8/src/profiler/tick-sample.cc
@@ -118,6 +118,12 @@ bool SimulatorHelper::FillRegisters(Isolate* isolate,
   state->sp = reinterpret_cast<void*>(simulator->get_register(Simulator::sp));
   state->fp = reinterpret_cast<void*>(simulator->get_register(Simulator::fp));
   state->lr = reinterpret_cast<void*>(simulator->get_lr());
+#elif V8_TARGET_ARCH_SW64
+  if (!simulator->has_bad_pc()) {
+    state->pc = reinterpret_cast<void*>(simulator->get_pc());
+  }
+  state->sp = reinterpret_cast<void*>(simulator->get_register(Simulator::sp));
+  state->fp = reinterpret_cast<void*>(simulator->get_register(Simulator::fp));
 #elif V8_TARGET_ARCH_S390
   if (!simulator->has_bad_pc()) {
     state->pc = reinterpret_cast<void*>(simulator->get_pc());
diff --git a/src/3rdparty/chromium/v8/src/regexp/regexp-macro-assembler-arch.h b/src/3rdparty/chromium/v8/src/regexp/regexp-macro-assembler-arch.h
index 8ec12a0ae6..32500421d2 100644
--- a/src/3rdparty/chromium/v8/src/regexp/regexp-macro-assembler-arch.h
+++ b/src/3rdparty/chromium/v8/src/regexp/regexp-macro-assembler-arch.h
@@ -23,6 +23,8 @@
 #include "src/regexp/mips64/regexp-macro-assembler-mips64.h"
 #elif V8_TARGET_ARCH_S390
 #include "src/regexp/s390/regexp-macro-assembler-s390.h"
+#elif V8_TARGET_ARCH_SW64
+#include "src/regexp/sw64/regexp-macro-assembler-sw64.h"
 #else
 #error Unsupported target architecture.
 #endif
diff --git a/src/3rdparty/chromium/v8/src/regexp/regexp-macro-assembler.h b/src/3rdparty/chromium/v8/src/regexp/regexp-macro-assembler.h
index 52465610cb..8f14fd218f 100644
--- a/src/3rdparty/chromium/v8/src/regexp/regexp-macro-assembler.h
+++ b/src/3rdparty/chromium/v8/src/regexp/regexp-macro-assembler.h
@@ -48,6 +48,7 @@ class RegExpMacroAssembler {
     kPPCImplementation,
     kX64Implementation,
     kX87Implementation,
+    kSW64Implementation,
     kBytecodeImplementation
   };
 
diff --git a/src/3rdparty/chromium/v8/src/regexp/regexp.cc b/src/3rdparty/chromium/v8/src/regexp/regexp.cc
index 569acdab48..72c59f5030 100644
--- a/src/3rdparty/chromium/v8/src/regexp/regexp.cc
+++ b/src/3rdparty/chromium/v8/src/regexp/regexp.cc
@@ -815,6 +815,9 @@ bool RegExpImpl::Compile(Isolate* isolate, Zone* zone, RegExpCompileData* data,
 #elif V8_TARGET_ARCH_MIPS64
     macro_assembler.reset(new RegExpMacroAssemblerMIPS(isolate, zone, mode,
                                                        output_register_count));
+#elif V8_TARGET_ARCH_SW64
+    macro_assembler.reset(new RegExpMacroAssemblerSW64(isolate, zone, mode,
+                                                       output_register_count));
 #else
 #error "Unsupported architecture"
 #endif
diff --git a/src/3rdparty/chromium/v8/src/regexp/sw64/OWNERS b/src/3rdparty/chromium/v8/src/regexp/sw64/OWNERS
new file mode 100755
index 0000000000..42582e993a
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/regexp/sw64/OWNERS
@@ -0,0 +1,3 @@
+ivica.bogosavljevic@sw64.com
+Miran.Karic@sw64.com
+sreten.kovacevic@sw64.com
diff --git a/src/3rdparty/chromium/v8/src/regexp/sw64/regexp-macro-assembler-sw64.cc b/src/3rdparty/chromium/v8/src/regexp/sw64/regexp-macro-assembler-sw64.cc
new file mode 100755
index 0000000000..7561532f3c
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/regexp/sw64/regexp-macro-assembler-sw64.cc
@@ -0,0 +1,1370 @@
+// Copyright 2012 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#if V8_TARGET_ARCH_SW64
+
+#include "src/regexp/sw64/regexp-macro-assembler-sw64.h"
+
+#include "src/codegen/assembler-inl.h"
+#include "src/codegen/macro-assembler.h"
+#include "src/logging/log.h"
+#include "src/objects/objects-inl.h"
+#include "src/regexp/regexp-macro-assembler.h"
+#include "src/regexp/regexp-stack.h"
+#include "src/snapshot/embedded/embedded-data.h"
+#include "src/strings/unicode.h"
+
+namespace v8 {
+namespace internal {
+
+/* clang-format off
+ *
+ * This assembler uses the following register assignment convention
+ * - t3 : Temporarily stores the index of capture start after a matching pass
+ *        for a global regexp.
+ * - a5 : Pointer to current Code object including heap object tag.
+ * - t9 : Current position in input, as negative offset from end of string.
+ *        Please notice that this is the byte offset, not the character offset!
+ * - t10: Currently loaded character. Must be loaded using
+ *        LoadCurrentCharacter before using any of the dispatch methods.
+ * - t0 : Points to tip of backtrack stack
+ * - t1 : Unused.
+ * - t2 : End of input (points to byte after last character in input).
+ * - fp : Frame pointer. Used to access arguments, local variables and
+ *         RegExp registers.
+ * - sp : Points to tip of C stack.
+ *
+ * The remaining registers are free for computations.
+ * Each call to a public method should retain this convention.
+ *
+ * TODO(plind): O32 documented here with intent of having single 32/64 codebase
+ *              in the future.
+ *
+ * The O32 stack will have the following structure:
+ *
+ *  - fp[72]  Isolate* isolate   (address of the current isolate)
+ *  - fp[68]  direct_call  (if 1, direct call from JavaScript code,
+ *                          if 0, call through the runtime system).
+ *  - fp[64]  stack_area_base (High end of the memory area to use as
+ *                             backtracking stack).
+ *  - fp[60]  capture array size (may fit multiple sets of matches)
+ *  - fp[44..59]  SW64 O32 four argument slots
+ *  - fp[40]  int* capture_array (int[num_saved_registers_], for output).
+ *  --- sp when called ---
+ *  - fp[36]  return address      (lr).
+ *  - fp[32]  old frame pointer   (r11).
+ *  - fp[0..31]  backup of registers s0..s7.
+ *  --- frame pointer ----
+ *  - fp[-4]  end of input       (address of end of string).
+ *  - fp[-8]  start of input     (address of first character in string).
+ *  - fp[-12] start index        (character index of start).
+ *  - fp[-16] void* input_string (location of a handle containing the string).
+ *  - fp[-20] success counter    (only for global regexps to count matches).
+ *  - fp[-24] Offset of location before start of input (effectively character
+ *            string start - 1). Used to initialize capture registers to a
+ *            non-position.
+ *  - fp[-28] At start (if 1, we are starting at the start of the
+ *    string, otherwise 0)
+ *  - fp[-32] register 0         (Only positions must be stored in the first
+ *  -         register 1          num_saved_registers_ registers)
+ *  -         ...
+ *  -         register num_registers-1
+ *  --- sp ---
+ *
+ *
+ * The N64 stack will have the following structure:
+ *
+ *  - fp[80]  Isolate* isolate   (address of the current isolate)               kIsolate
+ *                                                                              kStackFrameHeader
+ *  --- sp when called ---
+ *  - fp[72]  ra                 Return from RegExp code (ra).                  kReturnAddress
+ *  - fp[64]  s9, old-fp         Old fp, callee saved(s9).
+ *  - fp[0..63]  s0..s7          Callee-saved registers s0..s7.
+ *  --- frame pointer ----
+ *  - fp[-8]  direct_call        (1 = direct call from JS, 0 = from runtime)    kDirectCall
+ *  - fp[-16] stack_base         (Top of backtracking stack).                   kStackHighEnd
+ *  - fp[-24] capture array size (may fit multiple sets of matches)             kNumOutputRegisters
+ *  - fp[-32] int* capture_array (int[num_saved_registers_], for output).       kRegisterOutput
+ *  - fp[-40] end of input       (address of end of string).                    kInputEnd
+ *  - fp[-48] start of input     (address of first character in string).        kInputStart
+ *  - fp[-56] start index        (character index of start).                    kStartIndex
+ *  - fp[-64] void* input_string (location of a handle containing the string).  kInputString
+ *  - fp[-72] success counter    (only for global regexps to count matches).    kSuccessfulCaptures
+ *  - fp[-80] Offset of location before start of input (effectively character   kStringStartMinusOne
+ *            position -1). Used to initialize capture registers to a
+ *            non-position.
+ *  --------- The following output registers are 32-bit values. ---------
+ *  - fp[-88] register 0         (Only positions must be stored in the first    kRegisterZero
+ *  -         register 1          num_saved_registers_ registers)
+ *  -         ...
+ *  -         register num_registers-1
+ *  --- sp ---
+ *
+ *
+ * The sw64 stack will have the following structure:
+ *
+ *  - fp[80]  Isolate* isolate   (address of the current isolate)               kIsolate
+ *  - fp[72]  direct_call        (1 = direct call from JS, 0 = from runtime)    kDirectCall
+ *  - fp[64]  stack_base         (Top of backtracking stack).                   kStackHighEnd
+ *                                                                              kStackFrameHeader
+ *  --- sp when called ---
+ *  - fp[56]  ra                 Return from RegExp code (ra).                  kReturnAddress
+ *  - fp[48]  fp                 fp, callee saved(s6).
+ *  - fp[0..47]  s0..s5          Callee-saved registers s0..s5.
+ *  --- frame pointer ----
+ *  - fp[-8]  capture array size (may fit multiple sets of matches)             kNumOutputRegisters
+ *  - fp[-16] int* capture_array (int[num_saved_registers_], for output).       kRegisterOutput
+ *  - fp[-24] end of input       (address of end of string).                    kInputEnd
+ *  - fp[-32] start of input     (address of first character in string).        kInputStart
+ *  - fp[-40] start index        (character index of start).                    kStartIndex
+ *  - fp[-48] void* input_string (location of a handle containing the string).  kInputString
+ *  - fp[-56] success counter    (only for global regexps to count matches).    kSuccessfulCaptures
+ *  - fp[-64] Offset of location before start of input (effectively character   kInputStartMinusOne
+ *            position -1). Used to initialize capture registers to a
+ *            non-position.
+ *  --------- The following output registers are 32-bit values. ---------
+ *  - fp[-72] register 0         (Only positions must be stored in the first    kRegisterZero
+ *  -         register 1          num_saved_registers_ registers)
+ *  -         ...
+ *  -         register num_registers-1
+ *  --- sp ---
+ *
+ * The first num_saved_registers_ registers are initialized to point to
+ * "character -1" in the string (i.e., char_size() bytes before the first
+ * character of the string). The remaining registers start out as garbage.
+ *
+ * The data up to the return address must be placed there by the calling
+ * code and the remaining arguments are passed in registers, e.g. by calling the
+ * code entry as cast to a function with the signature:
+ * int (*match)(String input_string,
+ *              int start_index,
+ *              Address start,
+ *              Address end,
+ *              int* capture_output_array,
+ *              int num_capture_registers,
+ *              byte* stack_area_base,
+ *              bool direct_call = false,
+ *              Isolate* isolate);
+ * The call is performed by NativeRegExpMacroAssembler::Execute()
+ * (in regexp-macro-assembler.cc) via the GeneratedCode wrapper.
+ *
+ * clang-format on
+ */
+
+#define __ ACCESS_MASM(masm_)
+
+const int RegExpMacroAssemblerSW64::kRegExpCodeSize;
+
+RegExpMacroAssemblerSW64::RegExpMacroAssemblerSW64(Isolate* isolate, Zone* zone,
+                                                   Mode mode,
+                                                   int registers_to_save)
+    : NativeRegExpMacroAssembler(isolate, zone),
+      masm_(new MacroAssembler(isolate, CodeObjectRequired::kYes,
+                               NewAssemblerBuffer(kRegExpCodeSize))),
+      mode_(mode),
+      num_registers_(registers_to_save),
+      num_saved_registers_(registers_to_save),
+      entry_label_(),
+      start_label_(),
+      success_label_(),
+      backtrack_label_(),
+      exit_label_(),
+      internal_failure_label_() {
+  masm_->set_root_array_available(false);
+
+  DCHECK_EQ(0, registers_to_save % 2);
+  __ jmp(&entry_label_);   // We'll write the entry code later.
+  // If the code gets too big or corrupted, an internal exception will be
+  // raised, and we will exit right away.
+  __ bind(&internal_failure_label_);
+  __ li(v0, Operand(FAILURE));
+  __ Ret();
+  __ bind(&start_label_);  // And then continue from here.
+}
+
+RegExpMacroAssemblerSW64::~RegExpMacroAssemblerSW64() {
+  delete masm_;
+  // Unuse labels in case we throw away the assembler without calling GetCode.
+  entry_label_.Unuse();
+  start_label_.Unuse();
+  success_label_.Unuse();
+  backtrack_label_.Unuse();
+  exit_label_.Unuse();
+  check_preempt_label_.Unuse();
+  stack_overflow_label_.Unuse();
+  internal_failure_label_.Unuse();
+}
+
+
+int RegExpMacroAssemblerSW64::stack_limit_slack()  {
+  return RegExpStack::kStackLimitSlack;
+}
+
+
+void RegExpMacroAssemblerSW64::AdvanceCurrentPosition(int by) {
+  if (by != 0) {
+    __ Addl(current_input_offset(),
+            current_input_offset(), Operand(by * char_size()));
+  }
+}
+
+
+void RegExpMacroAssemblerSW64::AdvanceRegister(int reg, int by) {
+  DCHECK_LE(0, reg);
+  DCHECK_GT(num_registers_, reg);
+  if (by != 0) {
+    __ Ldl(a0, register_location(reg));
+    __ Addl(a0, a0, Operand(by));
+    __ Stl(a0, register_location(reg));
+  }
+}
+
+
+void RegExpMacroAssemblerSW64::Backtrack() {
+  CheckPreemption();
+  if (has_backtrack_limit()) {
+    Label next;
+    __ Ldl(a0, MemOperand(frame_pointer(), kBacktrackCount));
+    __ Addl(a0, a0, Operand(1));
+    __ Stl(a0, MemOperand(frame_pointer(), kBacktrackCount));
+    __ Branch(&next, ne, a0, Operand(backtrack_limit()));
+
+    // Exceeded limits are treated as a failed match.
+      Fail();
+
+    __ bind(&next);
+  }
+  // Pop Code offset from backtrack stack, add Code and jump to location.
+  Pop(a0);
+  __ Addl(a0, a0, code_pointer());
+  __ Jump(a0);
+}
+
+
+void RegExpMacroAssemblerSW64::Bind(Label* label) {
+  __ bind(label);
+}
+
+
+void RegExpMacroAssemblerSW64::CheckCharacter(uint32_t c, Label* on_equal) {
+  BranchOrBacktrack(on_equal, eq, current_character(), Operand(c));
+}
+
+
+void RegExpMacroAssemblerSW64::CheckCharacterGT(uc16 limit, Label* on_greater) {
+  BranchOrBacktrack(on_greater, gt, current_character(), Operand(limit));
+}
+
+
+void RegExpMacroAssemblerSW64::CheckAtStart(int cp_offset, Label* on_at_start) {
+  __ Ldl(a1, MemOperand(frame_pointer(), kStringStartMinusOne));
+  __ Addl(a0, current_input_offset(),
+           Operand(-char_size() + cp_offset * char_size()));
+  BranchOrBacktrack(on_at_start, eq, a0, Operand(a1));
+}
+
+
+void RegExpMacroAssemblerSW64::CheckNotAtStart(int cp_offset,
+                                               Label* on_not_at_start) {
+  __ Ldl(a1, MemOperand(frame_pointer(), kStringStartMinusOne));
+  __ Addl(a0, current_input_offset(),
+           Operand(-char_size() + cp_offset * char_size()));
+  BranchOrBacktrack(on_not_at_start, ne, a0, Operand(a1));
+}
+
+
+void RegExpMacroAssemblerSW64::CheckCharacterLT(uc16 limit, Label* on_less) {
+  BranchOrBacktrack(on_less, lt, current_character(), Operand(limit));
+}
+
+
+void RegExpMacroAssemblerSW64::CheckGreedyLoop(Label* on_equal) {
+  Label backtrack_non_equal;
+  __ Ldw(a0, MemOperand(backtrack_stackpointer(), 0));
+  __ Branch(&backtrack_non_equal, ne, current_input_offset(), Operand(a0));
+  __ Addl(backtrack_stackpointer(),
+          backtrack_stackpointer(),
+          Operand(kIntSize));
+  __ bind(&backtrack_non_equal);
+  BranchOrBacktrack(on_equal, eq, current_input_offset(), Operand(a0));
+}
+
+
+void RegExpMacroAssemblerSW64::CheckNotBackReferenceIgnoreCase(
+    int start_reg, bool read_backward, bool unicode, Label* on_no_match) {
+  Label fallthrough;
+  __ Ldl(a0, register_location(start_reg));      // Index of start of capture.
+  __ Ldl(a1, register_location(start_reg + 1));  // Index of end of capture.
+  __ Subl(a1, a1, a0);  // Length of capture.
+
+  // At this point, the capture registers are either both set or both cleared.
+  // If the capture length is zero, then the capture is either empty or cleared.
+  // Fall through in both cases.
+  __ Branch(&fallthrough, eq, a1, Operand(zero_reg));
+
+  if (read_backward) {
+    __ Ldl(t1, MemOperand(frame_pointer(), kStringStartMinusOne));
+    __ Addl(t1, t1, a1);
+    BranchOrBacktrack(on_no_match, le, current_input_offset(), Operand(t1));
+  } else {
+    __ Addl(t1, a1, current_input_offset());
+    // Check that there are enough characters left in the input.
+    BranchOrBacktrack(on_no_match, gt, t1, Operand(zero_reg));
+  }
+
+  if (mode_ == LATIN1) {
+    Label success;
+    Label fail;
+    Label loop_check;
+
+    // a0 - offset of start of capture.
+    // a1 - length of capture.
+    __ Addl(a0, a0, Operand(end_of_input_address()));
+    __ Addl(a2, end_of_input_address(), Operand(current_input_offset()));
+    if (read_backward) {
+      __ Subl(a2, a2, Operand(a1));
+    }
+    __ Addl(a1, a0, Operand(a1));
+
+    // a0 - Address of start of capture.
+    // a1 - Address of end of capture.
+    // a2 - Address of current input position.
+
+    Label loop;
+    __ bind(&loop);
+    __ Ldbu(a3, MemOperand(a0, 0));
+    __ addl(a0, char_size(), a0);
+    __ Ldbu(a4, MemOperand(a2, 0));
+    __ addl(a2, char_size(), a2);
+
+    __ Branch(&loop_check, eq, a4, Operand(a3));
+
+    // Mismatch, try case-insensitive match (converting letters to lower-case).
+    __ Or(a3, a3, Operand(0x20));  // Convert capture character to lower-case.
+    __ Or(a4, a4, Operand(0x20));  // Also convert input character.
+    __ Branch(&fail, ne, a4, Operand(a3));
+    __ Subl(a3, a3, Operand('a'));
+    __ Branch(&loop_check, ls, a3, Operand('z' - 'a'));
+    // Latin-1: Check for values in range [224,254] but not 247.
+    __ Subl(a3, a3, Operand(224 - 'a'));
+    // Weren't Latin-1 letters.
+    __ Branch(&fail, hi, a3, Operand(254 - 224));
+    // Check for 247.
+    __ Branch(&fail, eq, a3, Operand(247 - 224));
+
+    __ bind(&loop_check);
+    __ Branch(&loop, lt, a0, Operand(a1));
+    __ jmp(&success);
+
+    __ bind(&fail);
+    GoTo(on_no_match);
+
+    __ bind(&success);
+    // Compute new value of character position after the matched part.
+    __ Subl(current_input_offset(), a2, end_of_input_address());
+    if (read_backward) {
+      __ Ldl(t1, register_location(start_reg));  // Index of start of capture.
+      __ Ldl(a2, register_location(start_reg + 1));  // Index of end of capture.
+      __ Addl(current_input_offset(), current_input_offset(), Operand(t1));
+      __ Subl(current_input_offset(), current_input_offset(), Operand(a2));
+    }
+  } else {
+    DCHECK(mode_ == UC16);
+    // Put regexp engine registers on stack.
+    RegList regexp_registers_to_retain = current_input_offset().bit() |
+        current_character().bit() | backtrack_stackpointer().bit();
+    __ MultiPush(regexp_registers_to_retain);
+
+    int argument_count = 4;
+    __ PrepareCallCFunction(argument_count, a2);
+
+    // a0 - offset of start of capture.
+    // a1 - length of capture.
+
+    // Put arguments into arguments registers.
+    // Parameters are
+    //   a0: Address byte_offset1 - Address captured substring's start.
+    //   a1: Address byte_offset2 - Address of current character position.
+    //   a2: size_t byte_length - length of capture in bytes(!).
+    //   a3: Isolate* isolate.
+
+    // Address of start of capture.
+    __ Addl(a0, a0, Operand(end_of_input_address()));
+    // Length of capture.
+    __ mov(a2, a1);
+    // Save length in callee-save register for use on return.
+    __ mov(s3, a1);  // Can we use s3 here?
+    // Address of current input position.
+    __ Addl(a1, current_input_offset(), Operand(end_of_input_address()));
+    if (read_backward) {
+      __ Subl(a1, a1, Operand(s3));
+    }
+    // Isolate.
+      __ li(a3, Operand(ExternalReference::isolate_address(masm_->isolate())));
+
+    {
+      AllowExternalCallThatCantCauseGC scope(masm_);
+      ExternalReference function =
+          unicode ? ExternalReference::re_case_insensitive_compare_unicode(
+                        isolate())
+                  : ExternalReference::re_case_insensitive_compare_non_unicode(
+                        isolate());
+      __ CallCFunction(function, argument_count);
+    }
+
+    // Restore regexp engine registers.
+    __ MultiPop(regexp_registers_to_retain);
+    __ li(code_pointer(), Operand(masm_->CodeObject()), CONSTANT_SIZE);
+    __ Ldl(end_of_input_address(), MemOperand(frame_pointer(), kInputEnd));
+
+    // Check if function returned non-zero for success or zero for failure.
+    BranchOrBacktrack(on_no_match, eq, v0, Operand(zero_reg));
+    // On success, increment position by length of capture.
+    if (read_backward) {
+      __ Subl(current_input_offset(), current_input_offset(), Operand(s3));
+    } else {
+      __ Addl(current_input_offset(), current_input_offset(), Operand(s3));
+    }
+  }
+
+  __ bind(&fallthrough);
+}
+
+
+void RegExpMacroAssemblerSW64::CheckNotBackReference(int start_reg,
+                                                     bool read_backward,
+                                                     Label* on_no_match) {
+  Label fallthrough;
+
+  // Find length of back-referenced capture.
+  __ Ldl(a0, register_location(start_reg));
+  __ Ldl(a1, register_location(start_reg + 1));
+  __ Subl(a1, a1, a0);  // Length to check.
+
+  // At this point, the capture registers are either both set or both cleared.
+  // If the capture length is zero, then the capture is either empty or cleared.
+  // Fall through in both cases.
+  __ Branch(&fallthrough, eq, a1, Operand(zero_reg));
+
+  if (read_backward) {
+    __ Ldl(t1, MemOperand(frame_pointer(), kStringStartMinusOne));
+    __ Addl(t1, t1, a1);
+    BranchOrBacktrack(on_no_match, le, current_input_offset(), Operand(t1));
+  } else {
+    __ Addl(t1, a1, current_input_offset());
+    // Check that there are enough characters left in the input.
+    BranchOrBacktrack(on_no_match, gt, t1, Operand(zero_reg));
+  }
+
+  // Compute pointers to match string and capture string.
+  __ Addl(a0, a0, Operand(end_of_input_address()));
+  __ Addl(a2, end_of_input_address(), Operand(current_input_offset()));
+  if (read_backward) {
+    __ Subl(a2, a2, Operand(a1));
+  }
+  __ Addl(a1, a1, Operand(a0));
+
+  Label loop;
+  __ bind(&loop);
+  if (mode_ == LATIN1) {
+    __ Ldbu(a3, MemOperand(a0, 0));
+    __ addl(a0, char_size(), a0);
+    __ Ldbu(a4, MemOperand(a2, 0));
+    __ addl(a2, char_size(), a2);
+  } else {
+    DCHECK(mode_ == UC16);
+    __ Ldhu(a3, MemOperand(a0, 0));
+    __ addl(a0, char_size(), a0);
+    __ Ldhu(a4, MemOperand(a2, 0));
+    __ addl(a2, char_size(), a2);
+  }
+  BranchOrBacktrack(on_no_match, ne, a3, Operand(a4));
+  __ Branch(&loop, lt, a0, Operand(a1));
+
+  // Move current character position to position after match.
+  __ Subl(current_input_offset(), a2, end_of_input_address());
+  if (read_backward) {
+    __ Ldl(t1, register_location(start_reg));      // Index of start of capture.
+    __ Ldl(a2, register_location(start_reg + 1));  // Index of end of capture.
+    __ Addl(current_input_offset(), current_input_offset(), Operand(t1));
+    __ Subl(current_input_offset(), current_input_offset(), Operand(a2));
+  }
+  __ bind(&fallthrough);
+}
+
+
+void RegExpMacroAssemblerSW64::CheckNotCharacter(uint32_t c,
+                                                 Label* on_not_equal) {
+  BranchOrBacktrack(on_not_equal, ne, current_character(), Operand(c));
+}
+
+
+void RegExpMacroAssemblerSW64::CheckCharacterAfterAnd(uint32_t c,
+                                                      uint32_t mask,
+                                                      Label* on_equal) {
+  __ And(a0, current_character(), Operand(mask));
+  Operand rhs = (c == 0) ? Operand(zero_reg) : Operand(c);
+  BranchOrBacktrack(on_equal, eq, a0, rhs);
+}
+
+
+void RegExpMacroAssemblerSW64::CheckNotCharacterAfterAnd(uint32_t c,
+                                                         uint32_t mask,
+                                                         Label* on_not_equal) {
+  __ And(a0, current_character(), Operand(mask));
+  Operand rhs = (c == 0) ? Operand(zero_reg) : Operand(c);
+  BranchOrBacktrack(on_not_equal, ne, a0, rhs);
+}
+
+
+void RegExpMacroAssemblerSW64::CheckNotCharacterAfterMinusAnd(
+    uc16 c,
+    uc16 minus,
+    uc16 mask,
+    Label* on_not_equal) {
+  DCHECK_GT(String::kMaxUtf16CodeUnit, minus);
+  __ Subl(a0, current_character(), Operand(minus));
+  __ And(a0, a0, Operand(mask));
+  BranchOrBacktrack(on_not_equal, ne, a0, Operand(c));
+}
+
+
+void RegExpMacroAssemblerSW64::CheckCharacterInRange(
+    uc16 from,
+    uc16 to,
+    Label* on_in_range) {
+  __ Subl(a0, current_character(), Operand(from));
+  // Unsigned lower-or-same condition.
+  BranchOrBacktrack(on_in_range, ls, a0, Operand(to - from));
+}
+
+
+void RegExpMacroAssemblerSW64::CheckCharacterNotInRange(
+    uc16 from,
+    uc16 to,
+    Label* on_not_in_range) {
+  __ Subl(a0, current_character(), Operand(from));
+  // Unsigned higher condition.
+  BranchOrBacktrack(on_not_in_range, hi, a0, Operand(to - from));
+}
+
+
+void RegExpMacroAssemblerSW64::CheckBitInTable(
+    Handle<ByteArray> table,
+    Label* on_bit_set) {
+  __ li(a0, Operand(table));
+  if (mode_ != LATIN1 || kTableMask != String::kMaxOneByteCharCode) {
+    __ And(a1, current_character(), Operand(kTableSize - 1));
+    __ Addl(a0, a0, a1);
+  } else {
+    __ Addl(a0, a0, current_character());
+  }
+
+  __ Ldbu(a0, FieldMemOperand(a0, ByteArray::kHeaderSize));
+  BranchOrBacktrack(on_bit_set, ne, a0, Operand(zero_reg));
+}
+
+
+bool RegExpMacroAssemblerSW64::CheckSpecialCharacterClass(uc16 type,
+                                                          Label* on_no_match) {
+  // Range checks (c in min..max) are generally implemented by an unsigned
+  // (c - min) <= (max - min) check.
+  switch (type) {
+  case 's':
+    // Match space-characters.
+    if (mode_ == LATIN1) {
+      // One byte space characters are '\t'..'\r', ' ' and \u00a0.
+      Label success;
+      __ Branch(&success, eq, current_character(), Operand(' '));
+      // Check range 0x09..0x0D.
+      __ Subl(a0, current_character(), Operand('\t'));
+      __ Branch(&success, ls, a0, Operand('\r' - '\t'));
+      // \u00a0 (NBSP).
+      BranchOrBacktrack(on_no_match, ne, a0, Operand(0x00A0 - '\t'));
+      __ bind(&success);
+      return true;
+    }
+    return false;
+  case 'S':
+    // The emitted code for generic character classes is good enough.
+    return false;
+  case 'd':
+    // Match Latin1 digits ('0'..'9').
+    __ Subl(a0, current_character(), Operand('0'));
+    BranchOrBacktrack(on_no_match, hi, a0, Operand('9' - '0'));
+    return true;
+  case 'D':
+    // Match non Latin1-digits.
+    __ Subl(a0, current_character(), Operand('0'));
+    BranchOrBacktrack(on_no_match, ls, a0, Operand('9' - '0'));
+    return true;
+  case '.': {
+    // Match non-newlines (not 0x0A('\n'), 0x0D('\r'), 0x2028 and 0x2029).
+    __ Xor(a0, current_character(), Operand(0x01));
+    // See if current character is '\n'^1 or '\r'^1, i.e., 0x0B or 0x0C.
+    __ Subl(a0, a0, Operand(0x0B));
+    BranchOrBacktrack(on_no_match, ls, a0, Operand(0x0C - 0x0B));
+    if (mode_ == UC16) {
+      // Compare original value to 0x2028 and 0x2029, using the already
+      // computed (current_char ^ 0x01 - 0x0B). I.e., check for
+      // 0x201D (0x2028 - 0x0B) or 0x201E.
+      __ Subl(a0, a0, Operand(0x2028 - 0x0B));
+      BranchOrBacktrack(on_no_match, ls, a0, Operand(1));
+    }
+    return true;
+  }
+  case 'n': {
+    // Match newlines (0x0A('\n'), 0x0D('\r'), 0x2028 and 0x2029).
+    __ Xor(a0, current_character(), Operand(0x01));
+    // See if current character is '\n'^1 or '\r'^1, i.e., 0x0B or 0x0C.
+    __ Subl(a0, a0, Operand(0x0B));
+    if (mode_ == LATIN1) {
+      BranchOrBacktrack(on_no_match, hi, a0, Operand(0x0C - 0x0B));
+    } else {
+      Label done;
+      BranchOrBacktrack(&done, ls, a0, Operand(0x0C - 0x0B));
+      // Compare original value to 0x2028 and 0x2029, using the already
+      // computed (current_char ^ 0x01 - 0x0B). I.e., check for
+      // 0x201D (0x2028 - 0x0B) or 0x201E.
+      __ Subl(a0, a0, Operand(0x2028 - 0x0B));
+      BranchOrBacktrack(on_no_match, hi, a0, Operand(1));
+      __ bind(&done);
+    }
+    return true;
+  }
+  case 'w': {
+    if (mode_ != LATIN1) {
+      // Table is 256 entries, so all Latin1 characters can be tested.
+      BranchOrBacktrack(on_no_match, hi, current_character(), Operand('z'));
+    }
+    ExternalReference map = ExternalReference::re_word_character_map(isolate());
+    __ li(a0, Operand(map));
+    __ Addl(a0, a0, current_character());
+    __ Ldbu(a0, MemOperand(a0, 0));
+    BranchOrBacktrack(on_no_match, eq, a0, Operand(zero_reg));
+    return true;
+  }
+  case 'W': {
+    Label done;
+    if (mode_ != LATIN1) {
+      // Table is 256 entries, so all Latin1 characters can be tested.
+      __ Branch(&done, hi, current_character(), Operand('z'));
+    }
+    ExternalReference map = ExternalReference::re_word_character_map(isolate());
+    __ li(a0, Operand(map));
+    __ Addl(a0, a0, current_character());
+    __ Ldbu(a0, MemOperand(a0, 0));
+    BranchOrBacktrack(on_no_match, ne, a0, Operand(zero_reg));
+    if (mode_ != LATIN1) {
+      __ bind(&done);
+    }
+    return true;
+  }
+  case '*':
+    // Match any character.
+    return true;
+  // No custom implementation (yet): s(UC16), S(UC16).
+  default:
+    return false;
+  }
+}
+
+
+void RegExpMacroAssemblerSW64::Fail() {
+  __ li(v0, Operand(FAILURE));
+  __ jmp(&exit_label_);
+}
+
+
+Handle<HeapObject> RegExpMacroAssemblerSW64::GetCode(Handle<String> source) {
+  Label return_v0;
+  if (masm_->has_exception()) {
+    // If the code gets corrupted due to long regular expressions and lack of
+    // space on trampolines, an internal exception flag is set. If this case
+    // is detected, we will jump into exit sequence right away.
+    __ bind_to(&entry_label_, internal_failure_label_.pos());
+  } else {
+    // Finalize code - write the entry point code now we know how many
+    // registers we need.
+
+    // Entry code:
+    __ bind(&entry_label_);
+
+    // Tell the system that we have a stack frame.  Because the type is MANUAL,
+    // no is generated.
+    FrameScope scope(masm_, StackFrame::MANUAL);
+
+    // Actually emit code to start a new stack frame.
+    // Push arguments
+    // Save callee-save registers.
+    // Start new stack frame.
+    // Store link register in existing stack-cell.
+    // Order here should correspond to order of offset constants in header file.
+    // TODO(plind): we save s0..s5, but ONLY use s3 here - use the regs
+    // or dont save.
+    RegList registers_to_retain = s0.bit() | s1.bit() | s2.bit() |
+        s3.bit() | s4.bit() | s5.bit() | fp.bit();
+    RegList argument_registers = a0.bit() | a1.bit() | a2.bit() | a3.bit() |
+        a4.bit() | a5.bit();
+
+    __ MultiPush(registers_to_retain | ra.bit()); __ MultiPush(argument_registers);
+    // Set frame pointer in space for it if this is not a direct call
+    // from generated code.
+    __ Addl(frame_pointer(), sp, Operand(6 * kPointerSize));  // 6 argument regs
+    STATIC_ASSERT(kSuccessfulCaptures == kInputString - kSystemPointerSize);
+    __ mov(a0, zero_reg);
+    __ push(a0);  // Make room for success counter and initialize it to 0.
+    STATIC_ASSERT(kStringStartMinusOne ==
+                  kSuccessfulCaptures - kSystemPointerSize);
+    __ push(a0);  // Make room for "string start - 1" constant.
+    STATIC_ASSERT(kBacktrackCount == kStringStartMinusOne - kSystemPointerSize);
+    __ push(a0);  // The backtrack counter
+
+    // Check if we have space on the stack for registers.
+    Label stack_limit_hit;
+    Label stack_ok;
+
+    ExternalReference stack_limit =
+        ExternalReference::address_of_jslimit(masm_->isolate());
+    __ li(a0, Operand(stack_limit));
+    __ Ldl(a0, MemOperand(a0));
+    __ Subl(a0, sp, a0);
+    // Handle it if the stack pointer is already below the stack limit.
+    __ Branch(&stack_limit_hit, le, a0, Operand(zero_reg));
+    // Check if there is room for the variable number of registers above
+    // the stack limit.
+    __ Branch(&stack_ok, hs, a0, Operand(num_registers_ * kPointerSize));
+    // Exit with OutOfMemory exception. There is not enough space on the stack
+    // for our working registers.
+    __ li(v0, Operand(EXCEPTION));
+    __ jmp(&return_v0);
+
+    __ bind(&stack_limit_hit);
+    CallCheckStackGuardState(a0);
+    // If returned value is non-zero, we exit with the returned value as result.
+    __ Branch(&return_v0, ne, v0, Operand(zero_reg));
+
+    __ bind(&stack_ok);
+    // Allocate space on stack for registers.
+    __ Subl(sp, sp, Operand(num_registers_ * kPointerSize));
+    // Load string end.
+    __ Ldl(end_of_input_address(), MemOperand(frame_pointer(), kInputEnd));
+    // Load input start.
+    __ Ldl(a0, MemOperand(frame_pointer(), kInputStart));
+    // Find negative length (offset of start relative to end).
+    __ Subl(current_input_offset(), a0, end_of_input_address());
+    // Set a0 to address of char before start of the input string
+    // (effectively string position -1).
+    __ Ldl(a1, MemOperand(frame_pointer(), kStartIndex));
+    __ Subl(a0, current_input_offset(), Operand(char_size()));
+    __ slll(a1, (mode_ == UC16) ? 1 : 0, t1);
+    __ Subl(a0, a0, t1);
+    // Store this value in a local variable, for use when clearing
+    // position registers.
+    __ Stl(a0, MemOperand(frame_pointer(), kStringStartMinusOne));
+
+    // Initialize code pointer register
+    __ li(code_pointer(), Operand(masm_->CodeObject()), CONSTANT_SIZE);
+
+    Label load_char_start_regexp, start_regexp;
+    // Load newline if index is at start, previous character otherwise.
+    __ Branch(&load_char_start_regexp, ne, a1, Operand(zero_reg));
+    __ li(current_character(), Operand('\n'));
+    __ jmp(&start_regexp);
+
+    // Global regexp restarts matching here.
+    __ bind(&load_char_start_regexp);
+    // Load previous char as initial value of current character register.
+    LoadCurrentCharacterUnchecked(-1, 1);
+    __ bind(&start_regexp);
+
+    // Initialize on-stack registers.
+    if (num_saved_registers_ > 0) {  // Always is, if generated from a regexp.
+      // Fill saved registers with initial value = start offset - 1.
+      if (num_saved_registers_ > 8) {
+        // Address of register 0.
+        __ Addl(a1, frame_pointer(), Operand(kRegisterZero));
+        __ li(a2, Operand(num_saved_registers_));
+        Label init_loop;
+        __ bind(&init_loop);
+        __ Stl(a0, MemOperand(a1));
+        __ Addl(a1, a1, Operand(-kPointerSize));
+        __ Subl(a2, a2, Operand(1));
+        __ Branch(&init_loop, ne, a2, Operand(zero_reg));
+      } else {
+        for (int i = 0; i < num_saved_registers_; i++) {
+          __ Stl(a0, register_location(i));
+        }
+      }
+    }
+
+    // Initialize backtrack stack pointer.
+    __ Ldl(backtrack_stackpointer(), MemOperand(frame_pointer(), kStackHighEnd));
+
+    __ jmp(&start_label_);
+
+
+    // Exit code:
+    if (success_label_.is_linked()) {
+      // Save captures when successful.
+      __ bind(&success_label_);
+      if (num_saved_registers_ > 0) {
+        // Copy captures to output.
+        __ Ldl(a1, MemOperand(frame_pointer(), kInputStart));
+        __ Ldl(a0, MemOperand(frame_pointer(), kRegisterOutput));
+        __ Ldl(a2, MemOperand(frame_pointer(), kStartIndex));
+        __ Subl(a1, end_of_input_address(), a1);
+        // a1 is length of input in bytes.
+        if (mode_ == UC16) {
+          __ srll(a1, 1, a1);
+        }
+        // a1 is length of input in characters.
+        __ Addl(a1, a1, Operand(a2));
+        // a1 is length of string in characters.
+
+        DCHECK_EQ(0, num_saved_registers_ % 2);
+        // Always an even number of capture registers. This allows us to
+        // unroll the loop once to add an operation between a load of a register
+        // and the following use of that register.
+        for (int i = 0; i < num_saved_registers_; i += 2) {
+          __ Ldl(a2, register_location(i));
+          __ Ldl(a3, register_location(i + 1));
+          if (i == 0 && global_with_zero_length_check()) {
+            // Keep capture start in a4 for the zero-length check later.
+            __ mov(t3, a2);
+          }
+          if (mode_ == UC16) {
+            __ sral(a2, 1, a2);
+            __ Addl(a2, a2, a1);
+            __ sral(a3, 1, a3);
+            __ Addl(a3, a3, a1);
+          } else {
+            __ Addl(a2, a1, Operand(a2));
+            __ Addl(a3, a1, Operand(a3));
+          }
+          // V8 expects the output to be an int32_t array.
+          __ Stw(a2, MemOperand(a0));
+          __ Addl(a0, a0, kIntSize);
+          __ Stw(a3, MemOperand(a0));
+          __ Addl(a0, a0, kIntSize);
+        }
+      }
+
+      if (global()) {
+        // Restart matching if the regular expression is flagged as global.
+        __ Ldl(a0, MemOperand(frame_pointer(), kSuccessfulCaptures));
+        __ Ldl(a1, MemOperand(frame_pointer(), kNumOutputRegisters));
+        __ Ldl(a2, MemOperand(frame_pointer(), kRegisterOutput));
+        // Increment success counter.
+        __ Addl(a0, a0, 1);
+        __ Stl(a0, MemOperand(frame_pointer(), kSuccessfulCaptures));
+        // Capture results have been stored, so the number of remaining global
+        // output registers is reduced by the number of stored captures.
+        __ Subl(a1, a1, num_saved_registers_);
+        // Check whether we have enough room for another set of capture results.
+        __ mov(v0, a0);
+        __ Branch(&return_v0, lt, a1, Operand(num_saved_registers_));
+
+        __ Stl(a1, MemOperand(frame_pointer(), kNumOutputRegisters));
+        // Advance the location for output.
+        __ Addl(a2, a2, num_saved_registers_ * kIntSize);
+        __ Stl(a2, MemOperand(frame_pointer(), kRegisterOutput));
+
+        // Prepare a0 to initialize registers with its value in the next run.
+        __ Ldl(a0, MemOperand(frame_pointer(), kStringStartMinusOne));
+
+        if (global_with_zero_length_check()) {
+          // Special case for zero-length matches.
+          // t3: capture start index
+          // Not a zero-length match, restart.
+          __ Branch(
+              &load_char_start_regexp, ne, current_input_offset(), Operand(t3));
+          // Offset from the end is zero if we already reached the end.
+          __ Branch(&exit_label_, eq, current_input_offset(),
+                    Operand(zero_reg));
+          // Advance current position after a zero-length match.
+          Label advance;
+          __ bind(&advance);
+          __ Addl(current_input_offset(),
+                  current_input_offset(),
+                  Operand((mode_ == UC16) ? 2 : 1));
+          if (global_unicode()) CheckNotInSurrogatePair(0, &advance);
+        }
+
+        __ Branch(&load_char_start_regexp);
+      } else {
+        __ li(v0, Operand(SUCCESS));
+      }
+    }
+    // Exit and return v0.
+    __ bind(&exit_label_);
+    if (global()) {
+      __ Ldl(v0, MemOperand(frame_pointer(), kSuccessfulCaptures));
+    }
+
+    __ bind(&return_v0);
+    // Skip sp past regexp registers and local variables..
+    __ mov(sp, frame_pointer());
+    // Restore registers s0..s7 and return (restoring ra to pc).
+    __ MultiPop(registers_to_retain | ra.bit());
+    __ Ret();
+
+    // Backtrack code (branch target for conditional backtracks).
+    if (backtrack_label_.is_linked()) {
+      __ bind(&backtrack_label_);
+      Backtrack();
+    }
+
+    Label exit_with_exception;
+
+    // Preempt-code.
+    if (check_preempt_label_.is_linked()) {
+      SafeCallTarget(&check_preempt_label_);
+      // Put regexp engine registers on stack.
+      RegList regexp_registers_to_retain = current_input_offset().bit() |
+          current_character().bit() | backtrack_stackpointer().bit();
+      __ MultiPush(regexp_registers_to_retain);
+      CallCheckStackGuardState(a0);
+      __ MultiPop(regexp_registers_to_retain);
+      // If returning non-zero, we should end execution with the given
+      // result as return value.
+      __ Branch(&return_v0, ne, v0, Operand(zero_reg));
+
+      // String might have moved: Reload end of string from frame.
+      __ Ldl(end_of_input_address(), MemOperand(frame_pointer(), kInputEnd));
+      __ li(code_pointer(), Operand(masm_->CodeObject()), CONSTANT_SIZE);
+      SafeReturn();
+    }
+
+    // Backtrack stack overflow code.
+    if (stack_overflow_label_.is_linked()) {
+      SafeCallTarget(&stack_overflow_label_);
+      // Reached if the backtrack-stack limit has been hit.
+      // Put regexp engine registers on stack first.
+      RegList regexp_registers = current_input_offset().bit() |
+          current_character().bit();
+      __ MultiPush(regexp_registers);
+
+      // Call GrowStack(backtrack_stackpointer(), &stack_base)
+      static const int num_arguments = 3;
+      __ PrepareCallCFunction(num_arguments, a0);
+      __ mov(a0, backtrack_stackpointer());
+      __ Addl(a1, frame_pointer(), Operand(kStackHighEnd));
+      __ li(a2, Operand(ExternalReference::isolate_address(masm_->isolate())));
+      ExternalReference grow_stack =
+          ExternalReference::re_grow_stack(masm_->isolate());
+      __ CallCFunction(grow_stack, num_arguments);
+      // Restore regexp registers.
+      __ MultiPop(regexp_registers);
+      // If return nullptr, we have failed to grow the stack, and
+      // must exit with a stack-overflow exception.
+      __ Branch(&exit_with_exception, eq, v0, Operand(zero_reg));
+      // Otherwise use return value as new stack pointer.
+      __ mov(backtrack_stackpointer(), v0);
+      // Restore saved registers and continue.
+      __ li(code_pointer(), Operand(masm_->CodeObject()), CONSTANT_SIZE);
+      __ Ldl(end_of_input_address(), MemOperand(frame_pointer(), kInputEnd));
+      SafeReturn();
+    }
+
+    if (exit_with_exception.is_linked()) {
+      // If any of the code above needed to exit with an exception.
+      __ bind(&exit_with_exception);
+      // Exit with Result EXCEPTION(-1) to signal thrown exception.
+      __ li(v0, Operand(EXCEPTION));
+      __ jmp(&return_v0);
+    }
+  }
+
+  CodeDesc code_desc;
+  masm_->GetCode(isolate(), &code_desc);
+  Handle<Code> code =
+      Factory::CodeBuilder(isolate(), code_desc, CodeKind::REGEXP)
+          .set_self_reference(masm_->CodeObject())
+          .Build();
+  LOG(masm_->isolate(),
+      RegExpCodeCreateEvent(Handle<AbstractCode>::cast(code), source));
+  return Handle<HeapObject>::cast(code);
+}
+
+
+void RegExpMacroAssemblerSW64::GoTo(Label* to) {
+  if (to == nullptr) {
+    Backtrack();
+    return;
+  }
+  __ jmp(to);
+  return;
+}
+
+
+void RegExpMacroAssemblerSW64::IfRegisterGE(int reg,
+                                            int comparand,
+                                            Label* if_ge) {
+  __ Ldl(a0, register_location(reg));
+  BranchOrBacktrack(if_ge, ge, a0, Operand(comparand));
+}
+
+
+void RegExpMacroAssemblerSW64::IfRegisterLT(int reg,
+                                            int comparand,
+                                            Label* if_lt) {
+  __ Ldl(a0, register_location(reg));
+  BranchOrBacktrack(if_lt, lt, a0, Operand(comparand));
+}
+
+
+void RegExpMacroAssemblerSW64::IfRegisterEqPos(int reg,
+                                               Label* if_eq) {
+  __ Ldl(a0, register_location(reg));
+  BranchOrBacktrack(if_eq, eq, a0, Operand(current_input_offset()));
+}
+
+
+RegExpMacroAssembler::IrregexpImplementation
+    RegExpMacroAssemblerSW64::Implementation() {
+  return kSW64Implementation;
+}
+
+
+void RegExpMacroAssemblerSW64::PopCurrentPosition() {
+  Pop(current_input_offset());
+}
+
+
+void RegExpMacroAssemblerSW64::PopRegister(int register_index) {
+  Pop(a0);
+  __ Stl(a0, register_location(register_index));
+}
+
+
+void RegExpMacroAssemblerSW64::PushBacktrack(Label* label) {
+  if (label->is_bound()) {
+    int target = label->pos();
+    __ li(a0, Operand(target + Code::kHeaderSize - kHeapObjectTag));
+  } else {
+    Assembler::BlockTrampolinePoolScope block_trampoline_pool(masm_);
+    Label after_constant;
+    __ Branch(&after_constant);
+    int offset = masm_->pc_offset();
+    int cp_offset = offset + Code::kHeaderSize - kHeapObjectTag;
+    __ emitSW(0);
+    masm_->label_at_put(label, offset);
+    __ bind(&after_constant);
+    if (is_int16(cp_offset)) {
+      __ Ldwu(a0, MemOperand(code_pointer(), cp_offset));
+    } else {
+      __ Addl(a0, code_pointer(), cp_offset);
+      __ Ldwu(a0, MemOperand(a0, 0));
+    }
+  }
+  Push(a0);
+  CheckStackLimit();
+}
+
+
+void RegExpMacroAssemblerSW64::PushCurrentPosition() {
+  Push(current_input_offset());
+}
+
+
+void RegExpMacroAssemblerSW64::PushRegister(int register_index,
+                                            StackCheckFlag check_stack_limit) {
+  __ Ldl(a0, register_location(register_index));
+  Push(a0);
+  if (check_stack_limit) CheckStackLimit();
+}
+
+
+void RegExpMacroAssemblerSW64::ReadCurrentPositionFromRegister(int reg) {
+  __ Ldl(current_input_offset(), register_location(reg));
+}
+
+
+void RegExpMacroAssemblerSW64::ReadStackPointerFromRegister(int reg) {
+  __ Ldl(backtrack_stackpointer(), register_location(reg));
+  __ Ldl(a0, MemOperand(frame_pointer(), kStackHighEnd));
+  __ Addl(backtrack_stackpointer(), backtrack_stackpointer(), Operand(a0));
+}
+
+
+void RegExpMacroAssemblerSW64::SetCurrentPositionFromEnd(int by) {
+  Label after_position;
+  __ Branch(&after_position,
+            ge,
+            current_input_offset(),
+            Operand(-by * char_size()));
+  __ li(current_input_offset(), -by * char_size());
+  // On RegExp code entry (where this operation is used), the character before
+  // the current position is expected to be already loaded.
+  // We have advanced the position, so it's safe to read backwards.
+  LoadCurrentCharacterUnchecked(-1, 1);
+  __ bind(&after_position);
+}
+
+
+void RegExpMacroAssemblerSW64::SetRegister(int register_index, int to) {
+  DCHECK(register_index >= num_saved_registers_);  // Reserved for positions!
+  __ li(a0, Operand(to));
+  __ Stl(a0, register_location(register_index));
+}
+
+
+bool RegExpMacroAssemblerSW64::Succeed() {
+  __ jmp(&success_label_);
+  return global();
+}
+
+
+void RegExpMacroAssemblerSW64::WriteCurrentPositionToRegister(int reg,
+                                                              int cp_offset) {
+  if (cp_offset == 0) {
+    __ Stl(current_input_offset(), register_location(reg));
+  } else {
+    __ Addl(a0, current_input_offset(), Operand(cp_offset * char_size()));
+    __ Stl(a0, register_location(reg));
+  }
+}
+
+
+void RegExpMacroAssemblerSW64::ClearRegisters(int reg_from, int reg_to) {
+  DCHECK(reg_from <= reg_to);
+  __ Ldl(a0, MemOperand(frame_pointer(), kStringStartMinusOne));
+  for (int reg = reg_from; reg <= reg_to; reg++) {
+    __ Stl(a0, register_location(reg));
+  }
+}
+
+
+void RegExpMacroAssemblerSW64::WriteStackPointerToRegister(int reg) {
+  __ Ldl(a1, MemOperand(frame_pointer(), kStackHighEnd));
+  __ Subl(a0, backtrack_stackpointer(), a1);
+  __ Stl(a0, register_location(reg));
+}
+
+
+bool RegExpMacroAssemblerSW64::CanReadUnaligned() {
+  return false;
+}
+
+
+// Private methods:
+
+void RegExpMacroAssemblerSW64::CallCheckStackGuardState(Register scratch) {
+  DCHECK(!isolate()->IsGeneratingEmbeddedBuiltins());
+  DCHECK(!masm_->options().isolate_independent_code);
+
+  int stack_alignment = base::OS::ActivationFrameAlignment();
+
+  // Align the stack pointer and save the original sp value on the stack.
+  __ mov(scratch, sp);
+  __ Subl(sp, sp, Operand(kPointerSize));
+  DCHECK(base::bits::IsPowerOfTwo(stack_alignment));
+  __ And(sp, sp, Operand(-stack_alignment));
+  __ Stl(scratch, MemOperand(sp));
+
+  __ mov(a2, frame_pointer());
+  // Code of self.
+  __ li(a1, Operand(masm_->CodeObject()), CONSTANT_SIZE);
+
+  // We need to make room for the return address on the stack.
+  DCHECK(IsAligned(stack_alignment, kPointerSize));
+  __ Subl(sp, sp, Operand(stack_alignment));
+
+  // The stack pointer now points to cell where the return address will be
+  // written. Arguments are in registers, meaning we treat the return address as
+  // argument 5. Since DirectCEntry will handle allocating space for the C
+  // argument slots, we don't need to care about that here. This is how the
+  // stack will look (sp meaning the value of sp at this moment):
+  // [sp + 3] - empty slot if needed for alignment.
+  // [sp + 2] - saved sp.
+  // [sp + 1] - second word reserved for return value.
+  // [sp + 0] - first word reserved for return value.
+
+  // a0 will point to the return address, placed by DirectCEntry.
+  __ mov(a0, sp);
+
+  ExternalReference stack_guard_check =
+      ExternalReference::re_check_stack_guard_state(masm_->isolate());
+  __ li(t12, Operand(stack_guard_check));
+
+    EmbeddedData d = EmbeddedData::FromBlob();
+    CHECK(Builtins::IsIsolateIndependent(Builtins::kDirectCEntry));
+    Address entry = d.InstructionStartOfBuiltin(Builtins::kDirectCEntry);
+    __ li(kScratchReg, Operand(entry, RelocInfo::OFF_HEAP_TARGET));
+    __ Call(kScratchReg);
+
+  // DirectCEntryStub allocated space for the C argument slots so we have to
+  // drop them with the return address from the stack with loading saved sp.
+  // At this point stack must look:
+  // [sp + 7] - empty slot if needed for alignment.
+  // [sp + 6] - saved sp.
+  // [sp + 5] - second word reserved for return value.
+  // [sp + 4] - first word reserved for return value.
+  // [sp + 3] - C argument slot.
+  // [sp + 2] - C argument slot.
+  // [sp + 1] - C argument slot.
+  // [sp + 0] - C argument slot.
+  __ Ldl(sp, MemOperand(sp, stack_alignment + kCArgsSlotsSize));
+
+  __ li(code_pointer(), Operand(masm_->CodeObject()));
+}
+
+
+// Helper function for reading a value out of a stack frame.
+template <typename T>
+static T& frame_entry(Address re_frame, int frame_offset) {
+  return reinterpret_cast<T&>(Memory<int32_t>(re_frame + frame_offset));
+}
+
+
+template <typename T>
+static T* frame_entry_address(Address re_frame, int frame_offset) {
+  return reinterpret_cast<T*>(re_frame + frame_offset);
+}
+
+int64_t RegExpMacroAssemblerSW64::CheckStackGuardState(Address* return_address,
+                                                       Address raw_code,
+                                                       Address re_frame) {
+  Code re_code = Code::cast(Object(raw_code));
+  return NativeRegExpMacroAssembler::CheckStackGuardState(
+      frame_entry<Isolate*>(re_frame, kIsolate),
+      static_cast<int>(frame_entry<int64_t>(re_frame, kStartIndex)),
+      static_cast<RegExp::CallOrigin>(
+          frame_entry<int64_t>(re_frame, kDirectCall)),
+      return_address, re_code,
+      frame_entry_address<Address>(re_frame, kInputString),
+      frame_entry_address<const byte*>(re_frame, kInputStart),
+      frame_entry_address<const byte*>(re_frame, kInputEnd));
+}
+
+
+MemOperand RegExpMacroAssemblerSW64::register_location(int register_index) {
+  DCHECK(register_index < (1<<30));
+  if (num_registers_ <= register_index) {
+    num_registers_ = register_index + 1;
+  }
+  return MemOperand(frame_pointer(),
+                    kRegisterZero - register_index * kPointerSize);
+}
+
+
+void RegExpMacroAssemblerSW64::CheckPosition(int cp_offset,
+                                             Label* on_outside_input) {
+  if (cp_offset >= 0) {
+    BranchOrBacktrack(on_outside_input, ge, current_input_offset(),
+                      Operand(-cp_offset * char_size()));
+  } else {
+    __ Ldl(a1, MemOperand(frame_pointer(), kStringStartMinusOne));
+    __ Addl(a0, current_input_offset(), Operand(cp_offset * char_size()));
+    BranchOrBacktrack(on_outside_input, le, a0, Operand(a1));
+  }
+}
+
+
+void RegExpMacroAssemblerSW64::BranchOrBacktrack(Label* to,
+                                                 Condition condition,
+                                                 Register rs,
+                                                 const Operand& rt) {
+  if (condition == al) {  // Unconditional.
+    if (to == nullptr) {
+      Backtrack();
+      return;
+    }
+    __ jmp(to);
+    return;
+  }
+  if (to == nullptr) {
+    __ Branch(&backtrack_label_, condition, rs, rt);
+    return;
+  }
+  __ Branch(to, condition, rs, rt);
+}
+
+
+void RegExpMacroAssemblerSW64::SafeCall(Label* to,
+                                        Condition cond,
+                                        Register rs,
+                                        const Operand& rt) {
+  __ BranchAndLink(to, cond, rs, rt);
+}
+
+
+void RegExpMacroAssemblerSW64::SafeReturn() {
+  __ pop(ra);
+  __ Addl(t1, ra, Operand(masm_->CodeObject()));
+  __ Jump(t1);
+}
+
+
+void RegExpMacroAssemblerSW64::SafeCallTarget(Label* name) {
+  __ bind(name);
+  __ Subl(ra, ra, Operand(masm_->CodeObject()));
+  __ push(ra);
+}
+
+
+void RegExpMacroAssemblerSW64::Push(Register source) {
+  DCHECK(source != backtrack_stackpointer());
+  __ Addl(backtrack_stackpointer(),
+          backtrack_stackpointer(),
+          Operand(-kIntSize));
+  __ Stw(source, MemOperand(backtrack_stackpointer()));
+}
+
+
+void RegExpMacroAssemblerSW64::Pop(Register target) {
+  DCHECK(target != backtrack_stackpointer());
+  __ Ldw(target, MemOperand(backtrack_stackpointer()));
+  __ Addl(backtrack_stackpointer(), backtrack_stackpointer(), kIntSize);
+}
+
+
+void RegExpMacroAssemblerSW64::CheckPreemption() {
+  // Check for preemption.
+  ExternalReference stack_limit =
+      ExternalReference::address_of_jslimit(masm_->isolate());
+  __ li(a0, Operand(stack_limit));
+  __ Ldl(a0, MemOperand(a0));
+  SafeCall(&check_preempt_label_, ls, sp, Operand(a0));
+}
+
+
+void RegExpMacroAssemblerSW64::CheckStackLimit() {
+  ExternalReference stack_limit =
+      ExternalReference::address_of_regexp_stack_limit_address(
+          masm_->isolate());
+
+  __ li(a0, Operand(stack_limit));
+  __ Ldl(a0, MemOperand(a0));
+  SafeCall(&stack_overflow_label_, ls, backtrack_stackpointer(), Operand(a0));
+}
+
+
+void RegExpMacroAssemblerSW64::LoadCurrentCharacterUnchecked(int cp_offset,
+                                                             int characters) {
+  Register offset = current_input_offset();
+  if (cp_offset != 0) {
+    // t3 is not being used to store the capture start index at this point.
+    __ Addl(t3, current_input_offset(), Operand(cp_offset * char_size()));
+    offset = t3;
+  }
+  // We assume that we cannot do unaligned loads on SW64, so this function
+  // must only be used to load a single character at a time.
+  DCHECK_EQ(1, characters);
+  __ Addl(t1, end_of_input_address(), Operand(offset));
+  if (mode_ == LATIN1) {
+    __ Ldbu(current_character(), MemOperand(t1, 0));
+  } else {
+    DCHECK(mode_ == UC16);
+    __ Ldhu(current_character(), MemOperand(t1, 0));
+  }
+}
+
+#undef __
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_TARGET_ARCH_SW64
diff --git a/src/3rdparty/chromium/v8/src/regexp/sw64/regexp-macro-assembler-sw64.h b/src/3rdparty/chromium/v8/src/regexp/sw64/regexp-macro-assembler-sw64.h
new file mode 100755
index 0000000000..ab4c62db6e
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/regexp/sw64/regexp-macro-assembler-sw64.h
@@ -0,0 +1,224 @@
+// Copyright 2011 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef V8_REGEXP_SW64_REGEXP_MACRO_ASSEMBLER_SW64_H_
+#define V8_REGEXP_SW64_REGEXP_MACRO_ASSEMBLER_SW64_H_
+
+#include "src/codegen/macro-assembler.h"
+#include "src/codegen/sw64/assembler-sw64.h"
+#include "src/regexp/regexp-macro-assembler.h"
+
+namespace v8 {
+namespace internal {
+
+class V8_EXPORT_PRIVATE RegExpMacroAssemblerSW64
+    : public NativeRegExpMacroAssembler {
+ public:
+  RegExpMacroAssemblerSW64(Isolate* isolate, Zone* zone, Mode mode,
+                           int registers_to_save);
+  virtual ~RegExpMacroAssemblerSW64();
+  virtual int stack_limit_slack();
+  virtual void AdvanceCurrentPosition(int by);
+  virtual void AdvanceRegister(int reg, int by);
+  virtual void Backtrack();
+  virtual void Bind(Label* label);
+  virtual void CheckAtStart(int cp_offset, Label* on_at_start);
+  virtual void CheckCharacter(uint32_t c, Label* on_equal);
+  virtual void CheckCharacterAfterAnd(uint32_t c,
+                                      uint32_t mask,
+                                      Label* on_equal);
+  virtual void CheckCharacterGT(uc16 limit, Label* on_greater);
+  virtual void CheckCharacterLT(uc16 limit, Label* on_less);
+  // A "greedy loop" is a loop that is both greedy and with a simple
+  // body. It has a particularly simple implementation.
+  virtual void CheckGreedyLoop(Label* on_tos_equals_current_position);
+  virtual void CheckNotAtStart(int cp_offset, Label* on_not_at_start);
+  virtual void CheckNotBackReference(int start_reg, bool read_backward,
+                                     Label* on_no_match);
+  virtual void CheckNotBackReferenceIgnoreCase(int start_reg,
+                                               bool read_backward, bool unicode,
+                                               Label* on_no_match);
+  virtual void CheckNotCharacter(uint32_t c, Label* on_not_equal);
+  virtual void CheckNotCharacterAfterAnd(uint32_t c,
+                                         uint32_t mask,
+                                         Label* on_not_equal);
+  virtual void CheckNotCharacterAfterMinusAnd(uc16 c,
+                                              uc16 minus,
+                                              uc16 mask,
+                                              Label* on_not_equal);
+  virtual void CheckCharacterInRange(uc16 from,
+                                     uc16 to,
+                                     Label* on_in_range);
+  virtual void CheckCharacterNotInRange(uc16 from,
+                                        uc16 to,
+                                        Label* on_not_in_range);
+  virtual void CheckBitInTable(Handle<ByteArray> table, Label* on_bit_set);
+
+  // Checks whether the given offset from the current position is before
+  // the end of the string.
+  virtual void CheckPosition(int cp_offset, Label* on_outside_input);
+  virtual bool CheckSpecialCharacterClass(uc16 type,
+                                          Label* on_no_match);
+  virtual void Fail();
+  virtual Handle<HeapObject> GetCode(Handle<String> source);
+  virtual void GoTo(Label* label);
+  virtual void IfRegisterGE(int reg, int comparand, Label* if_ge);
+  virtual void IfRegisterLT(int reg, int comparand, Label* if_lt);
+  virtual void IfRegisterEqPos(int reg, Label* if_eq);
+  virtual IrregexpImplementation Implementation();
+  virtual void LoadCurrentCharacterUnchecked(int cp_offset,
+                                             int character_count);
+  virtual void PopCurrentPosition();
+  virtual void PopRegister(int register_index);
+  virtual void PushBacktrack(Label* label);
+  virtual void PushCurrentPosition();
+  virtual void PushRegister(int register_index,
+                            StackCheckFlag check_stack_limit);
+  virtual void ReadCurrentPositionFromRegister(int reg);
+  virtual void ReadStackPointerFromRegister(int reg);
+  virtual void SetCurrentPositionFromEnd(int by);
+  virtual void SetRegister(int register_index, int to);
+  virtual bool Succeed();
+  virtual void WriteCurrentPositionToRegister(int reg, int cp_offset);
+  virtual void ClearRegisters(int reg_from, int reg_to);
+  virtual void WriteStackPointerToRegister(int reg);
+  virtual bool CanReadUnaligned();
+
+  // Called from RegExp if the stack-guard is triggered.
+  // If the code object is relocated, the return address is fixed before
+  // returning.
+  // {raw_code} is an Address because this is called via ExternalReference.
+  static int64_t CheckStackGuardState(Address* return_address, Address raw_code,
+                                      Address re_frame);
+
+  void print_regexp_frame_constants();
+
+ private:
+  // Offsets from frame_pointer() of function parameters and stored registers.
+  static const int kFramePointer = 0;
+
+  // Above the frame pointer - Stored registers and stack passed parameters.
+  // Registers s0 to s5, fp, and ra.
+  static const int kStoredRegisters = kFramePointer;
+  // Return address (stored from link register, read into pc on return).
+
+  // TODO(plind): This 7 - is 6 s-regs (s0..s5) plus fp.
+  static const int kReturnAddress = kStoredRegisters + 7 * kPointerSize;
+
+  // Stack frame header.
+  static const int kStackFrameHeader = kReturnAddress;
+  // Stack parameters placed by caller.
+  static const int kStackHighEnd = kStackFrameHeader + kPointerSize;
+  static const int kDirectCall = kStackHighEnd + kPointerSize;
+  static const int kIsolate = kDirectCall + kPointerSize;
+
+  // Below the frame pointer.
+  // Register parameters stored by setup code.
+  static const int kNumOutputRegisters = kFramePointer - kPointerSize;
+  static const int kRegisterOutput = kNumOutputRegisters - kPointerSize;
+  static const int kInputEnd = kRegisterOutput - kPointerSize;
+  static const int kInputStart = kInputEnd - kPointerSize;
+  static const int kStartIndex = kInputStart - kPointerSize;
+  static const int kInputString = kStartIndex - kPointerSize;
+  // When adding local variables remember to push space for them in
+  // the frame in GetCode.
+  static const int kSuccessfulCaptures = kInputString - kPointerSize;
+  static const int kStringStartMinusOne = kSuccessfulCaptures - kPointerSize;
+  static const int kBacktrackCount = kStringStartMinusOne - kSystemPointerSize;
+  // First register address. Following registers are below it on the stack.
+  static const int kRegisterZero = kBacktrackCount - kSystemPointerSize;
+
+  // Initial size of code buffer.
+  static const int kRegExpCodeSize = 1024;
+
+  // Check whether preemption has been requested.
+  void CheckPreemption();
+
+  // Check whether we are exceeding the stack limit on the backtrack stack.
+  void CheckStackLimit();
+
+
+  // Generate a call to CheckStackGuardState.
+  void CallCheckStackGuardState(Register scratch);
+
+  // The ebp-relative location of a regexp register.
+  MemOperand register_location(int register_index);
+
+  // Register holding the current input position as negative offset from
+  // the end of the string.
+  inline Register current_input_offset() { return t9; }
+
+  // The register containing the current character after LoadCurrentCharacter.
+  inline Register current_character() { return t10; }
+
+  // Register holding address of the end of the input string.
+  inline Register end_of_input_address() { return t2; }
+
+  // Register holding the frame address. Local variables, parameters and
+  // regexp registers are addressed relative to this.
+  inline Register frame_pointer() { return fp; }
+
+  // The register containing the backtrack stack top. Provides a meaningful
+  // name to the register.
+  inline Register backtrack_stackpointer() { return t0; }
+
+  // Register holding pointer to the current code object.
+  inline Register code_pointer() { return a5; }
+
+  // Byte size of chars in the string to match (decided by the Mode argument).
+  inline int char_size() { return static_cast<int>(mode_); }
+
+  // Equivalent to a conditional branch to the label, unless the label
+  // is nullptr, in which case it is a conditional Backtrack.
+  void BranchOrBacktrack(Label* to,
+                         Condition condition,
+                         Register rs,
+                         const Operand& rt);
+
+  // Call and return internally in the generated code in a way that
+  // is GC-safe (i.e., doesn't leave absolute code addresses on the stack)
+  inline void SafeCall(Label* to,
+                       Condition cond,
+                       Register rs,
+                       const Operand& rt);
+  inline void SafeReturn();
+  inline void SafeCallTarget(Label* name);
+
+  // Pushes the value of a register on the backtrack stack. Decrements the
+  // stack pointer by a word size and stores the register's value there.
+  inline void Push(Register source);
+
+  // Pops a value from the backtrack stack. Reads the word at the stack pointer
+  // and increments it by a word size.
+  inline void Pop(Register target);
+
+  Isolate* isolate() const { return masm_->isolate(); }
+
+  MacroAssembler* masm_;
+
+  // Which mode to generate code for (Latin1 or UC16).
+  Mode mode_;
+
+  // One greater than maximal register index actually used.
+  int num_registers_;
+
+  // Number of registers to output at the end (the saved registers
+  // are always 0..num_saved_registers_-1).
+  int num_saved_registers_;
+
+  // Labels used internally.
+  Label entry_label_;
+  Label start_label_;
+  Label success_label_;
+  Label backtrack_label_;
+  Label exit_label_;
+  Label check_preempt_label_;
+  Label stack_overflow_label_;
+  Label internal_failure_label_;
+};
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_REGEXP_SW64_REGEXP_MACRO_ASSEMBLER_SW64_H_
diff --git a/src/3rdparty/chromium/v8/src/runtime/runtime-atomics.cc b/src/3rdparty/chromium/v8/src/runtime/runtime-atomics.cc
index 2b17ad52b1..5a2aad049a 100644
--- a/src/3rdparty/chromium/v8/src/runtime/runtime-atomics.cc
+++ b/src/3rdparty/chromium/v8/src/runtime/runtime-atomics.cc
@@ -19,7 +19,7 @@ namespace internal {
 
 // Other platforms have CSA support, see builtins-sharedarraybuffer-gen.h.
 #if V8_TARGET_ARCH_MIPS || V8_TARGET_ARCH_MIPS64 || V8_TARGET_ARCH_PPC64 || \
-    V8_TARGET_ARCH_PPC || V8_TARGET_ARCH_S390 || V8_TARGET_ARCH_S390X
+    V8_TARGET_ARCH_PPC || V8_TARGET_ARCH_S390 || V8_TARGET_ARCH_S390X || V8_TARGET_ARCH_SW64
 
 namespace {
 
diff --git a/src/3rdparty/chromium/v8/src/runtime/runtime-utils.h b/src/3rdparty/chromium/v8/src/runtime/runtime-utils.h
index 170c0bcdbc..fa60252092 100644
--- a/src/3rdparty/chromium/v8/src/runtime/runtime-utils.h
+++ b/src/3rdparty/chromium/v8/src/runtime/runtime-utils.h
@@ -119,10 +119,22 @@ namespace internal {
 // In Win64 calling convention, a struct of two pointers is returned in memory,
 // allocated by the caller, and passed as a pointer in a hidden first parameter.
 #ifdef V8_HOST_ARCH_64_BIT
+#ifdef SW64
+typedef Address ObjectPair;
+#else
 struct ObjectPair {
   Address x;
   Address y;
 };
+#endif
+
+#ifdef SW64
+static inline ObjectPair MakePair(Object x, Object y) {
+  // mov a1 to a5
+  __asm__ __volatile__ ("bis %0,$31,$21\n\t"::"r"(y.ptr()));
+  return x.ptr();
+}
+#else
 
 static inline ObjectPair MakePair(Object x, Object y) {
   ObjectPair result = {x.ptr(), y.ptr()};
@@ -130,6 +142,7 @@ static inline ObjectPair MakePair(Object x, Object y) {
   // In Win64 they are assigned to a hidden first argument.
   return result;
 }
+#endif
 #else
 using ObjectPair = uint64_t;
 static inline ObjectPair MakePair(Object x, Object y) {
diff --git a/src/3rdparty/chromium/v8/src/snapshot/deserializer.h b/src/3rdparty/chromium/v8/src/snapshot/deserializer.h
index ae410bacd3..178d9f2b58 100644
--- a/src/3rdparty/chromium/v8/src/snapshot/deserializer.h
+++ b/src/3rdparty/chromium/v8/src/snapshot/deserializer.h
@@ -31,7 +31,7 @@ class Object;
 // of objects found in code.
 #if defined(V8_TARGET_ARCH_MIPS) || defined(V8_TARGET_ARCH_MIPS64) || \
     defined(V8_TARGET_ARCH_PPC) || defined(V8_TARGET_ARCH_S390) ||    \
-    defined(V8_TARGET_ARCH_PPC64) || V8_EMBEDDED_CONSTANT_POOL
+    defined(V8_TARGET_ARCH_PPC64) || V8_EMBEDDED_CONSTANT_POOL || defined(V8_TARGET_ARCH_SW64)
 #define V8_CODE_EMBEDS_OBJECT_POINTER 1
 #else
 #define V8_CODE_EMBEDS_OBJECT_POINTER 0
diff --git a/src/3rdparty/chromium/v8/src/snapshot/embedded/platform-embedded-file-writer-generic.cc b/src/3rdparty/chromium/v8/src/snapshot/embedded/platform-embedded-file-writer-generic.cc
index 070aaf51b6..6b3f01ba7f 100644
--- a/src/3rdparty/chromium/v8/src/snapshot/embedded/platform-embedded-file-writer-generic.cc
+++ b/src/3rdparty/chromium/v8/src/snapshot/embedded/platform-embedded-file-writer-generic.cc
@@ -151,7 +151,7 @@ int PlatformEmbeddedFileWriterGeneric::IndentedDataDirective(
 
 DataDirective PlatformEmbeddedFileWriterGeneric::ByteChunkDataDirective()
     const {
-#if defined(V8_TARGET_ARCH_MIPS) || defined(V8_TARGET_ARCH_MIPS64)
+#if defined(V8_TARGET_ARCH_MIPS) || defined(V8_TARGET_ARCH_MIPS64)|| defined(V8_TARGET_ARCH_SW64)
   // MIPS uses a fixed 4 byte instruction set, using .long
   // to prevent any unnecessary padding.
   return kLong;
diff --git a/src/3rdparty/chromium/v8/src/wasm/baseline/liftoff-assembler-defs.h b/src/3rdparty/chromium/v8/src/wasm/baseline/liftoff-assembler-defs.h
index 781fb87dbc..4770d6acfe 100644
--- a/src/3rdparty/chromium/v8/src/wasm/baseline/liftoff-assembler-defs.h
+++ b/src/3rdparty/chromium/v8/src/wasm/baseline/liftoff-assembler-defs.h
@@ -46,6 +46,22 @@ constexpr RegList kLiftoffAssemblerGpCacheRegs =
 constexpr RegList kLiftoffAssemblerFpCacheRegs = DoubleRegister::ListOf(
     f0, f2, f4, f6, f8, f10, f12, f14, f16, f18, f20, f22, f24, f26);
 
+#elif V8_TARGET_ARCH_SW64
+
+// t7, t8 used as two scratch regs instead of s3, s4, so delete t7, t8;
+// t5, t6 same to at, t4 used as v1.
+constexpr RegList kLiftoffAssemblerGpCacheRegs =
+    Register::ListOf(a0, a1, a2, a3, a4, a5, t0, t1, t2, t3, t4,
+                     t9, t10, s5, v0);
+
+// f30 used as kDoubleCompareReg;
+// f27, f28, f29 used as kScratchDoubleReg*;
+constexpr RegList kLiftoffAssemblerFpCacheRegs =
+    DoubleRegister::ListOf(f0, f1, f2, f3, f4, f5, f6, f7,
+                           f8, f9, f10, f11, f12, f13, f14, f15,
+                           f16, f17, f18, f19, f20, f21,
+                           f22, f23, f24, f25, f26);
+
 #elif V8_TARGET_ARCH_ARM
 
 // r7: cp, r10: root, r11: fp, r12: ip, r13: sp, r14: lr, r15: pc.
@@ -103,6 +119,19 @@ constexpr Condition kUnsignedLessEqual = ule;
 constexpr Condition kUnsignedGreaterThan = ugt;
 constexpr Condition kUnsignedGreaterEqual = uge;
 
+#elif V8_TARGET_ARCH_SW64
+
+constexpr Condition kEqual = eq;
+constexpr Condition kUnequal = ne;
+constexpr Condition kSignedLessThan = lt;
+constexpr Condition kSignedLessEqual = le;
+constexpr Condition kSignedGreaterThan = gt;
+constexpr Condition kSignedGreaterEqual = ge;
+constexpr Condition kUnsignedLessThan = ult;
+constexpr Condition kUnsignedLessEqual = ule;
+constexpr Condition kUnsignedGreaterThan = ugt;
+constexpr Condition kUnsignedGreaterEqual = uge;
+
 #elif V8_TARGET_ARCH_ARM || V8_TARGET_ARCH_ARM64
 
 constexpr Condition kEqual = eq;
diff --git a/src/3rdparty/chromium/v8/src/wasm/baseline/liftoff-assembler.h b/src/3rdparty/chromium/v8/src/wasm/baseline/liftoff-assembler.h
index e2bd99841f..e2d937644b 100644
--- a/src/3rdparty/chromium/v8/src/wasm/baseline/liftoff-assembler.h
+++ b/src/3rdparty/chromium/v8/src/wasm/baseline/liftoff-assembler.h
@@ -1353,6 +1353,8 @@ class LiftoffStackSlots {
 #include "src/wasm/baseline/mips64/liftoff-assembler-mips64.h"
 #elif V8_TARGET_ARCH_S390
 #include "src/wasm/baseline/s390/liftoff-assembler-s390.h"
+#elif V8_TARGET_ARCH_SW64
+#include "src/wasm/baseline/sw64/liftoff-assembler-sw64.h"
 #else
 #error Unsupported architecture.
 #endif
diff --git a/src/3rdparty/chromium/v8/src/wasm/baseline/sw64/liftoff-assembler-sw64.h b/src/3rdparty/chromium/v8/src/wasm/baseline/sw64/liftoff-assembler-sw64.h
new file mode 100755
index 0000000000..76e48d0a62
--- /dev/null
+++ b/src/3rdparty/chromium/v8/src/wasm/baseline/sw64/liftoff-assembler-sw64.h
@@ -0,0 +1,2951 @@
+// Copyright 2017 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef V8_WASM_BASELINE_SW64_LIFTOFF_ASSEMBLER_SW64_H_
+#define V8_WASM_BASELINE_SW64_LIFTOFF_ASSEMBLER_SW64_H_
+
+#include "src/wasm/baseline/liftoff-assembler.h"
+
+namespace v8 {
+namespace internal {
+namespace wasm {
+
+namespace liftoff {
+
+// Liftoff Frames.
+//
+//  slot      Frame
+//       +--------------------+---------------------------
+//  n+4  | optional padding slot to keep the stack 16 byte aligned.
+//  n+3  |   parameter n      |
+//  ...  |       ...          |
+//   4   |   parameter 1      | or parameter 2
+//   3   |   parameter 0      | or parameter 1
+//   2   |  (result address)  | or parameter 0
+//  -----+--------------------+---------------------------
+//   1   | return addr (ra)   |
+//   0   | previous frame (fp)|
+//  -----+--------------------+  <-- frame ptr (fp)
+//  -1   | 0xa: WASM          |
+//  -2   |     instance       |
+//  -----+--------------------+---------------------------
+//  -3   |     slot 0         |   ^
+//  -4   |     slot 1         |   |
+//       |                    | Frame slots
+//       |                    |   |
+//       |                    |   v
+//       | optional padding slot to keep the stack 16 byte aligned.
+//  -----+--------------------+  <-- stack ptr (sp)
+//
+
+// fp-8 holds the stack marker, fp-16 is the instance parameter.
+constexpr int kInstanceOffset = 16;
+
+inline MemOperand GetStackSlot(int offset) { return MemOperand(fp, -offset); }
+
+inline MemOperand GetInstanceOperand() { return GetStackSlot(kInstanceOffset); }
+
+inline MemOperand GetMemOp(LiftoffAssembler* assm, Register addr,
+                           Register offset, uint32_t offset_imm) {
+  if (is_uint31(offset_imm)) {
+    if (offset == no_reg) return MemOperand(addr, offset_imm);
+    assm->addl(addr, offset, kScratchReg);
+    return MemOperand(kScratchReg, offset_imm);
+  }
+  // Offset immediate does not fit in 31 bits.
+  assm->li(kScratchReg, offset_imm);
+  assm->addl(kScratchReg, addr, kScratchReg);
+  if (offset != no_reg) {
+    assm->addl(kScratchReg, offset, kScratchReg);
+  }
+  return MemOperand(kScratchReg, 0);
+}
+
+inline void Load(LiftoffAssembler* assm, LiftoffRegister dst, MemOperand src,
+                 ValueType type) {
+  switch (type.kind()) {
+    case ValueType::kI32:
+      assm->Ldw(dst.gp(), src);
+      break;
+    case ValueType::kI64:
+    case ValueType::kRef:
+    case ValueType::kOptRef:
+      assm->Ldl(dst.gp(), src);
+      break;
+    case ValueType::kF32:
+      assm->Flds(dst.fp(), src);
+      break;
+    case ValueType::kF64:
+      assm->Fldd(dst.fp(), src);
+      break;
+//    case ValueType::kS128:
+//      assm->ld_b(dst.fp().toW(), src);
+//      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+inline void Store(LiftoffAssembler* assm, Register base, int32_t offset,
+                  LiftoffRegister src, ValueType type) {
+  MemOperand dst(base, offset);
+  switch (type.kind()) {
+    case ValueType::kI32:
+      assm->Ustw(src.gp(), dst);
+      break;
+    case ValueType::kI64:
+      assm->Ustl(src.gp(), dst);
+      break;
+    case ValueType::kF32:
+      assm->Ufsts(src.fp(), dst, t11);
+      break;
+    case ValueType::kF64:
+      assm->Ufstd(src.fp(), dst, t11);
+      break;
+//    case ValueType::kS128:
+//      assm->st_b(src.fp().toW(), dst);
+//      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+inline void push(LiftoffAssembler* assm, LiftoffRegister reg, ValueType type) {
+  switch (type.kind()) {
+    case ValueType::kI32:
+      assm->subl(sp, kSystemPointerSize, sp);
+      assm->Stw(reg.gp(), MemOperand(sp, 0));
+      break;
+    case ValueType::kI64:
+      assm->push(reg.gp());
+      break;
+    case ValueType::kF32:
+      assm->subl(sp, kSystemPointerSize, sp);
+      assm->Fsts(reg.fp(), MemOperand(sp, 0));
+      break;
+    case ValueType::kF64:
+      assm->subl(sp, kSystemPointerSize, sp);
+      assm->Fstd(reg.fp(), MemOperand(sp, 0));
+      break;
+//    case ValueType::kS128:
+//      assm->daddiu(sp, sp, -kSystemPointerSize * 2);
+//      assm->st_b(reg.fp().toW(), MemOperand(sp, 0));
+//      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+}  // namespace liftoff
+
+int LiftoffAssembler::PrepareStackFrame() {
+  int offset = pc_offset();
+  // When constant that represents size of stack frame can't be represented
+  // as 16bit we need three instructions to add it to sp, so we reserve space
+  // for this case.
+  addl(sp, 0, sp);
+  nop();
+  nop();
+  return offset;
+}
+
+void LiftoffAssembler::PrepareTailCall(int num_callee_stack_params,
+                                       int stack_param_delta) {
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+
+  // Push the return address and frame pointer to complete the stack frame.
+  Ldl(scratch, MemOperand(fp, 8));
+  Push(scratch);
+  Ldl(scratch, MemOperand(fp, 0));
+  Push(scratch);
+
+  // Shift the whole frame upwards.
+  int slot_count = num_callee_stack_params + 2;
+  for (int i = slot_count - 1; i >= 0; --i) {
+    Ldl(scratch, MemOperand(sp, i * 8));
+    Stl(scratch, MemOperand(fp, (i - stack_param_delta) * 8));
+  }
+
+  // Set the new stack and frame pointer.
+  subl(fp, stack_param_delta * 8, sp);
+  Pop(ra, fp);
+}
+
+void LiftoffAssembler::PatchPrepareStackFrame(int offset, int frame_size) {
+  // We can't run out of space, just pass anything big enough to not cause the
+  // assembler to try to grow the buffer.
+  constexpr int kAvailableSpace = 256;
+  TurboAssembler patching_assembler(
+      nullptr, AssemblerOptions{}, CodeObjectRequired::kNo,
+      ExternalAssemblerBuffer(buffer_start_ + offset, kAvailableSpace));
+  // If bytes can be represented as 16bit, addl will be generated and two
+  // nops will stay untouched. Otherwise, lui-ori sequence will load it to
+  // register and, as third instruction, addl will be generated.
+  patching_assembler.Addl(sp, sp, Operand(-frame_size));
+}
+
+void LiftoffAssembler::FinishCode() {}
+
+void LiftoffAssembler::AbortCompilation() {}
+
+// static
+constexpr int LiftoffAssembler::StaticStackFrameSize() {
+  return liftoff::kInstanceOffset;
+}
+
+int LiftoffAssembler::SlotSizeForType(ValueType type) {
+  switch (type.kind()) {
+    case ValueType::kS128:
+      return type.element_size_bytes();
+    default:
+      return kStackSlotSize;
+  }
+}
+
+bool LiftoffAssembler::NeedsAlignment(ValueType type) {
+  return type.kind() == ValueType::kS128 || type.is_reference_type();
+}
+
+void LiftoffAssembler::LoadConstant(LiftoffRegister reg, WasmValue value,
+                                    RelocInfo::Mode rmode) {
+  switch (value.type().kind()) {
+    case ValueType::kI32:
+      TurboAssembler::li(reg.gp(), Operand(value.to_i32(), rmode));
+      break;
+    case ValueType::kI64:
+      TurboAssembler::li(reg.gp(), Operand(value.to_i64(), rmode));
+      break;
+    case ValueType::kF32:
+      TurboAssembler::Move(reg.fp(), value.to_f32_boxed().get_bits());
+      break;
+    case ValueType::kF64:
+      TurboAssembler::Move(reg.fp(), value.to_f64_boxed().get_bits());
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+void LiftoffAssembler::LoadFromInstance(Register dst, uint32_t offset,
+                                        int size) {
+  DCHECK_LE(offset, kMaxInt);
+  Ldl(dst, liftoff::GetInstanceOperand());
+  DCHECK(size == 4 || size == 8);
+  if (size == 4) {
+    Ldw(dst, MemOperand(dst, offset));
+  } else {
+    Ldl(dst, MemOperand(dst, offset));
+  }
+}
+
+void LiftoffAssembler::LoadTaggedPointerFromInstance(Register dst,
+                                                     uint32_t offset) {
+  LoadFromInstance(dst, offset, kTaggedSize);
+}
+
+void LiftoffAssembler::SpillInstance(Register instance) {
+  Stl(instance, liftoff::GetInstanceOperand());
+}
+
+void LiftoffAssembler::FillInstanceInto(Register dst) {
+  Ldl(dst, liftoff::GetInstanceOperand());
+}
+
+void LiftoffAssembler::LoadTaggedPointer(Register dst, Register src_addr,
+                                         Register offset_reg,
+                                         int32_t offset_imm,
+                                         LiftoffRegList pinned) {
+  DCHECK_GE(offset_imm, 0);
+  STATIC_ASSERT(kTaggedSize == kInt64Size);
+  Load(LiftoffRegister(dst), src_addr, offset_reg,
+       static_cast<uint32_t>(offset_imm), LoadType::kI64Load, pinned);
+}
+
+void LiftoffAssembler::StoreTaggedPointer(Register dst_addr,
+                                          int32_t offset_imm,
+                                          LiftoffRegister src,
+                                          LiftoffRegList pinned) {
+  bailout(kRefTypes, "GlobalSet");
+}
+
+void LiftoffAssembler::Load(LiftoffRegister dst, Register src_addr,
+                            Register offset_reg, uint32_t offset_imm,
+                            LoadType type, LiftoffRegList pinned,
+                            uint32_t* protected_load_pc, bool is_load_mem) {
+  MemOperand src_op = liftoff::GetMemOp(this, src_addr, offset_reg, offset_imm);
+
+  if (protected_load_pc) *protected_load_pc = pc_offset();
+  switch (type.value()) {
+    case LoadType::kI32Load8U:
+    case LoadType::kI64Load8U:
+      Ldbu(dst.gp(), src_op);
+      break;
+    case LoadType::kI32Load8S:
+    case LoadType::kI64Load8S:
+      Ldb(dst.gp(), src_op);
+      break;
+    case LoadType::kI32Load16U:
+    case LoadType::kI64Load16U:
+      TurboAssembler::Uldhu(dst.gp(), src_op);
+      break;
+    case LoadType::kI32Load16S:
+    case LoadType::kI64Load16S:
+      TurboAssembler::Uldh(dst.gp(), src_op);
+      break;
+    case LoadType::kI64Load32U:
+      TurboAssembler::Uldwu(dst.gp(), src_op);
+      break;
+    case LoadType::kI32Load:
+    case LoadType::kI64Load32S:
+      TurboAssembler::Uldw(dst.gp(), src_op);
+      break;
+    case LoadType::kI64Load:
+      TurboAssembler::Uldl(dst.gp(), src_op);
+      break;
+    case LoadType::kF32Load:
+      TurboAssembler::Uflds(dst.fp(), src_op, t11);
+      break;
+    case LoadType::kF64Load:
+      TurboAssembler::Ufldd(dst.fp(), src_op, t11);
+      break;
+    default:
+      UNREACHABLE();
+  }
+
+#if defined(V8_TARGET_BIG_ENDIAN)
+  if (is_load_mem) {
+    pinned.set(src_op.rm());
+    liftoff::ChangeEndiannessLoad(this, dst, type, pinned);
+  }
+#endif
+}
+
+void LiftoffAssembler::Store(Register dst_addr, Register offset_reg,
+                             uint32_t offset_imm, LiftoffRegister src,
+                             StoreType type, LiftoffRegList pinned,
+                             uint32_t* protected_store_pc, bool is_store_mem) {
+  MemOperand dst_op = liftoff::GetMemOp(this, dst_addr, offset_reg, offset_imm);
+
+#if defined(V8_TARGET_BIG_ENDIAN)
+  if (is_store_mem) {
+    pinned.set(dst_op.rm());
+    LiftoffRegister tmp = GetUnusedRegister(src.reg_class(), pinned);
+    // Save original value.
+    Move(tmp, src, type.value_type());
+
+    src = tmp;
+    pinned.set(tmp);
+    liftoff::ChangeEndiannessStore(this, src, type, pinned);
+  }
+#endif
+
+  if (protected_store_pc) *protected_store_pc = pc_offset();
+  switch (type.value()) {
+    case StoreType::kI32Store8:
+    case StoreType::kI64Store8:
+      Stb(src.gp(), dst_op);
+      break;
+    case StoreType::kI32Store16:
+    case StoreType::kI64Store16:
+      TurboAssembler::Usth(src.gp(), dst_op, t11);
+      break;
+    case StoreType::kI32Store:
+    case StoreType::kI64Store32:
+      TurboAssembler::Ustw(src.gp(), dst_op);
+      break;
+    case StoreType::kI64Store:
+      TurboAssembler::Ustl(src.gp(), dst_op);
+      break;
+    case StoreType::kF32Store:
+      TurboAssembler::Ufsts(src.fp(), dst_op, t11);
+      break;
+    case StoreType::kF64Store:
+      TurboAssembler::Ufstd(src.fp(), dst_op, t11);
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+void LiftoffAssembler::AtomicLoad(LiftoffRegister dst, Register src_addr,
+                                  Register offset_reg, uint32_t offset_imm,
+                                  LoadType type, LiftoffRegList pinned) {
+  bailout(kAtomics, "AtomicLoad");
+}
+
+void LiftoffAssembler::AtomicStore(Register dst_addr, Register offset_reg,
+                                   uint32_t offset_imm, LiftoffRegister src,
+                                   StoreType type, LiftoffRegList pinned) {
+  bailout(kAtomics, "AtomicStore");
+}
+
+void LiftoffAssembler::AtomicAdd(Register dst_addr, Register offset_reg,
+                                 uint32_t offset_imm, LiftoffRegister value,
+                                 LiftoffRegister result, StoreType type) {
+  bailout(kAtomics, "AtomicAdd");
+}
+
+void LiftoffAssembler::AtomicSub(Register dst_addr, Register offset_reg,
+                                 uint32_t offset_imm, LiftoffRegister value,
+                                 LiftoffRegister result, StoreType type) {
+  bailout(kAtomics, "AtomicSub");
+}
+
+void LiftoffAssembler::AtomicAnd(Register dst_addr, Register offset_reg,
+                                 uint32_t offset_imm, LiftoffRegister value,
+                                 LiftoffRegister result, StoreType type) {
+  bailout(kAtomics, "AtomicAnd");
+}
+
+void LiftoffAssembler::AtomicOr(Register dst_addr, Register offset_reg,
+                                uint32_t offset_imm, LiftoffRegister value,
+                                LiftoffRegister result, StoreType type) {
+  bailout(kAtomics, "AtomicOr");
+}
+
+void LiftoffAssembler::AtomicXor(Register dst_addr, Register offset_reg,
+                                 uint32_t offset_imm, LiftoffRegister value,
+                                 LiftoffRegister result, StoreType type) {
+  bailout(kAtomics, "AtomicXor");
+}
+
+void LiftoffAssembler::AtomicExchange(Register dst_addr, Register offset_reg,
+                                      uint32_t offset_imm,
+                                      LiftoffRegister value,
+                                      LiftoffRegister result, StoreType type) {
+  bailout(kAtomics, "AtomicExchange");
+}
+
+void LiftoffAssembler::AtomicCompareExchange(
+    Register dst_addr, Register offset_reg, uint32_t offset_imm,
+    LiftoffRegister expected, LiftoffRegister new_value, LiftoffRegister result,
+    StoreType type) {
+  bailout(kAtomics, "AtomicCompareExchange");
+}
+
+//SKTODO
+void LiftoffAssembler::AtomicFence() {memb();}
+
+void LiftoffAssembler::LoadCallerFrameSlot(LiftoffRegister dst,
+                                           uint32_t caller_slot_idx,
+                                           ValueType type) {
+  MemOperand src(fp, kSystemPointerSize * (caller_slot_idx + 1));
+  liftoff::Load(this, dst, src, type);
+}
+
+void LiftoffAssembler::StoreCallerFrameSlot(LiftoffRegister src,
+                                            uint32_t caller_slot_idx,
+                                            ValueType type) {
+  int32_t offset = kSystemPointerSize * (caller_slot_idx + 1);
+  liftoff::Store(this, fp, offset, src, type);
+}
+
+void LiftoffAssembler::LoadReturnStackSlot(LiftoffRegister dst, int offset,
+                                           ValueType type) {
+  liftoff::Load(this, dst, MemOperand(sp, offset), type);
+}
+
+void LiftoffAssembler::MoveStackValue(uint32_t dst_offset, uint32_t src_offset,
+                                      ValueType type) {
+  DCHECK_NE(dst_offset, src_offset);
+  LiftoffRegister reg = GetUnusedRegister(reg_class_for(type), {});
+  Fill(reg, src_offset, type);
+  Spill(dst_offset, reg, type);
+}
+
+void LiftoffAssembler::Move(Register dst, Register src, ValueType type) {
+  DCHECK_NE(dst, src);
+  // TODO(ksreten): Handle different sizes here.
+  TurboAssembler::Move(dst, src);
+}
+
+void LiftoffAssembler::Move(DoubleRegister dst, DoubleRegister src,
+                            ValueType type) {
+  DCHECK_NE(dst, src);
+  TurboAssembler::Move(dst, src);
+}
+
+void LiftoffAssembler::Spill(int offset, LiftoffRegister reg, ValueType type) {
+  RecordUsedSpillOffset(offset);
+  MemOperand dst = liftoff::GetStackSlot(offset);
+  switch (type.kind()) {
+    case ValueType::kI32:
+      Stw(reg.gp(), dst);
+      break;
+    case ValueType::kI64:
+    case ValueType::kRef:
+    case ValueType::kOptRef:
+      Stl(reg.gp(), dst);
+      break;
+    case ValueType::kF32:
+      Fsts(reg.fp(), dst);
+      break;
+    case ValueType::kF64:
+      TurboAssembler::Fstd(reg.fp(), dst);
+      break;
+//    case ValueType::kS128:
+//      TurboAssembler::st_b(reg.fp().toW(), dst);
+//      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+void LiftoffAssembler::Spill(int offset, WasmValue value) {
+  RecordUsedSpillOffset(offset);
+  MemOperand dst = liftoff::GetStackSlot(offset);
+  switch (value.type().kind()) {
+    case ValueType::kI32: {
+      LiftoffRegister tmp = GetUnusedRegister(kGpReg, {});
+      TurboAssembler::li(tmp.gp(), Operand(value.to_i32()));
+      Stw(tmp.gp(), dst);
+      break;
+    }
+    case ValueType::kI64:
+    case ValueType::kRef:
+    case ValueType::kOptRef: {
+      LiftoffRegister tmp = GetUnusedRegister(kGpReg, {});
+      TurboAssembler::li(tmp.gp(), value.to_i64());
+      Stl(tmp.gp(), dst);
+      break;
+    }
+    default:
+      // kWasmF32 and kWasmF64 are unreachable, since those
+      // constants are not tracked.
+      UNREACHABLE();
+  }
+}
+
+void LiftoffAssembler::Fill(LiftoffRegister reg, int offset, ValueType type) {
+  MemOperand src = liftoff::GetStackSlot(offset);
+  switch (type.kind()) {
+    case ValueType::kI32:
+      Ldw(reg.gp(), src);
+      break;
+    case ValueType::kI64:
+    case ValueType::kRef:
+    case ValueType::kOptRef:
+      Ldl(reg.gp(), src);
+      break;
+    case ValueType::kF32:
+      Flds(reg.fp(), src);
+      break;
+    case ValueType::kF64:
+      TurboAssembler::Fldd(reg.fp(), src);
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+void LiftoffAssembler::FillI64Half(Register, int offset, RegPairHalf) {
+  UNREACHABLE();
+}
+
+void LiftoffAssembler::FillStackSlotsWithZero(int start, int size) {
+  DCHECK_LT(0, size);
+  RecordUsedSpillOffset(start + size);
+
+  if (size <= 12 * kStackSlotSize) {
+    // Special straight-line code for up to 12 slots. Generates one
+    // instruction per slot (<= 12 instructions total).
+    uint32_t remainder = size;
+    for (; remainder >= kStackSlotSize; remainder -= kStackSlotSize) {
+      Stl(zero_reg, liftoff::GetStackSlot(start + remainder));
+    }
+    DCHECK(remainder == 4 || remainder == 0);
+    if (remainder) {
+      Stw(zero_reg, liftoff::GetStackSlot(start + remainder));
+    }
+  } else {
+    // General case for bigger counts (12 instructions).
+    // Use a0 for start address (inclusive), a1 for end address (exclusive).
+    Push(a1, a0);
+    Addl(a0, fp, Operand(-start - size));
+    Addl(a1, fp, Operand(-start));
+
+    Label loop;
+    bind(&loop);
+    Stl(zero_reg, MemOperand(a0));
+    addl(a0, kSystemPointerSize, a0);
+    BranchShort(&loop, ne, a0, Operand(a1));
+
+    Pop(a1, a0);
+  }
+}
+
+void LiftoffAssembler::emit_i64_clz(LiftoffRegister dst, LiftoffRegister src) {
+  TurboAssembler::Dclz(dst.gp(), src.gp());
+}
+
+void LiftoffAssembler::emit_i64_ctz(LiftoffRegister dst, LiftoffRegister src) {
+  TurboAssembler::Dctz(dst.gp(), src.gp());
+}
+
+bool LiftoffAssembler::emit_i64_popcnt(LiftoffRegister dst,
+                                       LiftoffRegister src) {
+  TurboAssembler::Dpopcnt(dst.gp(), src.gp());
+  return true;
+}
+
+void LiftoffAssembler::emit_i32_mul(Register dst, Register lhs, Register rhs) {
+  TurboAssembler::Mulw(dst, lhs, rhs);
+}
+
+void LiftoffAssembler::emit_i32_divs(Register dst, Register lhs, Register rhs,
+                                     Label* trap_div_by_zero,
+                                     Label* trap_div_unrepresentable) {
+  TurboAssembler::Branch(trap_div_by_zero, eq, rhs, Operand(zero_reg));
+
+  // Check if lhs == kMinInt and rhs == -1, since this case is unrepresentable.
+  TurboAssembler::li(kScratchReg, 1);
+  TurboAssembler::li(kScratchReg2, 1);
+  TurboAssembler::LoadZeroOnCondition(kScratchReg, lhs, Operand(kMinInt), eq);
+  TurboAssembler::LoadZeroOnCondition(kScratchReg2, rhs, Operand(-1), eq);
+  addl(kScratchReg, kScratchReg2,kScratchReg);
+  TurboAssembler::Branch(trap_div_unrepresentable, eq, kScratchReg,
+                         Operand(zero_reg));
+
+//  addw(lhs, 0, lhs);
+//  addw(rhs, 0, rhs);
+  ifmovd(lhs, kScratchDoubleReg1);
+  ifmovd(rhs, kScratchDoubleReg2);
+  fcvtld(kScratchDoubleReg1, kScratchDoubleReg1);
+  fcvtld(kScratchDoubleReg2, kScratchDoubleReg2);
+  fdivd(kScratchDoubleReg1, kScratchDoubleReg2, kScratchDoubleReg1);
+  fcvtdl_z(kScratchDoubleReg1, kScratchDoubleReg2);
+  fimovd(kScratchDoubleReg2, dst);
+  //TurboAssembler::Div(dst, lhs, rhs);
+}
+
+void LiftoffAssembler::emit_i32_divu(Register dst, Register lhs, Register rhs,
+                                     Label* trap_div_by_zero) {
+  TurboAssembler::Branch(trap_div_by_zero, eq, rhs, Operand(zero_reg));
+  zapnot(lhs, 0xf, lhs);
+  zapnot(rhs, 0xf, rhs);
+  ifmovd(lhs, kScratchDoubleReg1);
+  ifmovd(rhs, kScratchDoubleReg2);
+  fcvtld(kScratchDoubleReg1, kScratchDoubleReg1);
+  fcvtld(kScratchDoubleReg2, kScratchDoubleReg2);
+  fdivd(kScratchDoubleReg1, kScratchDoubleReg2, kScratchDoubleReg1);
+  fcvtdl_z(kScratchDoubleReg1, kScratchDoubleReg2);
+  fimovd(kScratchDoubleReg2,dst);
+  //TurboAssembler::Divu(dst, lhs, rhs);
+}
+
+void LiftoffAssembler::emit_i32_rems(Register dst, Register lhs, Register rhs,
+                                     Label* trap_div_by_zero) {
+  TurboAssembler::Branch(trap_div_by_zero, eq, rhs, Operand(zero_reg));
+//  addw(lhs, 0, lhs);
+//  addw(rhs, 0, rhs);
+  ifmovd(lhs, kScratchDoubleReg1);
+  ifmovd(rhs, kScratchDoubleReg2);
+  fcvtld(kScratchDoubleReg1, kScratchDoubleReg1);
+  fcvtld(kScratchDoubleReg2, kScratchDoubleReg2);
+  fdivd(kScratchDoubleReg1, kScratchDoubleReg2, kScratchDoubleReg1);
+  fcvtdl_z(kScratchDoubleReg1, kScratchDoubleReg2);
+  fimovd(kScratchDoubleReg2, kScratchReg);
+  mulw(kScratchReg, rhs, kScratchReg);
+  subw(lhs, kScratchReg, dst);
+  //TurboAssembler::Mod(dst, lhs, rhs);
+}
+
+void LiftoffAssembler::emit_i32_remu(Register dst, Register lhs, Register rhs,
+                                     Label* trap_div_by_zero) {
+  TurboAssembler::Branch(trap_div_by_zero, eq, rhs, Operand(zero_reg));
+  zapnot(lhs, 0xf, lhs);
+  zapnot(rhs, 0xf, rhs);
+  ifmovd(lhs, kScratchDoubleReg1);
+  ifmovd(rhs, kScratchDoubleReg2);
+  fcvtld(kScratchDoubleReg1, kScratchDoubleReg1);
+  fcvtld(kScratchDoubleReg2, kScratchDoubleReg2);
+  fdivd(kScratchDoubleReg1, kScratchDoubleReg2, kScratchDoubleReg1);
+  fcvtdl_z(kScratchDoubleReg1, kScratchDoubleReg2);
+  fimovd(kScratchDoubleReg2, kScratchReg);
+  mulw(kScratchReg, rhs, kScratchReg);
+  subw(lhs, kScratchReg, dst);
+  //TurboAssembler::Modu(dst, lhs, rhs);
+}
+
+#define I32_BINOP(name, instruction)                                 \
+  void LiftoffAssembler::emit_i32_##name(Register dst, Register lhs, \
+                                         Register rhs) {             \
+    instruction(lhs, rhs, dst);                                      \
+  }
+
+// clang-format off
+I32_BINOP(add, addw)
+I32_BINOP(sub, subw)
+I32_BINOP(and, and_ins)
+I32_BINOP(or, or_ins)
+I32_BINOP(xor, xor_ins)
+// clang-format on
+
+#undef I32_BINOP
+
+#define I32_BINOP_I(name, instruction)                                  \
+  void LiftoffAssembler::emit_i32_##name##i(Register dst, Register lhs, \
+                                            int32_t imm) {              \
+    instruction(dst, lhs, Operand(imm));                                \
+  }
+
+// clang-format off
+I32_BINOP_I(add, Addw)
+I32_BINOP_I(and, And)
+I32_BINOP_I(or, Or)
+I32_BINOP_I(xor, Xor)
+// clang-format on
+
+#undef I32_BINOP_I
+
+void LiftoffAssembler::emit_i32_clz(Register dst, Register src) {
+  TurboAssembler::Clz(dst, src);
+}
+
+void LiftoffAssembler::emit_i32_ctz(Register dst, Register src) {
+  TurboAssembler::Ctz(dst, src);
+}
+
+bool LiftoffAssembler::emit_i32_popcnt(Register dst, Register src) {
+  TurboAssembler::Popcnt(dst, src);
+  return true;
+}
+
+#define I32_SHIFTOP(name, instruction)                               \
+  void LiftoffAssembler::emit_i32_##name(Register dst, Register src, \
+                                         Register amount) {          \
+    instruction(dst, src, amount);                                   \
+  }
+#define I32_SHIFTOP_I(name, instruction)                             \
+  I32_SHIFTOP(name, instruction)                                     \
+  void LiftoffAssembler::emit_i32_##name##i(Register dst, Register src, \
+                                         int amount) {               \
+    instruction(dst, src, (amount & 31));                            \
+  }
+
+I32_SHIFTOP_I(shl, Sllw)
+I32_SHIFTOP_I(sar, Sraw)
+I32_SHIFTOP_I(shr, Srlw)
+
+#undef I32_SHIFTOP
+#undef I32_SHIFTOP_I
+
+void LiftoffAssembler::emit_i64_mul(LiftoffRegister dst, LiftoffRegister lhs,
+                                    LiftoffRegister rhs) {
+  TurboAssembler::Mull(dst.gp(), lhs.gp(), rhs.gp());
+}
+
+bool LiftoffAssembler::emit_i64_divs(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs,
+                                     Label* trap_div_by_zero,
+                                     Label* trap_div_unrepresentable) {
+  //TODO
+  return false;
+}
+
+bool LiftoffAssembler::emit_i64_divu(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs,
+                                     Label* trap_div_by_zero) {
+  //TODO
+  return false;
+  //TurboAssembler::Branch(trap_div_by_zero, eq, rhs.gp(), Operand(zero_reg));
+  //TurboAssembler::Ddivu(dst.gp(), lhs.gp(), rhs.gp());
+  //return true;
+}
+
+bool LiftoffAssembler::emit_i64_rems(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs,
+                                     Label* trap_div_by_zero) {
+  //TODO
+  return false;
+  //TurboAssembler::Branch(trap_div_by_zero, eq, rhs.gp(), Operand(zero_reg));
+  //TurboAssembler::Dmod(dst.gp(), lhs.gp(), rhs.gp());
+  //return true;
+}
+
+bool LiftoffAssembler::emit_i64_remu(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs,
+                                     Label* trap_div_by_zero) {
+  //TODO
+  return false;
+}
+
+#define I64_BINOP(name, instruction)                                   \
+  void LiftoffAssembler::emit_i64_##name(                              \
+      LiftoffRegister dst, LiftoffRegister lhs, LiftoffRegister rhs) { \
+    instruction(lhs.gp(), rhs.gp(), dst.gp());                         \
+  }
+
+// clang-format off
+I64_BINOP(add, addl)
+I64_BINOP(sub, subl)
+I64_BINOP(and, and_ins)
+I64_BINOP(or, or_ins)
+I64_BINOP(xor, xor_ins)
+// clang-format on
+
+#undef I64_BINOP
+
+#define I64_BINOP_I(name, instruction)                         \
+  void LiftoffAssembler::emit_i64_##name##i(                   \
+      LiftoffRegister dst, LiftoffRegister lhs, int32_t imm) { \
+    instruction(dst.gp(), lhs.gp(), Operand(imm));             \
+  }
+
+// clang-format off
+I64_BINOP_I(add, Addl)
+I64_BINOP_I(and, And)
+I64_BINOP_I(or, Or)
+I64_BINOP_I(xor, Xor)
+// clang-format on
+
+#undef I64_BINOP_I
+
+#define I64_SHIFTOP(name, instruction)                                         \
+  void LiftoffAssembler::emit_i64_##name(                          \
+      LiftoffRegister dst, LiftoffRegister src, Register amount) { \
+    instruction(src.gp(), amount, dst.gp());                       \
+  }
+#define I64_SHIFTOP_I(name, instruction)                                    \
+  I64_SHIFTOP(name, instruction)                                            \
+  void LiftoffAssembler::emit_i64_##name##i(LiftoffRegister dst,               \
+                                         LiftoffRegister src, int amount) { \
+      instruction(src.gp(), (amount & 63), dst.gp());                            \
+  }
+
+I64_SHIFTOP_I(shl, slll)
+I64_SHIFTOP_I(sar, sral)
+I64_SHIFTOP_I(shr, srll)
+
+#undef I64_SHIFTOP
+#undef I64_SHIFTOP_I
+
+void LiftoffAssembler::emit_u32_to_intptr(Register dst, Register src) {
+//  Dext(dst, src, 0, 32);
+  zapnot(src, 0xf, dst);
+}
+
+void LiftoffAssembler::emit_f32_neg(DoubleRegister dst, DoubleRegister src) {
+  TurboAssembler::Fnegs(dst, src);
+}
+
+void LiftoffAssembler::emit_f64_neg(DoubleRegister dst, DoubleRegister src) {
+  TurboAssembler::Fnegd(dst, src);
+}
+
+void LiftoffAssembler::emit_f32_min(DoubleRegister dst, DoubleRegister lhs,
+                                    DoubleRegister rhs) {
+  Label ool, done;
+  TurboAssembler::Float32Min(dst, lhs, rhs, &ool);
+  Branch(&done);
+
+  bind(&ool);
+  TurboAssembler::Float32MinOutOfLine(dst, lhs, rhs);
+  bind(&done);
+}
+
+void LiftoffAssembler::emit_f32_max(DoubleRegister dst, DoubleRegister lhs,
+                                    DoubleRegister rhs) {
+  Label ool, done;
+  TurboAssembler::Float32Max(dst, lhs, rhs, &ool);
+  Branch(&done);
+
+  bind(&ool);
+  TurboAssembler::Float32MaxOutOfLine(dst, lhs, rhs);
+  bind(&done);
+}
+
+void LiftoffAssembler::emit_f32_copysign(DoubleRegister dst, DoubleRegister lhs,
+                                         DoubleRegister rhs) {
+  bailout(kComplexOperation, "f32_copysign");
+}
+
+void LiftoffAssembler::emit_f64_min(DoubleRegister dst, DoubleRegister lhs,
+                                    DoubleRegister rhs) {
+  Label ool, done;
+  TurboAssembler::Float64Min(dst, lhs, rhs, &ool);
+  Branch(&done);
+
+  bind(&ool);
+  TurboAssembler::Float64MinOutOfLine(dst, lhs, rhs);
+  bind(&done);
+}
+
+void LiftoffAssembler::emit_f64_max(DoubleRegister dst, DoubleRegister lhs,
+                                    DoubleRegister rhs) {
+  Label ool, done;
+  TurboAssembler::Float64Max(dst, lhs, rhs, &ool);
+  Branch(&done);
+
+  bind(&ool);
+  TurboAssembler::Float64MaxOutOfLine(dst, lhs, rhs);
+  bind(&done);
+}
+
+void LiftoffAssembler::emit_f64_copysign(DoubleRegister dst, DoubleRegister lhs,
+                                         DoubleRegister rhs) {
+  bailout(kComplexOperation, "f64_copysign");
+}
+
+#define FP_BINOP(name, instruction)                                          \
+  void LiftoffAssembler::emit_##name(DoubleRegister dst, DoubleRegister lhs, \
+                                     DoubleRegister rhs) {                   \
+    instruction(lhs, rhs, dst);                                              \
+  }
+#define FP_UNOP_X(name, instruction)                                             \
+  void LiftoffAssembler::emit_##name(DoubleRegister dst, DoubleRegister src) { \
+    instruction(src, dst);                                                     \
+  }
+
+
+#define FP_UNOP(name, instruction)                                             \
+  void LiftoffAssembler::emit_##name(DoubleRegister dst, DoubleRegister src) { \
+    TurboAssembler::instruction(dst, src);                                     \
+  }
+
+#define FP_UNOP_RETURN_TRUE(name, instruction)                                 \
+  bool LiftoffAssembler::emit_##name(DoubleRegister dst, DoubleRegister src) { \
+    instruction(dst, src);                                                     \
+    return true;                                                               \
+  }
+
+FP_BINOP(f32_add, fadds)
+FP_BINOP(f32_sub, fsubs)
+FP_BINOP(f32_mul, fmuls)
+FP_BINOP(f32_div, fdivs)
+FP_UNOP(f32_abs, Abs_sw)
+FP_UNOP_RETURN_TRUE(f32_ceil, Ceil_s_s)
+FP_UNOP_RETURN_TRUE(f32_floor, Floor_s_s)
+FP_UNOP_RETURN_TRUE(f32_trunc, Trunc_s_s)
+FP_UNOP_RETURN_TRUE(f32_nearest_int, Round_s_s)
+FP_UNOP_X(f32_sqrt, fsqrts)
+FP_BINOP(f64_add, faddd)
+FP_BINOP(f64_sub, fsubd)
+FP_BINOP(f64_mul, fmuld)
+FP_BINOP(f64_div, fdivd)
+FP_UNOP(f64_abs, Abs_sw)
+FP_UNOP_RETURN_TRUE(f64_ceil, Ceil_d_d)
+FP_UNOP_RETURN_TRUE(f64_floor, Floor_d_d)
+FP_UNOP_RETURN_TRUE(f64_trunc, Trunc_d_d)
+FP_UNOP_RETURN_TRUE(f64_nearest_int, Round_d_d)
+FP_UNOP_X(f64_sqrt, fsqrtd)
+
+#undef FP_BINOP
+#undef FP_UNOP
+#undef FP_UNOP_RETURN_TRUE
+
+bool LiftoffAssembler::emit_type_conversion(WasmOpcode opcode,
+                                            LiftoffRegister dst,
+                                            LiftoffRegister src, Label* trap) {
+  switch (opcode) {
+    case kExprI32ConvertI64:
+      addw(src.gp(), zero_reg, dst.gp()); //TurboAssembler::Ext(dst.gp(), src.gp(), 0, 32);
+      return true;
+    case kExprI32SConvertF32: {
+      LiftoffRegister rounded =
+          GetUnusedRegister(kFpReg, LiftoffRegList::ForRegs(src));
+      LiftoffRegister converted_back =
+          GetUnusedRegister(kFpReg, LiftoffRegList::ForRegs(src, rounded));
+
+      // Real conversion.
+      TurboAssembler::Trunc_s_s(rounded.fp(), src.fp());
+      ftruncsw(rounded.fp(), kScratchDoubleReg);
+      fimovs(kScratchDoubleReg, dst.gp());
+      // Avoid INT32_MAX as an overflow indicator and use INT32_MIN instead,
+      // because INT32_MIN allows easier out-of-bounds detection.
+      TurboAssembler::Addw(kScratchReg, dst.gp(), 1);
+      TurboAssembler::Cmplt(kScratchReg2, kScratchReg, dst.gp());
+      TurboAssembler::Selne(dst.gp(), kScratchReg, kScratchReg2);
+
+      // Checking if trap.
+      ifmovs(dst.gp(), kScratchDoubleReg);
+      fcvtws(kScratchDoubleReg, converted_back.fp());
+      TurboAssembler::CompareF32(EQ, rounded.fp(), converted_back.fp());
+      TurboAssembler::BranchFalseF(trap);
+      return true;
+    }
+    case kExprI32UConvertF32: {
+      LiftoffRegister rounded =
+          GetUnusedRegister(kFpReg, LiftoffRegList::ForRegs(src));
+      LiftoffRegister converted_back =
+          GetUnusedRegister(kFpReg, LiftoffRegList::ForRegs(src, rounded));
+
+      // Real conversion.
+      TurboAssembler::Trunc_s_s(rounded.fp(), src.fp());
+      TurboAssembler::Trunc_uw_s(dst.gp(), rounded.fp(), kScratchDoubleReg);
+      // Avoid UINT32_MAX as an overflow indicator and use 0 instead,
+      // because 0 allows easier out-of-bounds detection.
+      TurboAssembler::Addw(kScratchReg, dst.gp(), 1);
+      TurboAssembler::Seleq(dst.gp(), zero_reg, kScratchReg);
+
+      // Checking if trap.
+      TurboAssembler::Cvt_d_uw(converted_back.fp(), dst.gp());
+      fcvtds_(converted_back.fp(), converted_back.fp());
+      TurboAssembler::CompareF32(EQ, rounded.fp(), converted_back.fp());
+      TurboAssembler::BranchFalseF(trap);
+      return true;
+    }
+    case kExprI32SConvertF64: {
+      LiftoffRegister rounded =
+          GetUnusedRegister(kFpReg, LiftoffRegList::ForRegs(src));
+      LiftoffRegister converted_back =
+          GetUnusedRegister(kFpReg, LiftoffRegList::ForRegs(src, rounded));
+
+      // Real conversion.
+      TurboAssembler::Trunc_d_d(rounded.fp(), src.fp());
+      ftruncdw(rounded.fp(), kScratchDoubleReg);
+      fimovs(kScratchDoubleReg, dst.gp());
+
+      // Checking if trap.
+      fcvtwd(kScratchDoubleReg, converted_back.fp());
+      TurboAssembler::CompareF64(EQ, rounded.fp(), converted_back.fp());
+      TurboAssembler::BranchFalseF(trap);
+      return true;
+    }
+    case kExprI32UConvertF64: {
+      LiftoffRegister rounded =
+          GetUnusedRegister(kFpReg, LiftoffRegList::ForRegs(src));
+      LiftoffRegister converted_back =
+          GetUnusedRegister(kFpReg, LiftoffRegList::ForRegs(src, rounded));
+
+      // Real conversion.
+      TurboAssembler::Trunc_d_d(rounded.fp(), src.fp());
+      TurboAssembler::Trunc_uw_d(dst.gp(), rounded.fp(), kScratchDoubleReg);
+
+      // Checking if trap.
+      TurboAssembler::Cvt_d_uw(converted_back.fp(), dst.gp());
+      TurboAssembler::CompareF64(EQ, rounded.fp(), converted_back.fp());
+      TurboAssembler::BranchFalseF(trap);
+      return true;
+    }
+    case kExprI32ReinterpretF32:
+      fimovs(src.fp(), dst.gp());
+      return true;
+    case kExprI64SConvertI32:
+      addw(src.gp(), 0, dst.gp()); //ZHJ Sllw(dst.gp(), src.gp(), 0);
+      return true;
+    case kExprI64UConvertI32:
+      zapnot(src.gp(), 0xf, dst.gp()); //ZHJ TurboAssembler::Dext(dst.gp(), src.gp(), 0, 32);
+      return true;
+    case kExprI64SConvertF32: {
+      LiftoffRegister rounded =
+          GetUnusedRegister(kFpReg, LiftoffRegList::ForRegs(src));
+      LiftoffRegister converted_back =
+          GetUnusedRegister(kFpReg, LiftoffRegList::ForRegs(src, rounded));
+
+      // Real conversion.
+      TurboAssembler::Trunc_s_s(rounded.fp(), src.fp());
+      ftruncsl(rounded.fp(), kScratchDoubleReg);
+      fimovd(kScratchDoubleReg, dst.gp());
+      // Avoid INT64_MAX as an overflow indicator and use INT64_MIN instead,
+      // because INT64_MIN allows easier out-of-bounds detection.
+      TurboAssembler::Addl(kScratchReg, dst.gp(), 1);
+      TurboAssembler::Cmplt(kScratchReg2, kScratchReg, dst.gp());
+      TurboAssembler::Selne(dst.gp(), kScratchReg, kScratchReg2);
+
+      // Checking if trap.
+      ifmovd(dst.gp(), kScratchDoubleReg);
+      fcvtls(kScratchDoubleReg, converted_back.fp());
+      TurboAssembler::CompareF32(EQ, rounded.fp(), converted_back.fp());
+      TurboAssembler::BranchFalseF(trap);
+      return true;
+    }
+    case kExprI64UConvertF32: {
+      // Real conversion.
+      TurboAssembler::Trunc_ul_s(dst.gp(), src.fp(), kScratchDoubleReg,
+                                 kScratchReg);
+
+      // Checking if trap.
+      TurboAssembler::Branch(trap, eq, kScratchReg, Operand(zero_reg));
+      return true;
+    }
+    case kExprI64SConvertF64: {
+      LiftoffRegister rounded =
+          GetUnusedRegister(kFpReg, LiftoffRegList::ForRegs(src));
+      LiftoffRegister converted_back =
+          GetUnusedRegister(kFpReg, LiftoffRegList::ForRegs(src, rounded));
+
+      // Real conversion.
+      TurboAssembler::Trunc_d_d(rounded.fp(), src.fp());
+      ftruncdl(rounded.fp(), kScratchDoubleReg);
+      fimovd(kScratchDoubleReg, dst.gp());
+      // Avoid INT64_MAX as an overflow indicator and use INT64_MIN instead,
+      // because INT64_MIN allows easier out-of-bounds detection.
+      TurboAssembler::Addl(kScratchReg, dst.gp(), 1);
+      TurboAssembler::Cmplt(kScratchReg2, kScratchReg, dst.gp());
+      TurboAssembler::Selne(dst.gp(), kScratchReg, kScratchReg2);
+
+      // Checking if trap.
+      ifmovd(dst.gp(), kScratchDoubleReg);
+      fcvtld(kScratchDoubleReg, converted_back.fp());
+      TurboAssembler::CompareF64(EQ, rounded.fp(), converted_back.fp());
+      TurboAssembler::BranchFalseF(trap);
+      return true;
+    }
+    case kExprI64UConvertF64: {
+      // Real conversion.
+      TurboAssembler::Trunc_ul_d(dst.gp(), src.fp(), kScratchDoubleReg,
+                                 kScratchReg);
+
+      // Checking if trap.
+      TurboAssembler::Branch(trap, eq, kScratchReg, Operand(zero_reg));
+      return true;
+    }
+    case kExprI64ReinterpretF64:
+      fimovd(src.fp(), dst.gp());
+      return true;
+    case kExprF32SConvertI32: {
+      LiftoffRegister scratch =
+          GetUnusedRegister(kFpReg, LiftoffRegList::ForRegs(dst));
+      ifmovs(src.gp(), scratch.fp());
+      fcvtws(scratch.fp(), dst.fp());
+      return true;
+    }
+    case kExprF32UConvertI32:
+      TurboAssembler::Cvt_s_uw(dst.fp(), src.gp());
+      return true;
+    case kExprF32ConvertF64:
+      fcvtds(src.fp(), dst.fp());
+      return true;
+    case kExprF32ReinterpretI32:
+      ifmovs(src.gp(), dst.fp());
+      return true;
+    case kExprF64SConvertI32: {
+      LiftoffRegister scratch =
+          GetUnusedRegister(kFpReg, LiftoffRegList::ForRegs(dst));
+      ifmovs(src.gp(), scratch.fp());
+      fcvtwd(scratch.fp(), dst.fp());
+      return true;
+    }
+    case kExprF64UConvertI32:
+      TurboAssembler::Cvt_d_uw(dst.fp(), src.gp());
+      return true;
+    case kExprF64ConvertF32:
+      fcvtsd(src.fp(), dst.fp());
+      return true;
+    case kExprF64ReinterpretI64:
+      ifmovd(src.gp(), dst.fp());
+      return true;
+    case kExprI32SConvertSatF32:
+      bailout(kNonTrappingFloatToInt, "kExprI32SConvertSatF32");
+      return true;
+    case kExprI32UConvertSatF32:
+      bailout(kNonTrappingFloatToInt, "kExprI32UConvertSatF32");
+      return true;
+    case kExprI32SConvertSatF64:
+      bailout(kNonTrappingFloatToInt, "kExprI32SConvertSatF64");
+      return true;
+    case kExprI32UConvertSatF64:
+      bailout(kNonTrappingFloatToInt, "kExprI32UConvertSatF64");
+      return true;
+    case kExprI64SConvertSatF32:
+      bailout(kNonTrappingFloatToInt, "kExprI64SConvertSatF32");
+      return true;
+    case kExprI64UConvertSatF32:
+      bailout(kNonTrappingFloatToInt, "kExprI64UConvertSatF32");
+      return true;
+    case kExprI64SConvertSatF64:
+      bailout(kNonTrappingFloatToInt, "kExprI64SConvertSatF64");
+      return true;
+    case kExprI64UConvertSatF64:
+      bailout(kNonTrappingFloatToInt, "kExprI64UConvertSatF64");
+      return true;
+    default:
+      return false;
+  }
+}
+
+void LiftoffAssembler::emit_i32_signextend_i8(Register dst, Register src) {
+  bailout(kComplexOperation, "i32_signextend_i8");
+}
+
+void LiftoffAssembler::emit_i32_signextend_i16(Register dst, Register src) {
+  bailout(kComplexOperation, "i32_signextend_i16");
+}
+
+void LiftoffAssembler::emit_i64_signextend_i8(LiftoffRegister dst,
+                                              LiftoffRegister src) {
+  bailout(kComplexOperation, "i64_signextend_i8");
+}
+
+void LiftoffAssembler::emit_i64_signextend_i16(LiftoffRegister dst,
+                                               LiftoffRegister src) {
+  bailout(kComplexOperation, "i64_signextend_i16");
+}
+
+void LiftoffAssembler::emit_i64_signextend_i32(LiftoffRegister dst,
+                                               LiftoffRegister src) {
+  bailout(kComplexOperation, "i64_signextend_i32");
+}
+
+void LiftoffAssembler::emit_jump(Label* label) {
+  TurboAssembler::Branch(label);
+}
+
+void LiftoffAssembler::emit_jump(Register target) {
+  TurboAssembler::Jump(target);
+}
+
+void LiftoffAssembler::emit_cond_jump(Condition cond, Label* label,
+                                      ValueType type, Register lhs,
+                                      Register rhs) {
+  if (rhs != no_reg) {
+    TurboAssembler::Branch(label, cond, lhs, Operand(rhs));
+  } else {
+    TurboAssembler::Branch(label, cond, lhs, Operand(zero_reg));
+}
+}
+
+void LiftoffAssembler::emit_i32_eqz(Register dst, Register src) {
+  cmpeq(src, 0, dst);
+}
+
+void LiftoffAssembler::emit_i32_set_cond(Condition cond, Register dst,
+                                         Register lhs, Register rhs) {
+  Register tmp = dst;
+  if (dst == lhs || dst == rhs) {
+    tmp = GetUnusedRegister(kGpReg, LiftoffRegList::ForRegs(lhs, rhs)).gp();
+  }
+  // Write 1 as result.
+  TurboAssembler::li(tmp, 1);
+
+  // If negative condition is true, write 0 as result.
+  Condition neg_cond = NegateCondition(cond);
+  TurboAssembler::LoadZeroOnCondition(tmp, lhs, Operand(rhs), neg_cond);
+
+  // If tmp != dst, result will be moved.
+  TurboAssembler::Move(dst, tmp);
+}
+
+void LiftoffAssembler::emit_i64_eqz(Register dst, LiftoffRegister src) {
+  cmpeq(src.gp(), 0, dst);
+}
+
+void LiftoffAssembler::emit_i64_set_cond(Condition cond, Register dst,
+                                         LiftoffRegister lhs,
+                                         LiftoffRegister rhs) {
+  Register tmp = dst;
+  if (dst == lhs.gp() || dst == rhs.gp()) {
+    tmp = GetUnusedRegister(kGpReg, LiftoffRegList::ForRegs(lhs, rhs)).gp();
+  }
+  // Write 1 as result.
+  TurboAssembler::li(tmp, 1);
+
+  // If negative condition is true, write 0 as result.
+  Condition neg_cond = NegateCondition(cond);
+  TurboAssembler::LoadZeroOnCondition(tmp, lhs.gp(), Operand(rhs.gp()),
+                                      neg_cond);
+
+  // If tmp != dst, result will be moved.
+  TurboAssembler::Move(dst, tmp);
+}
+
+namespace liftoff {
+
+inline FPUCondition ConditionToConditionCmpFPU(Condition condition,
+                                               bool* predicate) {
+  switch (condition) {
+    case kEqual:
+      *predicate = true;
+      return EQ;
+    case kUnequal:
+      *predicate = false;
+      return EQ;
+    case kUnsignedLessThan:
+      *predicate = true;
+      return OLT;
+    case kUnsignedGreaterEqual:
+      *predicate = false;
+      return OLT;
+    case kUnsignedLessEqual:
+      *predicate = true;
+      return OLE;
+    case kUnsignedGreaterThan:
+      *predicate = false;
+      return OLE;
+    default:
+      *predicate = true;
+      break;
+  }
+  UNREACHABLE();
+}
+//SKTODO
+#if 0
+inline void EmitAnyTrue(LiftoffAssembler* assm, LiftoffRegister dst,
+                        LiftoffRegister src) {
+  Label all_false;
+  assm->BranchMSA(&all_false, MSA_BRANCH_V, all_zero, src.fp().toW(),
+                  USE_DELAY_SLOT);
+  assm->li(dst.gp(), 0l);
+  assm->li(dst.gp(), 1);
+  assm->bind(&all_false);
+}
+
+inline void EmitAllTrue(LiftoffAssembler* assm, LiftoffRegister dst,
+                        LiftoffRegister src, MSABranchDF msa_branch_df) {
+  Label all_true;
+  assm->BranchMSA(&all_true, msa_branch_df, all_not_zero, src.fp().toW(),
+                  USE_DELAY_SLOT);
+  assm->li(dst.gp(), 1);
+  assm->li(dst.gp(), 0l);
+  assm->bind(&all_true);
+}
+#endif
+}  // namespace liftoff
+
+void LiftoffAssembler::emit_f32_set_cond(Condition cond, Register dst,
+                                         DoubleRegister lhs,
+                                         DoubleRegister rhs) {
+  Label not_nan, cont;
+  TurboAssembler::CompareIsNanF32(lhs, rhs);
+  TurboAssembler::BranchFalseF(&not_nan);
+  // If one of the operands is NaN, return 1 for f32.ne, else 0.
+  if (cond == ne) {
+    TurboAssembler::li(dst, 1);
+  } else {
+    TurboAssembler::Move(dst, zero_reg);
+  }
+  TurboAssembler::Branch(&cont);
+
+  bind(&not_nan);
+
+  TurboAssembler::li(dst, 1);
+  bool predicate;
+  FPUCondition fcond = liftoff::ConditionToConditionCmpFPU(cond, &predicate);
+  TurboAssembler::CompareF32(fcond, lhs, rhs);
+  if (predicate) {
+    TurboAssembler::LoadZeroIfNotFPUCondition(dst);
+  } else {
+    TurboAssembler::LoadZeroIfFPUCondition(dst);
+  }
+
+  bind(&cont);
+}
+
+void LiftoffAssembler::emit_f64_set_cond(Condition cond, Register dst,
+                                         DoubleRegister lhs,
+                                         DoubleRegister rhs) {
+  Label not_nan, cont;
+  TurboAssembler::CompareIsNanF64(lhs, rhs);
+  TurboAssembler::BranchFalseF(&not_nan);
+  // If one of the operands is NaN, return 1 for f64.ne, else 0.
+  if (cond == ne) {
+    TurboAssembler::li(dst, 1);
+  } else {
+    TurboAssembler::Move(dst, zero_reg);
+  }
+  TurboAssembler::Branch(&cont);
+
+  bind(&not_nan);
+
+  TurboAssembler::li(dst, 1);
+  bool predicate;
+  FPUCondition fcond = liftoff::ConditionToConditionCmpFPU(cond, &predicate);
+  TurboAssembler::CompareF64(fcond, lhs, rhs);
+  if (predicate) {
+    TurboAssembler::LoadZeroIfNotFPUCondition(dst);
+  } else {
+    TurboAssembler::LoadZeroIfFPUCondition(dst);
+  }
+
+  bind(&cont);
+}
+
+bool LiftoffAssembler::emit_select(LiftoffRegister dst, Register condition,
+                                   LiftoffRegister true_value,
+                                   LiftoffRegister false_value,
+                                   ValueType type) {
+  return false;
+}
+
+//SKTODO
+void LiftoffAssembler::LoadTransform(LiftoffRegister dst, Register src_addr,
+                                     Register offset_reg, uint32_t offset_imm,
+                                     LoadType type,
+                                     LoadTransformationKind transform,
+                                     uint32_t* protected_load_pc) {
+  UNREACHABLE();
+#if 0
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  Daddu(scratch, src_addr, offset_reg);
+  MemOperand src_op = MemOperand(scratch, offset_imm);
+  MSARegister dst_msa = dst.fp().toW();
+  *protected_load_pc = pc_offset();
+  MachineType memtype = type.mem_type();
+
+  if (transform == LoadTransformationKind::kExtend) {
+    Ld(scratch, src_op);
+    if (memtype == MachineType::Int8()) {
+      fill_d(dst_msa, scratch);
+      clti_s_b(kSimd128ScratchReg, dst_msa, 0);
+      ilvr_b(dst_msa, kSimd128ScratchReg, dst_msa);
+    } else if (memtype == MachineType::Uint8()) {
+      xor_v(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
+      fill_d(dst_msa, scratch);
+      ilvr_b(dst_msa, kSimd128RegZero, dst_msa);
+    } else if (memtype == MachineType::Int16()) {
+      fill_d(dst_msa, scratch);
+      clti_s_h(kSimd128ScratchReg, dst_msa, 0);
+      ilvr_h(dst_msa, kSimd128ScratchReg, dst_msa);
+    } else if (memtype == MachineType::Uint16()) {
+      xor_v(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
+      fill_d(dst_msa, scratch);
+      ilvr_h(dst_msa, kSimd128RegZero, dst_msa);
+    } else if (memtype == MachineType::Int32()) {
+      fill_d(dst_msa, scratch);
+      clti_s_w(kSimd128ScratchReg, dst_msa, 0);
+      ilvr_w(dst_msa, kSimd128ScratchReg, dst_msa);
+    } else if (memtype == MachineType::Uint32()) {
+      xor_v(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
+      fill_d(dst_msa, scratch);
+      ilvr_w(dst_msa, kSimd128RegZero, dst_msa);
+    }
+  } else {
+    DCHECK_EQ(LoadTransformationKind::kSplat, transform);
+    if (memtype == MachineType::Int8()) {
+      Lb(scratch, src_op);
+      fill_b(dst_msa, scratch);
+    } else if (memtype == MachineType::Int16()) {
+      Lh(scratch, src_op);
+      fill_h(dst_msa, scratch);
+    } else if (memtype == MachineType::Int32()) {
+      Lw(scratch, src_op);
+      fill_w(dst_msa, scratch);
+    } else if (memtype == MachineType::Int64()) {
+      Ld(scratch, src_op);
+      fill_d(dst_msa, scratch);
+    }
+  }
+#endif
+}
+
+void LiftoffAssembler::emit_i8x16_shuffle(LiftoffRegister dst,
+                                          LiftoffRegister lhs,
+                                          LiftoffRegister rhs,
+                                          const uint8_t shuffle[16],
+                                          bool is_swizzle) {
+  UNREACHABLE();
+#if 0
+  MSARegister dst_msa = dst.fp().toW();
+  MSARegister lhs_msa = lhs.fp().toW();
+  MSARegister rhs_msa = rhs.fp().toW();
+
+  uint64_t control_hi = 0;
+  uint64_t control_low = 0;
+  for (int i = 7; i >= 0; i--) {
+    control_hi <<= 8;
+    control_hi |= shuffle[i + 8];
+    control_low <<= 8;
+    control_low |= shuffle[i];
+  }
+
+  if (dst_msa == lhs_msa) {
+    move_v(kSimd128ScratchReg, lhs_msa);
+    lhs_msa = kSimd128ScratchReg;
+  } else if (dst_msa == rhs_msa) {
+    move_v(kSimd128ScratchReg, rhs_msa);
+    rhs_msa = kSimd128ScratchReg;
+  }
+
+  li(kScratchReg, control_low);
+  insert_d(dst_msa, 0, kScratchReg);
+  li(kScratchReg, control_hi);
+  insert_d(dst_msa, 1, kScratchReg);
+  vshf_b(dst_msa, rhs_msa, lhs_msa);
+#endif
+}
+
+void LiftoffAssembler::emit_i8x16_swizzle(LiftoffRegister dst,
+                                          LiftoffRegister lhs,
+                                          LiftoffRegister rhs) {
+  UNREACHABLE();
+#if 0
+  MSARegister dst_msa = dst.fp().toW();
+  MSARegister lhs_msa = lhs.fp().toW();
+  MSARegister rhs_msa = rhs.fp().toW();
+
+  if (dst == lhs) {
+    move_v(kSimd128ScratchReg, lhs_msa);
+    lhs_msa = kSimd128ScratchReg;
+  }
+  xor_v(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
+  move_v(dst_msa, rhs_msa);
+  vshf_b(dst_msa, kSimd128RegZero, lhs_msa);
+#endif 
+}
+
+void LiftoffAssembler::emit_i8x16_splat(LiftoffRegister dst,
+                                        LiftoffRegister src) {
+  UNREACHABLE();
+//  fill_b(dst.fp().toW(), src.gp());
+}
+
+void LiftoffAssembler::emit_i16x8_splat(LiftoffRegister dst,
+                                        LiftoffRegister src) {
+  UNREACHABLE();
+//  fill_h(dst.fp().toW(), src.gp());
+}
+
+void LiftoffAssembler::emit_i32x4_splat(LiftoffRegister dst,
+                                        LiftoffRegister src) {
+  UNREACHABLE();
+//  fill_w(dst.fp().toW(), src.gp());
+}
+
+void LiftoffAssembler::emit_i64x2_splat(LiftoffRegister dst,
+                                        LiftoffRegister src) {
+  UNREACHABLE();
+//  fill_d(dst.fp().toW(), src.gp());
+}
+
+void LiftoffAssembler::emit_f32x4_splat(LiftoffRegister dst,
+                                        LiftoffRegister src) {
+  UNREACHABLE();
+//  TurboAssembler::FmoveLow(kScratchReg, src.fp());
+//  fill_w(dst.fp().toW(), kScratchReg);
+}
+
+void LiftoffAssembler::emit_f64x2_splat(LiftoffRegister dst,
+                                        LiftoffRegister src) {
+  UNREACHABLE();
+//  TurboAssembler::Move(kScratchReg, src.fp());
+//  fill_d(dst.fp().toW(), kScratchReg);
+  }
+
+void LiftoffAssembler::emit_i8x16_eq(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs) {
+  UNREACHABLE();
+//  ceq_b(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i8x16_ne(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs) {
+  UNREACHABLE();
+//  ceq_b(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+//  nor_v(dst.fp().toW(), dst.fp().toW(), dst.fp().toW());
+}
+
+void LiftoffAssembler::emit_i8x16_gt_s(LiftoffRegister dst, LiftoffRegister lhs,
+                                       LiftoffRegister rhs) {
+  UNREACHABLE();
+//  clt_s_b(dst.fp().toW(), rhs.fp().toW(), lhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i8x16_gt_u(LiftoffRegister dst, LiftoffRegister lhs,
+                                       LiftoffRegister rhs) {
+  UNREACHABLE();
+//  clt_u_b(dst.fp().toW(), rhs.fp().toW(), lhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i8x16_ge_s(LiftoffRegister dst, LiftoffRegister lhs,
+                                       LiftoffRegister rhs) {
+  UNREACHABLE();
+//  cle_s_b(dst.fp().toW(), rhs.fp().toW(), lhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i8x16_ge_u(LiftoffRegister dst, LiftoffRegister lhs,
+                                       LiftoffRegister rhs) {
+  UNREACHABLE();
+//  cle_u_b(dst.fp().toW(), rhs.fp().toW(), lhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i16x8_eq(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs) {
+  UNREACHABLE();
+//  ceq_h(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i16x8_ne(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs) {
+  UNREACHABLE();
+//  ceq_h(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+//  nor_v(dst.fp().toW(), dst.fp().toW(), dst.fp().toW());
+}
+
+void LiftoffAssembler::emit_i16x8_gt_s(LiftoffRegister dst, LiftoffRegister lhs,
+                                       LiftoffRegister rhs) {
+  UNREACHABLE();
+//  clt_s_h(dst.fp().toW(), rhs.fp().toW(), lhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i16x8_gt_u(LiftoffRegister dst, LiftoffRegister lhs,
+                                       LiftoffRegister rhs) {
+  UNREACHABLE();
+//  clt_u_h(dst.fp().toW(), rhs.fp().toW(), lhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i16x8_ge_s(LiftoffRegister dst, LiftoffRegister lhs,
+                                       LiftoffRegister rhs) {
+  UNREACHABLE();
+//  cle_s_h(dst.fp().toW(), rhs.fp().toW(), lhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i16x8_ge_u(LiftoffRegister dst, LiftoffRegister lhs,
+                                       LiftoffRegister rhs) {
+  UNREACHABLE();
+//  cle_u_h(dst.fp().toW(), rhs.fp().toW(), lhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i32x4_eq(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs) {
+  UNREACHABLE();
+//  ceq_w(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i32x4_ne(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs) {
+  UNREACHABLE();
+//  ceq_w(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+//  nor_v(dst.fp().toW(), dst.fp().toW(), dst.fp().toW());
+}
+
+void LiftoffAssembler::emit_i32x4_gt_s(LiftoffRegister dst, LiftoffRegister lhs,
+                                       LiftoffRegister rhs) {
+  UNREACHABLE();
+//  clt_s_w(dst.fp().toW(), rhs.fp().toW(), lhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i32x4_gt_u(LiftoffRegister dst, LiftoffRegister lhs,
+                                       LiftoffRegister rhs) {
+  UNREACHABLE();
+//  clt_u_w(dst.fp().toW(), rhs.fp().toW(), lhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i32x4_ge_s(LiftoffRegister dst, LiftoffRegister lhs,
+                                       LiftoffRegister rhs) {
+  UNREACHABLE();
+//  cle_s_w(dst.fp().toW(), rhs.fp().toW(), lhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i32x4_ge_u(LiftoffRegister dst, LiftoffRegister lhs,
+                                       LiftoffRegister rhs) {
+  UNREACHABLE();
+//  cle_u_w(dst.fp().toW(), rhs.fp().toW(), lhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_f32x4_eq(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fceq_w(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_f32x4_ne(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fcune_w(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_f32x4_lt(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fclt_w(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_f32x4_le(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fcle_w(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_f64x2_eq(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fceq_d(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_f64x2_ne(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fcune_d(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_f64x2_lt(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fclt_d(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_f64x2_le(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fcle_d(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_s128_const(LiftoffRegister dst,
+                                       const uint8_t imms[16]) {
+  UNREACHABLE();
+#if 0
+  MSARegister dst_msa = dst.fp().toW();
+  uint64_t vals[2];
+  memcpy(vals, imms, sizeof(vals));
+  li(kScratchReg, vals[0]);
+  insert_d(dst_msa, 0, kScratchReg);
+  li(kScratchReg, vals[1]);
+  insert_d(dst_msa, 1, kScratchReg);
+#endif
+}
+
+void LiftoffAssembler::emit_s128_not(LiftoffRegister dst, LiftoffRegister src) {
+  UNREACHABLE();
+//  nor_v(dst.fp().toW(), src.fp().toW(), src.fp().toW());
+}
+
+void LiftoffAssembler::emit_s128_and(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs) {
+  UNREACHABLE();
+//  and_v(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_s128_or(LiftoffRegister dst, LiftoffRegister lhs,
+                                    LiftoffRegister rhs) {
+  UNREACHABLE();
+//  or_v(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_s128_xor(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs) {
+  UNREACHABLE();
+//  xor_v(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_s128_and_not(LiftoffRegister dst,
+                                         LiftoffRegister lhs,
+                                         LiftoffRegister rhs) {
+  UNREACHABLE();
+//  nor_v(kSimd128ScratchReg, rhs.fp().toW(), rhs.fp().toW());
+//  and_v(dst.fp().toW(), kSimd128ScratchReg, lhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_s128_select(LiftoffRegister dst,
+                                        LiftoffRegister src1,
+                                        LiftoffRegister src2,
+                                        LiftoffRegister mask) {
+  UNREACHABLE();
+#if 0
+  if (dst == mask) {
+    bsel_v(dst.fp().toW(), src2.fp().toW(), src1.fp().toW());
+  } else {
+    xor_v(kSimd128ScratchReg, src1.fp().toW(), src2.fp().toW());
+    and_v(kSimd128ScratchReg, kSimd128ScratchReg, mask.fp().toW());
+    xor_v(dst.fp().toW(), kSimd128ScratchReg, src2.fp().toW());
+  }
+#endif
+}
+
+void LiftoffAssembler::emit_i8x16_neg(LiftoffRegister dst,
+                                      LiftoffRegister src) {
+  UNREACHABLE();
+//  xor_v(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
+//  subv_b(dst.fp().toW(), kSimd128RegZero, src.fp().toW());
+}
+
+void LiftoffAssembler::emit_v8x16_anytrue(LiftoffRegister dst,
+                                          LiftoffRegister src) {
+  UNREACHABLE();
+//  liftoff::EmitAnyTrue(this, dst, src);
+}
+
+void LiftoffAssembler::emit_v8x16_alltrue(LiftoffRegister dst,
+                                          LiftoffRegister src) {
+  UNREACHABLE();
+//  liftoff::EmitAllTrue(this, dst, src, MSA_BRANCH_B);
+}
+
+void LiftoffAssembler::emit_i8x16_bitmask(LiftoffRegister dst,
+                                          LiftoffRegister src) {
+  UNREACHABLE();
+#if 0
+  MSARegister scratch0 = kSimd128RegZero;
+  MSARegister scratch1 = kSimd128ScratchReg;
+  srli_b(scratch0, src.fp().toW(), 7);
+  srli_h(scratch1, scratch0, 7);
+  or_v(scratch0, scratch0, scratch1);
+  srli_w(scratch1, scratch0, 14);
+  or_v(scratch0, scratch0, scratch1);
+  srli_d(scratch1, scratch0, 28);
+  or_v(scratch0, scratch0, scratch1);
+  shf_w(scratch1, scratch0, 0x0E);
+  ilvev_b(scratch0, scratch1, scratch0);
+  copy_u_h(dst.gp(), scratch0, 0);
+#endif
+}
+
+void LiftoffAssembler::emit_i8x16_shl(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fill_b(kSimd128ScratchReg, rhs.gp());
+//  sll_b(dst.fp().toW(), lhs.fp().toW(), kSimd128ScratchReg);
+}
+
+void LiftoffAssembler::emit_i8x16_shli(LiftoffRegister dst, LiftoffRegister lhs,
+                                       int32_t rhs) {
+  UNREACHABLE();
+//  slli_b(dst.fp().toW(), lhs.fp().toW(), rhs & 7);
+}
+
+void LiftoffAssembler::emit_i8x16_shr_s(LiftoffRegister dst,
+                                        LiftoffRegister lhs,
+                                        LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fill_b(kSimd128ScratchReg, rhs.gp());
+//  sra_b(dst.fp().toW(), lhs.fp().toW(), kSimd128ScratchReg);
+}
+
+void LiftoffAssembler::emit_i8x16_shri_s(LiftoffRegister dst,
+                                         LiftoffRegister lhs, int32_t rhs) {
+  UNREACHABLE();
+//  srai_b(dst.fp().toW(), lhs.fp().toW(), rhs & 7);
+}
+
+void LiftoffAssembler::emit_i8x16_shr_u(LiftoffRegister dst,
+                                        LiftoffRegister lhs,
+                                        LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fill_b(kSimd128ScratchReg, rhs.gp());
+//  srl_b(dst.fp().toW(), lhs.fp().toW(), kSimd128ScratchReg);
+}
+
+void LiftoffAssembler::emit_i8x16_shri_u(LiftoffRegister dst,
+                                         LiftoffRegister lhs, int32_t rhs) {
+  UNREACHABLE();
+//  srli_b(dst.fp().toW(), lhs.fp().toW(), rhs & 7);
+}
+
+void LiftoffAssembler::emit_i8x16_add(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+//  addv_b(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i8x16_add_saturate_s(LiftoffRegister dst,
+                                            LiftoffRegister lhs,
+                                            LiftoffRegister rhs) {
+  UNREACHABLE();
+//  adds_s_b(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i8x16_add_saturate_u(LiftoffRegister dst,
+                                            LiftoffRegister lhs,
+                                            LiftoffRegister rhs) {
+  UNREACHABLE();
+//  adds_u_b(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i8x16_sub(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+//  subv_b(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i8x16_sub_saturate_s(LiftoffRegister dst,
+                                            LiftoffRegister lhs,
+                                            LiftoffRegister rhs) {
+  UNREACHABLE();
+//  subs_s_b(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i8x16_sub_saturate_u(LiftoffRegister dst,
+                                            LiftoffRegister lhs,
+                                            LiftoffRegister rhs) {
+  UNREACHABLE();
+//  subs_u_b(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i8x16_mul(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+//  mulv_b(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i8x16_min_s(LiftoffRegister dst,
+                                        LiftoffRegister lhs,
+                                        LiftoffRegister rhs) {
+  UNREACHABLE();
+//  min_s_b(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i8x16_min_u(LiftoffRegister dst,
+                                        LiftoffRegister lhs,
+                                        LiftoffRegister rhs) {
+  UNREACHABLE();
+//  min_u_b(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i8x16_max_s(LiftoffRegister dst,
+                                        LiftoffRegister lhs,
+                                        LiftoffRegister rhs) {
+  UNREACHABLE();
+//  max_s_b(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i8x16_max_u(LiftoffRegister dst,
+                                        LiftoffRegister lhs,
+                                        LiftoffRegister rhs) {
+  UNREACHABLE();
+//  max_u_b(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i16x8_neg(LiftoffRegister dst,
+                                      LiftoffRegister src) {
+  UNREACHABLE();
+//  xor_v(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
+//  subv_h(dst.fp().toW(), kSimd128RegZero, src.fp().toW());
+}
+
+void LiftoffAssembler::emit_v16x8_anytrue(LiftoffRegister dst,
+                                          LiftoffRegister src) {
+  UNREACHABLE();
+//  liftoff::EmitAnyTrue(this, dst, src);
+}
+
+void LiftoffAssembler::emit_v16x8_alltrue(LiftoffRegister dst,
+                                          LiftoffRegister src) {
+  UNREACHABLE();
+//  liftoff::EmitAllTrue(this, dst, src, MSA_BRANCH_H);
+}
+
+void LiftoffAssembler::emit_i16x8_bitmask(LiftoffRegister dst,
+                                          LiftoffRegister src) {
+  UNREACHABLE();
+#if 0
+  MSARegister scratch0 = kSimd128RegZero;
+  MSARegister scratch1 = kSimd128ScratchReg;
+  srli_h(scratch0, src.fp().toW(), 15);
+  srli_w(scratch1, scratch0, 15);
+  or_v(scratch0, scratch0, scratch1);
+  srli_d(scratch1, scratch0, 30);
+  or_v(scratch0, scratch0, scratch1);
+  shf_w(scratch1, scratch0, 0x0E);
+  slli_d(scratch1, scratch1, 4);
+  or_v(scratch0, scratch0, scratch1);
+  copy_u_b(dst.gp(), scratch0, 0);
+#endif
+}
+
+void LiftoffAssembler::emit_i16x8_shl(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fill_h(kSimd128ScratchReg, rhs.gp());
+//  sll_h(dst.fp().toW(), lhs.fp().toW(), kSimd128ScratchReg);
+}
+
+void LiftoffAssembler::emit_i16x8_shli(LiftoffRegister dst, LiftoffRegister lhs,
+                                       int32_t rhs) {
+  UNREACHABLE();
+//  slli_h(dst.fp().toW(), lhs.fp().toW(), rhs & 15);
+}
+
+void LiftoffAssembler::emit_i16x8_shr_s(LiftoffRegister dst,
+                                        LiftoffRegister lhs,
+                                        LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fill_h(kSimd128ScratchReg, rhs.gp());
+//  sra_h(dst.fp().toW(), lhs.fp().toW(), kSimd128ScratchReg);
+}
+
+void LiftoffAssembler::emit_i16x8_shri_s(LiftoffRegister dst,
+                                         LiftoffRegister lhs, int32_t rhs) {
+  UNREACHABLE();
+//  srai_h(dst.fp().toW(), lhs.fp().toW(), rhs & 15);
+}
+
+void LiftoffAssembler::emit_i16x8_shr_u(LiftoffRegister dst,
+                                        LiftoffRegister lhs,
+                                        LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fill_h(kSimd128ScratchReg, rhs.gp());
+//  srl_h(dst.fp().toW(), lhs.fp().toW(), kSimd128ScratchReg);
+}
+
+void LiftoffAssembler::emit_i16x8_shri_u(LiftoffRegister dst,
+                                         LiftoffRegister lhs, int32_t rhs) {
+  UNREACHABLE();
+//  srli_h(dst.fp().toW(), lhs.fp().toW(), rhs & 15);
+}
+
+void LiftoffAssembler::emit_i16x8_add(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+//  addv_h(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i16x8_add_saturate_s(LiftoffRegister dst,
+                                            LiftoffRegister lhs,
+                                            LiftoffRegister rhs) {
+  UNREACHABLE();
+//  adds_s_h(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i16x8_add_saturate_u(LiftoffRegister dst,
+                                            LiftoffRegister lhs,
+                                            LiftoffRegister rhs) {
+  UNREACHABLE();
+//  adds_u_h(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i16x8_sub(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+//  subv_h(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i16x8_sub_saturate_s(LiftoffRegister dst,
+                                            LiftoffRegister lhs,
+                                            LiftoffRegister rhs) {
+  UNREACHABLE();
+//  subs_s_h(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i16x8_sub_saturate_u(LiftoffRegister dst,
+                                            LiftoffRegister lhs,
+                                            LiftoffRegister rhs) {
+  UNREACHABLE();
+//  subs_u_h(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i16x8_mul(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+//  mulv_h(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i16x8_min_s(LiftoffRegister dst,
+                                        LiftoffRegister lhs,
+                                        LiftoffRegister rhs) {
+  UNREACHABLE();
+//  min_s_h(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i16x8_min_u(LiftoffRegister dst,
+                                        LiftoffRegister lhs,
+                                        LiftoffRegister rhs) {
+  UNREACHABLE();
+//  min_u_h(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i16x8_max_s(LiftoffRegister dst,
+                                        LiftoffRegister lhs,
+                                        LiftoffRegister rhs) {
+  UNREACHABLE();
+//  max_s_h(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i16x8_max_u(LiftoffRegister dst,
+                                        LiftoffRegister lhs,
+                                        LiftoffRegister rhs) {
+  UNREACHABLE();
+//  max_u_h(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i32x4_neg(LiftoffRegister dst,
+                                      LiftoffRegister src) {
+  UNREACHABLE();
+//  xor_v(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
+//  subv_w(dst.fp().toW(), kSimd128RegZero, src.fp().toW());
+}
+
+void LiftoffAssembler::emit_v32x4_anytrue(LiftoffRegister dst,
+                                          LiftoffRegister src) {
+  UNREACHABLE();
+//  liftoff::EmitAnyTrue(this, dst, src);
+}
+
+void LiftoffAssembler::emit_v32x4_alltrue(LiftoffRegister dst,
+                                          LiftoffRegister src) {
+  UNREACHABLE();
+//  liftoff::EmitAllTrue(this, dst, src, MSA_BRANCH_W);
+}
+
+void LiftoffAssembler::emit_i32x4_bitmask(LiftoffRegister dst,
+                                          LiftoffRegister src) {
+  UNREACHABLE();
+#if 0
+  MSARegister scratch0 = kSimd128RegZero;
+  MSARegister scratch1 = kSimd128ScratchReg;
+  srli_w(scratch0, src.fp().toW(), 31);
+  srli_d(scratch1, scratch0, 31);
+  or_v(scratch0, scratch0, scratch1);
+  shf_w(scratch1, scratch0, 0x0E);
+  slli_d(scratch1, scratch1, 2);
+  or_v(scratch0, scratch0, scratch1);
+  copy_u_b(dst.gp(), scratch0, 0);
+#endif
+}
+
+void LiftoffAssembler::emit_i32x4_shl(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fill_w(kSimd128ScratchReg, rhs.gp());
+//  sll_w(dst.fp().toW(), lhs.fp().toW(), kSimd128ScratchReg);
+}
+
+void LiftoffAssembler::emit_i32x4_shli(LiftoffRegister dst, LiftoffRegister lhs,
+                                       int32_t rhs) {
+  UNREACHABLE();
+//  slli_w(dst.fp().toW(), lhs.fp().toW(), rhs & 31);
+}
+
+void LiftoffAssembler::emit_i32x4_shr_s(LiftoffRegister dst,
+                                        LiftoffRegister lhs,
+                                        LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fill_w(kSimd128ScratchReg, rhs.gp());
+//  sra_w(dst.fp().toW(), lhs.fp().toW(), kSimd128ScratchReg);
+}
+
+void LiftoffAssembler::emit_i32x4_shri_s(LiftoffRegister dst,
+                                         LiftoffRegister lhs, int32_t rhs) {
+  UNREACHABLE();
+//  srai_w(dst.fp().toW(), lhs.fp().toW(), rhs & 31);
+}
+
+void LiftoffAssembler::emit_i32x4_shr_u(LiftoffRegister dst,
+                                        LiftoffRegister lhs,
+                                        LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fill_w(kSimd128ScratchReg, rhs.gp());
+//  srl_w(dst.fp().toW(), lhs.fp().toW(), kSimd128ScratchReg);
+}
+
+void LiftoffAssembler::emit_i32x4_shri_u(LiftoffRegister dst,
+                                         LiftoffRegister lhs, int32_t rhs) {
+  UNREACHABLE();
+//  srli_w(dst.fp().toW(), lhs.fp().toW(), rhs & 31);
+}
+
+void LiftoffAssembler::emit_i32x4_add(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+//  addv_w(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i32x4_sub(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+//  subv_w(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i32x4_mul(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+//  mulv_w(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i32x4_min_s(LiftoffRegister dst,
+                                        LiftoffRegister lhs,
+                                        LiftoffRegister rhs) {
+  UNREACHABLE();
+//  min_s_w(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i32x4_min_u(LiftoffRegister dst,
+                                        LiftoffRegister lhs,
+                                        LiftoffRegister rhs) {
+  UNREACHABLE();
+//  min_u_w(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i32x4_max_s(LiftoffRegister dst,
+                                        LiftoffRegister lhs,
+                                        LiftoffRegister rhs) {
+  UNREACHABLE();
+//  max_s_w(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i32x4_max_u(LiftoffRegister dst,
+                                        LiftoffRegister lhs,
+                                        LiftoffRegister rhs) {
+  UNREACHABLE();
+//  max_u_w(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i64x2_neg(LiftoffRegister dst,
+                                      LiftoffRegister src) {
+  UNREACHABLE();
+//  xor_v(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
+//  subv_d(dst.fp().toW(), kSimd128RegZero, src.fp().toW());
+}
+
+void LiftoffAssembler::emit_i64x2_shl(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fill_d(kSimd128ScratchReg, rhs.gp());
+//  sll_d(dst.fp().toW(), lhs.fp().toW(), kSimd128ScratchReg);
+}
+
+void LiftoffAssembler::emit_i64x2_shli(LiftoffRegister dst, LiftoffRegister lhs,
+                                       int32_t rhs) {
+  UNREACHABLE();
+//  slli_d(dst.fp().toW(), lhs.fp().toW(), rhs & 63);
+}
+
+void LiftoffAssembler::emit_i64x2_shr_s(LiftoffRegister dst,
+                                        LiftoffRegister lhs,
+                                        LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fill_d(kSimd128ScratchReg, rhs.gp());
+//  sra_d(dst.fp().toW(), lhs.fp().toW(), kSimd128ScratchReg);
+}
+
+void LiftoffAssembler::emit_i64x2_shri_s(LiftoffRegister dst,
+                                         LiftoffRegister lhs, int32_t rhs) {
+  UNREACHABLE();
+//  srai_d(dst.fp().toW(), lhs.fp().toW(), rhs & 63);
+}
+
+void LiftoffAssembler::emit_i64x2_shr_u(LiftoffRegister dst,
+                                        LiftoffRegister lhs,
+                                        LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fill_d(kSimd128ScratchReg, rhs.gp());
+//  srl_d(dst.fp().toW(), lhs.fp().toW(), kSimd128ScratchReg);
+}
+
+void LiftoffAssembler::emit_i64x2_shri_u(LiftoffRegister dst,
+                                         LiftoffRegister lhs, int32_t rhs) {
+  UNREACHABLE();
+//  srli_d(dst.fp().toW(), lhs.fp().toW(), rhs & 63);
+}
+
+void LiftoffAssembler::emit_i64x2_add(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+//  addv_d(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i64x2_sub(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+//  subv_d(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i64x2_mul(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+//  mulv_d(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_f32x4_abs(LiftoffRegister dst,
+                                      LiftoffRegister src) {
+  UNREACHABLE();
+//  bclri_w(dst.fp().toW(), src.fp().toW(), 31);
+}
+
+void LiftoffAssembler::emit_f32x4_neg(LiftoffRegister dst,
+                                      LiftoffRegister src) {
+  UNREACHABLE();
+//  bnegi_w(dst.fp().toW(), src.fp().toW(), 31);
+}
+
+void LiftoffAssembler::emit_f32x4_sqrt(LiftoffRegister dst,
+                                       LiftoffRegister src) {
+  UNREACHABLE();
+}
+
+bool LiftoffAssembler::emit_f32x4_ceil(LiftoffRegister dst,
+                                       LiftoffRegister src) {
+  UNREACHABLE();
+}
+
+bool LiftoffAssembler::emit_f32x4_floor(LiftoffRegister dst,
+                                        LiftoffRegister src) {
+  UNREACHABLE();
+}
+
+bool LiftoffAssembler::emit_f32x4_trunc(LiftoffRegister dst,
+                                        LiftoffRegister src) {
+  UNREACHABLE();
+}
+
+bool LiftoffAssembler::emit_f32x4_nearest_int(LiftoffRegister dst,
+                                              LiftoffRegister src) {
+  UNREACHABLE();
+}
+
+void LiftoffAssembler::emit_f32x4_add(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fadd_w(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_f32x4_sub(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fsub_w(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_f32x4_mul(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fmul_w(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_f32x4_div(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fdiv_w(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_f32x4_min(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+#if 0
+  MSARegister dst_msa = dst.fp().toW();
+  MSARegister lhs_msa = lhs.fp().toW();
+  MSARegister rhs_msa = rhs.fp().toW();
+  MSARegister scratch0 = kSimd128RegZero;
+  MSARegister scratch1 = kSimd128ScratchReg;
+  // If inputs are -0.0. and +0.0, then write -0.0 to scratch1.
+  // scratch1 = (lhs == rhs) ?  (lhs | rhs) : (rhs | rhs).
+  fseq_w(scratch0, lhs_msa, rhs_msa);
+  bsel_v(scratch0, rhs_msa, lhs_msa);
+  or_v(scratch1, scratch0, rhs_msa);
+  // scratch0 = isNaN(scratch1) ? scratch1: lhs.
+  fseq_w(scratch0, scratch1, scratch1);
+  bsel_v(scratch0, scratch1, lhs_msa);
+  // dst = (scratch1 <= scratch0) ? scratch1 : scratch0.
+  fsle_w(dst_msa, scratch1, scratch0);
+  bsel_v(dst_msa, scratch0, scratch1);
+#endif 
+}
+
+void LiftoffAssembler::emit_f32x4_max(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+#if 0
+  MSARegister dst_msa = dst.fp().toW();
+  MSARegister lhs_msa = lhs.fp().toW();
+  MSARegister rhs_msa = rhs.fp().toW();
+  MSARegister scratch0 = kSimd128RegZero;
+  MSARegister scratch1 = kSimd128ScratchReg;
+  // If inputs are -0.0. and +0.0, then write +0.0 to scratch1.
+  // scratch1 = (lhs == rhs) ?  (lhs | rhs) : (rhs | rhs).
+  fseq_w(scratch0, lhs_msa, rhs_msa);
+  bsel_v(scratch0, rhs_msa, lhs_msa);
+  and_v(scratch1, scratch0, rhs_msa);
+  // scratch0 = isNaN(scratch1) ? scratch1: lhs.
+  fseq_w(scratch0, scratch1, scratch1);
+  bsel_v(scratch0, scratch1, lhs_msa);
+  // dst = (scratch0 <= scratch1) ? scratch1 : scratch0.
+  fsle_w(dst_msa, scratch0, scratch1);
+  bsel_v(dst_msa, scratch0, scratch1);
+#endif 
+}
+
+void LiftoffAssembler::emit_f32x4_pmin(LiftoffRegister dst, LiftoffRegister lhs,
+                                       LiftoffRegister rhs) {
+  UNREACHABLE();
+}
+
+void LiftoffAssembler::emit_f32x4_pmax(LiftoffRegister dst, LiftoffRegister lhs,
+                                       LiftoffRegister rhs) {
+  UNREACHABLE();
+}
+
+void LiftoffAssembler::emit_f64x2_abs(LiftoffRegister dst,
+                                      LiftoffRegister src) {
+  UNREACHABLE();
+//  bclri_d(dst.fp().toW(), src.fp().toW(), 63);
+}
+
+void LiftoffAssembler::emit_f64x2_neg(LiftoffRegister dst,
+                                      LiftoffRegister src) {
+  UNREACHABLE();
+//  bnegi_d(dst.fp().toW(), src.fp().toW(), 63);
+}
+
+void LiftoffAssembler::emit_f64x2_sqrt(LiftoffRegister dst,
+                                       LiftoffRegister src) {
+  UNREACHABLE();
+}
+
+bool LiftoffAssembler::emit_f64x2_ceil(LiftoffRegister dst,
+                                       LiftoffRegister src) {
+  UNREACHABLE();
+}
+
+bool LiftoffAssembler::emit_f64x2_floor(LiftoffRegister dst,
+                                        LiftoffRegister src) {
+  UNREACHABLE();
+}
+
+bool LiftoffAssembler::emit_f64x2_trunc(LiftoffRegister dst,
+                                        LiftoffRegister src) {
+  UNREACHABLE();
+}
+
+bool LiftoffAssembler::emit_f64x2_nearest_int(LiftoffRegister dst,
+                                              LiftoffRegister src) {
+  UNREACHABLE();
+}
+
+void LiftoffAssembler::emit_f64x2_add(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fadd_d(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_f64x2_sub(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fsub_d(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_f64x2_mul(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fmul_d(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_f64x2_div(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+//  fdiv_d(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_f64x2_min(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+#if 0
+  MSARegister dst_msa = dst.fp().toW();
+  MSARegister lhs_msa = lhs.fp().toW();
+  MSARegister rhs_msa = rhs.fp().toW();
+  MSARegister scratch0 = kSimd128RegZero;
+  MSARegister scratch1 = kSimd128ScratchReg;
+  // If inputs are -0.0. and +0.0, then write -0.0 to scratch1.
+  // scratch1 = (lhs == rhs) ?  (lhs | rhs) : (rhs | rhs).
+  fseq_d(scratch0, lhs_msa, rhs_msa);
+  bsel_v(scratch0, rhs_msa, lhs_msa);
+  or_v(scratch1, scratch0, rhs_msa);
+  // scratch0 = isNaN(scratch1) ? scratch1: lhs.
+  fseq_d(scratch0, scratch1, scratch1);
+  bsel_v(scratch0, scratch1, lhs_msa);
+  // dst = (scratch1 <= scratch0) ? scratch1 : scratch0.
+  fsle_d(dst_msa, scratch1, scratch0);
+  bsel_v(dst_msa, scratch0, scratch1);
+#endif
+}
+
+void LiftoffAssembler::emit_f64x2_max(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  UNREACHABLE();
+#if 0
+  MSARegister dst_msa = dst.fp().toW();
+  MSARegister lhs_msa = lhs.fp().toW();
+  MSARegister rhs_msa = rhs.fp().toW();
+  MSARegister scratch0 = kSimd128RegZero;
+  MSARegister scratch1 = kSimd128ScratchReg;
+  // If inputs are -0.0. and +0.0, then write +0.0 to scratch1.
+  // scratch1 = (lhs == rhs) ?  (lhs | rhs) : (rhs | rhs).
+  fseq_d(scratch0, lhs_msa, rhs_msa);
+  bsel_v(scratch0, rhs_msa, lhs_msa);
+  and_v(scratch1, scratch0, rhs_msa);
+  // scratch0 = isNaN(scratch1) ? scratch1: lhs.
+  fseq_d(scratch0, scratch1, scratch1);
+  bsel_v(scratch0, scratch1, lhs_msa);
+  // dst = (scratch0 <= scratch1) ? scratch1 : scratch0.
+  fsle_d(dst_msa, scratch0, scratch1);
+  bsel_v(dst_msa, scratch0, scratch1);
+  // Canonicalize the result.
+  fmax_d(dst_msa, dst_msa, dst_msa);
+#endif
+}
+
+void LiftoffAssembler::emit_f64x2_pmin(LiftoffRegister dst, LiftoffRegister lhs,
+                                       LiftoffRegister rhs) {
+  UNREACHABLE();
+}
+
+void LiftoffAssembler::emit_f64x2_pmax(LiftoffRegister dst, LiftoffRegister lhs,
+                                       LiftoffRegister rhs) {
+  UNREACHABLE();
+}
+
+void LiftoffAssembler::emit_i32x4_sconvert_f32x4(LiftoffRegister dst,
+                                                 LiftoffRegister src) {
+  UNREACHABLE();
+//  ftrunc_s_w(dst.fp().toW(), src.fp().toW());
+}
+
+void LiftoffAssembler::emit_i32x4_uconvert_f32x4(LiftoffRegister dst,
+                                                 LiftoffRegister src) {
+  UNREACHABLE();
+//  ftrunc_u_w(dst.fp().toW(), src.fp().toW());
+}
+
+void LiftoffAssembler::emit_f32x4_sconvert_i32x4(LiftoffRegister dst,
+                                                 LiftoffRegister src) {
+  UNREACHABLE();
+//  ffint_s_w(dst.fp().toW(), src.fp().toW());
+}
+
+void LiftoffAssembler::emit_f32x4_uconvert_i32x4(LiftoffRegister dst,
+                                                 LiftoffRegister src) {
+  UNREACHABLE();
+//  ffint_u_w(dst.fp().toW(), src.fp().toW());
+}
+
+void LiftoffAssembler::emit_i8x16_sconvert_i16x8(LiftoffRegister dst,
+                                                 LiftoffRegister lhs,
+                                                 LiftoffRegister rhs) {
+  UNREACHABLE();
+//  saturate_s_h(kSimd128ScratchReg, lhs.fp().toW(), 7);
+//  saturate_s_h(dst.fp().toW(), lhs.fp().toW(), 7);
+//  pckev_b(dst.fp().toW(), dst.fp().toW(), kSimd128ScratchReg);
+}
+
+void LiftoffAssembler::emit_i8x16_uconvert_i16x8(LiftoffRegister dst,
+                                                 LiftoffRegister lhs,
+                                                 LiftoffRegister rhs) {
+  UNREACHABLE();
+#if 0
+  xor_v(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
+  max_s_h(kSimd128ScratchReg, kSimd128RegZero, lhs.fp().toW());
+  saturate_u_h(kSimd128ScratchReg, kSimd128ScratchReg, 7);
+  max_s_h(dst.fp().toW(), kSimd128RegZero, rhs.fp().toW());
+  saturate_u_h(dst.fp().toW(), dst.fp().toW(), 7);
+  pckev_b(dst.fp().toW(), dst.fp().toW(), kSimd128ScratchReg);
+#endif
+}
+
+void LiftoffAssembler::emit_i16x8_sconvert_i32x4(LiftoffRegister dst,
+                                                 LiftoffRegister lhs,
+                                                 LiftoffRegister rhs) {
+  UNREACHABLE();
+//  saturate_s_w(kSimd128ScratchReg, lhs.fp().toW(), 15);
+//  saturate_s_w(dst.fp().toW(), lhs.fp().toW(), 15);
+//  pckev_h(dst.fp().toW(), dst.fp().toW(), kSimd128ScratchReg);
+}
+
+void LiftoffAssembler::emit_i16x8_uconvert_i32x4(LiftoffRegister dst,
+                                                 LiftoffRegister lhs,
+                                                 LiftoffRegister rhs) {
+  UNREACHABLE();
+#if 0
+  xor_v(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
+  max_s_w(kSimd128ScratchReg, kSimd128RegZero, lhs.fp().toW());
+  saturate_u_w(kSimd128ScratchReg, kSimd128ScratchReg, 15);
+  max_s_w(dst.fp().toW(), kSimd128RegZero, rhs.fp().toW());
+  saturate_u_w(dst.fp().toW(), dst.fp().toW(), 15);
+  pckev_h(dst.fp().toW(), dst.fp().toW(), kSimd128ScratchReg);
+#endif
+}
+
+void LiftoffAssembler::emit_i16x8_sconvert_i8x16_low(LiftoffRegister dst,
+                                                     LiftoffRegister src) {
+  UNREACHABLE();
+//  ilvr_b(kSimd128ScratchReg, src.fp().toW(), src.fp().toW());
+//  slli_h(dst.fp().toW(), kSimd128ScratchReg, 8);
+//  srai_h(dst.fp().toW(), dst.fp().toW(), 8);
+}
+
+void LiftoffAssembler::emit_i16x8_sconvert_i8x16_high(LiftoffRegister dst,
+                                                      LiftoffRegister src) {
+  UNREACHABLE();
+//  ilvl_b(kSimd128ScratchReg, src.fp().toW(), src.fp().toW());
+//  slli_h(dst.fp().toW(), kSimd128ScratchReg, 8);
+//  srai_h(dst.fp().toW(), dst.fp().toW(), 8);
+}
+
+void LiftoffAssembler::emit_i16x8_uconvert_i8x16_low(LiftoffRegister dst,
+                                                     LiftoffRegister src) {
+  UNREACHABLE();
+//  xor_v(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
+//  ilvr_b(dst.fp().toW(), kSimd128RegZero, src.fp().toW());
+}
+
+void LiftoffAssembler::emit_i16x8_uconvert_i8x16_high(LiftoffRegister dst,
+                                                      LiftoffRegister src) {
+  UNREACHABLE();
+//  xor_v(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
+//  ilvl_b(dst.fp().toW(), kSimd128RegZero, src.fp().toW());
+}
+
+void LiftoffAssembler::emit_i32x4_sconvert_i16x8_low(LiftoffRegister dst,
+                                                     LiftoffRegister src) {
+  UNREACHABLE();
+//  ilvr_h(kSimd128ScratchReg, src.fp().toW(), src.fp().toW());
+//  slli_w(dst.fp().toW(), kSimd128ScratchReg, 16);
+//  srai_w(dst.fp().toW(), dst.fp().toW(), 16);
+}
+
+void LiftoffAssembler::emit_i32x4_sconvert_i16x8_high(LiftoffRegister dst,
+                                                      LiftoffRegister src) {
+  UNREACHABLE();
+//  ilvl_h(kSimd128ScratchReg, src.fp().toW(), src.fp().toW());
+//  slli_w(dst.fp().toW(), kSimd128ScratchReg, 16);
+//  srai_w(dst.fp().toW(), dst.fp().toW(), 16);
+}
+
+void LiftoffAssembler::emit_i32x4_uconvert_i16x8_low(LiftoffRegister dst,
+                                                     LiftoffRegister src) {
+  UNREACHABLE();
+//  xor_v(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
+//  ilvr_h(dst.fp().toW(), kSimd128RegZero, src.fp().toW());
+}
+
+void LiftoffAssembler::emit_i32x4_uconvert_i16x8_high(LiftoffRegister dst,
+                                                      LiftoffRegister src) {
+  UNREACHABLE();
+//  xor_v(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
+//  ilvl_h(dst.fp().toW(), kSimd128RegZero, src.fp().toW());
+}
+
+void LiftoffAssembler::emit_i8x16_rounding_average_u(LiftoffRegister dst,
+                                                     LiftoffRegister lhs,
+                                                     LiftoffRegister rhs) {
+  UNREACHABLE();
+//  aver_u_b(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i16x8_rounding_average_u(LiftoffRegister dst,
+                                                     LiftoffRegister lhs,
+                                                     LiftoffRegister rhs) {
+  UNREACHABLE();
+//  aver_u_h(dst.fp().toW(), lhs.fp().toW(), rhs.fp().toW());
+}
+
+void LiftoffAssembler::emit_i8x16_abs(LiftoffRegister dst,
+                                      LiftoffRegister src) {
+  UNREACHABLE();
+//  xor_v(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
+//  asub_s_b(dst.fp().toW(), src.fp().toW(), kSimd128RegZero);
+}
+
+void LiftoffAssembler::emit_i16x8_abs(LiftoffRegister dst,
+                                      LiftoffRegister src) {
+  UNREACHABLE();
+//  xor_v(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
+//  asub_s_h(dst.fp().toW(), src.fp().toW(), kSimd128RegZero);
+}
+
+void LiftoffAssembler::emit_i32x4_abs(LiftoffRegister dst,
+                                      LiftoffRegister src) {
+  UNREACHABLE();
+//  xor_v(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
+//  asub_s_w(dst.fp().toW(), src.fp().toW(), kSimd128RegZero);
+}
+
+void LiftoffAssembler::emit_i8x16_extract_lane_s(LiftoffRegister dst,
+                                                 LiftoffRegister lhs,
+                                                 uint8_t imm_lane_idx) {
+  UNREACHABLE();
+//  copy_s_b(dst.gp(), lhs.fp().toW(), imm_lane_idx);
+}
+
+void LiftoffAssembler::emit_i8x16_extract_lane_u(LiftoffRegister dst,
+                                                 LiftoffRegister lhs,
+                                                 uint8_t imm_lane_idx) {
+  UNREACHABLE();
+//  copy_u_b(dst.gp(), lhs.fp().toW(), imm_lane_idx);
+}
+
+void LiftoffAssembler::emit_i16x8_extract_lane_s(LiftoffRegister dst,
+                                                 LiftoffRegister lhs,
+                                                 uint8_t imm_lane_idx) {
+  UNREACHABLE();
+//  copy_s_h(dst.gp(), lhs.fp().toW(), imm_lane_idx);
+}
+
+void LiftoffAssembler::emit_i16x8_extract_lane_u(LiftoffRegister dst,
+                                                 LiftoffRegister lhs,
+                                                 uint8_t imm_lane_idx) {
+  UNREACHABLE();
+//  copy_u_h(dst.gp(), lhs.fp().toW(), imm_lane_idx);
+}
+
+void LiftoffAssembler::emit_i32x4_extract_lane(LiftoffRegister dst,
+                                               LiftoffRegister lhs,
+                                               uint8_t imm_lane_idx) {
+  UNREACHABLE();
+//  copy_s_w(dst.gp(), lhs.fp().toW(), imm_lane_idx);
+}
+
+void LiftoffAssembler::emit_i64x2_extract_lane(LiftoffRegister dst,
+                                               LiftoffRegister lhs,
+                                               uint8_t imm_lane_idx) {
+  UNREACHABLE();
+//  copy_s_d(dst.gp(), lhs.fp().toW(), imm_lane_idx);
+}
+
+void LiftoffAssembler::emit_f32x4_extract_lane(LiftoffRegister dst,
+                                               LiftoffRegister lhs,
+                                               uint8_t imm_lane_idx) {
+  UNREACHABLE();
+//  copy_u_w(kScratchReg, lhs.fp().toW(), imm_lane_idx);
+//  TurboAssembler::FmoveLow(dst.fp(), kScratchReg);
+}
+
+void LiftoffAssembler::emit_f64x2_extract_lane(LiftoffRegister dst,
+                                               LiftoffRegister lhs,
+                                               uint8_t imm_lane_idx) {
+  UNREACHABLE();
+//  copy_s_d(kScratchReg, lhs.fp().toW(), imm_lane_idx);
+//  TurboAssembler::Move(dst.fp(), kScratchReg);
+}
+
+void LiftoffAssembler::emit_i8x16_replace_lane(LiftoffRegister dst,
+                                               LiftoffRegister src1,
+                                               LiftoffRegister src2,
+                                               uint8_t imm_lane_idx) {
+  UNREACHABLE();
+#if 0
+  if (dst != src1) {
+    move_v(dst.fp().toW(), src1.fp().toW());
+  }
+  insert_b(dst.fp().toW(), imm_lane_idx, src2.gp());
+#endif
+}
+
+void LiftoffAssembler::emit_i16x8_replace_lane(LiftoffRegister dst,
+                                               LiftoffRegister src1,
+                                               LiftoffRegister src2,
+                                               uint8_t imm_lane_idx) {
+  UNREACHABLE();
+#if 0
+  if (dst != src1) {
+    move_v(dst.fp().toW(), src1.fp().toW());
+  }
+  insert_h(dst.fp().toW(), imm_lane_idx, src2.gp());
+#endif
+}
+
+void LiftoffAssembler::emit_i32x4_replace_lane(LiftoffRegister dst,
+                                               LiftoffRegister src1,
+                                               LiftoffRegister src2,
+                                               uint8_t imm_lane_idx) {
+  UNREACHABLE();
+#if 0
+  if (dst != src1) {
+    move_v(dst.fp().toW(), src1.fp().toW());
+  }
+  insert_w(dst.fp().toW(), imm_lane_idx, src2.gp());
+#endif
+}
+
+void LiftoffAssembler::emit_i64x2_replace_lane(LiftoffRegister dst,
+                                               LiftoffRegister src1,
+                                               LiftoffRegister src2,
+                                               uint8_t imm_lane_idx) {
+  UNREACHABLE();
+#if 0
+  if (dst != src1) {
+    move_v(dst.fp().toW(), src1.fp().toW());
+  }
+  insert_d(dst.fp().toW(), imm_lane_idx, src2.gp());
+#endif
+}
+
+void LiftoffAssembler::emit_f32x4_replace_lane(LiftoffRegister dst,
+                                               LiftoffRegister src1,
+                                               LiftoffRegister src2,
+                                               uint8_t imm_lane_idx) {
+  UNREACHABLE();
+#if 0
+  TurboAssembler::FmoveLow(kScratchReg, src2.fp());
+  if (dst != src1) {
+    move_v(dst.fp().toW(), src1.fp().toW());
+  }
+  insert_w(dst.fp().toW(), imm_lane_idx, kScratchReg);
+#endif
+}
+
+void LiftoffAssembler::emit_f64x2_replace_lane(LiftoffRegister dst,
+                                               LiftoffRegister src1,
+                                               LiftoffRegister src2,
+                                               uint8_t imm_lane_idx) {
+ UNREACHABLE();
+#if 0
+  TurboAssembler::Move(kScratchReg, src2.fp());
+  if (dst != src1) {
+    move_v(dst.fp().toW(), src1.fp().toW());
+  }
+  insert_d(dst.fp().toW(), imm_lane_idx, kScratchReg);
+#endif
+}
+
+void LiftoffAssembler::StackCheck(Label* ool_code, Register limit_address) {
+  TurboAssembler::Uldl(limit_address, MemOperand(limit_address));
+  TurboAssembler::Branch(ool_code, ule, sp, Operand(limit_address));
+}
+
+void LiftoffAssembler::CallTrapCallbackForTesting() {
+  PrepareCallCFunction(0, GetUnusedRegister(kGpReg, {}).gp());
+  CallCFunction(ExternalReference::wasm_call_trap_callback_for_testing(), 0);
+}
+
+void LiftoffAssembler::AssertUnreachable(AbortReason reason) {
+  if (emit_debug_code()) Abort(reason);
+}
+
+void LiftoffAssembler::PushRegisters(LiftoffRegList regs) {
+  LiftoffRegList gp_regs = regs & kGpCacheRegList;
+  unsigned num_gp_regs = gp_regs.GetNumRegsSet();
+  if (num_gp_regs) {
+    unsigned offset = num_gp_regs * kSystemPointerSize;
+    subl(sp, offset, sp);
+    while (!gp_regs.is_empty()) {
+      LiftoffRegister reg = gp_regs.GetFirstRegSet();
+      offset -= kSystemPointerSize;
+      Stl(reg.gp(), MemOperand(sp, offset));
+      gp_regs.clear(reg);
+    }
+    DCHECK_EQ(offset, 0);
+  }
+  LiftoffRegList fp_regs = regs & kFpCacheRegList;
+  unsigned num_fp_regs = fp_regs.GetNumRegsSet();
+  if (num_fp_regs) {
+    unsigned slot_size = IsEnabled(SW64_SIMD) ? 16 : 8;
+    subl(sp, (num_fp_regs * slot_size), sp);
+    unsigned offset = 0;
+    while (!fp_regs.is_empty()) {
+      LiftoffRegister reg = fp_regs.GetFirstRegSet();
+      TurboAssembler::Fstd(reg.fp(), MemOperand(sp, offset));
+      fp_regs.clear(reg);
+      offset += slot_size;
+    }
+    DCHECK_EQ(offset, num_fp_regs * slot_size);
+  }
+}
+
+void LiftoffAssembler::PopRegisters(LiftoffRegList regs) {
+  LiftoffRegList fp_regs = regs & kFpCacheRegList;
+  unsigned fp_offset = 0;
+  while (!fp_regs.is_empty()) {
+    LiftoffRegister reg = fp_regs.GetFirstRegSet();
+    TurboAssembler::Fldd(reg.fp(), MemOperand(sp, fp_offset));
+    fp_regs.clear(reg);
+    fp_offset += sizeof(double);
+  }
+  if (fp_offset) addl(sp, fp_offset, sp);
+  LiftoffRegList gp_regs = regs & kGpCacheRegList;
+  unsigned gp_offset = 0;
+  while (!gp_regs.is_empty()) {
+    LiftoffRegister reg = gp_regs.GetLastRegSet();
+    Ldl(reg.gp(), MemOperand(sp, gp_offset));
+    gp_regs.clear(reg);
+    gp_offset += kSystemPointerSize;
+  }
+  addl(sp, gp_offset, sp);
+}
+
+void LiftoffAssembler::DropStackSlotsAndRet(uint32_t num_stack_slots) {
+  DCHECK_LT(num_stack_slots,
+            (1 << 16) / kSystemPointerSize);  // 16 bit immediate
+  TurboAssembler::DropAndRet(static_cast<int>(num_stack_slots));
+}
+
+void LiftoffAssembler::CallC(const wasm::FunctionSig* sig,
+                             const LiftoffRegister* args,
+                             const LiftoffRegister* rets,
+                             ValueType out_argument_type, int stack_bytes,
+                             ExternalReference ext_ref) {
+  Addl(sp, sp, -stack_bytes);
+
+  int arg_bytes = 0;
+  for (ValueType param_type : sig->parameters()) {
+    liftoff::Store(this, sp, arg_bytes, *args++, param_type);
+    arg_bytes += param_type.element_size_bytes();
+  }
+  DCHECK_LE(arg_bytes, stack_bytes);
+
+  // Pass a pointer to the buffer with the arguments to the C function.
+  // On sw64, the first argument is passed in {a0}.
+  constexpr Register kFirstArgReg = a0;
+  mov(kFirstArgReg, sp);
+
+  // Now call the C function.
+  constexpr int kNumCCallArgs = 1;
+  PrepareCallCFunction(kNumCCallArgs, kScratchReg);
+  CallCFunction(ext_ref, kNumCCallArgs);
+
+  // Move return value to the right register.
+  const LiftoffRegister* next_result_reg = rets;
+  if (sig->return_count() > 0) {
+    DCHECK_EQ(1, sig->return_count());
+    constexpr Register kReturnReg = v0;
+    if (kReturnReg != next_result_reg->gp()) {
+      Move(*next_result_reg, LiftoffRegister(kReturnReg), sig->GetReturn(0));
+    }
+    ++next_result_reg;
+  }
+
+  // Load potential output value from the buffer on the stack.
+  if (out_argument_type != kWasmStmt) {
+    liftoff::Load(this, *next_result_reg, MemOperand(sp, 0), out_argument_type);
+  }
+
+  Addl(sp, sp, stack_bytes);
+}
+
+void LiftoffAssembler::CallNativeWasmCode(Address addr) {
+  Call(addr, RelocInfo::WASM_CALL);
+}
+
+void LiftoffAssembler::TailCallNativeWasmCode(Address addr) {
+  Jump(addr, RelocInfo::WASM_CALL);
+}
+
+void LiftoffAssembler::CallIndirect(const wasm::FunctionSig* sig,
+                                    compiler::CallDescriptor* call_descriptor,
+                                    Register target) {
+  if (target == no_reg) {
+    pop(kScratchReg);
+    Call(kScratchReg);
+  } else {
+    Call(target);
+  }
+}
+
+void LiftoffAssembler::TailCallIndirect(Register target) {
+  if (target == no_reg) {
+    Pop(kScratchReg);
+    Jump(kScratchReg);
+  } else {
+    Jump(target);
+  }
+}
+
+void LiftoffAssembler::CallRuntimeStub(WasmCode::RuntimeStubId sid) {
+  // A direct call to a wasm runtime stub defined in this module.
+  // Just encode the stub index. This will be patched at relocation.
+  Call(static_cast<Address>(sid), RelocInfo::WASM_STUB_CALL);
+}
+
+void LiftoffAssembler::AllocateStackSlot(Register addr, uint32_t size) {
+  Addl(sp, sp, -size);
+  TurboAssembler::Move(addr, sp);
+}
+
+void LiftoffAssembler::DeallocateStackSlot(uint32_t size) {
+  Addl(sp, sp, size);
+}
+
+void LiftoffStackSlots::Construct() {
+  for (auto& slot : slots_) {
+    const LiftoffAssembler::VarState& src = slot.src_;
+    switch (src.loc()) {
+      case LiftoffAssembler::VarState::kStack:
+        asm_->Ldl(kScratchReg, liftoff::GetStackSlot(slot.src_offset_));
+        asm_->push(kScratchReg);
+        break;
+      case LiftoffAssembler::VarState::kRegister:
+        liftoff::push(asm_, src.reg(), src.type());
+        break;
+      case LiftoffAssembler::VarState::kIntConst: {
+        asm_->li(kScratchReg, Operand(src.i32_const()));
+        asm_->push(kScratchReg);
+        break;
+      }
+    }
+  }
+}
+
+}  // namespace wasm
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_WASM_BASELINE_SW64_LIFTOFF_ASSEMBLER_SW64_H_
diff --git a/src/3rdparty/chromium/v8/src/wasm/jump-table-assembler.cc b/src/3rdparty/chromium/v8/src/wasm/jump-table-assembler.cc
index 90cdad4672..83e045fcb9 100644
--- a/src/3rdparty/chromium/v8/src/wasm/jump-table-assembler.cc
+++ b/src/3rdparty/chromium/v8/src/wasm/jump-table-assembler.cc
@@ -307,6 +307,41 @@ void JumpTableAssembler::NopBytes(int bytes) {
   }
 }
 
+#elif V8_TARGET_ARCH_SW64
+void JumpTableAssembler::EmitLazyCompileJumpSlot(uint32_t func_index,
+                                                 Address lazy_compile_target) {
+  int start = pc_offset();
+  li(kWasmCompileLazyFuncIndexRegister, func_index);  // max. 2 instr
+  // Jump produces max. 4 instructions for 32-bit platform
+  // and max. 6 instructions for 64-bit platform.
+  Jump(lazy_compile_target, RelocInfo::NONE);
+  int nop_bytes = start + kLazyCompileTableSlotSize - pc_offset();
+  DCHECK_EQ(nop_bytes % kInstrSize, 0);
+  for (int i = 0; i < nop_bytes; i += kInstrSize) nop();
+}
+
+bool JumpTableAssembler::EmitJumpSlot(Address target) {
+  PatchAndJump(target);
+  return true;
+}
+
+void JumpTableAssembler::EmitFarJumpSlot(Address target) {
+  JumpToInstructionStream(target);
+}
+
+// static
+void JumpTableAssembler::PatchFarJumpSlot(Address slot, Address target) {
+  UNREACHABLE();
+}
+
+void JumpTableAssembler::NopBytes(int bytes) {
+  DCHECK_LE(0, bytes);
+  DCHECK_EQ(0, bytes % kInstrSize);
+  for (; bytes > 0; bytes -= kInstrSize) {
+    nop();
+  }
+}
+
 #else
 #error Unknown architecture.
 #endif
diff --git a/src/3rdparty/chromium/v8/src/wasm/jump-table-assembler.h b/src/3rdparty/chromium/v8/src/wasm/jump-table-assembler.h
index 253f0bc018..1023e5e581 100644
--- a/src/3rdparty/chromium/v8/src/wasm/jump-table-assembler.h
+++ b/src/3rdparty/chromium/v8/src/wasm/jump-table-assembler.h
@@ -215,6 +215,11 @@ class V8_EXPORT_PRIVATE JumpTableAssembler : public MacroAssembler {
   static constexpr int kJumpTableSlotSize = 8 * kInstrSize;
   static constexpr int kFarJumpTableSlotSize = 6 * kInstrSize;
   static constexpr int kLazyCompileTableSlotSize = 8 * kInstrSize;
+#elif V8_TARGET_ARCH_SW64
+  static constexpr int kJumpTableLineSize = 10 * kInstrSize;
+  static constexpr int kJumpTableSlotSize = 10 * kInstrSize;
+  static constexpr int kFarJumpTableSlotSize = 5 * kInstrSize;
+  static constexpr int kLazyCompileTableSlotSize = 10 * kInstrSize;
 #else
 #error Unknown architecture.
 #endif
diff --git a/src/3rdparty/chromium/v8/src/wasm/wasm-debug.cc b/src/3rdparty/chromium/v8/src/wasm/wasm-debug.cc
index d05caa4144..9b8265600f 100644
--- a/src/3rdparty/chromium/v8/src/wasm/wasm-debug.cc
+++ b/src/3rdparty/chromium/v8/src/wasm/wasm-debug.cc
@@ -776,8 +776,12 @@ class DebugInfoImpl {
           debug_break_fp +
           WasmDebugBreakFrameConstants::GetPushedFpRegisterOffset(code);
       if (type == kWasmF32) {
-        return WasmValue(ReadUnalignedValue<float>(spilled_addr));
       } else if (type == kWasmF64) {
+#ifdef V8_TARGET_ARCH_SW64
+        return WasmValue((float)(ReadUnalignedValue<double>(spilled_addr)));
+#else
+        return WasmValue(ReadUnalignedValue<float>(spilled_addr));
+#endif
         return WasmValue(ReadUnalignedValue<double>(spilled_addr));
       } else if (type == kWasmS128) {
         return WasmValue(Simd128(ReadUnalignedValue<int16>(spilled_addr)));
diff --git a/src/3rdparty/chromium/v8/src/wasm/wasm-linkage.h b/src/3rdparty/chromium/v8/src/wasm/wasm-linkage.h
index 7e56ea6eae..0a69bc18e2 100644
--- a/src/3rdparty/chromium/v8/src/wasm/wasm-linkage.h
+++ b/src/3rdparty/chromium/v8/src/wasm/wasm-linkage.h
@@ -84,6 +84,15 @@ constexpr Register kGpReturnRegisters[] = {r3, r4};
 constexpr DoubleRegister kFpParamRegisters[] = {d1, d2, d3, d4, d5, d6, d7, d8};
 constexpr DoubleRegister kFpReturnRegisters[] = {d1, d2};
 
+#elif V8_TARGET_ARCH_SW64
+// ===========================================================================
+// == sw64 ===================================================================
+// ===========================================================================
+constexpr Register kGpParamRegisters[] = {a0, a2, a3, a4, a5};
+constexpr Register kGpReturnRegisters[] = {v0, t4};  // t4 used as v1
+constexpr DoubleRegister kFpParamRegisters[] = {f16, f17, f18, f19, f20, f21};
+constexpr DoubleRegister kFpReturnRegisters[] = {f0, f1};
+
 #elif V8_TARGET_ARCH_S390X
 // ===========================================================================
 // == s390x ==================================================================
diff --git a/src/3rdparty/gn/tools/gn/args.cc b/src/3rdparty/gn/tools/gn/args.cc
index 802c3731d5..0089612ed4 100644
--- a/src/3rdparty/gn/tools/gn/args.cc
+++ b/src/3rdparty/gn/tools/gn/args.cc
@@ -329,6 +329,7 @@ void Args::SetSystemVarsLocked(Scope* dest) const {
   static const char kMips64[] = "mips64el";
   static const char kS390X[] = "s390x";
   static const char kPPC64[] = "ppc64";
+  static const char kSW64[] = "sw_64";
   const char* arch = nullptr;
 
   // Set the host CPU architecture based on the underlying OS, not
@@ -353,6 +354,8 @@ void Args::SetSystemVarsLocked(Scope* dest) const {
     // This allows us to use the same toolchain as ppc64 BE
     // and specific flags are included using the host_byteorder logic.
     arch = kPPC64;
+  else if (os_arch == "sw_64")
+    arch = kSW64;
   else
     CHECK(false) << "OS architecture not handled. (" << os_arch << ")";
 
diff --git a/src/3rdparty/gn/util/build_config.h b/src/3rdparty/gn/util/build_config.h
index addd7cfb08..1e2decee9d 100644
--- a/src/3rdparty/gn/util/build_config.h
+++ b/src/3rdparty/gn/util/build_config.h
@@ -172,6 +172,11 @@
 #define ARCH_CPU_32_BITS 1
 #define ARCH_CPU_BIG_ENDIAN 1
 #endif
+#elif defined(__sw_64__)
+#define ARCH_CPU_SW64_FAMILY 1
+#define ARCH_CPU_SW64 1
+#define ARCH_CPU_64_BITS 1
+#define ARCH_CPU_LITTLE_ENDIAN 1
 #else
 #error Please add support for your architecture in build_config.h
 #endif
-- 
2.20.1

